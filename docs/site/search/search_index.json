{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cahier \u00b6 2019 190604 Pape, Katrin, 2019 190524 Kopec, Ashley M. 2019 190517 Lynn, Christopher W., 2019 190517 Avena-Koenigsberger, A., 2019 190507 Nelson, Peter T, 2019 190427 Jaafra, Y., Laurent, J.L. 2018 190410 DeAngelis D., Diaz S., 2019 190311 Bassett, D. S., Bullmore, E.T. 2017 2018 180904 180805 Xia 2018 180728 Schellekens 2018 180719 Betzel 2017 180326 Harari 2011 180307 Avena Koenigsberger 2017 180220 Anzellotti 2018 2017 171125 NIPS 171124 NIPS 170704 Logan Cross 170616 CogMTG Hamamoto 170329 Bassett 2017 170319 TNI v88n11 170317 Bassett, D.S. 2017 170314 Yoshimori, T. 170216 Hasegawa, T. 2016 160604 Kano, M. 160526 Osumi semi 160523 Okada, Y.","title":"Home"},{"location":"160523_okada_y/","text":"16-05-23 Okada, Yukinori \u00b6 HP genome \u3092\u7d10\u89e3\u304f --> genome \u3092\u6d3b\u7528\u3059\u308b GWAS drug repositioning Ref. \u00b6 Genetics of rheumatoid arthritis contributes to biology and drug discovery Japanese Population Structure, Based on SNP Genotypes from 7003 Individuals Compared to Other Ethnic Groups: Effects on Population-Based Association Studies \u7d71\u8a08\u5b66\u304c\u6700\u5f37\u306e\u5b66\u554f\u3067\u3042\u308b genome.gov/GWASstudies/ Genetic studies of body mass index yield new insights for obesity biology Construction of a population-specific HLA imputation reference panel and its application to Graves' disease risk in Japanese New data and an old puzzle: the negative association between schizophrenia and rheumatoid arthritis Lessons learned from the fate of AstraZeneca\u2019s drug pipeline: a five-dimensional framework","title":"160523 Okada, Y."},{"location":"160523_okada_y/#ref","text":"Genetics of rheumatoid arthritis contributes to biology and drug discovery Japanese Population Structure, Based on SNP Genotypes from 7003 Individuals Compared to Other Ethnic Groups: Effects on Population-Based Association Studies \u7d71\u8a08\u5b66\u304c\u6700\u5f37\u306e\u5b66\u554f\u3067\u3042\u308b genome.gov/GWASstudies/ Genetic studies of body mass index yield new insights for obesity biology Construction of a population-specific HLA imputation reference panel and its application to Graves' disease risk in Japanese New data and an old puzzle: the negative association between schizophrenia and rheumatoid arthritis Lessons learned from the fate of AstraZeneca\u2019s drug pipeline: a five-dimensional framework","title":"Ref."},{"location":"160526_osumi_semi/","text":"16-05-26 Osumi Semi \u00b6 Rapid Quantification of Adult and Developing Mouse Spatial Vision Using a Virtual Optomotor System 80% basic \u60c5\u5831\u3092\u8133\u306f\u3069\u3046\u3084\u3063\u3066\u51e6\u7406\u3057\u3066\u3044\u308b\u306e\u304b\uff1f Refractive state and visual acuity in the hooded rat \u7d14\u5316\u3059\u308b\u306b\u306f\u6642\u9593\u304c\u304b\u304b\u308b \u5b66\u7fd2 =>\u3000\u8996\u529b\u6e2c\u5b9a\u306b\u306f\u6642\u9593\u304c\u304b\u304b\u308b \u5b50\u4f9b\u306e\u30de\u30a6\u30b9\u306f\u6e2c\u5b9a\u3067\u304d\u306a\u3044 \u767a\u751f\u306f\u6e2c\u5b9a\u3067\u304d\u306a\u3044 optomotor response\u3000\u8996\u904b\u52d5\u53cd\u5c04 \u3092\u5229\u7528\u3057\u3088\u3046 visual acuity (2\u70b9\u306e\u8b58\u5225) contrast sensitivity ref. 1. Refractive state and visual acuity in the hooded rat \u4eba\u3078\u306e\u5fdc\u7528\u306f \u8996\u529b\u3000 \u57fa\u6e96\u306f\u4e21\u773c\u306e\u9593\u3067\u3044\u3044\u306e\uff1f","title":"160526 Osumi semi"},{"location":"160604_kano_m/","text":"16-06-04 Kano, Michiko \u00b6 Kano, Michiko Ref. 1. An fMRI-Based Neurologic Signature of Physical Pain 2. Multivariate morphological brain signatures predict patients with chronic abdominal pain from healthy control subjects Ref. 1. Resting-state functional MRI in depression unmasks increased connectivity between networks via the dorsal nexus 2. shoaner 2016 cognitive behavioral therapy 3. http://search.proquest.com/openview/c7681dda220035093b9858bbcd7f0095/1?pq-origsite=gscholar","title":"160604 Kano, M."},{"location":"170216_hasegawa_t/","text":"17-02-16 Hasegawa, Takafumi \u00b6 Hasegwa, T Parkinson, James. 1817. \"Essay on Shaking Palsy\" Reselpin Carisson SN Striate DA 20% => Sx 0-3 y: \u30cf\u30cd\u30e0\u30fc\u30f3 3-5 y: Wearing off 5-10 y: Dyskinesia 10-15 y: DA resistance 15-20 y: Hallucination +3-4 y : Death 1: Olf, Med 2: RBD 3-4: Motor Progression inh Marker Prion Hypothesis \u00b6 Oshima. 2016 SSRI Kanno, 2012 RBD \u00b6 Synnucleinopathy 70-80% 12-15y Olf \u00b6 \"Sniffing out dementia\" MIBG HCM ratio Prinon \u00b6 Kikneh, 2016 Sampsom, 2016, Gut microbiota","title":"170216 Hasegawa, T."},{"location":"170216_hasegawa_t/#prion_hypothesis","text":"Oshima. 2016 SSRI Kanno, 2012","title":"Prion Hypothesis"},{"location":"170216_hasegawa_t/#rbd","text":"Synnucleinopathy 70-80% 12-15y","title":"RBD"},{"location":"170216_hasegawa_t/#olf","text":"\"Sniffing out dementia\" MIBG HCM ratio","title":"Olf"},{"location":"170216_hasegawa_t/#prinon","text":"Kikneh, 2016 Sampsom, 2016, Gut microbiota","title":"Prinon"},{"location":"170314_yoshimori_t/","text":"17-03-14 Yoshimori T \u00b6 Yoshimori Lab 1-2% of prot --> lysis 80% of AA --> biosynthesis of protein selective atg 1-14 core atg autophagosome lc3 (atg8) Kabeya, 2000, Embo J from where ? \u00b6 Hayashi, 2009, Nat Cell Biol cradle model er vs. mito contact site mito er ca, lipid Hamasaki, 2013, Nature iii pi3 k xenophagy \u00b6 Nakagawa, 2004, Sci gas selectivity Kageyama, 2011, Mol Biol Cell enodosme Fujita, 2013, J Cell Biol autophagy recognize damaged endosome lysophagy \u00b6 Maejima, 2013, Embo J damaged lysosome Hr-UAemia nephliitis lysosome fusion w/ autophagosome \u00b6 inpp5e Hasegawa, 2016, Embo J Joubert synd Bielas, 2009, Nat Genet cd, pd senda synd viei synd , cancer, joubert synd Fatty liver \u00b6 Tanaka, 2016, Hepato rubicon Matsumaga, 2009, J Cell Biol fatty diet incrased rubicon fatty liver acquired Melanosome \u00b6 Murase, 2013, J Invest Derm QA \u00b6 Virus infexn \u00b6 polio autophagosome PD \u00b6 pink1 park","title":"170314 Yoshimori, T."},{"location":"170314_yoshimori_t/#from_where","text":"Hayashi, 2009, Nat Cell Biol cradle model er vs. mito contact site mito er ca, lipid Hamasaki, 2013, Nature iii pi3 k","title":"from where ?"},{"location":"170314_yoshimori_t/#xenophagy","text":"Nakagawa, 2004, Sci gas selectivity Kageyama, 2011, Mol Biol Cell enodosme Fujita, 2013, J Cell Biol autophagy recognize damaged endosome","title":"xenophagy"},{"location":"170314_yoshimori_t/#lysophagy","text":"Maejima, 2013, Embo J damaged lysosome Hr-UAemia nephliitis","title":"lysophagy"},{"location":"170314_yoshimori_t/#lysosome_fusion_w_autophagosome","text":"inpp5e Hasegawa, 2016, Embo J Joubert synd Bielas, 2009, Nat Genet cd, pd senda synd viei synd , cancer, joubert synd","title":"lysosome fusion w/ autophagosome"},{"location":"170314_yoshimori_t/#fatty_liver","text":"Tanaka, 2016, Hepato rubicon Matsumaga, 2009, J Cell Biol fatty diet incrased rubicon fatty liver acquired","title":"Fatty liver"},{"location":"170314_yoshimori_t/#melanosome","text":"Murase, 2013, J Invest Derm","title":"Melanosome"},{"location":"170314_yoshimori_t/#qa","text":"","title":"QA"},{"location":"170314_yoshimori_t/#virus_infexn","text":"polio autophagosome","title":"Virus infexn"},{"location":"170314_yoshimori_t/#pd","text":"pink1 park","title":"PD"},{"location":"170317_bassett_d_s_2017/","text":"Bassett, D.S. 2017 \u00b6 Bassett, D.S. and Mattar, M.G., 2017. A Network Neuroscience of Human Learning: Potential to Inform Quantitative Theories of Brain and Behavior. Trends in Cognitive Sciences. in Mendeley \u00a71 Learning as a Network Phenomenone \u00b6 \u00b61 \u00b62 a new literature is emerging that focuses on the effects of learning at a coarser level \u2013 between entire brain regions \u00b63 broad-scale changes in neurophysi-ological dynamics across distributed neural circuits or networks When learning produces such distributed network changes\u2013refl ected either in anatomy or function\u2013 it is useful to consider quantitative methods that can not only describe that network architecture but also predict its dynamics \u00b64 In this review... unique approach to describing neural systems in terms of associations in the brain the reconfigurations underlying its adaptive processe how network neuroscience could provide a quantitative framework that complements existing models of learning by cohesively accounting for network structure in neurophysiological and behavioral data \u00a72 Network Neuroscience \u00b6 \u00b61 Network science: a subfield of complex systems science \u00b62 \"network neuroscinece\" emcompass: the use of network science in understanding connectivity patterns the impact of these connectivity patterns on animal behavior describe the architecture of relational data \u00b63 E, V, forming a dyad , dyads can change in their strength over time \u00b64 mesurement of local structure measurement of mesoscale & global structure local : e.g., clustering coeeficient of graph mesoscale : e.g., modularity of G (measures the presence/strength of local clusters of interconnected nodes) \u00b65 G structure can have distincet implications for how the system functions For e.g, Random G : N has equal prob of connecting to any other Ns : transmit info quickly, NOT local info integration Regular G : N connetcts to equal # of neighbor : local info integration, NOT global transmission of info Modular G : evolvabiloty : contain groups of Ns that can change / adapt their func w/o perturbin other groups Hierachial modular network : similat to human brain : specialization of nested func & adaptability \u00b66 functional brain network : fMRI, MEG, EEG : Ns - region/voxel; Es - functional connectivity infer circuits \u00b67 DWI Structural network : WM microstructure : Vs - ; Es - estimated strength of WM tracts \u00a73 Dynamic Networks During Learning \u00b6 \u00b68 Temporal network : each G represents interaction pattern in single time-window : ensemble of Gs - evlution of interaction patterns N in one time-window is connected to iteself in neighboring time-windows using interlayer link traditional stat : adjacency matrix new stat : adjacency tensor \u00b69 Within-scan : functional imaging Across-scan : functiona & structural imaging \u00b610 Base on Hebbian learning, \u0394 in struc & func: driven by patterns of activity and connectivity functional connectivity e.g., 1. detect memory rehabilitation trainning intervention following stroke 2. predict future abilty of individual to learn motor skills 3. words of artificial language \u00b611 motor skill learning network reconfigulation (learning) * struc \u0394: * func \u0394 : \u00a74 Reconfigulation of Network Modulates During Learning \u00b6 \u00b612 in motor skill learning, network stat \u0394: \u2191 clustering coefficients \u2191 # of network connections \u2191 connection strength \u2193 communication distances * \u0394 in network centrality learning requires \u0394 in modular organization Modular network : contains local clusters of densely interconnected Ns recruitments of & integration between systems is altered \u00b613 reconfigulation are charcteristics of a flexible brain networks organizaition individual \u0394 predicts future learning rate \u00b6 14 modularity plays role in behavioral adaptability \u00b615 modularity is marker of high congnitive func modularity \u221d working memory flexible rearrangement \u221d \u0394 memory acuracy & cognitive flexibility \u00a75 Challenges & Oppotunities \u00b6 \u00b616 ? States / Traits ? depends on the type of learning ? Can predict ? brains flexible ? modulate w/ mood / pharmochological intervention ? modulated by NMDA / NE / other \u00b617 Dynamic Network Drivers of Seizure Generation, Propagation and Termination in Human Neocortical Epilepsy \u00a76 Torward a Network-Based Theory of Learning \u00b6 \u00a77 Conculuding Remarks & Future Directions \u00b6","title":"170317 Bassett, D.S. 2017"},{"location":"170317_bassett_d_s_2017/#1_learning_as_a_network_phenomenone","text":"\u00b61 \u00b62 a new literature is emerging that focuses on the effects of learning at a coarser level \u2013 between entire brain regions \u00b63 broad-scale changes in neurophysi-ological dynamics across distributed neural circuits or networks When learning produces such distributed network changes\u2013refl ected either in anatomy or function\u2013 it is useful to consider quantitative methods that can not only describe that network architecture but also predict its dynamics \u00b64 In this review... unique approach to describing neural systems in terms of associations in the brain the reconfigurations underlying its adaptive processe how network neuroscience could provide a quantitative framework that complements existing models of learning by cohesively accounting for network structure in neurophysiological and behavioral data","title":"\u00a71 Learning as a Network Phenomenone"},{"location":"170317_bassett_d_s_2017/#2_network_neuroscience","text":"\u00b61 Network science: a subfield of complex systems science \u00b62 \"network neuroscinece\" emcompass: the use of network science in understanding connectivity patterns the impact of these connectivity patterns on animal behavior describe the architecture of relational data \u00b63 E, V, forming a dyad , dyads can change in their strength over time \u00b64 mesurement of local structure measurement of mesoscale & global structure local : e.g., clustering coeeficient of graph mesoscale : e.g., modularity of G (measures the presence/strength of local clusters of interconnected nodes) \u00b65 G structure can have distincet implications for how the system functions For e.g, Random G : N has equal prob of connecting to any other Ns : transmit info quickly, NOT local info integration Regular G : N connetcts to equal # of neighbor : local info integration, NOT global transmission of info Modular G : evolvabiloty : contain groups of Ns that can change / adapt their func w/o perturbin other groups Hierachial modular network : similat to human brain : specialization of nested func & adaptability \u00b66 functional brain network : fMRI, MEG, EEG : Ns - region/voxel; Es - functional connectivity infer circuits \u00b67 DWI Structural network : WM microstructure : Vs - ; Es - estimated strength of WM tracts","title":"\u00a72 Network Neuroscience"},{"location":"170317_bassett_d_s_2017/#3_dynamic_networks_during_learning","text":"\u00b68 Temporal network : each G represents interaction pattern in single time-window : ensemble of Gs - evlution of interaction patterns N in one time-window is connected to iteself in neighboring time-windows using interlayer link traditional stat : adjacency matrix new stat : adjacency tensor \u00b69 Within-scan : functional imaging Across-scan : functiona & structural imaging \u00b610 Base on Hebbian learning, \u0394 in struc & func: driven by patterns of activity and connectivity functional connectivity e.g., 1. detect memory rehabilitation trainning intervention following stroke 2. predict future abilty of individual to learn motor skills 3. words of artificial language \u00b611 motor skill learning network reconfigulation (learning) * struc \u0394: * func \u0394 :","title":"\u00a73 Dynamic Networks During Learning"},{"location":"170317_bassett_d_s_2017/#4_reconfigulation_of_network_modulates_during_learning","text":"\u00b612 in motor skill learning, network stat \u0394: \u2191 clustering coefficients \u2191 # of network connections \u2191 connection strength \u2193 communication distances * \u0394 in network centrality learning requires \u0394 in modular organization Modular network : contains local clusters of densely interconnected Ns recruitments of & integration between systems is altered \u00b613 reconfigulation are charcteristics of a flexible brain networks organizaition individual \u0394 predicts future learning rate \u00b6 14 modularity plays role in behavioral adaptability \u00b615 modularity is marker of high congnitive func modularity \u221d working memory flexible rearrangement \u221d \u0394 memory acuracy & cognitive flexibility","title":"\u00a74 Reconfigulation of Network Modulates During Learning"},{"location":"170317_bassett_d_s_2017/#5_challenges_oppotunities","text":"\u00b616 ? States / Traits ? depends on the type of learning ? Can predict ? brains flexible ? modulate w/ mood / pharmochological intervention ? modulated by NMDA / NE / other \u00b617 Dynamic Network Drivers of Seizure Generation, Propagation and Termination in Human Neocortical Epilepsy","title":"\u00a75 Challenges &amp; Oppotunities"},{"location":"170317_bassett_d_s_2017/#6_torward_a_network-based_theory_of_learning","text":"","title":"\u00a76 Torward a Network-Based Theory of Learning"},{"location":"170317_bassett_d_s_2017/#7_conculuding_remarks_future_directions","text":"","title":"\u00a77 Conculuding Remarks &amp; Future Directions"},{"location":"170319_tni_v88n11/","text":"NeuroImages vol.88 \u00b6 A spiritual visual hallucination from a right parieto-occipital seizure Ictal guardian angel. Wong, V.S. and Kellogg, M.A., 2017. Neurology, 88(11), pp.e101-e102.","title":"170319 TNI v88n11"},{"location":"170329_bassett_2017/","text":"17-03-29 Bassett, D.S. 2017 \u00b6 Bassett, D.S. and Sporns, O., 2017. Network neuroscience. Nature Neuroscience, 20(3), pp.353-364. Mendeley \u00a71 Intro \u00b6 \u00a72 Network mapping and obervation \u00b6 \u00b62 dense reconstructure : \u00a73 Network analysis and modeling \u00b6 \u00a74 Current frontiers \u00b6 \u00a74-1 Network dynamics \u00b6 \u00a74-2 Prediction \u00b6 \u00a74-3 Perturbation, manipulation and control \u00b6 \u00a74-4 Crossing levels \u00b6 \u00a7 5 Conclusion \u00b6 01 01:","title":"170329 Bassett 2017"},{"location":"170329_bassett_2017/#1_intro","text":"","title":"\u00a71 Intro"},{"location":"170329_bassett_2017/#2_network_mapping_and_obervation","text":"\u00b62 dense reconstructure :","title":"\u00a72 Network mapping and obervation"},{"location":"170329_bassett_2017/#3_network_analysis_and_modeling","text":"","title":"\u00a73 Network analysis and modeling"},{"location":"170329_bassett_2017/#4_current_frontiers","text":"","title":"\u00a74 Current frontiers"},{"location":"170329_bassett_2017/#4-1_network_dynamics","text":"","title":"\u00a74-1 Network dynamics"},{"location":"170329_bassett_2017/#4-2_prediction","text":"","title":"\u00a74-2 Prediction"},{"location":"170329_bassett_2017/#4-3_perturbation_manipulation_and_control","text":"","title":"\u00a74-3 Perturbation, manipulation and control"},{"location":"170329_bassett_2017/#4-4_crossing_levels","text":"","title":"\u00a74-4 Crossing levels"},{"location":"170329_bassett_2017/#5_conclusion","text":"01 01:","title":"\u00a7 5 Conclusion"},{"location":"170616_cogmtg_hamamoto/","text":"killen 1996 dissatisfaction fairbuen 1993 relapsing tanja 2014 dankanalis 2016 vooks 2013; mohr 2010 looking own body other rating body katzman 1996 holland 2013 nevonen 2001 EDI2 - control of mind & - body image EBA Extrastriate FBA Fusiform","title":"170616 CogMTG Hamamoto"},{"location":"170704_logan_cross/","text":"17-07-04 Logan Cross \u00b6 evernote Presenter: Logan Cross His Boss: John Doherty About Deep Q-Network \u00b6 Q-network DQN\u00a4\u00ce\u00c9\u00fa\u00a4\u00a4\u00c1\u00a2\u00a4\u00c1\u00a1\u00a1\u00a3\u00ab\u00a1\u00a1Deep Q-Network\u00a4\u00f2Chainer\u00a4\u00c7\u0095\u00f8\u00a4\u00a4\u00a4\u00bf Convolutional neural network \u00b6 Wiki Fig: DQN Fig: Google DeepMind Suzuki Qs: Why not use Representational similarity analysis MVPA ... \u00b6 r01...","title":"170704 Logan Cross"},{"location":"170704_logan_cross/#about_deep_q-network","text":"Q-network DQN\u00a4\u00ce\u00c9\u00fa\u00a4\u00a4\u00c1\u00a2\u00a4\u00c1\u00a1\u00a1\u00a3\u00ab\u00a1\u00a1Deep Q-Network\u00a4\u00f2Chainer\u00a4\u00c7\u0095\u00f8\u00a4\u00a4\u00a4\u00bf","title":"About Deep Q-Network"},{"location":"170704_logan_cross/#convolutional_neural_network","text":"Wiki Fig: DQN Fig: Google DeepMind Suzuki Qs: Why not use Representational similarity analysis MVPA","title":"Convolutional neural network"},{"location":"170704_logan_cross/#_1","text":"r01...","title":"..."},{"location":"171124_NIPS/","text":"17-11-24 NIPS \u00b6 171124_NIPS.m4a Mushiake \u00b6 Sadato \u00b6 Sadato lab .. .. .. Koike 2012 Takahiro Koike Saito 2010 Isoda \u00b6 turn taking .. Isoda 2011 .. Yoshida 2012 mirror or mentalizing Eifuku \u00b6 lab Eifuku 2004 sao 2008 perception identity semantic","title":"171124 NIPS"},{"location":"171124_NIPS/#mushiake","text":"","title":"Mushiake"},{"location":"171124_NIPS/#sadato","text":"Sadato lab .. .. .. Koike 2012 Takahiro Koike Saito 2010","title":"Sadato"},{"location":"171124_NIPS/#isoda","text":"turn taking .. Isoda 2011 .. Yoshida 2012 mirror or mentalizing","title":"Isoda"},{"location":"171124_NIPS/#eifuku","text":"lab Eifuku 2004 sao 2008 perception identity semantic","title":"Eifuku"},{"location":"171125_NIPS/","text":"17-11-25 NIPS \u00b6 Sugiura \u00b6 .. .. Kikuchi, Sugiura Suzuki \u00b6 .. .. .. \u5f62\u614b \u7d71\u5408\u5931\u8a8d \u9023\u5408 \u8272\u5f69 \u8272\u899a\u5931\u8a8d \u8272\u5f69\u5931\u8a8d \u5931\u540d\u8f9e cavian r01...","title":"171125 NIPS"},{"location":"171125_NIPS/#sugiura","text":".. .. Kikuchi, Sugiura","title":"Sugiura"},{"location":"171125_NIPS/#suzuki","text":".. .. .. \u5f62\u614b \u7d71\u5408\u5931\u8a8d \u9023\u5408 \u8272\u5f69 \u8272\u899a\u5931\u8a8d \u8272\u5f69\u5931\u8a8d \u5931\u540d\u8f9e cavian r01...","title":"Suzuki"},{"location":"180220_anzellotti_2018/","text":"18-02-20 Anzellotti, 2018 \u00b6 ![cover] Anzellotti, Stefano , Coutanche, Marc N. , 2018 T1 - Beyond Functional Connectivity: Investigating Networks of Multivariate Representations AU - Anzellotti, Stefano AU - Coutanche, Marc N. JF - Trends in Cognitive Sciences UR - http://dx.doi.org/10.1016/j.tics.2017.12.002 Origin Mendeley Contents \u00b6 1 . Incorporation Multivariate Information into Our Understanding of Brain Networks 2 . From Univariate to Multivariate Signal 2.1 . Multivoxel Pattern Analysis 3 . Moving beyound Univariate Approaches to Connectivity 4 . Modeling Regions' Multivariate Responses 4.1 . Informational Connectivity 4.2 . Multivariate 4.3 . Multivariate 5 . Modeling Interactiona 5.1 . Informational 5.2 . Structural 5.3 . Multivariate Pattern 5.4 . Nonlinear 6 . From Studying 7 . Concluding 1 Incorporation Multivariate Information into Our Understanding of Brain Networks \u00b6 \u00b601 \u00b6 \u00b602 \u00b6 ? what is the relationship between teh information encoded in one brain reginon and the information encoded in another Functional Connectivity -> Multivariate analysis outline: advances / how this information is neglected recent tech significance of the change of perspective 2 From Univariate to Multivariate Signal \u00b6 \u00b603 \u00b6 univariate diff can reflect a wide variety of neural computations 2.1 Multivoxel Pattern Analysis \u00b6 \u00b604 \u00b6 MVPA : ... \u00b605 \u00b6 3 Moving beyond Univariate Approaches to Connectivity \u00b6 \u00b606 \u00b6 traditional FC NOT measure fluctuations in info that is represented in multivox pattern fig1 \u00b6 ![fig1] \u00b607 \u00b6 2 approaches: 1. Informational connectivity 2. MVPA they examine the relationship between patterns of multiple brain regions , such as using the patterns from one region to predict patterns in another region \u00b608 \u00b6 diff multivariate connectivity techs vary in: - how describe multivariate signal - classifier - points in multidim - model regional interaction - correlation - linear regression - nonlinear model 4 Modeling Regions' Multivariate Responses 4.1 Informational Connectivity \u00b6 \u00b609 \u00b6 Informational connectivity : ref.24 4.2 . Multivariate Pattern Dependence \u00b6","title":"180220 Anzellotti 2018"},{"location":"180220_anzellotti_2018/#contents","text":"1 . Incorporation Multivariate Information into Our Understanding of Brain Networks 2 . From Univariate to Multivariate Signal 2.1 . Multivoxel Pattern Analysis 3 . Moving beyound Univariate Approaches to Connectivity 4 . Modeling Regions' Multivariate Responses 4.1 . Informational Connectivity 4.2 . Multivariate 4.3 . Multivariate 5 . Modeling Interactiona 5.1 . Informational 5.2 . Structural 5.3 . Multivariate Pattern 5.4 . Nonlinear 6 . From Studying 7 . Concluding","title":"Contents"},{"location":"180220_anzellotti_2018/#1_incorporation_multivariate_information_into_our_understanding_of_brain_networks","text":"","title":"1 Incorporation Multivariate Information into Our Understanding of Brain Networks"},{"location":"180220_anzellotti_2018/#01","text":"","title":"\u00b601"},{"location":"180220_anzellotti_2018/#02","text":"? what is the relationship between teh information encoded in one brain reginon and the information encoded in another Functional Connectivity -> Multivariate analysis outline: advances / how this information is neglected recent tech significance of the change of perspective","title":"\u00b602"},{"location":"180220_anzellotti_2018/#2_from_univariate_to_multivariate_signal","text":"","title":"2 From Univariate to Multivariate Signal"},{"location":"180220_anzellotti_2018/#03","text":"univariate diff can reflect a wide variety of neural computations","title":"\u00b603"},{"location":"180220_anzellotti_2018/#21_multivoxel_pattern_analysis","text":"","title":"2.1 Multivoxel Pattern Analysis"},{"location":"180220_anzellotti_2018/#04","text":"MVPA : ...","title":"\u00b604"},{"location":"180220_anzellotti_2018/#05","text":"","title":"\u00b605"},{"location":"180220_anzellotti_2018/#3_moving_beyond_univariate_approaches_to_connectivity","text":"","title":"3 Moving beyond Univariate Approaches to Connectivity"},{"location":"180220_anzellotti_2018/#06","text":"traditional FC NOT measure fluctuations in info that is represented in multivox pattern","title":"\u00b606"},{"location":"180220_anzellotti_2018/#fig1","text":"![fig1]","title":"fig1"},{"location":"180220_anzellotti_2018/#07","text":"2 approaches: 1. Informational connectivity 2. MVPA they examine the relationship between patterns of multiple brain regions , such as using the patterns from one region to predict patterns in another region","title":"\u00b607"},{"location":"180220_anzellotti_2018/#08","text":"diff multivariate connectivity techs vary in: - how describe multivariate signal - classifier - points in multidim - model regional interaction - correlation - linear regression - nonlinear model 4 Modeling Regions' Multivariate Responses","title":"\u00b608"},{"location":"180220_anzellotti_2018/#41_informational_connectivity","text":"","title":"4.1 Informational Connectivity"},{"location":"180220_anzellotti_2018/#09","text":"Informational connectivity : ref.24","title":"\u00b609"},{"location":"180220_anzellotti_2018/#42_multivariate_pattern_dependence","text":"","title":"4.2. Multivariate Pattern Dependence"},{"location":"180307_avena-koenigsberger_2017/","text":"2018-03-07 Avena-Koenigsberger A, 2017 \u00b6 AU - Avena-Koenigsberger, Andrea AU - Misic, Bratislav AU - Sporns, Olaf TI - Communication dynamics in complex brain networks JO - Nature Reviews Neuroscience UR - http://dx.doi.org/10.1038/nrn.2017.149 https://www.nature.com/articles/nrn.2017.149 0. Intro \u00b6 1. Conceptual framework \u00b6 Structural Communicational Statistical (Functional) : time-averaged 2. Network topology and communication \u00b6 2.1. Routing communication \u00b6 2.2. Parallel communication \u00b6 2.3. Communication efficienct \u00b6 3. Network dynamics and communication \u00b6 3.1. Flow-based communication models \u00b6 3.2. Integration and segregation through routing and diffusion \u00b6 3.3. Communication process as a function of time \u00b6 4. Network computation and communication \u00b6 4.1. Communication dynamics as effective connectivity \u00b6 4.2. computation by networks \u00b6 Conclusion \u00b6","title":"180307 Avena Koenigsberger 2017"},{"location":"180307_avena-koenigsberger_2017/#0_intro","text":"","title":"0. Intro"},{"location":"180307_avena-koenigsberger_2017/#1_conceptual_framework","text":"Structural Communicational Statistical (Functional) : time-averaged","title":"1. Conceptual framework"},{"location":"180307_avena-koenigsberger_2017/#2_network_topology_and_communication","text":"","title":"2. Network topology and communication"},{"location":"180307_avena-koenigsberger_2017/#21_routing_communication","text":"","title":"2.1. Routing communication"},{"location":"180307_avena-koenigsberger_2017/#22_parallel_communication","text":"","title":"2.2. Parallel communication"},{"location":"180307_avena-koenigsberger_2017/#23_communication_efficienct","text":"","title":"2.3. Communication efficienct"},{"location":"180307_avena-koenigsberger_2017/#3_network_dynamics_and_communication","text":"","title":"3. Network dynamics and communication"},{"location":"180307_avena-koenigsberger_2017/#31_flow-based_communication_models","text":"","title":"3.1. Flow-based communication models"},{"location":"180307_avena-koenigsberger_2017/#32_integration_and_segregation_through_routing_and_diffusion","text":"","title":"3.2. Integration and segregation through routing and diffusion"},{"location":"180307_avena-koenigsberger_2017/#33_communication_process_as_a_function_of_time","text":"","title":"3.3. Communication process as a function of time"},{"location":"180307_avena-koenigsberger_2017/#4_network_computation_and_communication","text":"","title":"4. Network computation and communication"},{"location":"180307_avena-koenigsberger_2017/#41_communication_dynamics_as_effective_connectivity","text":"","title":"4.1. Communication dynamics as effective connectivity"},{"location":"180307_avena-koenigsberger_2017/#42_computation_by_networks","text":"","title":"4.2. computation by networks"},{"location":"180307_avena-koenigsberger_2017/#conclusion","text":"","title":"Conclusion"},{"location":"180326_harari_2011/","text":"2018-03-26 Harari 2011 \u00b6 TY - BOOK T1 - Sapiens: A Brief History of Humankind A1 - Harari, Y.N. SN - 9780062316103 UR - https://books.google.co.jp/books?id=FmyBAwAAQBAJ Y1 - 2015 PB - HarperCollins ER - Sapiens: A Brief History of Humankind Yuval Noah Harari 2011 Table of Contents \u00b6 1 : The cognitive revolution : 1.01 : An animal of no significance 1.02 : The tree of knowledge 1.03 : A day in the life of Adam and Eve 1.04 : The flood 2 : The agricultural revolution : 2.05 : History's biggest fraud 2.06 : Building pyramids 2.07 : Memory overload 2.08 : There is no justice in history 3 : The unification of humankind : 3.09 : The arrow of history 3.10 : The scent of money 3.11 : Imperial visions 3.12 : The law of religion 3.13 : The secret of success 4 : The scientific revolution : 4.14 : The discovery of ignorance 4.15 : The marriage of science and empire 4.16 : The capitalist creed 4.17 : The wheels of industry 4.18 : A permanent revolution 4.19 : And they lived happily ever after 4.20 : The end of Homo sapiens 5 : Afterword: The animal that became a god. 5.21 : 1 The cognitive revolution \u00b6 1.01 An animal of no significance \u00b6 Ann Gibbons, 2007, Food for thought https://scholar.google.co.jp/scholar?hl=en&num=20&as_sdt=0%2C5&q=denisova+genome+interbreeding+melanesia+review&btnG=&oq=denisova+genome+interbreeding+melanesia+revie 1.02 The tree of knowledge \u00b6 circumstance v. Gossip \u00b6 Grooming, Gossip, and the Evolution of Language, Robin Dunbar fiction \u00b6 believe \"as many as six impossible things before breakfast\" Peugeot \u00b6 Chimpanzee Politics: Power and Sex among Apes corporation <- corpus fiction, social construct (\u793e\u4f1a\u7684\u69cb\u6210\u6982\u5ff5), Genome \u00b6 . History & biology \u00b6 prehistory : ... 1.03 A day in the life of Adam and Eve \u00b6 http://science.sciencemag.org/content/sci/316/5831/1558.full.pdf","title":"180326 Harari 2011"},{"location":"180326_harari_2011/#table_of_contents","text":"1 : The cognitive revolution : 1.01 : An animal of no significance 1.02 : The tree of knowledge 1.03 : A day in the life of Adam and Eve 1.04 : The flood 2 : The agricultural revolution : 2.05 : History's biggest fraud 2.06 : Building pyramids 2.07 : Memory overload 2.08 : There is no justice in history 3 : The unification of humankind : 3.09 : The arrow of history 3.10 : The scent of money 3.11 : Imperial visions 3.12 : The law of religion 3.13 : The secret of success 4 : The scientific revolution : 4.14 : The discovery of ignorance 4.15 : The marriage of science and empire 4.16 : The capitalist creed 4.17 : The wheels of industry 4.18 : A permanent revolution 4.19 : And they lived happily ever after 4.20 : The end of Homo sapiens 5 : Afterword: The animal that became a god. 5.21 :","title":"Table of Contents"},{"location":"180326_harari_2011/#1_the_cognitive_revolution","text":"","title":"1 The cognitive revolution"},{"location":"180326_harari_2011/#101_an_animal_of_no_significance","text":"Ann Gibbons, 2007, Food for thought https://scholar.google.co.jp/scholar?hl=en&num=20&as_sdt=0%2C5&q=denisova+genome+interbreeding+melanesia+review&btnG=&oq=denisova+genome+interbreeding+melanesia+revie","title":"1.01 An animal of no significance"},{"location":"180326_harari_2011/#102_the_tree_of_knowledge","text":"","title":"1.02 The tree of knowledge"},{"location":"180326_harari_2011/#circumstance_v_gossip","text":"Grooming, Gossip, and the Evolution of Language, Robin Dunbar","title":"circumstance v. Gossip"},{"location":"180326_harari_2011/#fiction","text":"believe \"as many as six impossible things before breakfast\"","title":"fiction"},{"location":"180326_harari_2011/#peugeot","text":"Chimpanzee Politics: Power and Sex among Apes corporation <- corpus fiction, social construct (\u793e\u4f1a\u7684\u69cb\u6210\u6982\u5ff5),","title":"Peugeot"},{"location":"180326_harari_2011/#genome","text":".","title":"Genome"},{"location":"180326_harari_2011/#history_biology","text":"prehistory : ...","title":"History &amp; biology"},{"location":"180326_harari_2011/#103_a_day_in_the_life_of_adam_and_eve","text":"http://science.sciencemag.org/content/sci/316/5831/1558.full.pdf","title":"1.03 A day in the life of Adam and Eve"},{"location":"180719_betzel2017/","text":"18-07-19 Multi-scale brain networks \u00b6 Betzel, R. F., & Bassett, D. S. (2017). Multi-scale brain networks. Neuroimage, 160, 73-83. @article{betzel2017multi, title = {Multi-scale brain networks}, author = {Betzel, Richard F and Bassett, Danielle S}, journal = {Neuroimage}, volume = {160}, pages = {73--83}, year = {2017}, publisher = {Elsevier} } Original | Mendeley Overview 1 Intro 2 Functional and structural brain networks 3 Multi-scale network analysis 3.1 Multi-scale community structure 3.1.1 Multi-scale community structure in the neuroimaging literature 3.1.2 Implementation and practical considerations 3.1.3 Selecting the resolution parameter 3.1.4 Consensus community structure and communities of interest 3.2 Multi-scale rich club and core\u2013periphery organization 3.3 Multi-scale temporal networks 3.3.1 Multi-scale, multi-layer network analysis 3.3.2 Practical considerations 3.4 Multi-scale spatial networks 4 Conclusion and future directions 1. Intro \u00b6 \"scale\": spatial cell /synapse brain region / large-scale fiber tract temporal sub-millisecond entire lifespan / evolutionary changes across different species topological individual nodes network focus on... networks algorithms (e.g., community detection) multi-scale temporal networks & multi-layer techniques 2. Functional and structural brain networks \u00b6 connectivity: structured / anatomical connectivity (SC) functional (FC) connectivity mat: \\mathbf{A} element: A_{i,j} 3. Multi-scale network analysis \u00b6 Simpson 2016 topological degree Takeuchi 2015 spectrum path length : average # of steps Santernecchi 2014 mesoscale: - community structure Fortunato 2010 - cores & peripheries - rich clubs 3.1 Multi-scale community structure \u00b6 \\begin{align*} \\tag{eq.1} Q = \\sum_{ij} \\big[ A_{ij} - P_{ij} \\big] \\delta(\\sigma_i\\sigma_j) \\end{align*} Q : modularity, i , j : node index A_{ij} : weight of observed connection between nodes i & j P_{ij} : weight of expected connection \\sigma_i \\in [1, \\cdots, K] : communities node i is assigned \\delta(\\sigma_i\\sigma_j) : Kronecker delta fn \\begin{align*} \\tag{app.1} \\delta(\\sigma_i \\sigma_j) = \\begin{cases} 1 \\quad \\text{if arguments are the same}\\\\ 0 \\quad \\text{if NOT} \\end{cases} \\end{align*} \\begin{align*} \\tag{eq.2} Q(\\gamma) = \\sum_{ij} \\big[ A_{ij} - \\gamma P_{ij} \\big] \\delta(\\sigma_i \\sigma_j) \\end{align*} 4. Conclusion and future directions \u00b6 img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; } /*img{width: 50%; float: right;}*/","title":"180719 Betzel 2017"},{"location":"180719_betzel2017/#1_intro","text":"\"scale\": spatial cell /synapse brain region / large-scale fiber tract temporal sub-millisecond entire lifespan / evolutionary changes across different species topological individual nodes network focus on... networks algorithms (e.g., community detection) multi-scale temporal networks & multi-layer techniques","title":"1. Intro"},{"location":"180719_betzel2017/#2_functional_and_structural_brain_networks","text":"connectivity: structured / anatomical connectivity (SC) functional (FC) connectivity mat: \\mathbf{A} element: A_{i,j}","title":"2. Functional and structural brain networks"},{"location":"180719_betzel2017/#3_multi-scale_network_analysis","text":"Simpson 2016 topological degree Takeuchi 2015 spectrum path length : average # of steps Santernecchi 2014 mesoscale: - community structure Fortunato 2010 - cores & peripheries - rich clubs","title":"3. Multi-scale network analysis"},{"location":"180719_betzel2017/#31_multi-scale_community_structure","text":"\\begin{align*} \\tag{eq.1} Q = \\sum_{ij} \\big[ A_{ij} - P_{ij} \\big] \\delta(\\sigma_i\\sigma_j) \\end{align*} Q : modularity, i , j : node index A_{ij} : weight of observed connection between nodes i & j P_{ij} : weight of expected connection \\sigma_i \\in [1, \\cdots, K] : communities node i is assigned \\delta(\\sigma_i\\sigma_j) : Kronecker delta fn \\begin{align*} \\tag{app.1} \\delta(\\sigma_i \\sigma_j) = \\begin{cases} 1 \\quad \\text{if arguments are the same}\\\\ 0 \\quad \\text{if NOT} \\end{cases} \\end{align*} \\begin{align*} \\tag{eq.2} Q(\\gamma) = \\sum_{ij} \\big[ A_{ij} - \\gamma P_{ij} \\big] \\delta(\\sigma_i \\sigma_j) \\end{align*}","title":"3.1 Multi-scale community structure"},{"location":"180719_betzel2017/#4_conclusion_and_future_directions","text":"img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; } /*img{width: 50%; float: right;}*/","title":"4. Conclusion and future directions"},{"location":"180728_schellekens2018/","text":"18-07-28 schellekens2018 \u00b6 Schellekens, W., Petridou, N., & Ramsey, N. F. (2018). Detailed somatotopy in primary motor and somatosensory cortex revealed by Gaussian population receptive fields. NeuroImage, 179, 337-347. @article{schellekens2018detailed, title = \"Detailed somatotopy in primary motor and somatosensory cortex revealed by Gaussian population receptive fields\", journal = \"NeuroImage\", volume = \"179\", pages = \"337 - 347\", year = \"2018\", issn = \"1053-8119\", doi = \"https://doi.org/10.1016/j.neuroimage.2018.06.062\", url = \"http://www.sciencedirect.com/science/article/pii/S105381191830569X\", author = \"Wouter Schellekens and Natalia Petridou and Nick F. Ramsey\", keywords = \"Somatotopy, Motor cortex, Somatosensory cortex, Population receptive fields, Sensorimotor integration, High-field fMRI\" } Original | [Mendeley][mend] 1. \u00b6 [ref][ref01] img{width: 400px; float: right;}","title":"180728 Schellekens 2018"},{"location":"180805_xia2018/","text":"18-08-05 \u00b6 Linked dimensions of psychopathology and connectivity in functional brain networks Xia, C. H., Ma, Z., Ciric, R., Gu, S., Betzel, R. F., Kaczkurkin, A. N., ... & Cui, Z. (2018). Linked dimensions of psychopathology and connectivity in functional brain networks. Nature Communications, 9(1), 3003. Original | Mendeley | Github Overview \u00b6 Intro Results 2.1 Linked dimensions of psychopathology and connectivity 2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories 2.3 Common and dissociable patterns of connectivity 2.4 Developmental effects and sex differences 2.5 Linked dimensions are replicated in an independent sample Discussion Methods 4.1 Participants 4.2 Psychiatric assessment 4.3 Image acquisition 4.4 Structural preprocessing 4.5 Functional preprocessing 4.6 Network construction 4.7 Dimensionality reduction 4.8 Sparse canonical correlation analysis 4.9 Grid search for regularization parameters 4.10 Permutation testing 4.11 Resampling procedure 4.12 Network module analysis 4.13 Analysis of common connectivity features across dimensions 4.14 Analysis of age effects and sex differences 1. Intro \u00b6 Insel 2015 Bassett 2017 module-specific: Gordon 2014 Bassett 2018 between modules Satterthwaite 2013 \"neurodeve-lopmental connectopathies\" Paus 2005 limitations: 1. case-ctrl approach 2. driven by cov in the clinical symptomatolgy, rather than brain & behavior feat 3. youth studies: small samples Bzdok 2017 sCCA Witten 2009 Drysdale 2017 2. Result \u00b6 2.1 Linked dimensions of psychopathology and connectivity \u00b6 2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories \u00b6 2.3 Common and dissociable patterns of connectivity \u00b6 2.4 Developmental effects and sex differences \u00b6 2.5 Linked dimensions are replicated in an independent sample \u00b6 3. Discussion \u00b6 4. Online methods \u00b6 4.1 Participants \u00b6 4.2 Psychiatric assessment \u00b6 4.3 Image acq \u00b6 4.4 Structural prep \u00b6 4.5 Functional prep \u00b6 magnetic field inhomogeneities 4 initial vol realignmnet intensity outliers demeaning & removal of any linear / quatdric trends 4.6 Network construction \u00b6 4.7 Dimensinality reduction \u00b6 MAD (Median Absolute Deviation) Def: MAD (| \\mathbf{X}_i - \\text{median}(\\mathbf{X})|) 4.8 sCCA \u00b6 \\mathbf{X}_{n \\times p} , \\mathbf{Y}_{n \\times q} : mat n : num of obs (participants) p , q : num of var (feat) \\mathbf{u} , \\mathbf{v} c_1 , c_2 : constant \\begin{align*} \\tag{1} \\text{maximize } \\mathbf{u}^T\\mathbf{X}^T\\mathbf{Yv}, \\\\ \\text{subject to } ||\\mathbf{u}||^2_2 \u2264 1, ||\\mathbf{v}||^2_2 \u2264 1, \\\\ ||\\mathbf{u}||_1 \u2264 c_1, ||\\mathbf{v}||_1 \u2264 c_2 \\end{align*} \\begin{align*} L_1\\text{-norm:} \\quad ||\\mathbf{x}||_1 &:= \\sum_{i=1}^n |x_i| \\\\ L_2\\text{-norm:} \\quad ||\\mathbf{x}||_2 &:= \\sqrt{ \\sum_{i=1}^n{x_i^2}} \\end{align*} 4.9 Grid search for regularization parameters \u00b6 4.10 Permutation testing \u00b6 4.11 Resampling procedure \u00b6 4.12 Network module analysis \u00b6 4.13 Analysis of common connectivity features across dimensions \u00b6 4.14 Analysis of age effects and sex differences \u00b6 img{width: 50%; float: right;}","title":"180805 Xia 2018"},{"location":"180805_xia2018/#overview","text":"Intro Results 2.1 Linked dimensions of psychopathology and connectivity 2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories 2.3 Common and dissociable patterns of connectivity 2.4 Developmental effects and sex differences 2.5 Linked dimensions are replicated in an independent sample Discussion Methods 4.1 Participants 4.2 Psychiatric assessment 4.3 Image acquisition 4.4 Structural preprocessing 4.5 Functional preprocessing 4.6 Network construction 4.7 Dimensionality reduction 4.8 Sparse canonical correlation analysis 4.9 Grid search for regularization parameters 4.10 Permutation testing 4.11 Resampling procedure 4.12 Network module analysis 4.13 Analysis of common connectivity features across dimensions 4.14 Analysis of age effects and sex differences","title":"Overview"},{"location":"180805_xia2018/#1_intro","text":"Insel 2015 Bassett 2017 module-specific: Gordon 2014 Bassett 2018 between modules Satterthwaite 2013 \"neurodeve-lopmental connectopathies\" Paus 2005 limitations: 1. case-ctrl approach 2. driven by cov in the clinical symptomatolgy, rather than brain & behavior feat 3. youth studies: small samples Bzdok 2017 sCCA Witten 2009 Drysdale 2017","title":"1. Intro"},{"location":"180805_xia2018/#2_result","text":"","title":"2. Result"},{"location":"180805_xia2018/#21_linked_dimensions_of_psychopathology_and_connectivity","text":"","title":"2.1 Linked dimensions of psychopathology and connectivity"},{"location":"180805_xia2018/#22_brain-guided_dimensions_of_psychopathology_cross_clinical_diagnostic_categories","text":"","title":"2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories"},{"location":"180805_xia2018/#23_common_and_dissociable_patterns_of_connectivity","text":"","title":"2.3 Common and dissociable patterns of connectivity"},{"location":"180805_xia2018/#24_developmental_effects_and_sex_differences","text":"","title":"2.4 Developmental effects and sex differences"},{"location":"180805_xia2018/#25_linked_dimensions_are_replicated_in_an_independent_sample","text":"","title":"2.5 Linked dimensions are replicated in an independent sample"},{"location":"180805_xia2018/#3_discussion","text":"","title":"3. Discussion"},{"location":"180805_xia2018/#4_online_methods","text":"","title":"4. Online methods"},{"location":"180805_xia2018/#41_participants","text":"","title":"4.1 Participants"},{"location":"180805_xia2018/#42_psychiatric_assessment","text":"","title":"4.2 Psychiatric assessment"},{"location":"180805_xia2018/#43_image_acq","text":"","title":"4.3 Image acq"},{"location":"180805_xia2018/#44_structural_prep","text":"","title":"4.4 Structural prep"},{"location":"180805_xia2018/#45_functional_prep","text":"magnetic field inhomogeneities 4 initial vol realignmnet intensity outliers demeaning & removal of any linear / quatdric trends","title":"4.5 Functional prep"},{"location":"180805_xia2018/#46_network_construction","text":"","title":"4.6 Network construction"},{"location":"180805_xia2018/#47_dimensinality_reduction","text":"MAD (Median Absolute Deviation) Def: MAD (| \\mathbf{X}_i - \\text{median}(\\mathbf{X})|)","title":"4.7 Dimensinality reduction"},{"location":"180805_xia2018/#48_scca","text":"\\mathbf{X}_{n \\times p} , \\mathbf{Y}_{n \\times q} : mat n : num of obs (participants) p , q : num of var (feat) \\mathbf{u} , \\mathbf{v} c_1 , c_2 : constant \\begin{align*} \\tag{1} \\text{maximize } \\mathbf{u}^T\\mathbf{X}^T\\mathbf{Yv}, \\\\ \\text{subject to } ||\\mathbf{u}||^2_2 \u2264 1, ||\\mathbf{v}||^2_2 \u2264 1, \\\\ ||\\mathbf{u}||_1 \u2264 c_1, ||\\mathbf{v}||_1 \u2264 c_2 \\end{align*} \\begin{align*} L_1\\text{-norm:} \\quad ||\\mathbf{x}||_1 &:= \\sum_{i=1}^n |x_i| \\\\ L_2\\text{-norm:} \\quad ||\\mathbf{x}||_2 &:= \\sqrt{ \\sum_{i=1}^n{x_i^2}} \\end{align*}","title":"4.8 sCCA"},{"location":"180805_xia2018/#49_grid_search_for_regularization_parameters","text":"","title":"4.9 Grid search for regularization parameters"},{"location":"180805_xia2018/#410_permutation_testing","text":"","title":"4.10 Permutation testing"},{"location":"180805_xia2018/#411_resampling_procedure","text":"","title":"4.11 Resampling procedure"},{"location":"180805_xia2018/#412_network_module_analysis","text":"","title":"4.12 Network module analysis"},{"location":"180805_xia2018/#413_analysis_of_common_connectivity_features_across_dimensions","text":"","title":"4.13 Analysis of common connectivity features across dimensions"},{"location":"180805_xia2018/#414_analysis_of_age_effects_and_sex_differences","text":"img{width: 50%; float: right;}","title":"4.14 Analysis of age effects and sex differences"},{"location":"180904/","text":"18-09-04 \u00b6 Albert L\u00e1szl\u00f3 Barab\u00e1si, Network Science Original 0 Personal Intro \u00b6 1 Intro \u00b6 1. 1 Vulnerability Due to Interconnectivity \u00b6 1. 2 Networks at the Heart of Complex Systems \u00b6 1. 3 Two Forces Helped the Emergence of Network Science \u00b6 The Emergence of Network Maps \u00b6 The Universality of Network Characteristics \u00b6 1. 4 The Characteristics of Network Science \u00b6 Interdisciplinary Nature \u00b6 Empirical, Data Driven Nature \u00b6 Quantitative and Mathematical Nature \u00b6 Computational Nature \u00b6 1. 5 Societal Impact \u00b6 Economic Impact: From Web Search to Social Networking \u00b6 Health: From Drug Design to Metabolic Engineering \u00b6 Security: Fighting Terrorism \u00b6 Epidemics: from Forecasting to Halting Deadly Viruses \u00b6 Neuroscience: Mapping the Brain \u00b6 Management: Uncovering the Internal Structure of an Organization \u00b6 1. 6 Scientific Impact \u00b6 1. 7 Summary \u00b6 1. 8 Homework \u00b6 1. 9 \u00b6 2. Graph Theory \u00b6 3. Random Networks \u00b6 4. The Scale-Free Property \u00b6 5. The Barab\u00e1si-Albert Model \u00b6 6. Evolving Networks \u00b6 7. Degree Correlations \u00b6 8. Network Robustness \u00b6 9. Communities \u00b6 10. Spreading Phenomena \u00b6 [ref][ref01] img.right{ width: 50%; float: right; } img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; }","title":"180904"},{"location":"180904/#0_personal_intro","text":"","title":"0 Personal Intro"},{"location":"180904/#1_intro","text":"","title":"1 Intro"},{"location":"180904/#1_1_vulnerability_due_to_interconnectivity","text":"","title":"1. 1 Vulnerability Due to Interconnectivity"},{"location":"180904/#1_2_networks_at_the_heart_of_complex_systems","text":"","title":"1. 2 Networks at the Heart of Complex Systems"},{"location":"180904/#1_3_two_forces_helped_the_emergence_of_network_science","text":"","title":"1. 3 Two Forces Helped the Emergence of Network Science"},{"location":"180904/#the_emergence_of_network_maps","text":"","title":"The Emergence of Network Maps"},{"location":"180904/#the_universality_of_network_characteristics","text":"","title":"The Universality of Network Characteristics"},{"location":"180904/#1_4_the_characteristics_of_network_science","text":"","title":"1. 4 The Characteristics of Network Science"},{"location":"180904/#interdisciplinary_nature","text":"","title":"Interdisciplinary Nature"},{"location":"180904/#empirical_data_driven_nature","text":"","title":"Empirical, Data Driven Nature"},{"location":"180904/#quantitative_and_mathematical_nature","text":"","title":"Quantitative and Mathematical Nature"},{"location":"180904/#computational_nature","text":"","title":"Computational Nature"},{"location":"180904/#1_5_societal_impact","text":"","title":"1. 5 Societal Impact"},{"location":"180904/#economic_impact_from_web_search_to_social_networking","text":"","title":"Economic Impact: From Web Search to Social Networking"},{"location":"180904/#health_from_drug_design_to_metabolic_engineering","text":"","title":"Health: From Drug Design to Metabolic Engineering"},{"location":"180904/#security_fighting_terrorism","text":"","title":"Security: Fighting Terrorism"},{"location":"180904/#epidemics_from_forecasting_to_halting_deadly_viruses","text":"","title":"Epidemics: from Forecasting to Halting Deadly Viruses"},{"location":"180904/#neuroscience_mapping_the_brain","text":"","title":"Neuroscience: Mapping the Brain"},{"location":"180904/#management_uncovering_the_internal_structure_of_an_organization","text":"","title":"Management: Uncovering the Internal Structure of an Organization"},{"location":"180904/#1_6_scientific_impact","text":"","title":"1. 6 Scientific Impact"},{"location":"180904/#1_7_summary","text":"","title":"1. 7 Summary"},{"location":"180904/#1_8_homework","text":"","title":"1. 8 Homework"},{"location":"180904/#1_9","text":"","title":"1. 9"},{"location":"180904/#2_graph_theory","text":"","title":"2. Graph Theory"},{"location":"180904/#3_random_networks","text":"","title":"3. Random Networks"},{"location":"180904/#4_the_scale-free_property","text":"","title":"4. The Scale-Free Property"},{"location":"180904/#5_the_barabasi-albert_model","text":"","title":"5. The Barab\u00e1si-Albert Model"},{"location":"180904/#6_evolving_networks","text":"","title":"6. Evolving Networks"},{"location":"180904/#7_degree_correlations","text":"","title":"7. Degree Correlations"},{"location":"180904/#8_network_robustness","text":"","title":"8. Network Robustness"},{"location":"180904/#9_communities","text":"","title":"9. Communities"},{"location":"180904/#10_spreading_phenomena","text":"[ref][ref01] img.right{ width: 50%; float: right; } img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; }","title":"10. Spreading Phenomena"},{"location":"190311_BassettDS_BullmoreET2017/","text":"19-03-11 Small-World Brain Networks Revisited \u00b6 Bassett, D.S. and Bullmore, E.T., 2017. Small-world brain networks revisited. The Neuroscientist, 23(5), pp.499-516. Original | Mendeley Contents \u00b6 Abstract Small Worlds, Watts and Strogatz Small-World Brain Graphs What Have We (Not) Learnt Since 2006? Universality Economical Small-World Networks Small-Worldness Is Not the Whole Story Challenges to Small-Worldness Binary Graphs Weighted Graphs The Small-World Propensity Twenty-First Century Tract-Tracing Small-Worldness of Binary Tract-Tracing Networks Small-Worldness of Weighted Tract-Tracing Networks Weighted Small-Worldness and the Role of Edge Weights The Utility of Weak Connections Conclusions 0. Abstract \u00b6 It is nearly 20 years since the concept of a small-world network was first quantitatively defined, by a combination of high clustering and short path length; and about 10 years since this metric of complex network topology began to be widely applied to analysis of neuroimaging and other neuroscience data as part of the rapid growth of the new field of connectomics. Here, we review briefly the foundational concepts of graph theoretical estimation and generation of small-world networks. We take stock of some of the key developments in the field in the past decade and we consider in some detail the implications of recent studies using high-resolution tract-tracing methods to map the anatomical networks of the macaque and the mouse. In doing so, we draw attention to the important methodological distinction between topological analysis of binary or unweighted graphs, which have provided a popular but simple approach to brain network analysis in the past, and the topology of weighted graphs, which retain more biologically relevant information and are more appropriate to the increasingly sophisticated data on brain connectivity emerging from contemporary tract-tracing and other imaging studies. We conclude by highlighting some possible future trends in the further development of weighted small-worldness as part of a deeper and broader understanding of the topology and the functional value of the strong and weak links between areas of mammalian cortex. 1. Small Worlds, Watts and Strogatz \u00b6 Small-worldness now seems to be a ubiquitous characteristic of many complex systems; but its first, and still most familiar, appearance was in the form of social networks. We know that as individual agents (nodes) in a social network, we are connected by strong familial and friendship ties (edges) to a relatively few people who are likely also strongly connected to each other, forming a social clique, family or tribe. Yet we also know that we can travel far away from our tribal network, to physically remote cultures and places, and sometimes be surprised there to meet people\u2014often \u201cfriends-of-friends\u201d\u2014who are quite closely connected to our home tribe: \u201cit\u2019s a small world,\u201d we say. This common intuition was experimentally investigated by Milgram (1967) , who asked people in the Midwest of the United States (Omaha, Nebraska) to forward a letter addressed to an unknown individual in Boston by posting it to the friend or acquaintance in their social network that they thought might know someone else who would know the addressee ( Fig. 1 ). It was discovered, on average over multiple trials of this procedure, that the letters successfully reaching Boston had been passed through 6 intermediate postings, which was considered much less than expected given the geographical distance between source and target addresses. In the language of graph theory, the characteristic path length of Milgram\u2019s social networks was short. Figure 1. An illustration of the shortest path between Omaha and Boston in Milgram\u2019s social network experiment, published in Psychology Today in 1967 . Here, the results of multiple experiments are represented as a composite shortest path between the source (a person in Omaha) and the target (a person in Boston). A letter addressed to the target was given to the source, who was asked to send it on (with the same instructions) to the friend or acquaintance that they thought was most likely to know the target, or someone else who might know the target personally. It was found that most letters that eventually reached the correct address in Boston passed through six intermediaries between source and target (denoted 1st remove, 2nd remove, etc.), popularizing the notion that each of us is separated by no more than \u201csix degrees of freedom\u201d from any other individual in a geographically distributed social network. Reproduced with permission from Milgram (1967) . Famously, Watts and Strogatz (1998) combined this concept of path length (the minimum number of edges needed to make a connection between nodes) with a measure of topological clustering or cliquishness of edges between nodes ( Fig. 2 ). More formally, clustering measures the probability that the nodes j and k which are both directly connected to node i are also directly connected to each other; this is equivalent to measuring the proportion of closed triangular three-node motifs in a network ( Sporns and K\u00f6tter 2004 ). Watts and Strogatz (WS) explored the behavior of path length and clustering in a simple generative model (henceforth the WS model ) ( Fig. 3 ). Starting with a binary lattice network of N nodes each connected to the same number of nearest neighbors, by edges of identical weight (unity), the WS model iteratively rewires the lattice by randomly deleting an existing edge, between nodes i and j and replacing it by a new edge between node i and any node k \\neq j . They found that as the probability of random rewiring was incrementally increased from zero, so that the original lattice was progressively randomized, sparsely rewired networks demonstrated both high clustering (like a lattice) and short path length (like a random graph). By analogy to social networks, these algorithmically generated graphs were called small-world networks. Figure 2. Diagrams of clustering and path length in binary and weighted networks. (A) In a binary network, all edges have the same weight, and that is a weight equal to unity. In this example of a binary graph, if one wishes to walk along the shortest path from the orange node to the green node, then one would choose to walk along the edges highlighted in red, rather than along the edges highlighted in blue. We also note that the clustering coefficient of the green node is equal to 1 (all neighbors are also connected to each other to form a closed triangular motif), while the clustering coefficient of the orange node is =1 (only three out of five neighbors are also connected to each other). (B) In a weighted graph, edges can have different weights. In this example, edges have weights of \\frac{3}{3} = 1 , \\frac{2}{3} = 0.66 , and \\frac{1}{3} = 0.33 . If one wishes to traverse the graph from the orange node to the green node along the shortest path, one would choose to follow the path along the edges with weight equal to unity (stronger weights are equivalent to shorter topological distance). Note also that because the edges are now weighted, neither the orange nor the green nodes has a clustering coefficient equal to unity. Figure 3. The Watts\u2013Strogatz model and the generation of small-world networks. The canonical model of a small-world network is that described by Duncan Watts and Steve Strogatz in their 1998 article in Nature. The model begins with a regular lattice network in which each node is placed along the circumference of a circle, and is connected to its k nearest neighbors on that circle. Then, with probability p edges are rewired uniformly at random such that (1) at p = 0 the network is a lattice and (2) at p = 1 the network is random. Interestingly, at intermediate values of p the network has so-called \u201csmall-world\u201d characteristics with significant local clustering (from the lattice model) and short average path length facilitated by the topological short-cuts created during the random rewiring procedure. Because this architecture can be defined mathematically, small-world graphs have proven fundamental in understanding game theory ( Li and Cao 2009 ) and even testing analytical results in subfields of mathematics ( Konishi and Hara 2011 ). Yet, while this work provided a qualitative model of a small-world graph, it did not give a statistic to measure the degree of small-worldness in a particular data set. As a simple scalar measure of \u201csmall-worldness,\u201d Humphries and colleagues defined the small-world index, \u03c3 to be the ratio of the clustering coefficient (normalized by that expected in a random graph) to the average shortest path length (also normalized by that expected in a random graph) ( Humphries and others 2006 ). The intuition here is that this index should be large (in particular, \\sigma > 1 when the clustering coefficient is much greater than expected in the random graph, and the average shortest path length is comparable to that expected in a random graph. Since this initial definition, other extensions have been proposed and utilized ( Telesford and others 2011 ; Toppi and others 2012 ), building on the same general notions. In addition to introducing this generative model, Watts and Strogatz (1998) also showed how small-worldness could be estimated in naturally occurring networks. The hybrid combination of high clustering and short path length that emerged in sparsely rewired WS networks was proposed as a general quantitative measure of small-worldness in other networks. It was shown immediately that a nervous system was among the real-world networks that shared the small-world pattern of topological organization. Using data on the synaptic and gap junction connectivity between all N = 302 neurons in the nervous system of Caenorhabditis elegans ( White and others 1986 ), a binary undirected graph was constructed representing each neuron as an identical node and each synapse (~5000) or gap junction (~600) as an identical, unweighted and undirected edge between nodes. This graph of about 5600 edges between 302 nodes was sparsely connected: only about 12% of the maximum possible number of synaptic connections, \\frac{N^2-N}{2} = 45451 , actually existed. Compared with a random graph of \\frac{N^2-N}{2} = 45451 nodes, C. elegans had high clustering \\Gamma \\sim 5.6 and short path length \\Lambda \\sim 1.18 . Thus the C. elegans connectome was small-world, in the same quantitative sense as the networks generated by the WS model at low rewiring probabilities, less than 10%. But note that does not necessarily mean that the C. elegans connectome was biologically generated by the WS algorithm of random rewiring of established connections (axonal projections) between neurons. To put it another way, the WS model can generate small-world networks but not all small-world networks were generated by a WS model. (And the WS model does not seem like a biologically plausible generative model for brain networks ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]).) Small-World Brain Graphs \u00b6 Following the small-world analysis of C. elegans, pioneering topological studies of mammalian cortical networks used databases of tract-tracing experiments to demonstrate that the cat and macaque interareal anatomical networks shared similar small-world properties of short path length and high clustering ([Hilgetag and Kaiser 2004][2004KaiserM_HilgetagCC]; [Sporns and Zwi 2004][2004ZwiJD_SpornsO]). The first graph theoretical studies of neuroimaging data demonstrated that large-scale interareal networks of functional and structural connectivity in the human brain also had small-world properties ([Bassett and others 2006][2006BullmoreE_BassettDS]; [Salvador and others 2005][2005BullmoreE_SalvadorR]; [Vaessen and others 2010][2010BackesWH_VaessenMJ]). These and other seminal discoveries were central to the emergence of connectomics as a major growth point of network neuroscience ([Sporns and others 2005][2005K\u00f6tterR_SpornsO]). About 10 years ago, we reviewed these and other data in support of the idea that the brain is a small world network ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). Here, we aim to take another look at the concept of small-worldness, one or two decades since it was first formulated quantitatively and applied to brain network analysis at microscopic and macroscopic scales of anatomical resolution. First, we review some of the key questions about small-worldness that have been a focus of work in the period 2006\u20132016; then we review the technical evidence for small-worldness in high resolution tract-tracing data from the macaque and the mouse; finally, we highlight some likely trends in the further evolution of small-worldness as part of a deeper understanding of the topology of weighted brain graphs. 2. What Have We (Not) Learnt Since 2006? \u00b6 We have learnt a lot about complex topological organization of nervous systems since 2006, as evidenced by rapid growth in research articles, reviews, and citations related to \u201cbrain graphs\u201d and \u201cconnectomes\u201d ([Bullmore and Bassett 2011][2011BassettDS_Bullmore]; [Bullmore and Sporns 2009][2009SpornsO_BullmoreE]; [Pessoa 2014][2014PessoaL]); by the publication of several textbooks ([Fornito and others 2016][2016BullmoreET_FornitoA]; Sporns 2011 ); and by the recent launch of new specialist journals for network neuroscience. This emerging field of brain topology has grown much bigger than the foundational concept of small-worldness. But what have we learnt more specifically about brain small-worldness since 2006, and what do we still have to learn? Universality \u00b6 There is no doubt that small-worldness\u2014the combination of non-random clustering with near-random path length\u2014has been very frequently reported across a wide range of neuroscience studies. Small-world topology has been highly replicated across multiple species and scales from structural and functional MRI studies of large-scale brain networks in humans to multielectrode array recordings of cellular networks in cultures ([Bettencourt and others 2007][2007GrossGW_BettencourtLM]) and intact animals ([van den Heuvel and others 2016][2016SpornsO_VanDenHeuvelMP]). It seems reasonable to conclude that small-worldness is at least very common in network neuroscience; but is it a universal property of nervous systems? Universality is a strong claim and difficult to affirm conclusively. As Popper noted in his philosophy of science by hypothetical refutation ([Popper 1963][1963PopperKR]), the universal hypothesis that \u201call swans are white\u201d can only be affirmed conclusively by a complete survey of every swan in the world. Whereas it can be immediately and decisively refuted by the observation of a single black swan. Similarly, the claim that all brains have small-world topology has not yet been (and never will be) affirmed by a complete connectomic mapping of every brain in the world. Some apparent counter-examples of brain networks that do not have small-world topology have been reported and deserve careful consideration as possible Popperian black swans ([see below][]). However, we can provisionally conclude that enough evidence has amassed to judge that small-worldness is a nearly universal property of nervous systems. Indeed, it seems likely that brains are only one of a large \u201cuniversality class\u201d of small-world networks comprising also many other non-neural or non-biological complex systems. Such near-universality of small-worldness, or any other brain network parameter, has a number of implications. First, near-universality implies self-similarity . If the macroscale interareal network of the human brain is small-world, as is the microscale interneuronal network of the worm or the fly, then we should expect also that the microscale interneuronal network of the human brain is small-world. Self-similarity of small-worldness would be indexed by scale invariance of network path length and clustering parameters as the anatomical resolution \u201czooms in\u201d from macro- to microscales. Although there is abundant evidence for scaling, fractal or self-similar statistics in many aspects of brain network topology ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Klimm and others 2014][2014MuchaPJ_KlimmF]), experimental data do not yet exist that could support a multiscale, macro-to-micro analysis of small-worldness (and other network properties) in the same (human or mammalian) nervous system ( Bassett and Siebenhuhner 2013 ). Second, near-universality suggests some very general selection pressures might be operative on the evolution and development of nervous systems across scales and species. This line of thinking has led to the formulation of generative models that can simulate brain networks by some probabilistic growth rule or genetic algorithm. It has been found that simple generative models, that add edges to a network based on the spatial distance and the topological relationships between nodes, can recapitulate small-worldness and many other properties of the connectome on the basis of two (spatial and topological) parameters ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). This serves as a reminder that the network phenotype of small-worldness can be generated by many different mechanisms and the biological mechanisms controlling formation of small-world properties in brain networks currently remain unknown. Third, and from a somewhat more controversial perspective, universality might seem tantamount to triviality . If the brain is everywhere small-world, and so are almost all other complex systems in real life ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Gaiteri and others 2014][2014SibilleE_GaiteriC]; [Moslonka-Lefebvre and others 2011][2011PautassoM_Moslonka-LefebvreM]; [Sizemore and others 2016][2016BassettDS_SizemoreA]) (for a few exceptions, see [Koschutzki and others 2010][2010SchreiberF_KoschutzkiD]), then what is the small-worldness of the brain telling us that\u2019s of any interest specifically to neuroscience? There are two main answers to this important question, as we discuss in more detail below: (1) studies have recently succeeded in linking network topological metrics to biological concepts, like wiring cost ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bassett and others 2011a][2011GraftonST_BassettDS]; [Bullmore and Sporns 2012][2012SpornsO_BullomoreET]; [Rubinov and others 2015][2015BullmoreET_RubinovM]), and to biological phenotypes, like neuronal density ([A\u0107imovi\u0107 and others 2015][2015LinneML_A\u0107imovi\u0107J]; [van den Heuvel and others 2015][2015deReusMA_VanDenHeuvel]) or gene expression ([Fulcher and Fornito 2016][2016FornitoA_FulcherBD]) and (2) small-worldness is not the whole story of brain network organization ([Wang and Kennedy 2016][2016KennedyH_WangXJ]). Economical Small-World Networks \u00b6 At the risk of stating the obvious, small-worldness is a purely topological quantity that tells us nothing about the physical layout of the nodes or edges that constitute the graph ([Bassett and others 2010][2010BassettDS]; [Pessoa 2014][2014PessoaL]). However, it is equally obvious that brain networks are embedded in anatomical space ([Bassett and others 2011a][2011BassettDS]; [Klimm and others 2014][2014KlimmF]; [Lohse and others 2014][2014LohseC]). Somehow the abstract, dimensionless topology of small-worldness must be reconciled to the anatomy of the brain. It turns out that the small-world topology of brain networks is (almost) always economically embedded in physical space ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Kaiser and Hilgetag 2006][2006HilgetagCC_KaiserM]). For both clustering and path length, the two topological metrics combined in the hybrid small-world estimator, there is a strong relationship with brain anatomical space ([Bassett and others 2010][2010BassettDS]; [Bassett and others 2011a][2011BassettDS]; [Rubinov and others 2015][2015RubinovM]). The edges between clustered nodes tend to be shorter distance whereas the edges that mediate topological short cuts tend to traverse longer anatomical distances. Interpreting the Euclidean distance between brain regional nodes or neurons as a proxy for the wiring cost, that is, the total biological cost of building a physical connection and maintaining communication between nodes, it has been argued that the brain is an economical small-world network ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Latora and Marchiori 2001][2001MarchioriM_LatoraV]). Economical in this sense does not simply mean parsimonious or cheap; it is more closely related to the common-sense notion of \u201cvalue for money\u201d. Topologically clustered nodes are anatomically co-located and thereby nearly minimize wiring cost. But small-world brain networks are not naturally lattices and if they are computationally rewired strictly to minimize wiring cost then brain networks are topologically penalized, losing integrative capacity indexed by increased characteristic path length and thus reduced small-worldness scalar \\sigma The economical idea is that brain networks have been selected by the competition between a pressure to minimize biological cost versus a pressure to maximize topological integration. More formally, P_{i,j} \\sim f(d_{i,j})f(k_{i,j}) \\tag{1} , the probability of a connection between nodes i and j , P_{i,j} , is a product of: a function of the physical distance in mm between nodes d_{i,j} \u2014often used as a proxy for wiring cost \u2014and a function of the topological relationship between nodes k_{i,j} . Typically, the functions of cost and topology are each parameterized by a single parameter, for example, simple exponential and power law functions. Several variants of this approach have been published, exploring a range of different topological relationships k_{i,j} between nodes, for example, clustering and homophily ( Betzel and others 2016a ; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). Economical models can generally reproduce the small world properties of brain networks quite realistically: clustering and path length are both increased as a function of the cost parameter (Avena-Koenigsberger and others 2014). In other words, as the cost penalty becomes the dominant factor predicting the probability of a connection between nodes, economical models generate increasingly lattice-like networks, with strong spatial and topological clustering of connected nodes, approximating in the limit the minimal cost configuration of the network. The emergence of more integrative network features\u2014such as hubs mediating many intermodular connections\u2014typically depends on some degree of relaxation of the cost penalty (reduced distance parameter) relative to the parameter controlling the importance of (integrative) topological relationships between nodes in predicting their connectivity. Thus, small-world networks can be generated by economical models for a certain range of the two parameters controlling the competitive factors of (wiring) cost and (topological) value . Small-Worldness Is Not the Whole Story \u00b6 Before getting further into the details of small-worldness, as we do below in relation to recent tract-tracing results, it is important to acknowledge that the specific metrics of path length \\Lambda and clustering \\Gamma introduced by Watts and Strogatz (1998) , and the small-worldness scalar derived from them \\sigma = \\frac{\\Gamma}{\\Lambda} ( Humphries and others 2006 ), are a few global topological metrics that have been of central importance to the growth of complex network science generally. But more than 15 years after the first discovery of small-world properties in brain networks, the field of connectomics now extends into many other areas of topological analysis. There is much important recent work on topological properties like degree distribution and hubness ( Achard and others 2006 ), modularity ([Bassett and others 2011b][2011GraftonST_BassettDS]; [Chen and others 2008][2008EvansAC_ChenZJ]; Mattar and others 2015 ; [Meunier and others 2009][2009BullmoreE_MeunierD]; [Simon 1962][1962SimonH]; Sporns and Betzel 2016 ; [Stoop and others 2013][2013StoopR_StoopR]), core/periphery organization ([Bassett and others 2013][2013GraftonST_BassettDS]; Senden and others 2014 ; [van den Heuvel and Sporns 2011][2011SpornsO_VanDenHeuvelMP]), controllability ([Betzel and others 2016b][2016BassettDS_BetzelRF]; [Gu and others 2015][2015KahnAE_GuS]; Muldoon and others 2016b ) and navigability ( Guly\u00e1s and others 2015 ) that are not simply related to small-worldness. It is nothing like a complete description of the brain to say it is small world; we now turn to a more technical discussion of the evidence for small-worldness as a common property of nervous systems. 3. Challenges to Small-Worldness \u00b6 About 3 to 4 years ago, an important series of papers began to be published that could be regarded as \u201cblack swans\u201d refuting the general importance of small-worldness in an understanding of brain networks ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]; Markov and others 2013 ; [Markov and others 2014][2014SalletJ_MarkovNT]; [Song and others 2014][2014WangXJ_SongHF]): Previous studies of low density inter-areal graphs and apparent small-world properties are challenged by data that reveal high-density cortical graphs in which economy of connections is achieved by weight heterogeneity and distance-weight correlations. ( Markov and others 2013 ) Recent connectomic tract tracing reveals that, contrary to what was previously thought, the cortical inter-areal network has high density . This finding leads to a necessary revision of the relevance of some of the graph theoretical notions, such as the small world property . . ., that have been claimed to characterise the inter-areal cortical network. ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]) These remarks carried weight because they were based on sophisticated and highly sensitive measurements of mammalian cortical connectivity ( Fig. 4 ). In each one of multiple carefully standardized experiments in the macaque monkey, a fluorescent tracer was injected into a (target) cortical region where it was taken up by synaptic terminals and actively transported to the cell bodies of neurons projecting to the target region. When the animal\u2019s brain was subsequently examined microscopically, the retrograde transport of the tracer from the injection site resulted in a fluorescent signal in the (source) regions of cortex that were directly connected to the target region. The basic technology of anatomical tract-tracing had been used by neuroanatomists since the late 20th century; but in the first decades of the 21st century it was possible to increase the scale and precision of the measurements dramatically, enabling the construction of connectivity matrices that summarized the strength or weight of axonal projections between a large number of cortical areas. These next-generation tract-tracing data thus represented a new standard of knowledge about mammalian cortical connectivity, that was more continuously quantified than the binary or ordinal rating of connectivity from traditional tract-tracing experiments ([Stephan and others 2001][2001K\u00f6tterR_StephanKE]), and much less ambiguously related to the cellular substrates of brain networks than the statistical measures of functional connectivity ([Achard and others 2006][2006BullmoreET_AchardS]; [Zhang and others 2016][2016BassettDS_ZhangZ]) and structural covariance ([Alexander-Bloch and others 2013][2013BullmoreET_Alexander-BlochA]; [Bassett and others 2008][2008Meyer-LindenbergA_BassettDS]) used to build graphs from human neuroimaging data. It is clearly important to understand in some detail how the topology of brain networks can be modelled in contemporary tract-tracing data from the macaque (and subsequently the mouse ([Oh and others 2014][2014MortrudMT_OhSW]; [Rubinov and others 2015][2015BullmoreET_RubinovM])) and what these results tell us about the small-worldness of brain networks. Figure 4. High density of the macaque cortical graph excludes sparse small world architecture. (A) Comparison of the average shortest path length and density of the macaque cortical graph from (Markov and others 2013) with the graphs of previous studies (Felleman and Van Essen 1991; Honey and others 2007; Jouve and others 1998; Markov and others 2012; Modha and Singh 2010; Young 1993). Sequential removal of weak connections causes an increase in the path length. Black triangle: macaque cortical graph from Markov and others (2013); gray area: 95% confidence interval following random removal of connections from the macaque cortical graph from Markov and others (2013). Jouve et al., 1998 predicted indicates values of the graph inferred using the published algorithm. (B) Effect of density on Watts and Strogatz\u2019s formalization of a small-world network. Clustering and path length variations generated by edge rewiring with probability range indicated on the x-axis applied to regular lattices of increasingly higher densities. The pie charts show graph density encoded via colors for path length ( L and clustering coefficient (C). The y-axis indicates the path length ratio ( \\frac{L_p}{L_o} ) and clustering ratio ( \\frac{C_p}{C_o} ) of the randomly rewired network, where L_o and C_o are the path length and clustering of the regular lattice, respectively. The variables L_p and C_p are the same quantities measured for the network rewired with probability P . Hence, for each density value indicated in the L and C pie charts, the corresponding \\frac{L_p}{L_o} and \\frac{C_p}{C_o} curves can be identified. Three diagrams below the x-axis indicate the lattice (left), sparsely rewired (middle), and the randomized (right) networks. (C) The small-world coefficient \u03c3 (Humphries and others 2006) corresponding to each lattice rewiring. Color code is the same as in panel (B). Dashed lines in (B) and (C) indicate 42% and 48% density levels, respectively. Reproduced with permission from Markov and others (2013). Binary Graphs \u00b6 In general, a node represents a component of a system and an edge represents a connection or interaction between two nodes. Mathematically, we can capture these ideas with a graph G = (V,E) composed of a node set V and an edge set E ( Bollob\u00e1s 1979 , 1985 ). We store this information in an association or weight matrix W , whose ij th element indicates the strength or weight w_{i,j} of the edge between node i and node j A simple way of building a graph from such an association matrix is to apply a threshold \\tau to each element of the matrix, such that if w_{i,j} \u2265 \\tau then an edge is drawn between the corresponding nodes, but if w_{i,j} < \\tau no edge is drawn ([Achard and others 2006][2006BullmoreET_AchardS]). This thresholding operation thus binarizes the weight matrix and converts the continuously variable edge weights to either 1 (suprathreshold) or 0 (subthreshold). It was on this basis that almost all brain graphs were constructed in the 15 years or so following the seminal small-world analysis of a binary graph representing the cellular connectome of C. elegans ( Watts and Strogatz 1998 ). Most of the neuroimaging evidence for small-worldness in human brain networks, for example, is based on analysis of binary graphs constructed by thresholding a correlation coefficient or equivalent estimator of the weight of functional or structural connectivity or structural covariance between regions i and j ( van Wijk and others 2010 ). It is well recognized that construction of binary graphs represents an extreme simplification of brain networks; indeed, a binary undirected graph of homogenous nodes is as simple as it gets in graph theory ([Bassett and others 2012a][2012LimKO_BassettDS]). However, this approach has historically been preferred in neuroimaging because of limited signal-to-noise ratio in the data ([Achard and others 2006][2006BullmoreET_AchardS]). By varying the threshold \\tau used to construct a binary graph from a continuous weight matrix, the connection density of the network is made denser or sparser. If the threshold is low and many weak weights are added to the graph as edges then the connection density will increase; if the threshold is high and only the strongest weights are represented as edges, then the connection density will decrease. The connection density D is quantified by the number of edges E in the graph as a proportion of the total number of edges in a fully connected network of the same number of nodes N : D=\\frac{E}{\\frac{N^2\u2212N}{2}} Often, this proportion is translated into a percentage. In many neuroimaging studies, the threshold is set to a large value to control for the high levels of noise in MRI data, resulting in connection densities in the range 5% to 30% ( Lynall and others 2010 ). In many of the first generation tract tracing studies, the connectivity data were collected on a binary or ordinal scale, and not all possible connections had been experimentally measured, so these data were naturally modelled as binary graphs with connection densities ~30% a value that was constrained by the completeness and quality of the data ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). The small-world topology of a binary brain graph is defined by estimating two parameters in the data, path length L and clustering C ( Fig. 2A ), and comparing each of these observed parameters to their distributions under a specified null model ( Humphries and others 2006 ). More specifically, L = \\frac{1}{N}\\sum l_{i,j} is the global or characteristic path length, where l_{i,j} is the shortest path (geodesic) between nodes i] and j ; and C = \\frac{1}{N}\\sum c_{i,j} ( c_i := \\frac{L_i}{\\frac{k_i(k_i-1)}{2}} ) is the global clustering coefficient , where c_{i,j} is the number of closed triangular motifs including node i . Each of these parameters is normalized by its value in a binary graph representing the null hypothesis. For example, if the null hypothesis is that clustering of brain networks C_{\\text{brain}} is no different from the clustering of a random graph, then it is reasonable to generate an Erd\u00f6s\u2013Reny\u00ed graph for N nodes and D connection density, measure the clustering coefficient in the random graph C_{\\text{random}}] , and use the ratio between brain and random graph clustering coefficients as a test statistic for non-random clustering. We note that there are many other possible ways in which a null model could be sampled, besides using the classical Erd\u00f6s\u2013Reny\u00ed model, and this is an active area of methodological research ( Muldoon and others 2016a ). However, in general one can define the normalized clustering coefficient as \\Gamma = \\frac{C_{\\text{brain}}}{C_{\\text{random}}} . Likewise, the path length of the brain graph can be normalized by its value in a comparable random graph \\Lambda = \\frac{L_{\\text{brain}}}{L_{\\text{random}}} . A small-worldness scalar can then be simply defined as \\sigma = \\frac{\\Gamma}{\\Lambda} . With these definitions, small-world networks will have \\sigma > 1 , \\Gamma > 1 and \\Lambda \\sim 1 ( Humphries and others 2006 ). Weighted Graphs \u00b6 Although binary graph analysis has predominated to date in analysis of brain networks, this certainly does not represent the methodological limit of graph theory for connectomics. For example, provided the data are of sufficient quality, there is no need to threshold the weight matrix to estimate topological properties like clustering, path length, and small-worldness. Indeed, while the binarization procedure was common in early applications of graph theory to neural data ( van Wijk and others 2010 ), it remains fundamentally agnostic to architectural principles that may be encoded in edge weights ( Rubinov and Sporns 2011 ). This realization has more generally motivated the field to develop methods that remain sensitive to the patterns of weights on the edges ([Ginestet and others 2011][2011SimmonsA_GinestetCE]), and to the topologies present in weak versus strong weights ( Rubinov and Sporns 2011 ). These efforts have included the development of alternative thresholding schemes ([Bassett and others 2012a][2012LimKO_BassettDS]; [Lohse and others 2014][2014CarlsonJM_LohseC]) and fully weighted graph analysis ([Bassett and others 2011b][2011GraftonST_BassettDS]; Rubinov and Sporns 2011 ). The mathematical tools exist to estimate and simulate the topological properties of weighted networks, and analysis of weighted networks is akin to studying the geometry of the graph, rather than simply its topology ([Bassett and others 2012b][2012PorterMA_BassettDS]; [Bassett and others 2013][2013GraftonST_BassettDS]). For example, weighted analogues of binary metrics of clustering, path length and small-worldness can be defined formally ( Fig. 2B ). First, the weighted clustering coefficient of node i can be defined as C_{\\text{weighted}} = \\frac{1}{k_i(k_i\u22121)} \\sum_{j,k}(\\hat{w}_{ij} \\hat{w}_{jk} \\hat{w}_{ik})^{\\frac{1}{3}} , where k_i is the number of edges connected to node i , or degree of node i ([Onnela and others 2005][2005KaskiK_OnnelaJP]) (but see also [Barrat and others 2004][2004VespignaniA_BarratA]; [Zhang and Horvath 2005][2005HorvathS_ZhangB] for other similar definitions). The weighted path length can be defined as L_{\\text{weighted}} = \\frac{1}{N(N\u22121)} \\sum_{i \\neq j} \\delta_{ij} , where the topological distance between two nodes is given by \\delta_{ij} = \\frac{1}{w_{ij}} ([Newman 2001][2001NewmanMEJ]). These two statistics can be combined to construct a weighted metric of small-worldness ([Bola\u00f1os and others 2013][2013AviyenteS_Bola\u00f1osM]): \\sigma_{\\text{weighted}} = \\frac{\\Gamma_{\\text{weighted}}}{\\Lambda_{\\text{weighted}}} . With these definitions, small-world networks will have \\Gamma_{\\text{weighted}} > 1 , \\Lambda_{\\text{weighted}} \\sim 1 , and \\sigma_{\\text{weighted}} > 1 ( Humphries and others 2006 ). The Small-World Propensity \u00b6 There are several important limitations to the definitions of small-worldness described in the previous sections. First, the small-world scalar \\sigma (whether binary or weighted) can be greater than 1 even in cases when the normalized path length is much greater than one; because it is defined as a ratio, if \\gamma \\gg 1 and \\lambda > 1 , the scalar \\sigma > 1 This means that a small-world network will always have \\sigma > 1 , but not all networks with \\sigma > 1 will be small-world (some of them may have greater path length than random graphs). Second, the measure is strongly driven by the density of the graph, and denser networks will naturally have smaller values of \\sigma even if they are in fact generated from an identical small-world model. To address these and other limitations, Muldoon and colleagues recently developed a metric called the small-world propensity . Specifically, the small-world propensity, \\phi , reflects the deviation of a network\u2019s clustering coefficient, C_{\\text{brain}}] , and characteristic path length, L_{\\text{brain}} , from both lattice ( C_{\\text{lattice}} , L_{\\text{lattice}} ) and random ( C_{\\text{random}} , L_{\\text{random}} ) networks constructed with the same number of nodes and the same degree distribution: \\phi = 1 \u2212 \\sqrt{\\frac{\\Delta^2_C+\\Delta^2_L}{2}} , where \\Delta C = \\frac{C_{\\text{lattice}}\u2212C_{\\text{brain}}}{C_{\\text{lattice}}\u2212C_{\\text{random}}} and \\Delta L = \\frac{L_{\\text{brain}}\u2212L_{\\text{random}}}{L_{\\text{lattice}}\u2212L_{\\text{random}}} . The ratio \\Delta_{C/L} represents the fractional deviation of the metric C_{\\text{brain}} or L_{\\text{brain}} ) from its respective null model (a lattice or random network). This quantity can be calculated for binary networks (using binary definitions of clustering and path length) or for weighted networks (using weighted definitions of clustering and path length). Networks are considered small-world if they have small-world propensity 0.4 < \\phi \u2264 1 . However, this metric should be viewed as a continuous metric of small-worldness rather than a hard threshold ( Muldoon and others 2016a ). Importantly, the small-world propensity overcomes several limitations of previous scalar definitions of small-worldness ( Muldoon and others 2016a ). First, it can incorporate weighted estimates of both the clustering coefficient and path-length, thus being generally applicable to any neural data that can be represented as a weighted network. Second, it is density independent, meaning that it can be used to compare the relative small-worldness between two networks that have very different densities from one another. Third, the metric is informed by spatially-constrained null models ([Bassett and others 2015][2015DanielsKE_BassettDS]; [Expert and others 2011][2011LambiotteR_ExpertP]; [Papadopoulus and others 2016][2016BassettDS_Papadopoulus]) in which nodes have physical locations and the edges that correspond to the smallest Euclidean distance between nodes are assigned the highest weights ([Barth\u00e9lemy 2011][2011Barth\u00e9lemyM]) ( Fig. 5 ). Figure 5. Small-world propensity in weighted networks. Here, we illustrate an example of a generative small-world model, and its utility in estimating an empirical network\u2019s small-world propensity. (A) We can extend the concept of a Watts\u2013Strogatz model to weighted graphs by first building a lattice in which the edges are weighted by distance such that edges between spatially neighboring nodes have more strongly weighted than edges between spatially distant nodes. These edge weights can then be rewired with a probability, P, to create a weighted small-world network. (B) Weighted clustering coefficient and weighted path length can be estimated as a function of the rewiring parameter, P, and used to derive the small-world propensity of the graph compared with random and lattice benchmarks (Eq. 11). (C) Weighted small-world propensity calculated for the same network as in panel (B). Error bars represent the standard error of the mean calculated over 50 simulations, and the shaded regions represent the range denoted as small-world. (D) Weighted small-world propensity as a function of network density for a graph of 1000 nodes. Reproduced with permission from Muldoon and others (2016a). 4. Twenty-First Century Tract-Tracing \u00b6 The scale and quality of contemporary tract-tracing data, in both the macaque and the mouse, represents a step change in terms of sensitivity in detecting anatomical connections, or axonal projections, between cortical areas. Using retrograde tracer experiments it has proven possible to demonstrate reliably that pairs of regions in the macaque brain may be connected by one or a few axonal projections. Likewise, anterograde tracer experiments in the mouse have demonstrated that the minimal detectable weight of connectivity between cortical regions, that just exceeds the noise threshold, is equivalent to the projection of one or a few axons (Ypma and Bullmore 2016). This high sensitivity has led immediately to the recognition of a large number of weak and previously unreported axonal connections. In the macaque, it was estimated that 36% of connections identified by contemporary tract tracing were so-called new found projections (NFPs) that had not been described in the prior literature (Markov and others 2014). The existence of so many weak connections is reflected in the log normal distributions of connectivity weight, ranging over five to six orders of magnitude, in both the macaque and the mouse (Ercsey-Ravasz and others 2013; Oh and others 2014). In short, tract-tracing can now resolve connections approximately equivalent to a single axonal projection and approximately a million times weaker than the strongest anatomical connections or white matter tracts. How can we use graph theory to model the network organization of such highly sensitive, highly variable data? Perhaps the simplest approach, borrowing from prior studies of less high quality datasets, is to apply a threshold and convert the log-normal weight matrix into a binary adjacency matrix. If the threshold is defined by the noise distribution of the measurements then it will be very close to zero for these sophisticated experiments, and correspondingly the connection density of the binary graph will be high. In the macaque, the connection density of a binary graph of 29 visual cortical areas was estimated to be 66% (Markov and others 2013), considerably higher than historical estimates in the range of 25% to 45% (Fellemen and Van Essen 1991). In the mouse, the connection density of a binary graph of 308 areas of the whole cortex was estimated to be 53% (Rubinov and others 2015). In other words, the binary graphs generated from 21st century tract-tracing data are about twice as dense as the much sparser networks derived from human neuroimaging and 20th century tract-tracing. They are also considerably denser than brain networks constructed at a finer grained (ultimately cellular) resolution. For example, the connection density of the C elegans nervous system, which is still the only completely mapped synaptic connectome, is about 12%. It is easy to see that the connection density of a binary network depends on the number of neurons comprising each node. In the limit, if the nervous system is parcellated into two large nodes the connection density will certainly be 100%; as the same system is parcellated into a larger number of smaller nodes its connection density will monotonically decrease (Bassett and others 2011a; Zalesky and others 2010). Thus, the current interval estimate of mammalian cortical connection density ~55% to 65% is conditional both on the anatomical resolution of the parcellation scheme used to define the nodes and the sensitivity of the tract-tracing methods used to estimate the weights of the edges. Small-Worldness of Binary Tract-Tracing Networks \u00b6 Having constructed a high-density binary graph from tract-tracing data on mammalian cortex, it is straightforward to estimate its clustering and path length, using the same metrics as for sparser binary graphs. However, simply because there is a larger number of connections in the denser network, its clustering will be considerably higher (there will be more closed triangular motifs) and its path length will be shorter (there will be more direct, pairwise connections) than a sparser network. Indeed, the clustering and path length of any binary graph at 60% connection density will be close to the maximal clustering and minimal path length of a fully connected graph; and therefore the clustering and path length of a 60% dense brain network will be very similar to the clustering and path length of a 60% random network (Bassett and others 2009). This means that when clustering and path length in brain networks are normalized by their corresponding values in equally dense random networks, the scaled metrics \u0393 and \u039b will both be close to 1, and the small-world scalar \u03c3 will be close to its critical value of 1 (Markov and others 2013). For the macaque, at 66% connection density, \u0393=1.21\u00b10.014 , \u039b=1.00\u00b10.000 , and \u03c3=1.21\u00b10.014 ; for the mouse, at 53% connection density, \u0393=1.31\u00b10.004 , \u039b=1.00\u00b10.000 , and \u03c3=1.31\u00b10.004 (all given in mean \u00b1 standard deviation; Fig. 6A and C; Table 1). Since small-worldness has been traditionally defined as \u03c3>1 , these results suggest that dense binary graphs constructed from tract tracing data are small-world, although the macaque is more similar to a random network than the mouse. Figure 6. Binary and weighted small-worldness in mouse and macaque connectomes. For the macaque connectome reported in Markov and others (2013), we show (A) the binary network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (B) we show a weighted network analysis for the same data. For the mouse connectome reported in Rubinov and others (2015), we show (C) the weighted network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (D), we show a binary network analysis for the same data. In the boxplots, the gray dotted line shows the threshold value of \u03c3 = 1, and the purple area shows the range of values of 0.4 < \u03c6 \u2264 1 in which a network is considered small-world. These results do not look like a \u201cblack swan\u201d that refutes universal claims that the brain always embodies small-world network topology. Nor do they undermine the credibility of previous studies demonstrating small-worldness in sparser brain graphs. However, our view is that binary graph models are very unlikely to be an optimal strategy for network analysis of tract-tracing data, because they fail to take account of the extraordinary range of connectivity weights, distributed log normally over 6 orders of magnitude, that has been discovered in mammalian cortical networks (Ercsey-Ravasz and others 2013). The weakest connection between cortical areas is about a million times less weighted than the strongest connection: does it really make sense to set all these weights equivalently to 1 as edges in a binary graph? To ask the question is to answer it. Small-Worldness of Weighted Tract-Tracing Networks \u00b6 A weighted small-world analysis is easily done for these data (Fig. 6B and D). The weighted clustering and weighted path length metrics (Eqs. 8 and 9) are estimated directly from the weight matrices, and the ratio of weighted clustering to weighted path length is the scalar summary of weighted small-worldness \u03c3_{\\text{weighted}}>1 . In Figure 6, we directly compare binary and weighted graph theoretical results for the mouse (Oh and others 2014; Rubinov and others 2015) and macaque (Markov and others 2013) connectomes. Compared with the results of binary graph analysis, both mouse and macaque networks have increased clustering for the weighted graph analysis, and \u03c3 is increased for the macaque (see Table 1). The weighted graph of the mouse connectome is similarly small-world compared to the weighted macaque graph, as measured by \u03c3 , but is significantly more small-world as measured by the small-world propensity \u03d5 . However, classical estimates of small-worldness may depend in a non-trivial way on the density of the graph. This relationship becomes obvious if we estimate the topology of both weighted graphs as a function of connection density (Fig. 7). The classical small-world scalar \u03c3 is greatest when it is estimated for a sparse graph comprising less than 20% to 30% of the most strongly connected edges, and decreases progressively as the graph becomes denser. This might suggest that the macaque connectome seems less small-world than the mouse simply because it is denser. However, the small world propensity \u03d5 has the useful property that it is independent of network density and it is significantly greater, indicating more small-worldness, for the mouse than the macaque. This could be related to differences between the datasets in number of cortical areas and completeness of cortical coverage: the macaque dataset comprises fewer nodes of mostly visual cortex than the larger number of nodes across the whole mouse cortex. Figure 7. Dependence of small-world characteristics on network density. (A) Macaque and (B) mouse connectivity matrices in their natural state (left), as well as after thresholding to retain the 5% strongest (middle) or 25% strongest (right) connections. Weighted small-world metrics including the normalized clustering coefficient (\u0393), normalized path length (\u039b), small-world index (\u03c3), and small-world propensity (\u03c6) as a function of network density for the (C) macaque and (D) mouse connectivity matrices. Weighted Small-Worldness and the Role of Edge Weights \u00b6 Why does a weighted graph analysis provide stronger evidence for non-random clustering than a binary graph analysis applied to the same tract-tracing data? The most strongly weighted connections generally span the shortest physical distances between cortical areas (Ercsey-Ravasz and others 2013; Klimm and others 2014; Rubinov and others 2015). This is not surprising based on what we know about the importance of cost constraints on brain organization (Bassett and others 2009; Bassett and others 2010; Bullmore and Sporns 2012; Fornito and others 2011). Strong connectivity weights indicate a large number of axonal projections, a big bandwidth bundle, perhaps macroscopically visible as a white matter tract. Building and resourcing a high-bandwidth axonal signaling bundle is a significant biological cost that will increase as a function of connection distance: it is parsimonious to wire high bandwidth over short distances. Short distance connections are not only strongly weighted but also topologically clustered. So the strongest weights in both cortical networks define a topologically segregated and anatomically localized organization. A map of the sub-network formed by the strongest weights shows spatial and topological clusters of regions (Fig. 8). In the mouse, the strongly weighted clusters each comprise functionally specialized areas of cortex (visual, motor, etc.) that are known to be densely interconnected and anatomically localized (Rubinov and others 2015; Ypma and Bullmore 2016). Thus it is not surprising that weighting the topological analysis of mammalian cortical networks will provide stronger evidence for non-random clustering than unweighted analysis of binary graphs. Figure 8. The existence of weak links and their topology in the mouse connectome. Here, we show the properties of the 5% weakest and 5% strongest edges of the mouse cortical network. (A, B) Axial view of the mouse cortical network, red dots represent brain regions, blue lines represent the connections between them. Drawn are the (A) 5% weakest or (B) 5% strongest edges. Dot size corresponds to degree, the total number of incoming and outgoing edges connected to a node. In (B), the three nodes with highest degree have been labeled as follows: VISp, primary visual area; MOp, primary motor area; SSs, supplemental somatosensory area. The strong connections are spatially organized, mainly connecting spatially adjacent or contralaterally homologous regions. The weak connections span longer distances and are topologically more random than the strongest connections. (C) The distance distributions for (blue) the 5% weakest edges, (red) the 5% strongest edges, and (black) a random graph of the same size and connection density. (D) The degree distributions for the weakest and strongest connections of the mouse connectome, and a comparable random graph, color-coded as in panel (C). Reproduced with permission from Ypma and Bullmore (2016). The most weakly weighted connections are an area of active, ongoing research (discussed in more detail below) and it is inevitable that there is still much to learn about a feature of network organization\u2014replicable but very weak connections between large cortical areas\u2014that had not been measurable until recent advances in tract-tracing methodology. However, it is clear that weaker connections tend to subtend longer distances, and can be either more topologically random (Ypma and Bullmore 2016) than or similarly topologically organized to strong connections (Bassett and others 2012a). We conclude that graph theoretical analysis of tract-tracing connectomes should respect the quality of the data and use weighted topological metrics to reflect the wide ranging variation in anatomical connectivity, from single fibers to major tracts, that is now measurable in the mammalian brain (Wang and Kennedy 2016). Weighted graph analysis demonstrates clearly that both the macaque and mouse connectomes are small-world networks, as are the human, cat, and nematode brains (Muldoon and others 2016a). Binary graph analysis has usefully measured high connection density, due to the existence of many new anatomical connections, but binarization of these data is not the best way to understand their complex topology and its economical embedding in anatomical space (Bassett and others 2011a; Bassett and others 2012a; Klimm and others 2014; Rubinov and others 2015; Rubinov and Sporns 2011). Future studies will likely also pay more attention to the fact that most tract-tracing markers are axonally transported only in one direction: anterograde or retrograde. This means that the weight matrix could be modelled more completely as a weighted and directed graph, representing a further evolution in the use of graph theoretical methods to capture a richer and biologically more meaningful model of brain network organization than can be provided by binary graphs of unweighted and undirected edges. 5. The Utility of Weak Connections \u00b6 At this juncture, one might naturally ask, \u201cFrom a neuroscientific perspective, do we need techniques that account for edge weights? Do these weights indeed capture information of relevance for cognition and behavior?\u201d Neuroanatomical data suggest that the weights of structural connections may be driven by developmental growth rules (Ercsey-Ravasz and others 2013; Kaiser and Hilgetag 2006; Klimm and others 2014; Lohse and others 2014; Markov and others 2013), energetic and metabolic constraints (Bassett and others 2010), and physical limitations on the volume of neural systems, particularly brains encapsulated by bone (Sherbondy and others 2009). Yet the role of these edge weights in neural computations (Schneidman and others 2006) and higher order cognition has been less well studied. Recent studies have begun to elucidate the role of edge weights\u2014and particularly of weak connections\u2014in human cognition. In resting-state fMRI data, weak functional connections from lateral prefrontal cortex to regions within and outside the frontoparietal network have been shown to display individual differences in strength that predict individual differences in fluid intelligence (Cole and others 2012). The same general relationship was observed in a separate study in which individual differences in moderately weak, long-distance functional connections at rest were strongly correlated with full scale, verbal, and performance IQ (Santarnecchi and others 2014). Neither of these correlations were observed when considering strong connections. Indeed, the utility of weak edges appears to extend to psychiatric illness, where the highly organized topology of weak functional connections\u2014but not strong functional connections\u2014in resting-state fMRI were able to classify people with schizophrenia from healthy controls with high accuracy and specificity (Bassett and others 2012a). Interestingly, individual differences in these weak connections were significantly correlated with individual differences in cognitive scores and symptomatology. Together these results demonstrate that, indeed, methods that are sensitive to the strength (or weakness) of individual connections are imperative for progress to be made in understanding individual differences in cognitive abilities, and their alteration in psychiatric disease. Importantly, the utility of weak connections is not only evident at the large scale in human brains but also at the neuronal scale as measured in non-human species. In an influential article published in 2006 with Bialek and colleagues, Schneidman demonstrated that weak pairwise correlations implied strongly correlated network states in a neural population, suggesting the presence of strong collective behavior (Schneidman and others 2006). This result was initially counterintuitive as one might expect that weak correlations would be associated with the lack of collective behavior. However, the original observation has withstood the test of time, and has been validated in several additional studies including work at the level of tract tracing in macaque monkeys (Goulas and others 2015). Intuitively, the juxtaposition of weak correlations and cohesive, collective behavior is thought to be driven by the underlying sparsity of neuronal interactions (Ganmor and others 2011b), which contain a few non-trivial higher-order interaction terms (Ganmor and others 2011a). Indeed, these higher-order interactions are the topic of some interest both from a computational neuroscience perspective (Giusti and others 2016; Sizemore and others 2016), and from the perspective of neural coding (Giusti and others 2015). But perhaps the claim that weak connections are critically important for our understanding of neural systems should not be particularly surprising. Indeed, it is in fact an old story, first published at the inception of network science. In 1973, Granovetter wrote a seminal paper, titled \u201cThe strength of weak ties,\u201d which highlighted the critical importance of weakly connected components in global system dynamics (Granovetter 1973). Such weak connections are ubiquitous in many systems, from physician interactions (Bridewell and Das 2011) to ecosystem webs (Ulanowicz and others 2014) and atmospheric pathways (Lee and Su 2014). Looking forward, critical open questions lie in how these weak connections drive global dynamics, and how one can intervene in a system to manipulate those processes (Betzel and others 2016b; Gu and others 2015; Muldoon and others 2016b). Acknowledging the role of weak connections, weighted small-world organization plays a critical role in system functions that are particularly relevant to neural systems: including coherence, computation, and control and robustness (Novkovic and others 2016). Perhaps the most commonly studied function afforded by small-world architecture is the ability to transmit information, a characteristic that is common in networks of coupled oscillators (Barahona and Pecora 2002; Hong and others 2002; Nishikawa and others 2003) (although see Atay and others 2006, for a few notable exceptions). This capability supports enhanced computational power (Lago-Fern\u00e1ndez and others 2000), via swift flow and transport (Hwang and others 2010). In dynamic networks, oscillators coupled on small-world networks are much more sensitive to link changes than their random network counterparts (Kohar and others 2014), the time taken to reach synchronization is lowered, and the synchronized state is less stable over time, potentially enabling greater diversity of function. When such a system has both small-world topology and geometry, it directly impacts the network\u2019s ability to speed or slow spreading (Karsai and others 2011), a potentially useful characteristic for resilience to dementia which is thought to be caused by the spread of prions (Raj and others 2012; Raj and others 2015). The value of small-world architecture is not limited to its support of synchronization and information flow. Instead, it also supports a wide range of computations in neural circuits. From early neural network studies, it is clear that the exact topology of connectivity patterns between network elements directly supports trade-offs in the network\u2019s ability to learn new information versus retain old information in memory (Hermundstad and others 2011). When these patterns are organized in a small-world manner, evidence suggests that local computations can be integrated across distributed cell assemblies to support functions as diverse as somatosensation (Zippo and others 2013) and olfaction (Imam and others 2012). The mechanism by which small-worlds support these computations may stem from the fact that their topological structure tends to contain both large cavities and high-dimensional cliques (Sizemore and others 2016), which when embedded in a physical space can strongly constrain the geometric properties of the computation (Giusti and others 2015). While small-world structure can offer non-trivial advantages in terms of both communication and computation, it also directly informs the sorts of interventions that one could use to guide network dynamics and by extension system function. Indeed, computational studies have demonstrated that small-world network architecture requires specific control strategies if one wishes to stem the propagation of seizure activity (Ching and others 2012), control the spread of viruses (Kleczkowski and others 2012), or enhance recovery following injury (H\u00fcbler and Buchman 2008). To gain an intuition for how topology impacts control, we can consider the broad-scale degree distribution also characteristic of brain networks. Based on the Laplacian spectrum, one can observe that weakly connected nodes have the greatest potential to push the system into distant states, far away on an energy landscape (Pasqualetti and others 2014); conversely, strongly connected hubs have the greatest potential to push the system into many local states, nearby on the energy landscape (Gu and others 2015). Thus, control energy (such as that provided by brain stimulation) may be targeted to different locations in a small-world brain network to affect a specific change in brain dynamics (Muldoon and others 2016b). 6. Conclusions \u00b6 Small-worldness remains an important and viable concept in network neuroscience. Nearly 20 years on from the first analysis of the complex topology of a binary graph representing the nervous system of C. elegans, it has been established that small-worldness is a nearly universal and functionally valuable property of nervous systems economically embedded in anatomical space. Recent advances in tract-tracing connectomics do not refute small-worldness; rather they considerably enrich and deepen our understanding of what it means in the brain. The extraordinary precision of contemporary tract tracing, and the important discovery that mammalian cortical networks are denser than expected, mandates the adoption of more sophisticated techniques for weighted graph theoretical modeling of interareal connectomes. On this basis, we expect the next 10 years to yield further insights into the functional value of weak as well as strong connections in brain networks with weighted small-worldness. \u00b6","title":"190311 Bassett, D. S., Bullmore, E.T. 2017"},{"location":"190311_BassettDS_BullmoreET2017/#contents","text":"Abstract Small Worlds, Watts and Strogatz Small-World Brain Graphs What Have We (Not) Learnt Since 2006? Universality Economical Small-World Networks Small-Worldness Is Not the Whole Story Challenges to Small-Worldness Binary Graphs Weighted Graphs The Small-World Propensity Twenty-First Century Tract-Tracing Small-Worldness of Binary Tract-Tracing Networks Small-Worldness of Weighted Tract-Tracing Networks Weighted Small-Worldness and the Role of Edge Weights The Utility of Weak Connections Conclusions","title":"Contents"},{"location":"190311_BassettDS_BullmoreET2017/#0_abstract","text":"It is nearly 20 years since the concept of a small-world network was first quantitatively defined, by a combination of high clustering and short path length; and about 10 years since this metric of complex network topology began to be widely applied to analysis of neuroimaging and other neuroscience data as part of the rapid growth of the new field of connectomics. Here, we review briefly the foundational concepts of graph theoretical estimation and generation of small-world networks. We take stock of some of the key developments in the field in the past decade and we consider in some detail the implications of recent studies using high-resolution tract-tracing methods to map the anatomical networks of the macaque and the mouse. In doing so, we draw attention to the important methodological distinction between topological analysis of binary or unweighted graphs, which have provided a popular but simple approach to brain network analysis in the past, and the topology of weighted graphs, which retain more biologically relevant information and are more appropriate to the increasingly sophisticated data on brain connectivity emerging from contemporary tract-tracing and other imaging studies. We conclude by highlighting some possible future trends in the further development of weighted small-worldness as part of a deeper and broader understanding of the topology and the functional value of the strong and weak links between areas of mammalian cortex.","title":"0. Abstract"},{"location":"190311_BassettDS_BullmoreET2017/#1_small_worlds_watts_and_strogatz","text":"Small-worldness now seems to be a ubiquitous characteristic of many complex systems; but its first, and still most familiar, appearance was in the form of social networks. We know that as individual agents (nodes) in a social network, we are connected by strong familial and friendship ties (edges) to a relatively few people who are likely also strongly connected to each other, forming a social clique, family or tribe. Yet we also know that we can travel far away from our tribal network, to physically remote cultures and places, and sometimes be surprised there to meet people\u2014often \u201cfriends-of-friends\u201d\u2014who are quite closely connected to our home tribe: \u201cit\u2019s a small world,\u201d we say. This common intuition was experimentally investigated by Milgram (1967) , who asked people in the Midwest of the United States (Omaha, Nebraska) to forward a letter addressed to an unknown individual in Boston by posting it to the friend or acquaintance in their social network that they thought might know someone else who would know the addressee ( Fig. 1 ). It was discovered, on average over multiple trials of this procedure, that the letters successfully reaching Boston had been passed through 6 intermediate postings, which was considered much less than expected given the geographical distance between source and target addresses. In the language of graph theory, the characteristic path length of Milgram\u2019s social networks was short. Figure 1. An illustration of the shortest path between Omaha and Boston in Milgram\u2019s social network experiment, published in Psychology Today in 1967 . Here, the results of multiple experiments are represented as a composite shortest path between the source (a person in Omaha) and the target (a person in Boston). A letter addressed to the target was given to the source, who was asked to send it on (with the same instructions) to the friend or acquaintance that they thought was most likely to know the target, or someone else who might know the target personally. It was found that most letters that eventually reached the correct address in Boston passed through six intermediaries between source and target (denoted 1st remove, 2nd remove, etc.), popularizing the notion that each of us is separated by no more than \u201csix degrees of freedom\u201d from any other individual in a geographically distributed social network. Reproduced with permission from Milgram (1967) . Famously, Watts and Strogatz (1998) combined this concept of path length (the minimum number of edges needed to make a connection between nodes) with a measure of topological clustering or cliquishness of edges between nodes ( Fig. 2 ). More formally, clustering measures the probability that the nodes j and k which are both directly connected to node i are also directly connected to each other; this is equivalent to measuring the proportion of closed triangular three-node motifs in a network ( Sporns and K\u00f6tter 2004 ). Watts and Strogatz (WS) explored the behavior of path length and clustering in a simple generative model (henceforth the WS model ) ( Fig. 3 ). Starting with a binary lattice network of N nodes each connected to the same number of nearest neighbors, by edges of identical weight (unity), the WS model iteratively rewires the lattice by randomly deleting an existing edge, between nodes i and j and replacing it by a new edge between node i and any node k \\neq j . They found that as the probability of random rewiring was incrementally increased from zero, so that the original lattice was progressively randomized, sparsely rewired networks demonstrated both high clustering (like a lattice) and short path length (like a random graph). By analogy to social networks, these algorithmically generated graphs were called small-world networks. Figure 2. Diagrams of clustering and path length in binary and weighted networks. (A) In a binary network, all edges have the same weight, and that is a weight equal to unity. In this example of a binary graph, if one wishes to walk along the shortest path from the orange node to the green node, then one would choose to walk along the edges highlighted in red, rather than along the edges highlighted in blue. We also note that the clustering coefficient of the green node is equal to 1 (all neighbors are also connected to each other to form a closed triangular motif), while the clustering coefficient of the orange node is =1 (only three out of five neighbors are also connected to each other). (B) In a weighted graph, edges can have different weights. In this example, edges have weights of \\frac{3}{3} = 1 , \\frac{2}{3} = 0.66 , and \\frac{1}{3} = 0.33 . If one wishes to traverse the graph from the orange node to the green node along the shortest path, one would choose to follow the path along the edges with weight equal to unity (stronger weights are equivalent to shorter topological distance). Note also that because the edges are now weighted, neither the orange nor the green nodes has a clustering coefficient equal to unity. Figure 3. The Watts\u2013Strogatz model and the generation of small-world networks. The canonical model of a small-world network is that described by Duncan Watts and Steve Strogatz in their 1998 article in Nature. The model begins with a regular lattice network in which each node is placed along the circumference of a circle, and is connected to its k nearest neighbors on that circle. Then, with probability p edges are rewired uniformly at random such that (1) at p = 0 the network is a lattice and (2) at p = 1 the network is random. Interestingly, at intermediate values of p the network has so-called \u201csmall-world\u201d characteristics with significant local clustering (from the lattice model) and short average path length facilitated by the topological short-cuts created during the random rewiring procedure. Because this architecture can be defined mathematically, small-world graphs have proven fundamental in understanding game theory ( Li and Cao 2009 ) and even testing analytical results in subfields of mathematics ( Konishi and Hara 2011 ). Yet, while this work provided a qualitative model of a small-world graph, it did not give a statistic to measure the degree of small-worldness in a particular data set. As a simple scalar measure of \u201csmall-worldness,\u201d Humphries and colleagues defined the small-world index, \u03c3 to be the ratio of the clustering coefficient (normalized by that expected in a random graph) to the average shortest path length (also normalized by that expected in a random graph) ( Humphries and others 2006 ). The intuition here is that this index should be large (in particular, \\sigma > 1 when the clustering coefficient is much greater than expected in the random graph, and the average shortest path length is comparable to that expected in a random graph. Since this initial definition, other extensions have been proposed and utilized ( Telesford and others 2011 ; Toppi and others 2012 ), building on the same general notions. In addition to introducing this generative model, Watts and Strogatz (1998) also showed how small-worldness could be estimated in naturally occurring networks. The hybrid combination of high clustering and short path length that emerged in sparsely rewired WS networks was proposed as a general quantitative measure of small-worldness in other networks. It was shown immediately that a nervous system was among the real-world networks that shared the small-world pattern of topological organization. Using data on the synaptic and gap junction connectivity between all N = 302 neurons in the nervous system of Caenorhabditis elegans ( White and others 1986 ), a binary undirected graph was constructed representing each neuron as an identical node and each synapse (~5000) or gap junction (~600) as an identical, unweighted and undirected edge between nodes. This graph of about 5600 edges between 302 nodes was sparsely connected: only about 12% of the maximum possible number of synaptic connections, \\frac{N^2-N}{2} = 45451 , actually existed. Compared with a random graph of \\frac{N^2-N}{2} = 45451 nodes, C. elegans had high clustering \\Gamma \\sim 5.6 and short path length \\Lambda \\sim 1.18 . Thus the C. elegans connectome was small-world, in the same quantitative sense as the networks generated by the WS model at low rewiring probabilities, less than 10%. But note that does not necessarily mean that the C. elegans connectome was biologically generated by the WS algorithm of random rewiring of established connections (axonal projections) between neurons. To put it another way, the WS model can generate small-world networks but not all small-world networks were generated by a WS model. (And the WS model does not seem like a biologically plausible generative model for brain networks ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]).)","title":"1. Small Worlds, Watts and Strogatz"},{"location":"190311_BassettDS_BullmoreET2017/#small-world_brain_graphs","text":"Following the small-world analysis of C. elegans, pioneering topological studies of mammalian cortical networks used databases of tract-tracing experiments to demonstrate that the cat and macaque interareal anatomical networks shared similar small-world properties of short path length and high clustering ([Hilgetag and Kaiser 2004][2004KaiserM_HilgetagCC]; [Sporns and Zwi 2004][2004ZwiJD_SpornsO]). The first graph theoretical studies of neuroimaging data demonstrated that large-scale interareal networks of functional and structural connectivity in the human brain also had small-world properties ([Bassett and others 2006][2006BullmoreE_BassettDS]; [Salvador and others 2005][2005BullmoreE_SalvadorR]; [Vaessen and others 2010][2010BackesWH_VaessenMJ]). These and other seminal discoveries were central to the emergence of connectomics as a major growth point of network neuroscience ([Sporns and others 2005][2005K\u00f6tterR_SpornsO]). About 10 years ago, we reviewed these and other data in support of the idea that the brain is a small world network ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). Here, we aim to take another look at the concept of small-worldness, one or two decades since it was first formulated quantitatively and applied to brain network analysis at microscopic and macroscopic scales of anatomical resolution. First, we review some of the key questions about small-worldness that have been a focus of work in the period 2006\u20132016; then we review the technical evidence for small-worldness in high resolution tract-tracing data from the macaque and the mouse; finally, we highlight some likely trends in the further evolution of small-worldness as part of a deeper understanding of the topology of weighted brain graphs.","title":"Small-World Brain Graphs"},{"location":"190311_BassettDS_BullmoreET2017/#2_what_have_we_not_learnt_since_2006","text":"We have learnt a lot about complex topological organization of nervous systems since 2006, as evidenced by rapid growth in research articles, reviews, and citations related to \u201cbrain graphs\u201d and \u201cconnectomes\u201d ([Bullmore and Bassett 2011][2011BassettDS_Bullmore]; [Bullmore and Sporns 2009][2009SpornsO_BullmoreE]; [Pessoa 2014][2014PessoaL]); by the publication of several textbooks ([Fornito and others 2016][2016BullmoreET_FornitoA]; Sporns 2011 ); and by the recent launch of new specialist journals for network neuroscience. This emerging field of brain topology has grown much bigger than the foundational concept of small-worldness. But what have we learnt more specifically about brain small-worldness since 2006, and what do we still have to learn?","title":"2. What Have We (Not) Learnt Since 2006?"},{"location":"190311_BassettDS_BullmoreET2017/#universality","text":"There is no doubt that small-worldness\u2014the combination of non-random clustering with near-random path length\u2014has been very frequently reported across a wide range of neuroscience studies. Small-world topology has been highly replicated across multiple species and scales from structural and functional MRI studies of large-scale brain networks in humans to multielectrode array recordings of cellular networks in cultures ([Bettencourt and others 2007][2007GrossGW_BettencourtLM]) and intact animals ([van den Heuvel and others 2016][2016SpornsO_VanDenHeuvelMP]). It seems reasonable to conclude that small-worldness is at least very common in network neuroscience; but is it a universal property of nervous systems? Universality is a strong claim and difficult to affirm conclusively. As Popper noted in his philosophy of science by hypothetical refutation ([Popper 1963][1963PopperKR]), the universal hypothesis that \u201call swans are white\u201d can only be affirmed conclusively by a complete survey of every swan in the world. Whereas it can be immediately and decisively refuted by the observation of a single black swan. Similarly, the claim that all brains have small-world topology has not yet been (and never will be) affirmed by a complete connectomic mapping of every brain in the world. Some apparent counter-examples of brain networks that do not have small-world topology have been reported and deserve careful consideration as possible Popperian black swans ([see below][]). However, we can provisionally conclude that enough evidence has amassed to judge that small-worldness is a nearly universal property of nervous systems. Indeed, it seems likely that brains are only one of a large \u201cuniversality class\u201d of small-world networks comprising also many other non-neural or non-biological complex systems. Such near-universality of small-worldness, or any other brain network parameter, has a number of implications. First, near-universality implies self-similarity . If the macroscale interareal network of the human brain is small-world, as is the microscale interneuronal network of the worm or the fly, then we should expect also that the microscale interneuronal network of the human brain is small-world. Self-similarity of small-worldness would be indexed by scale invariance of network path length and clustering parameters as the anatomical resolution \u201czooms in\u201d from macro- to microscales. Although there is abundant evidence for scaling, fractal or self-similar statistics in many aspects of brain network topology ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Klimm and others 2014][2014MuchaPJ_KlimmF]), experimental data do not yet exist that could support a multiscale, macro-to-micro analysis of small-worldness (and other network properties) in the same (human or mammalian) nervous system ( Bassett and Siebenhuhner 2013 ). Second, near-universality suggests some very general selection pressures might be operative on the evolution and development of nervous systems across scales and species. This line of thinking has led to the formulation of generative models that can simulate brain networks by some probabilistic growth rule or genetic algorithm. It has been found that simple generative models, that add edges to a network based on the spatial distance and the topological relationships between nodes, can recapitulate small-worldness and many other properties of the connectome on the basis of two (spatial and topological) parameters ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). This serves as a reminder that the network phenotype of small-worldness can be generated by many different mechanisms and the biological mechanisms controlling formation of small-world properties in brain networks currently remain unknown. Third, and from a somewhat more controversial perspective, universality might seem tantamount to triviality . If the brain is everywhere small-world, and so are almost all other complex systems in real life ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Gaiteri and others 2014][2014SibilleE_GaiteriC]; [Moslonka-Lefebvre and others 2011][2011PautassoM_Moslonka-LefebvreM]; [Sizemore and others 2016][2016BassettDS_SizemoreA]) (for a few exceptions, see [Koschutzki and others 2010][2010SchreiberF_KoschutzkiD]), then what is the small-worldness of the brain telling us that\u2019s of any interest specifically to neuroscience? There are two main answers to this important question, as we discuss in more detail below: (1) studies have recently succeeded in linking network topological metrics to biological concepts, like wiring cost ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bassett and others 2011a][2011GraftonST_BassettDS]; [Bullmore and Sporns 2012][2012SpornsO_BullomoreET]; [Rubinov and others 2015][2015BullmoreET_RubinovM]), and to biological phenotypes, like neuronal density ([A\u0107imovi\u0107 and others 2015][2015LinneML_A\u0107imovi\u0107J]; [van den Heuvel and others 2015][2015deReusMA_VanDenHeuvel]) or gene expression ([Fulcher and Fornito 2016][2016FornitoA_FulcherBD]) and (2) small-worldness is not the whole story of brain network organization ([Wang and Kennedy 2016][2016KennedyH_WangXJ]).","title":"Universality"},{"location":"190311_BassettDS_BullmoreET2017/#economical_small-world_networks","text":"At the risk of stating the obvious, small-worldness is a purely topological quantity that tells us nothing about the physical layout of the nodes or edges that constitute the graph ([Bassett and others 2010][2010BassettDS]; [Pessoa 2014][2014PessoaL]). However, it is equally obvious that brain networks are embedded in anatomical space ([Bassett and others 2011a][2011BassettDS]; [Klimm and others 2014][2014KlimmF]; [Lohse and others 2014][2014LohseC]). Somehow the abstract, dimensionless topology of small-worldness must be reconciled to the anatomy of the brain. It turns out that the small-world topology of brain networks is (almost) always economically embedded in physical space ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Kaiser and Hilgetag 2006][2006HilgetagCC_KaiserM]). For both clustering and path length, the two topological metrics combined in the hybrid small-world estimator, there is a strong relationship with brain anatomical space ([Bassett and others 2010][2010BassettDS]; [Bassett and others 2011a][2011BassettDS]; [Rubinov and others 2015][2015RubinovM]). The edges between clustered nodes tend to be shorter distance whereas the edges that mediate topological short cuts tend to traverse longer anatomical distances. Interpreting the Euclidean distance between brain regional nodes or neurons as a proxy for the wiring cost, that is, the total biological cost of building a physical connection and maintaining communication between nodes, it has been argued that the brain is an economical small-world network ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Latora and Marchiori 2001][2001MarchioriM_LatoraV]). Economical in this sense does not simply mean parsimonious or cheap; it is more closely related to the common-sense notion of \u201cvalue for money\u201d. Topologically clustered nodes are anatomically co-located and thereby nearly minimize wiring cost. But small-world brain networks are not naturally lattices and if they are computationally rewired strictly to minimize wiring cost then brain networks are topologically penalized, losing integrative capacity indexed by increased characteristic path length and thus reduced small-worldness scalar \\sigma The economical idea is that brain networks have been selected by the competition between a pressure to minimize biological cost versus a pressure to maximize topological integration. More formally, P_{i,j} \\sim f(d_{i,j})f(k_{i,j}) \\tag{1} , the probability of a connection between nodes i and j , P_{i,j} , is a product of: a function of the physical distance in mm between nodes d_{i,j} \u2014often used as a proxy for wiring cost \u2014and a function of the topological relationship between nodes k_{i,j} . Typically, the functions of cost and topology are each parameterized by a single parameter, for example, simple exponential and power law functions. Several variants of this approach have been published, exploring a range of different topological relationships k_{i,j} between nodes, for example, clustering and homophily ( Betzel and others 2016a ; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). Economical models can generally reproduce the small world properties of brain networks quite realistically: clustering and path length are both increased as a function of the cost parameter (Avena-Koenigsberger and others 2014). In other words, as the cost penalty becomes the dominant factor predicting the probability of a connection between nodes, economical models generate increasingly lattice-like networks, with strong spatial and topological clustering of connected nodes, approximating in the limit the minimal cost configuration of the network. The emergence of more integrative network features\u2014such as hubs mediating many intermodular connections\u2014typically depends on some degree of relaxation of the cost penalty (reduced distance parameter) relative to the parameter controlling the importance of (integrative) topological relationships between nodes in predicting their connectivity. Thus, small-world networks can be generated by economical models for a certain range of the two parameters controlling the competitive factors of (wiring) cost and (topological) value .","title":"Economical Small-World Networks"},{"location":"190311_BassettDS_BullmoreET2017/#small-worldness_is_not_the_whole_story","text":"Before getting further into the details of small-worldness, as we do below in relation to recent tract-tracing results, it is important to acknowledge that the specific metrics of path length \\Lambda and clustering \\Gamma introduced by Watts and Strogatz (1998) , and the small-worldness scalar derived from them \\sigma = \\frac{\\Gamma}{\\Lambda} ( Humphries and others 2006 ), are a few global topological metrics that have been of central importance to the growth of complex network science generally. But more than 15 years after the first discovery of small-world properties in brain networks, the field of connectomics now extends into many other areas of topological analysis. There is much important recent work on topological properties like degree distribution and hubness ( Achard and others 2006 ), modularity ([Bassett and others 2011b][2011GraftonST_BassettDS]; [Chen and others 2008][2008EvansAC_ChenZJ]; Mattar and others 2015 ; [Meunier and others 2009][2009BullmoreE_MeunierD]; [Simon 1962][1962SimonH]; Sporns and Betzel 2016 ; [Stoop and others 2013][2013StoopR_StoopR]), core/periphery organization ([Bassett and others 2013][2013GraftonST_BassettDS]; Senden and others 2014 ; [van den Heuvel and Sporns 2011][2011SpornsO_VanDenHeuvelMP]), controllability ([Betzel and others 2016b][2016BassettDS_BetzelRF]; [Gu and others 2015][2015KahnAE_GuS]; Muldoon and others 2016b ) and navigability ( Guly\u00e1s and others 2015 ) that are not simply related to small-worldness. It is nothing like a complete description of the brain to say it is small world; we now turn to a more technical discussion of the evidence for small-worldness as a common property of nervous systems.","title":"Small-Worldness Is Not the Whole Story"},{"location":"190311_BassettDS_BullmoreET2017/#3_challenges_to_small-worldness","text":"About 3 to 4 years ago, an important series of papers began to be published that could be regarded as \u201cblack swans\u201d refuting the general importance of small-worldness in an understanding of brain networks ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]; Markov and others 2013 ; [Markov and others 2014][2014SalletJ_MarkovNT]; [Song and others 2014][2014WangXJ_SongHF]): Previous studies of low density inter-areal graphs and apparent small-world properties are challenged by data that reveal high-density cortical graphs in which economy of connections is achieved by weight heterogeneity and distance-weight correlations. ( Markov and others 2013 ) Recent connectomic tract tracing reveals that, contrary to what was previously thought, the cortical inter-areal network has high density . This finding leads to a necessary revision of the relevance of some of the graph theoretical notions, such as the small world property . . ., that have been claimed to characterise the inter-areal cortical network. ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]) These remarks carried weight because they were based on sophisticated and highly sensitive measurements of mammalian cortical connectivity ( Fig. 4 ). In each one of multiple carefully standardized experiments in the macaque monkey, a fluorescent tracer was injected into a (target) cortical region where it was taken up by synaptic terminals and actively transported to the cell bodies of neurons projecting to the target region. When the animal\u2019s brain was subsequently examined microscopically, the retrograde transport of the tracer from the injection site resulted in a fluorescent signal in the (source) regions of cortex that were directly connected to the target region. The basic technology of anatomical tract-tracing had been used by neuroanatomists since the late 20th century; but in the first decades of the 21st century it was possible to increase the scale and precision of the measurements dramatically, enabling the construction of connectivity matrices that summarized the strength or weight of axonal projections between a large number of cortical areas. These next-generation tract-tracing data thus represented a new standard of knowledge about mammalian cortical connectivity, that was more continuously quantified than the binary or ordinal rating of connectivity from traditional tract-tracing experiments ([Stephan and others 2001][2001K\u00f6tterR_StephanKE]), and much less ambiguously related to the cellular substrates of brain networks than the statistical measures of functional connectivity ([Achard and others 2006][2006BullmoreET_AchardS]; [Zhang and others 2016][2016BassettDS_ZhangZ]) and structural covariance ([Alexander-Bloch and others 2013][2013BullmoreET_Alexander-BlochA]; [Bassett and others 2008][2008Meyer-LindenbergA_BassettDS]) used to build graphs from human neuroimaging data. It is clearly important to understand in some detail how the topology of brain networks can be modelled in contemporary tract-tracing data from the macaque (and subsequently the mouse ([Oh and others 2014][2014MortrudMT_OhSW]; [Rubinov and others 2015][2015BullmoreET_RubinovM])) and what these results tell us about the small-worldness of brain networks. Figure 4. High density of the macaque cortical graph excludes sparse small world architecture. (A) Comparison of the average shortest path length and density of the macaque cortical graph from (Markov and others 2013) with the graphs of previous studies (Felleman and Van Essen 1991; Honey and others 2007; Jouve and others 1998; Markov and others 2012; Modha and Singh 2010; Young 1993). Sequential removal of weak connections causes an increase in the path length. Black triangle: macaque cortical graph from Markov and others (2013); gray area: 95% confidence interval following random removal of connections from the macaque cortical graph from Markov and others (2013). Jouve et al., 1998 predicted indicates values of the graph inferred using the published algorithm. (B) Effect of density on Watts and Strogatz\u2019s formalization of a small-world network. Clustering and path length variations generated by edge rewiring with probability range indicated on the x-axis applied to regular lattices of increasingly higher densities. The pie charts show graph density encoded via colors for path length ( L and clustering coefficient (C). The y-axis indicates the path length ratio ( \\frac{L_p}{L_o} ) and clustering ratio ( \\frac{C_p}{C_o} ) of the randomly rewired network, where L_o and C_o are the path length and clustering of the regular lattice, respectively. The variables L_p and C_p are the same quantities measured for the network rewired with probability P . Hence, for each density value indicated in the L and C pie charts, the corresponding \\frac{L_p}{L_o} and \\frac{C_p}{C_o} curves can be identified. Three diagrams below the x-axis indicate the lattice (left), sparsely rewired (middle), and the randomized (right) networks. (C) The small-world coefficient \u03c3 (Humphries and others 2006) corresponding to each lattice rewiring. Color code is the same as in panel (B). Dashed lines in (B) and (C) indicate 42% and 48% density levels, respectively. Reproduced with permission from Markov and others (2013).","title":"3. Challenges to Small-Worldness"},{"location":"190311_BassettDS_BullmoreET2017/#binary_graphs","text":"In general, a node represents a component of a system and an edge represents a connection or interaction between two nodes. Mathematically, we can capture these ideas with a graph G = (V,E) composed of a node set V and an edge set E ( Bollob\u00e1s 1979 , 1985 ). We store this information in an association or weight matrix W , whose ij th element indicates the strength or weight w_{i,j} of the edge between node i and node j A simple way of building a graph from such an association matrix is to apply a threshold \\tau to each element of the matrix, such that if w_{i,j} \u2265 \\tau then an edge is drawn between the corresponding nodes, but if w_{i,j} < \\tau no edge is drawn ([Achard and others 2006][2006BullmoreET_AchardS]). This thresholding operation thus binarizes the weight matrix and converts the continuously variable edge weights to either 1 (suprathreshold) or 0 (subthreshold). It was on this basis that almost all brain graphs were constructed in the 15 years or so following the seminal small-world analysis of a binary graph representing the cellular connectome of C. elegans ( Watts and Strogatz 1998 ). Most of the neuroimaging evidence for small-worldness in human brain networks, for example, is based on analysis of binary graphs constructed by thresholding a correlation coefficient or equivalent estimator of the weight of functional or structural connectivity or structural covariance between regions i and j ( van Wijk and others 2010 ). It is well recognized that construction of binary graphs represents an extreme simplification of brain networks; indeed, a binary undirected graph of homogenous nodes is as simple as it gets in graph theory ([Bassett and others 2012a][2012LimKO_BassettDS]). However, this approach has historically been preferred in neuroimaging because of limited signal-to-noise ratio in the data ([Achard and others 2006][2006BullmoreET_AchardS]). By varying the threshold \\tau used to construct a binary graph from a continuous weight matrix, the connection density of the network is made denser or sparser. If the threshold is low and many weak weights are added to the graph as edges then the connection density will increase; if the threshold is high and only the strongest weights are represented as edges, then the connection density will decrease. The connection density D is quantified by the number of edges E in the graph as a proportion of the total number of edges in a fully connected network of the same number of nodes N : D=\\frac{E}{\\frac{N^2\u2212N}{2}} Often, this proportion is translated into a percentage. In many neuroimaging studies, the threshold is set to a large value to control for the high levels of noise in MRI data, resulting in connection densities in the range 5% to 30% ( Lynall and others 2010 ). In many of the first generation tract tracing studies, the connectivity data were collected on a binary or ordinal scale, and not all possible connections had been experimentally measured, so these data were naturally modelled as binary graphs with connection densities ~30% a value that was constrained by the completeness and quality of the data ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). The small-world topology of a binary brain graph is defined by estimating two parameters in the data, path length L and clustering C ( Fig. 2A ), and comparing each of these observed parameters to their distributions under a specified null model ( Humphries and others 2006 ). More specifically, L = \\frac{1}{N}\\sum l_{i,j} is the global or characteristic path length, where l_{i,j} is the shortest path (geodesic) between nodes i] and j ; and C = \\frac{1}{N}\\sum c_{i,j} ( c_i := \\frac{L_i}{\\frac{k_i(k_i-1)}{2}} ) is the global clustering coefficient , where c_{i,j} is the number of closed triangular motifs including node i . Each of these parameters is normalized by its value in a binary graph representing the null hypothesis. For example, if the null hypothesis is that clustering of brain networks C_{\\text{brain}} is no different from the clustering of a random graph, then it is reasonable to generate an Erd\u00f6s\u2013Reny\u00ed graph for N nodes and D connection density, measure the clustering coefficient in the random graph C_{\\text{random}}] , and use the ratio between brain and random graph clustering coefficients as a test statistic for non-random clustering. We note that there are many other possible ways in which a null model could be sampled, besides using the classical Erd\u00f6s\u2013Reny\u00ed model, and this is an active area of methodological research ( Muldoon and others 2016a ). However, in general one can define the normalized clustering coefficient as \\Gamma = \\frac{C_{\\text{brain}}}{C_{\\text{random}}} . Likewise, the path length of the brain graph can be normalized by its value in a comparable random graph \\Lambda = \\frac{L_{\\text{brain}}}{L_{\\text{random}}} . A small-worldness scalar can then be simply defined as \\sigma = \\frac{\\Gamma}{\\Lambda} . With these definitions, small-world networks will have \\sigma > 1 , \\Gamma > 1 and \\Lambda \\sim 1 ( Humphries and others 2006 ).","title":"Binary Graphs"},{"location":"190311_BassettDS_BullmoreET2017/#weighted_graphs","text":"Although binary graph analysis has predominated to date in analysis of brain networks, this certainly does not represent the methodological limit of graph theory for connectomics. For example, provided the data are of sufficient quality, there is no need to threshold the weight matrix to estimate topological properties like clustering, path length, and small-worldness. Indeed, while the binarization procedure was common in early applications of graph theory to neural data ( van Wijk and others 2010 ), it remains fundamentally agnostic to architectural principles that may be encoded in edge weights ( Rubinov and Sporns 2011 ). This realization has more generally motivated the field to develop methods that remain sensitive to the patterns of weights on the edges ([Ginestet and others 2011][2011SimmonsA_GinestetCE]), and to the topologies present in weak versus strong weights ( Rubinov and Sporns 2011 ). These efforts have included the development of alternative thresholding schemes ([Bassett and others 2012a][2012LimKO_BassettDS]; [Lohse and others 2014][2014CarlsonJM_LohseC]) and fully weighted graph analysis ([Bassett and others 2011b][2011GraftonST_BassettDS]; Rubinov and Sporns 2011 ). The mathematical tools exist to estimate and simulate the topological properties of weighted networks, and analysis of weighted networks is akin to studying the geometry of the graph, rather than simply its topology ([Bassett and others 2012b][2012PorterMA_BassettDS]; [Bassett and others 2013][2013GraftonST_BassettDS]). For example, weighted analogues of binary metrics of clustering, path length and small-worldness can be defined formally ( Fig. 2B ). First, the weighted clustering coefficient of node i can be defined as C_{\\text{weighted}} = \\frac{1}{k_i(k_i\u22121)} \\sum_{j,k}(\\hat{w}_{ij} \\hat{w}_{jk} \\hat{w}_{ik})^{\\frac{1}{3}} , where k_i is the number of edges connected to node i , or degree of node i ([Onnela and others 2005][2005KaskiK_OnnelaJP]) (but see also [Barrat and others 2004][2004VespignaniA_BarratA]; [Zhang and Horvath 2005][2005HorvathS_ZhangB] for other similar definitions). The weighted path length can be defined as L_{\\text{weighted}} = \\frac{1}{N(N\u22121)} \\sum_{i \\neq j} \\delta_{ij} , where the topological distance between two nodes is given by \\delta_{ij} = \\frac{1}{w_{ij}} ([Newman 2001][2001NewmanMEJ]). These two statistics can be combined to construct a weighted metric of small-worldness ([Bola\u00f1os and others 2013][2013AviyenteS_Bola\u00f1osM]): \\sigma_{\\text{weighted}} = \\frac{\\Gamma_{\\text{weighted}}}{\\Lambda_{\\text{weighted}}} . With these definitions, small-world networks will have \\Gamma_{\\text{weighted}} > 1 , \\Lambda_{\\text{weighted}} \\sim 1 , and \\sigma_{\\text{weighted}} > 1 ( Humphries and others 2006 ).","title":"Weighted Graphs"},{"location":"190311_BassettDS_BullmoreET2017/#the_small-world_propensity","text":"There are several important limitations to the definitions of small-worldness described in the previous sections. First, the small-world scalar \\sigma (whether binary or weighted) can be greater than 1 even in cases when the normalized path length is much greater than one; because it is defined as a ratio, if \\gamma \\gg 1 and \\lambda > 1 , the scalar \\sigma > 1 This means that a small-world network will always have \\sigma > 1 , but not all networks with \\sigma > 1 will be small-world (some of them may have greater path length than random graphs). Second, the measure is strongly driven by the density of the graph, and denser networks will naturally have smaller values of \\sigma even if they are in fact generated from an identical small-world model. To address these and other limitations, Muldoon and colleagues recently developed a metric called the small-world propensity . Specifically, the small-world propensity, \\phi , reflects the deviation of a network\u2019s clustering coefficient, C_{\\text{brain}}] , and characteristic path length, L_{\\text{brain}} , from both lattice ( C_{\\text{lattice}} , L_{\\text{lattice}} ) and random ( C_{\\text{random}} , L_{\\text{random}} ) networks constructed with the same number of nodes and the same degree distribution: \\phi = 1 \u2212 \\sqrt{\\frac{\\Delta^2_C+\\Delta^2_L}{2}} , where \\Delta C = \\frac{C_{\\text{lattice}}\u2212C_{\\text{brain}}}{C_{\\text{lattice}}\u2212C_{\\text{random}}} and \\Delta L = \\frac{L_{\\text{brain}}\u2212L_{\\text{random}}}{L_{\\text{lattice}}\u2212L_{\\text{random}}} . The ratio \\Delta_{C/L} represents the fractional deviation of the metric C_{\\text{brain}} or L_{\\text{brain}} ) from its respective null model (a lattice or random network). This quantity can be calculated for binary networks (using binary definitions of clustering and path length) or for weighted networks (using weighted definitions of clustering and path length). Networks are considered small-world if they have small-world propensity 0.4 < \\phi \u2264 1 . However, this metric should be viewed as a continuous metric of small-worldness rather than a hard threshold ( Muldoon and others 2016a ). Importantly, the small-world propensity overcomes several limitations of previous scalar definitions of small-worldness ( Muldoon and others 2016a ). First, it can incorporate weighted estimates of both the clustering coefficient and path-length, thus being generally applicable to any neural data that can be represented as a weighted network. Second, it is density independent, meaning that it can be used to compare the relative small-worldness between two networks that have very different densities from one another. Third, the metric is informed by spatially-constrained null models ([Bassett and others 2015][2015DanielsKE_BassettDS]; [Expert and others 2011][2011LambiotteR_ExpertP]; [Papadopoulus and others 2016][2016BassettDS_Papadopoulus]) in which nodes have physical locations and the edges that correspond to the smallest Euclidean distance between nodes are assigned the highest weights ([Barth\u00e9lemy 2011][2011Barth\u00e9lemyM]) ( Fig. 5 ). Figure 5. Small-world propensity in weighted networks. Here, we illustrate an example of a generative small-world model, and its utility in estimating an empirical network\u2019s small-world propensity. (A) We can extend the concept of a Watts\u2013Strogatz model to weighted graphs by first building a lattice in which the edges are weighted by distance such that edges between spatially neighboring nodes have more strongly weighted than edges between spatially distant nodes. These edge weights can then be rewired with a probability, P, to create a weighted small-world network. (B) Weighted clustering coefficient and weighted path length can be estimated as a function of the rewiring parameter, P, and used to derive the small-world propensity of the graph compared with random and lattice benchmarks (Eq. 11). (C) Weighted small-world propensity calculated for the same network as in panel (B). Error bars represent the standard error of the mean calculated over 50 simulations, and the shaded regions represent the range denoted as small-world. (D) Weighted small-world propensity as a function of network density for a graph of 1000 nodes. Reproduced with permission from Muldoon and others (2016a).","title":"The Small-World Propensity"},{"location":"190311_BassettDS_BullmoreET2017/#4_twenty-first_century_tract-tracing","text":"The scale and quality of contemporary tract-tracing data, in both the macaque and the mouse, represents a step change in terms of sensitivity in detecting anatomical connections, or axonal projections, between cortical areas. Using retrograde tracer experiments it has proven possible to demonstrate reliably that pairs of regions in the macaque brain may be connected by one or a few axonal projections. Likewise, anterograde tracer experiments in the mouse have demonstrated that the minimal detectable weight of connectivity between cortical regions, that just exceeds the noise threshold, is equivalent to the projection of one or a few axons (Ypma and Bullmore 2016). This high sensitivity has led immediately to the recognition of a large number of weak and previously unreported axonal connections. In the macaque, it was estimated that 36% of connections identified by contemporary tract tracing were so-called new found projections (NFPs) that had not been described in the prior literature (Markov and others 2014). The existence of so many weak connections is reflected in the log normal distributions of connectivity weight, ranging over five to six orders of magnitude, in both the macaque and the mouse (Ercsey-Ravasz and others 2013; Oh and others 2014). In short, tract-tracing can now resolve connections approximately equivalent to a single axonal projection and approximately a million times weaker than the strongest anatomical connections or white matter tracts. How can we use graph theory to model the network organization of such highly sensitive, highly variable data? Perhaps the simplest approach, borrowing from prior studies of less high quality datasets, is to apply a threshold and convert the log-normal weight matrix into a binary adjacency matrix. If the threshold is defined by the noise distribution of the measurements then it will be very close to zero for these sophisticated experiments, and correspondingly the connection density of the binary graph will be high. In the macaque, the connection density of a binary graph of 29 visual cortical areas was estimated to be 66% (Markov and others 2013), considerably higher than historical estimates in the range of 25% to 45% (Fellemen and Van Essen 1991). In the mouse, the connection density of a binary graph of 308 areas of the whole cortex was estimated to be 53% (Rubinov and others 2015). In other words, the binary graphs generated from 21st century tract-tracing data are about twice as dense as the much sparser networks derived from human neuroimaging and 20th century tract-tracing. They are also considerably denser than brain networks constructed at a finer grained (ultimately cellular) resolution. For example, the connection density of the C elegans nervous system, which is still the only completely mapped synaptic connectome, is about 12%. It is easy to see that the connection density of a binary network depends on the number of neurons comprising each node. In the limit, if the nervous system is parcellated into two large nodes the connection density will certainly be 100%; as the same system is parcellated into a larger number of smaller nodes its connection density will monotonically decrease (Bassett and others 2011a; Zalesky and others 2010). Thus, the current interval estimate of mammalian cortical connection density ~55% to 65% is conditional both on the anatomical resolution of the parcellation scheme used to define the nodes and the sensitivity of the tract-tracing methods used to estimate the weights of the edges.","title":"4. Twenty-First Century Tract-Tracing"},{"location":"190311_BassettDS_BullmoreET2017/#small-worldness_of_binary_tract-tracing_networks","text":"Having constructed a high-density binary graph from tract-tracing data on mammalian cortex, it is straightforward to estimate its clustering and path length, using the same metrics as for sparser binary graphs. However, simply because there is a larger number of connections in the denser network, its clustering will be considerably higher (there will be more closed triangular motifs) and its path length will be shorter (there will be more direct, pairwise connections) than a sparser network. Indeed, the clustering and path length of any binary graph at 60% connection density will be close to the maximal clustering and minimal path length of a fully connected graph; and therefore the clustering and path length of a 60% dense brain network will be very similar to the clustering and path length of a 60% random network (Bassett and others 2009). This means that when clustering and path length in brain networks are normalized by their corresponding values in equally dense random networks, the scaled metrics \u0393 and \u039b will both be close to 1, and the small-world scalar \u03c3 will be close to its critical value of 1 (Markov and others 2013). For the macaque, at 66% connection density, \u0393=1.21\u00b10.014 , \u039b=1.00\u00b10.000 , and \u03c3=1.21\u00b10.014 ; for the mouse, at 53% connection density, \u0393=1.31\u00b10.004 , \u039b=1.00\u00b10.000 , and \u03c3=1.31\u00b10.004 (all given in mean \u00b1 standard deviation; Fig. 6A and C; Table 1). Since small-worldness has been traditionally defined as \u03c3>1 , these results suggest that dense binary graphs constructed from tract tracing data are small-world, although the macaque is more similar to a random network than the mouse. Figure 6. Binary and weighted small-worldness in mouse and macaque connectomes. For the macaque connectome reported in Markov and others (2013), we show (A) the binary network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (B) we show a weighted network analysis for the same data. For the mouse connectome reported in Rubinov and others (2015), we show (C) the weighted network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (D), we show a binary network analysis for the same data. In the boxplots, the gray dotted line shows the threshold value of \u03c3 = 1, and the purple area shows the range of values of 0.4 < \u03c6 \u2264 1 in which a network is considered small-world. These results do not look like a \u201cblack swan\u201d that refutes universal claims that the brain always embodies small-world network topology. Nor do they undermine the credibility of previous studies demonstrating small-worldness in sparser brain graphs. However, our view is that binary graph models are very unlikely to be an optimal strategy for network analysis of tract-tracing data, because they fail to take account of the extraordinary range of connectivity weights, distributed log normally over 6 orders of magnitude, that has been discovered in mammalian cortical networks (Ercsey-Ravasz and others 2013). The weakest connection between cortical areas is about a million times less weighted than the strongest connection: does it really make sense to set all these weights equivalently to 1 as edges in a binary graph? To ask the question is to answer it.","title":"Small-Worldness of Binary Tract-Tracing Networks"},{"location":"190311_BassettDS_BullmoreET2017/#small-worldness_of_weighted_tract-tracing_networks","text":"A weighted small-world analysis is easily done for these data (Fig. 6B and D). The weighted clustering and weighted path length metrics (Eqs. 8 and 9) are estimated directly from the weight matrices, and the ratio of weighted clustering to weighted path length is the scalar summary of weighted small-worldness \u03c3_{\\text{weighted}}>1 . In Figure 6, we directly compare binary and weighted graph theoretical results for the mouse (Oh and others 2014; Rubinov and others 2015) and macaque (Markov and others 2013) connectomes. Compared with the results of binary graph analysis, both mouse and macaque networks have increased clustering for the weighted graph analysis, and \u03c3 is increased for the macaque (see Table 1). The weighted graph of the mouse connectome is similarly small-world compared to the weighted macaque graph, as measured by \u03c3 , but is significantly more small-world as measured by the small-world propensity \u03d5 . However, classical estimates of small-worldness may depend in a non-trivial way on the density of the graph. This relationship becomes obvious if we estimate the topology of both weighted graphs as a function of connection density (Fig. 7). The classical small-world scalar \u03c3 is greatest when it is estimated for a sparse graph comprising less than 20% to 30% of the most strongly connected edges, and decreases progressively as the graph becomes denser. This might suggest that the macaque connectome seems less small-world than the mouse simply because it is denser. However, the small world propensity \u03d5 has the useful property that it is independent of network density and it is significantly greater, indicating more small-worldness, for the mouse than the macaque. This could be related to differences between the datasets in number of cortical areas and completeness of cortical coverage: the macaque dataset comprises fewer nodes of mostly visual cortex than the larger number of nodes across the whole mouse cortex. Figure 7. Dependence of small-world characteristics on network density. (A) Macaque and (B) mouse connectivity matrices in their natural state (left), as well as after thresholding to retain the 5% strongest (middle) or 25% strongest (right) connections. Weighted small-world metrics including the normalized clustering coefficient (\u0393), normalized path length (\u039b), small-world index (\u03c3), and small-world propensity (\u03c6) as a function of network density for the (C) macaque and (D) mouse connectivity matrices.","title":"Small-Worldness of Weighted Tract-Tracing Networks"},{"location":"190311_BassettDS_BullmoreET2017/#weighted_small-worldness_and_the_role_of_edge_weights","text":"Why does a weighted graph analysis provide stronger evidence for non-random clustering than a binary graph analysis applied to the same tract-tracing data? The most strongly weighted connections generally span the shortest physical distances between cortical areas (Ercsey-Ravasz and others 2013; Klimm and others 2014; Rubinov and others 2015). This is not surprising based on what we know about the importance of cost constraints on brain organization (Bassett and others 2009; Bassett and others 2010; Bullmore and Sporns 2012; Fornito and others 2011). Strong connectivity weights indicate a large number of axonal projections, a big bandwidth bundle, perhaps macroscopically visible as a white matter tract. Building and resourcing a high-bandwidth axonal signaling bundle is a significant biological cost that will increase as a function of connection distance: it is parsimonious to wire high bandwidth over short distances. Short distance connections are not only strongly weighted but also topologically clustered. So the strongest weights in both cortical networks define a topologically segregated and anatomically localized organization. A map of the sub-network formed by the strongest weights shows spatial and topological clusters of regions (Fig. 8). In the mouse, the strongly weighted clusters each comprise functionally specialized areas of cortex (visual, motor, etc.) that are known to be densely interconnected and anatomically localized (Rubinov and others 2015; Ypma and Bullmore 2016). Thus it is not surprising that weighting the topological analysis of mammalian cortical networks will provide stronger evidence for non-random clustering than unweighted analysis of binary graphs. Figure 8. The existence of weak links and their topology in the mouse connectome. Here, we show the properties of the 5% weakest and 5% strongest edges of the mouse cortical network. (A, B) Axial view of the mouse cortical network, red dots represent brain regions, blue lines represent the connections between them. Drawn are the (A) 5% weakest or (B) 5% strongest edges. Dot size corresponds to degree, the total number of incoming and outgoing edges connected to a node. In (B), the three nodes with highest degree have been labeled as follows: VISp, primary visual area; MOp, primary motor area; SSs, supplemental somatosensory area. The strong connections are spatially organized, mainly connecting spatially adjacent or contralaterally homologous regions. The weak connections span longer distances and are topologically more random than the strongest connections. (C) The distance distributions for (blue) the 5% weakest edges, (red) the 5% strongest edges, and (black) a random graph of the same size and connection density. (D) The degree distributions for the weakest and strongest connections of the mouse connectome, and a comparable random graph, color-coded as in panel (C). Reproduced with permission from Ypma and Bullmore (2016). The most weakly weighted connections are an area of active, ongoing research (discussed in more detail below) and it is inevitable that there is still much to learn about a feature of network organization\u2014replicable but very weak connections between large cortical areas\u2014that had not been measurable until recent advances in tract-tracing methodology. However, it is clear that weaker connections tend to subtend longer distances, and can be either more topologically random (Ypma and Bullmore 2016) than or similarly topologically organized to strong connections (Bassett and others 2012a). We conclude that graph theoretical analysis of tract-tracing connectomes should respect the quality of the data and use weighted topological metrics to reflect the wide ranging variation in anatomical connectivity, from single fibers to major tracts, that is now measurable in the mammalian brain (Wang and Kennedy 2016). Weighted graph analysis demonstrates clearly that both the macaque and mouse connectomes are small-world networks, as are the human, cat, and nematode brains (Muldoon and others 2016a). Binary graph analysis has usefully measured high connection density, due to the existence of many new anatomical connections, but binarization of these data is not the best way to understand their complex topology and its economical embedding in anatomical space (Bassett and others 2011a; Bassett and others 2012a; Klimm and others 2014; Rubinov and others 2015; Rubinov and Sporns 2011). Future studies will likely also pay more attention to the fact that most tract-tracing markers are axonally transported only in one direction: anterograde or retrograde. This means that the weight matrix could be modelled more completely as a weighted and directed graph, representing a further evolution in the use of graph theoretical methods to capture a richer and biologically more meaningful model of brain network organization than can be provided by binary graphs of unweighted and undirected edges.","title":"Weighted Small-Worldness and the Role of Edge Weights"},{"location":"190311_BassettDS_BullmoreET2017/#5_the_utility_of_weak_connections","text":"At this juncture, one might naturally ask, \u201cFrom a neuroscientific perspective, do we need techniques that account for edge weights? Do these weights indeed capture information of relevance for cognition and behavior?\u201d Neuroanatomical data suggest that the weights of structural connections may be driven by developmental growth rules (Ercsey-Ravasz and others 2013; Kaiser and Hilgetag 2006; Klimm and others 2014; Lohse and others 2014; Markov and others 2013), energetic and metabolic constraints (Bassett and others 2010), and physical limitations on the volume of neural systems, particularly brains encapsulated by bone (Sherbondy and others 2009). Yet the role of these edge weights in neural computations (Schneidman and others 2006) and higher order cognition has been less well studied. Recent studies have begun to elucidate the role of edge weights\u2014and particularly of weak connections\u2014in human cognition. In resting-state fMRI data, weak functional connections from lateral prefrontal cortex to regions within and outside the frontoparietal network have been shown to display individual differences in strength that predict individual differences in fluid intelligence (Cole and others 2012). The same general relationship was observed in a separate study in which individual differences in moderately weak, long-distance functional connections at rest were strongly correlated with full scale, verbal, and performance IQ (Santarnecchi and others 2014). Neither of these correlations were observed when considering strong connections. Indeed, the utility of weak edges appears to extend to psychiatric illness, where the highly organized topology of weak functional connections\u2014but not strong functional connections\u2014in resting-state fMRI were able to classify people with schizophrenia from healthy controls with high accuracy and specificity (Bassett and others 2012a). Interestingly, individual differences in these weak connections were significantly correlated with individual differences in cognitive scores and symptomatology. Together these results demonstrate that, indeed, methods that are sensitive to the strength (or weakness) of individual connections are imperative for progress to be made in understanding individual differences in cognitive abilities, and their alteration in psychiatric disease. Importantly, the utility of weak connections is not only evident at the large scale in human brains but also at the neuronal scale as measured in non-human species. In an influential article published in 2006 with Bialek and colleagues, Schneidman demonstrated that weak pairwise correlations implied strongly correlated network states in a neural population, suggesting the presence of strong collective behavior (Schneidman and others 2006). This result was initially counterintuitive as one might expect that weak correlations would be associated with the lack of collective behavior. However, the original observation has withstood the test of time, and has been validated in several additional studies including work at the level of tract tracing in macaque monkeys (Goulas and others 2015). Intuitively, the juxtaposition of weak correlations and cohesive, collective behavior is thought to be driven by the underlying sparsity of neuronal interactions (Ganmor and others 2011b), which contain a few non-trivial higher-order interaction terms (Ganmor and others 2011a). Indeed, these higher-order interactions are the topic of some interest both from a computational neuroscience perspective (Giusti and others 2016; Sizemore and others 2016), and from the perspective of neural coding (Giusti and others 2015). But perhaps the claim that weak connections are critically important for our understanding of neural systems should not be particularly surprising. Indeed, it is in fact an old story, first published at the inception of network science. In 1973, Granovetter wrote a seminal paper, titled \u201cThe strength of weak ties,\u201d which highlighted the critical importance of weakly connected components in global system dynamics (Granovetter 1973). Such weak connections are ubiquitous in many systems, from physician interactions (Bridewell and Das 2011) to ecosystem webs (Ulanowicz and others 2014) and atmospheric pathways (Lee and Su 2014). Looking forward, critical open questions lie in how these weak connections drive global dynamics, and how one can intervene in a system to manipulate those processes (Betzel and others 2016b; Gu and others 2015; Muldoon and others 2016b). Acknowledging the role of weak connections, weighted small-world organization plays a critical role in system functions that are particularly relevant to neural systems: including coherence, computation, and control and robustness (Novkovic and others 2016). Perhaps the most commonly studied function afforded by small-world architecture is the ability to transmit information, a characteristic that is common in networks of coupled oscillators (Barahona and Pecora 2002; Hong and others 2002; Nishikawa and others 2003) (although see Atay and others 2006, for a few notable exceptions). This capability supports enhanced computational power (Lago-Fern\u00e1ndez and others 2000), via swift flow and transport (Hwang and others 2010). In dynamic networks, oscillators coupled on small-world networks are much more sensitive to link changes than their random network counterparts (Kohar and others 2014), the time taken to reach synchronization is lowered, and the synchronized state is less stable over time, potentially enabling greater diversity of function. When such a system has both small-world topology and geometry, it directly impacts the network\u2019s ability to speed or slow spreading (Karsai and others 2011), a potentially useful characteristic for resilience to dementia which is thought to be caused by the spread of prions (Raj and others 2012; Raj and others 2015). The value of small-world architecture is not limited to its support of synchronization and information flow. Instead, it also supports a wide range of computations in neural circuits. From early neural network studies, it is clear that the exact topology of connectivity patterns between network elements directly supports trade-offs in the network\u2019s ability to learn new information versus retain old information in memory (Hermundstad and others 2011). When these patterns are organized in a small-world manner, evidence suggests that local computations can be integrated across distributed cell assemblies to support functions as diverse as somatosensation (Zippo and others 2013) and olfaction (Imam and others 2012). The mechanism by which small-worlds support these computations may stem from the fact that their topological structure tends to contain both large cavities and high-dimensional cliques (Sizemore and others 2016), which when embedded in a physical space can strongly constrain the geometric properties of the computation (Giusti and others 2015). While small-world structure can offer non-trivial advantages in terms of both communication and computation, it also directly informs the sorts of interventions that one could use to guide network dynamics and by extension system function. Indeed, computational studies have demonstrated that small-world network architecture requires specific control strategies if one wishes to stem the propagation of seizure activity (Ching and others 2012), control the spread of viruses (Kleczkowski and others 2012), or enhance recovery following injury (H\u00fcbler and Buchman 2008). To gain an intuition for how topology impacts control, we can consider the broad-scale degree distribution also characteristic of brain networks. Based on the Laplacian spectrum, one can observe that weakly connected nodes have the greatest potential to push the system into distant states, far away on an energy landscape (Pasqualetti and others 2014); conversely, strongly connected hubs have the greatest potential to push the system into many local states, nearby on the energy landscape (Gu and others 2015). Thus, control energy (such as that provided by brain stimulation) may be targeted to different locations in a small-world brain network to affect a specific change in brain dynamics (Muldoon and others 2016b).","title":"5. The Utility of Weak Connections"},{"location":"190311_BassettDS_BullmoreET2017/#6_conclusions","text":"Small-worldness remains an important and viable concept in network neuroscience. Nearly 20 years on from the first analysis of the complex topology of a binary graph representing the nervous system of C. elegans, it has been established that small-worldness is a nearly universal and functionally valuable property of nervous systems economically embedded in anatomical space. Recent advances in tract-tracing connectomics do not refute small-worldness; rather they considerably enrich and deepen our understanding of what it means in the brain. The extraordinary precision of contemporary tract tracing, and the important discovery that mammalian cortical networks are denser than expected, mandates the adoption of more sophisticated techniques for weighted graph theoretical modeling of interareal connectomes. On this basis, we expect the next 10 years to yield further insights into the functional value of weak as well as strong connections in brain networks with weighted small-worldness.","title":"6. Conclusions"},{"location":"190410_DeAngelisD_DiazS_2019/","text":"19-04-10 Decision-Making in Agent-Based Modeling: A Current Review and Future Prospectus \u00b6 Original | Mendeley ToC \u00b6 00. Abstract 01. Introduction 02. Decisions in Classical Population Models 03. Agent-based Modeling in Ecology 04. Movement Decisions and Their Consequences 04.01. When to Move? 04.02. Where to Move? 04.03. Collective Movement Behavior 05. Foraging Decisions and Population Interactions 06. Social Interactions in Populations 07. Developments in the Modeling of Decisions in Population Models 08. Prospectus 00. Abstract \u00b6 00.P01 \u00b6 All basic processes of ecological populations involve decisions; when and where to move, when and what to eat, and whether to fight or flee. Yet decisions and the underlying principles of decision-making have been difficult to integrate into the classical population-level models of ecology. Certainly, there is a long history of modeling individuals' searching behavior, diet selection, or conflict dynamics within social interactions. When all the individuals are given certain simple rules to govern their decision-making processes, the resultant population\u2013level models have yielded important generalizations and theory. But it is also recognized that such models do not represent the way real individuals decide on actions. Factors that influence a decision include the organism's environment with its dynamic rewards and risks , the complex internal state of the organism , and its imperfect knowledge of the environment . In the case of animals, it may also involve complex social factors, and experience and learning , which vary among individuals. The way that all factors are weighed and processed to lead to decisions is a major area of behavioral theory. 00.P02 \u00b6 Individual- / Agent-based model (IBM / ABM) While classic population-level modeling is limited in its ability to integrate decision-making in its actual complexity, the development of individual- or agent-based models (IBM/ABMs) (we use ABM throughout to designate both \u201cagent-based modeling\u201d and an \u201cagent-based model\u201d) has opened the possibility of describing the way that decisions are made, and their effects, in minute detail. Over the years, these models have increased in size and complexity. Current ABMs can simulate thousands of individuals in realistic environments, and with highly detailed internal physiology, perception and ability to process the perceptions and make decisions based on those and their internal states . The implementation of decision-making in ABMs ranges from fairly simple to highly complex; the process of an individual deciding on an action can occur through the use of logical and simple (if-then) rules to more sophisticated neural networks and genetic algorithms . The purpose of this paper is to give an overview of the ways in which decisions are integrated into a variety of ABMs and to give a prospectus on the future of modeling of decisions in ABMs. 01. Introduction \u00b6 01.P01 \u00b6 classical models: randomly moving w/ growth, reproducitin, mortality, interaction w/ env & other organisms e.g., logistric population model Lotka-Volterra predator-prey and competition model reaction-diffusion partial differential equation (PDE) model decision-making by individuals is unimportant? decision continually made e.g., single-celled animals (paramecia) ( 2009_Wagner ) social ants ( 2006_Deneubourg_Detrain ) The role of decision-making by individual organisms is largely ignored in the classical mathematical models of ecology, such as the logistic population models , Lotka-Volterra predator-prey and competition models , and their many variations. These models, as well as their extensions to space, as reaction-diffusion partial differential equation (PDE) models , treat organisms as randomly moving atoms, with added features of growth, reproduction, mortality, and interactions with their environment and other organisms. Because the classical models of ecological populations have been successful in revealing much about ecological systems, one might ask if decision-making by individuals is unimportant enough that it can be ignored at the population level, at least for simple organisms. But something like decisions are continually made, even by organisms perceived as simple. Bray (2009) notes that single-celled animals, like swimming paramecia, \u201ccontinually encounter different situations\u2026 and have to evaluate their options and assign priorities.\u201d Wagner (2009) asks \u201cDoes the bacterium choose to change direction?\u201d and concludes that this may be a matter of perspective. In more complex organisms, such as social ants, \u201ceach individual is a sensitive unit which can process a lot of information\u201d ( Detrain and Deneubourg, 2006 ). These actions are programmed into the organism's DNA, and are unconscious, but clearly decisions are being made, and we will follow Ydenberg (2010) (cited in Rypstra et al., 2015 ) in calling a decision \u201cwherever one or two (or more) options is/are selected .\u201d 01.P02 \u00b6 How important these decisions are at the population level and above is therefore an important question, even for organisms of low cognitive ability. Our perspective in this review is from that of population and community ecology and how modeling helps link individual behaviors to phenomena at the level of collections of individuals. We begin with a brief overview of how decisions have been incorporated in some classical analytic models of ecology. Then we introduce agent-based modeling (ABM) and describe how it has been used to simulate decision-making in individual movement, foraging behavior, population interactions, and social interactions within populations. This is not intended to be comprehensive, but to touch on a variety of ways ABM is used. Finally, we discuss more recent developments in modeling decisions within population models and present a prospectus for future directions. 02. Decisions in Classical Population Models \u00b6 02.P01 \u00b6 individual decision \u21d2 population < individual fitness \u21d0 ecological context The use of modeling to address the question of the effect of individual decisions on population level dynamics developed more slowly than modeling of the converse question of how ecological context influences the effects of decisions on individual fitness. The latter has been explored by behavioral ecologists using classical population models for several decades, focusing especially on decisions regarding foraging movement and its effects on the fitness of individuals. Additionally, some models have been able to incorporate decision-making when modeling collections of individuals, whole populations, and even multiple populations. 02.P02 \u00b6 models kinesis model (decisions on when to speed up/slow down/turn in response to detected local cond) restricted are search patch DSVM (Dynamic State Variable Modeling) Movement of animals toward favorable conditions could be decomposed into simple decisions on directed movement, or taxis, in relation to light, temperature, or resource gradients. Because it may be difficult for organisms to detect gradients, but possible to assess conditions at a current location, many models focused on kinesis, which involves decisions on when to speed up, slow down, or turn in response to detected local conditions (e.g., Gunn and Fraenkel, 1961 ; Sch\u00f6ne, 1984 ; Bell, 1991 ; Gr\u00fcnbaum, 1999 ; Gautestad, 2016 ). Other models used \u201crestricted area search\u201d in which organisms evaluate conditions within a limited area before making a movement choice (e.g., Humston et al., 2004 ). Movement decisions can be combined with decisions on settling at a place or leaving it, for which a variety of modeling approaches are used ( Lima and Zollner, 1996 ). At one extreme, animals may simply move in one direction until they find a spot to settle, or they may use spatial memory and learning to gain knowledge of the landscape such that they can choose the nearest detectable habitat patch ( Fahrig, 1988 ). Decisions on leaving a patch may increase as the level of resources is depleted. Mathematical theory has been applied to predict the optimal time to leave a patch, depending on its resource level relative to other patches and travel costs ( Charnov, 1976 ), or to predict what succession of patches, with varying risks and rewards, to choose in order to maximize fitness over longer times ( Mangel and Clark, 1988 ; Houston and McNamara, 1999 ; Clark and Mangel, 2000 ), an approach referred to as Dynamic State Variable Modeling (DSVM) . 02.P03 \u00b6 advective-dffusion model ( Skalski and Gilliam (2000) ) e.g., fish formation purposeful kinesis population density gradients ( Flierl (1999) ) The step from modeling individuals to modeling collections of individuals could in some cases be done using mathematical models, in which all individuals follow the same basic rules that could be incorporated into PDEs. For example, Skalski and Gilliam (2000) used an advective -diffusion model to simulate patterns of fish formation in which the only decisions involved swimming fast or slow and having an upstream directional bias rather than pure random movement. However, many observed movement patterns of collectives, such as of flocks of birds, schools of fish, swarms of insects, and patterns formed by herding mammals, are more complex. Modeling these patterns requires more than movement decisions based on abiotic conditions, but they can be approximated when the PDEs also incorporate terms that represent decisions to move up or down population density gradients . Such individual movement behavior differs from random walk and can result in various patterns of collections of organisms ( Patterson et al., 2008 ). \u201c Purposeful kinesis \u201d can alter diffusive patterns ( Gorban and \u00c7abukoglu, 2018 ), leading to positive density-dependent diffusion, or \u201c super-diffusion ,\u201d and other variations on diffusion through dependence on population density ( Topaz and Bertozzi, 2004 ; Lutscher, 2008 ; Almeida et al., 2015 ; Tilles and Petrovskii, 2016 ). For example, the phenomenon of clustering, in insect swarms and fish shoals, can occur when individuals accelerate in the direction of a positive density gradient ( Tyutyunov et al., 2004 ). Flierl et al. (1999) provide a general review of mathematical modeling of collective behavior. 02.P04 \u00b6 IFD (Ideal free distribution) e.g., predator-induced defenses Holling type 2 functional responses The influence of individual decisions on the level of whole populations and multi-population systems can also be studied when decision-making is incorporated into models of classical ecology, if the decisions are limited to simple rules, such as optimization of fitness in foraging ( MacArthur and Pianka, 1966 ; Charnov, 1976 ), or in diet selection ( Pulliam, 1974 ) and life history ( Roff, 1992 ). For example, if all individuals foraging on a spatial environment of habitat patches with different resource levels are assumed to move among them until no further movement would increase their fitness, the population would reach what is called an Ideal Free Distribution (IFD) ( Fretwell and Lucas, 1969 ). The IFD concept applies to a wider range of decisions, such as the choice that organisms in a population have in allocating resources in different proportions to foraging, defense, reproduction, etc. A particular example is that of predator-induced defenses , which are known to exist in many ecological systems and may involve changes in both morphology and behavior. The defended individuals are still edible but less so than undefended prey and, as a tradeoff, have lower resource intake rates than undefended prey. Recently it has been shown that the existence of inducible defenses in species at the bottom or middle of the food chain can affect the stability of the food chain, as well as the ability of the top predator to exert top-down control on the system ( Vos et al., 2004 ). In DeAngelis et al. (2007) , a system containing a predator, a prey with an inducible defense, and a resource of the prey, was studied using a differential equation model, with Holling type 2 functional responses describing the trophic interactions. The prey could choose between allocation to induced defense, with the tradeoff of lower resource uptake. The prey population evolved to switching between defense and non-defense, leading to a balance in which certain proportions of prey were in the undefended state and the rest in the defended state, the proportions depending on predator density. The prey in each state had equal fitness, so this was an IFD. This strategy of the prey influenced the populations of the whole food chain. 02.P05 \u00b6 game theory Mathematical models have also represented simple decisions on allocation of time and energy to foraging vs. avoiding or defending against predators ( Abrams, 1982 , 1993 ; Lima and Dill, 1990 ; Abrams and Matsuda, 1993 ; Lima and Zollner, 1996 ; Werner and Peacor, 2003 ). Other models, using differential equations, describe a forager in an environment of several prey, in which the forager could choose which prey to feed on, based on a maximization of long-term food intake (e.g., Feng et al., 2009 ). Another approach of classical mathematical theory in ecology is game theory , which can be used to determine the optimal strategy of an individual when the expected pay-off of a decision (e.g., to \u201cfight or flee\u201d when in a confrontation) depends on the decisions made by other individuals ( Riechert and Hammerstein, 1983 ). In all these cases models showed that optimal decisions taken by members of the populations can have large ecosystem consequences. 03. Agent-based Modeling in Ecology \u00b6 03.P01 \u00b6 2 trends: variability w/i populations in behaviors e.g., behavioral tendencies, personality ABM (Agent-based modeling) The fact that mathematical or analytic models based on a few simple decision rules can explain even complex patterns is remarkable, and such modeling is still a lively and important area of theory. But behaviorists recognized that the capacity of these simple models to represent the real decisions of organisms, determined by a multitude of inputs, was limited, and that inclusion of decision-making was important ( Dill, 1987 ). Two trends in ecology that are relevant to that problem have accelerated over the past two decades. One trend is the increasing recognition of marked variability within populations, not just in age, size, or stage, but also in behaviors of individuals within given classes. In fact, individuals across many taxa appear to have their own personalities, or temporally consistent \u201c behavioral tendencies \u201d ( Biro and Stamps, 2008 ; Beekman and Jordan, 2017 ). The prevalence of behavioral differences, or different personalities, that exists across animal taxa was reviewed in a pivotal paper ( Bolnick et al., 2003 ). Personality differences such as \u201cbold\u201d vs. \u201cconservative\u201d behavior in fish (e.g., Blake et al., 2018 ) exist and involve various aspects of behavior, such as responses to intra-and inter-specific competition ( Ara\u00fajo et al., 2011 ; Dall et al., 2012 ), and tradeoffs between growth and mortality ( Biro and Stamps, 2008 ) and early and late reproduction ( Wolf et al., 2011 ). Bolnick et al. (2011) discuss several ways in which this individual-level specialization can affect community dynamics, which is an impetus to including individual differences in models. Variation in individuals, and therefore in their possible decision-making, casts further doubt on the capability of analytic models to adequately represent real populations. The second trend is the rapid spread of individual- or agent-based modeling (ABM) , which has become an established approach with numerous modeling platforms and is encompassed by a vast literature. The latter development may provide a solution to the impasse of creating models of sophistication comparable to what is known about decision-making. 03.P02 \u00b6 ABM simulate autonomous \"agents\" state var representing internal states (behavioral states) unique history of interactions w/ env & other agents ABMs are ideally suited to accounting for individual differences in organisms. First applied to tree communities ( Botkin et al., 1972 ), in the last few decades ABMs have become well established in all areas of ecology. ABMs simulate the interactions of autonomous \u201cagents,\u201d generally representing individual organisms or other real-world entities, with other agents and with the external environment ( DeAngelis and Mooij, 2005 ). In ABMs, every individual of a population can, in principle, be simulated to almost any level of detail. Each agent may have state variables representing internal states, including behavioral states, and each can have a unique history of interactions with its environment and other agents ( DeAngelis and Grimm, 2014 ). Agent-based modeling attempts to capture the variation among individuals that is relevant to the questions being addressed. In particular, it can incorporate what is known about individual decision-making to explore the consequences in population and community models ( Parunak et al., 1998 ; Railsback, 2001 ; Vincenot, 2018 ). 03.P03 \u00b6 deterministic probabilistic An ABM can be used where decisions are complex and/or are in a setting of populations or communities. The simplest and most straightforward way to represent individual decision-making in an ABM is to utilize logical rules following the \u201cif-then\u201d structure. The behavior of an individual can be modeled when the \u201cif\u201d part contains a condition, and the proceeding \u201cthen\u201d part presents the individual's response. The rules governing decision-making processes can be set up in various ways. Strictly logical, deterministic rules would assign only one possible behavior to an individual in a particular circumstance ( Grimm and Railsback, 2005 ). Alternatively, a rule may be probabilistic , with a different probability for each choice in an array of possible actions in response to some stimulus. Rules may also be a combination of probabilistic and deterministic. For many animals, however, decision-making processes may be more complex, and can be influenced by many variables including the internal state of the individual, that may vary among individuals. As will be discussed later, ABMs are increasingly being used in situations where organisms are assumed to have incomplete knowledge of the surrounding environment, and differences in perception and navigation capacities, but are able to employ cognitive skills to make choices that are good proximate decisions, albeit less than optimal. In this way, ABMs are able to encapsulate the basic underlying principles of the decision-making process in a more realistic way than classical models, and may more effectively represent the way individuals actually decide on actions and how the resulting behaviors shape population-level processes ( Figure 1 ). Figure 1 \u00b6 individual complexity proximate decision-making, uncerteainty, emergence Figure 1. Schematic representation of the models and methodologies utilized to model decision-making in ecology. Along the horizontal axis, these models and methodologies are able to introduce greater individual complexity and represent more complex interactions. Along the vertical axis, the models' entities can be characterized by more proximate decision-making . There is more uncertainty introduced along this axis, but also a greater level of emergence , or higher-level behaviors resulting from lower-level interactions. 04. Movement Decisions and Their Consequences \u00b6 04.P01 \u00b6 compoponents: internal state physiological psychological external env spatially-explicit navigational capacity motion capacity ABMs have been used extensively to simulate the movement behavior of both terrestrial and marine organisms, due in large part to their flexibility in incorporating the various components that influence an individual's movement through space. These components include an individual's internal state, external environment, and navigational and motion capabilities ( Tang and Bennett, 2010 ; Figure 2 ). ABMs can represent a variety of dynamic physiological and psychological state variables comprising an organism's internal state, including commonly used bioenergetic variables (Tang and Bennett, 2010). When simulating movement through space, the external environment is generally spatially-explicit , often represented as grid cells, such that the location of agents and environmental attributes and the spatial relationship between them is explicitly defined ( Duning, 1995 ; Bauer and Klaassen, 2013 ). Figure 2 \u00b6 Figure 2. Schematic illustrating the major components influencing animal movement, all of which can be represented within an ABM ( Baguette et al., 2014 ). 04.P02 \u00b6 2 important decisions: when to move where to move By portraying an individual's dynamic environment and internal state in detail, ABMs can capture the two important decisions that an individual in motion must continually make: when to move , and where to move . The models can also be used to derive the consequences of many organisms moving and interacting with each other at the same time, which gives rise to spatial patterns. 04.01. When to Move? \u00b6 04.01.P01 \u00b6 internal state cond of the current area presence of competition & predation temporal changes prompt agents moement e.g., trout, bear ABMs for movement generally incorporate rules that dictate when an individual agent decides to move from its current location. For real-world organisms, the onset of movement at fine scales may depend on the individual's current internal state, including its physiological and psychological conditions, the condition of the current area that the individual is in, and the presence of competition and predation ( Semeniuk et al., 2011 ; Martin et al., 2013 ; Doherty and Driscoll, 2017 ). As such, ABMs simulating the fine-scale movement of individuals often keep track of temporal changes in the individual agent's internal state and its local surroundings . These changes generally prompt agent movement if they somehow increase (or at the very least, not decrease) the agent's fitness. For example, brown and rainbow trout agents decide to move from their position if the previous day's calculated ratio of mortality risk to growth is greater than expected. Thus, at a given time step, fish agents might move to maximize the ratio of growth to risk of mortality ( Van Winkle et al., 1998 ). Bear agents in an ABM simulating human-bear interactions decide to move to a new location if their current location has a low amount of food, or if they are threatened by human activity ( Marley et al., 2017 ). 04.01.P02 \u00b6 changes of many individuals through time influence movement e.g., green woodhoopoe An individual's decision on when to move may be influenced by the individual's life cycle and social factors. ABMs can track important biological changes of many individuals through time and be utilized to explore how these changes influence movement when placed within a social context. For example, Neuert et al. (1995) developed a model of the territorial group-living green woodhoopoe ( Phoeniculus purpureus ) to address the question of when a subdominant (and thus non-breeding) individual should decide to leave the group and scout for a territory on which it could breed, vs. waiting around to become high enough in status to breed at its natal site. In the model, the decision was based on its own age and rank, where the rank is correlated with age . The simple decision trait of higher propensity to go on scouting forays with increasing age provided the best agreement with empirical data. 04.01.P03 \u00b6 temporal changes in recoucese availability other factors e.g., timing of pink-footed geese migration are influenced by factors: minimal body stores maximal stores date temperature plant phenology fixed duration of stay Large scale movement, such as migration, may be triggered by temporal changes in resource availability ( Van Moorter et al., 2013 ) along with a number of other factors, and ABMs have been utilized to explore the potential decision-rules that may dictate the timing of such behavior for various species. For example, ( Duriez et al., 2009 ) assumed the following factors to be important for the timing of pink-footed geese migration; (1) having minimal body stores, (2) having maximal stores, (3) date, (4) temperature, (5) plant phenology, and (6) fixed duration of stay. The authors found that decision-rules related to food resources were important for dictating the onset of migration, but later in the season, decision-rules related to the geese agents' internal clocks and date are likely used. 04.01.P04 \u00b6 range expansion of population 3 phases initial probability of offspring dispersing from natal cell depending on population density transfer probability dependind on landscape composition of the cell & neghboring animals settlement probability depending on habitat suitability, presence of potential mate, density of conspeceifics Movement is also involved in the range expansion of a population , and Bocedi et al. (2014) modeled range expansion by considering three phases. Within the ABM, there is an initial probability of offspring dispersing from a natal cell , which can depend on population density. Then there is \u201c transfer probability ,\u201d the direction of which is weighted by the costs of moving to each adjacent cell, which depends on landscape composition of the cell and neighboring animals. In the final phase, four alternative strategies were compared to determine which best described settlement probability , and each of which was based on a combination of several factors, including habitat suitability, presence of a potential mate, and density of conspecifics . 04.02. Where to Move? \u00b6 04.02.P01 \u00b6 navigation capacity: links internal state and ecternal var Many organisms can process information about their environment and make movement decisions to satisfy internal desires. Changes in the internal state of an organism may result in changes in the organism's goals, movement decisions, and subsequent movement behavior ( Tang and Bennett, 2010 ). When deciding where to move, mobile animals rely on their navigation capacity, which links the animals' internal states and external variables and manifests itself as either non-oriented, oriented, or memory-driven movement ( Nathan et al., 2008 ; Doherty and Driscoll, 2017 ). The ABM framework lends itself to representing dynamic environmental cues, particularly when the spatiotemporal relationships between the agent and the environment is explicitly represented, as in spatially explicit ABMs. The internal states of individuals can be represented as dynamic state variables and integrated with the cognitive capabilities of individuals, which allows model agents to assess various movement decisions within complex landscapes and ultimately decide on their next destination ( Tang and Bennett, 2010 ). Additionally, some stochasticity in selecting an area to move to is incorporated within many ABMs by using a combination of probabilistic and logical rules, reflecting imperfect knowledge of the environment and perception capabilities. 04.02.P02 \u00b6 e.g., caribous destination cell influenced by daily energetic state reproductive energy requirement predation risk jaguars energy reserves (internal state) hippopotamus Many ABMs simulating animal movement explicitly represent and track various components of an individual agent's internal state in detail, often resulting in movement characteristics that closely mimic those of real-world organisms. For example, Semeniuk et al. (2012) explored potential habitat-selection strategies employed by woodland caribou in response to industrial features in the landscape and represented the caribou's internal states by primarily tracking an individual's energy gain and loss. The caribou's decision on selecting a destination cell was influenced by its daily energetic state, reproductive energy requirement, and predation risk. The way that the internal state of the agent influenced movement varied for each alternative habitat-selection strategy. The authors found that the behavioral strategy concerned with balancing daily energy intake, conserving energy for reproduction, and minimizing predation risk agreed with real-world data better than the other strategies ( Semeniuk et al., 2012 ). Watkins et al. (2015) kept track of energy reserves of jaguar agents within a model landscape representing central Belize . The landscape cells were characterized by attributes including food availability, the presence of marks from other jaguars, and the presence of roads. When agents decide to move, their decision-making process concerning cell selection depends on the habitat attributes underlying a specific cell and the agent's internal state, namely, energy reserves . The individual jaguar's energy reserve levels modulate the preference of different attributes; consequently, agents with high energy reserve levels may decide to move to a cell that does not necessarily have high food availability; see also Lewison and Carter (2004) for an ABM simulating hippopotamus foraging behavior. 04.02.P03 \u00b6 resistance cost When information on the dynamics of an individual's internal state is lacking, it may be appropriate to simulate movement by using simple decision rules based on empirical observations of the resistance that different habitat types may confer to the movement of real-world individuals. For example, Aben et al. (2014) developed and explored the effectiveness of an ABM in simulating forest bird movement, in which a bird agent's selection of a cell (habitat area) to move to at any given time step was partially determined by the land-cover class that characterized a given cell. Land-cover classes conferring more resistance to movement were given higher \u201ccost\u201d values . Spatial cells characterized as having a low cost to birds that are moving through the cells had a higher probability of being selected than cells characterized as having a high cost. Similarly, simple decisions rules governing the selection of destination cells may be based on the quality of the surrounding environment, such that agents generally move toward preferred or favorable areas. In an ABM developed to estimate landscape connectivity for bighorn sheep, each cell in the landscape is represented by landscape attributes including its proximity to escape terrain and the presence of roads ( Allen et al., 2016 ). Bighorn sheep agents have a higher probability of moving to cells closer to escape terrain and away from roads as these cells represent more favorable habitats to real-world bighorn sheep. The characteristics of the surrounding environment also played a major role for agents deciding on a destination in ABMs simulating movement for tiger ( Kanagaraj et al., 2013 ), tortoise ( Anad\u00f3n et al., 2012 ), and capercaillie ( Graf et al., 2007 ). 04.03. Collective Movement Behavior \u00b6 04.03.P01 \u00b6 single-agent multi-agent key to understanding collective behavior: principles of the bahavioral algorithms followed by individual how info flows between animals Many of the above models of animal movement are single-agent ABMs, as opposed to multi-agent ABMs used to simulate collections of interacting individuals. One of the enigmas of natural history is the striking coordinated collective movements of various taxa (birds, insects, fish, mammals, etc.). Investigation of these phenomena is based on the recognition that the movement characteristics of individuals are often influenced by the spatial position and movement of conspecifics, which results in collective movement ( Krause et al., 2010 ). Individuals must make decisions on the initialization and direction of movement, and these decisions somehow incorporate the states of their neighbors. As Sumpter (2006) notes, the key to understanding collective behavior lies in identifying the principles of the behavioral algorithms followed by individual animals and how information flows between the animals . The decision-making process of individuals involved in collective movement has been represented by a spectrum of simple to complex rules within ABMs. 04.03.P02 \u00b6 fish school ( Huth and Wissel (1992) ) rupulsion () attraction () Couzin 2002 : repulsion (move away from nearby neighbors) alignment (adopt the same direction) attractin (avoid becoming isolated) Both Sumpter (2006) and Couzin and Krause (2003) provide reviews of the use of ABM in describing collective behaviors of organisms based on relatively small sets of decision rules. In an early use of a spatially explicit ABM, Huth and Wissel (1992) showed that the complex movement of fish schools could be described by a few rules; the fish tend to keep a certain distance between themselves and a number of its close neighbors and move parallel to other fish when they are within a certain range. The fish agents decide to move away from neighbors if they are too close (repulsion) or align themselves with their neighbors if they are too far (attraction). Size sorting also occurs in fish schools, which may be represented by simple rules, where individuals tend to actively stay with individuals of similar size and avoid those of a different size ( Hemelrijk and Kunz, 2005 ). Couzin et al. (2002) proposed a similarly simple model in which individual animals followed three rules of thumb; (1) move away from very nearby neighbors, (2) adopt the same direction as those that are close by, and (3) avoid becoming isolated; thus, the authors utilized the simple rules of repulsion, alignment, and attraction. In taking into account the location of an individual's neighbors at all times, the authors' model was able to reproduce collective behavior often seen in nature, including swarm ing and torus behavior. 04.03.P03 \u00b6 Gueron 1996 : stress zone attraction zone neutral zone rear zone Hoare 2004 : \"zones of interactions\" influence the size of groups Dynamics of a small herd of mammal browsers were described by a more complex decision tree than those used by Huth and Wissel (1992) , and Couzin et al. (2002) . Gueron et al. (1996) integrated a combination of \u201cstress zones,\u201d \u201cattraction zones,\u201d \u201cneutral zones,\u201d and \u201c rear zones\u201d in the decision-making process of individuals moving within a herd. The various zones corresponded to specific distances extending from each individual, and each individual could invade the zones of other individuals. Probabilities and directions of movement depended on the type of zone an individual invaded. For example, if an individual sensed another individual within its \u201cstress zone,\u201d the individual slowed down to avoid collision. If neighbors were all on one side of the individual, the individual moved toward neighbors. Even though this sort of collective behavior is a much looser kind of group cohesion than fish schooling, the ABM showed that by simply integrating information about an individual's neighbors into the decision rules, many patterns of collective movement could emerge. Hoare et al. (2004) used a similar model to explain the group size distributions of fish, where dynamic \u201czones of interactions\u201d strongly influence the size of groups in simulations. 05. Foraging Decisions and Population Interactions \u00b6 05.P01 \u00b6 temporal / spatial patterns The use of ABM in modeling foraging behavior changes the emphasis from predicting the fitness of the individual based on its decisions to predicting temporal and spatial patterns formed by large collections of individuals foraging and competing for resources (though sometimes interacting positively). 05.P02 \u00b6 observations of real population in real locations A common, though not universal, characteristic of ABMs is that they are based on observations of real populations in real locations. Thus, the assumptions built into the models constitute hypotheses that can be tested against observational data. A few models will be mentioned here: Barnacle geese (Branta leucopsis) in Helgeland, Norway, white-fronted geese (Anser albifrons) in Lake Miyajimanuma, Japan, oystercatchers (Haematopus ostralegus) in the Exe estuary, England, salmonids in Little Jones Creek, California, USA, and dusky dolphins (Lagenorrhyncus obscurus) and killer whales (Orcinus orca) near Kaikoura, New Zealand. 05.P03 \u00b6 geese Bioenergetics model Kanarek et al. (2008) modeled barnacle geese that return to the same group of islands each year as a migration stop. The geese are assumed to vary in age, energy reserves, genetic disposition, and spatial memory of previously visited locations. A goose chooses a specific island based on a combination of factors; constraint due to rank in the dominance hierarchy, memories of previously visited sites and past reproductive success, inherited genetic influences toward site faithfulness, and knowledge of the available biomass density. Within the island the goose chooses to forage on a particular patch until its intake rate drops below a certain amount. Once a goose decides to leave a patch, it uses its knowledge to choose where to go next, assigning a score to a patch based on several factors. A bioenergetics model keeps track of the body mass of the goose, and the goose leaves the staging area either when a threshold of the amount of energy stored or when the end of the stopover period is reached. Each year that a goose returns to the same patch, its familiarity and ability to locate food increases as well as its hierarchical rank. 05.P04 \u00b6 dominance hierarchy A dominance hierarchy is also incorporated into the ABM of a migratory bird, the oystercatcher , feeding on mussels in a tidal estuary in England during the winter ( Stillman et al., 1997 ). A population of individual oystercatchers was modeled on a two-dimensional patch, and the individuals were assigned to places in a dominance hierarchy. When two individuals come within a certain distance, and one is handling prey, the other might attack to attempt to steal the prey, prompting the other to fight back, with the dominant individual always winning. The alternative behavior is avoidance, an action taken with higher probability by less dominant individuals. Avoidance subtracts less time from foraging than fighting. The authors assumed that individuals could calculate the costs vs. benefits of a given action and decide accordingly. The results of the model show, in accord with observations, much less incidence of interference than equivalent analytic models, because, given the differences in rank that are incorporated in the ABM, dominants wasted less time avoiding subdominants, and subdominants avoided fighting. 05.P05 \u00b6 white-fronted geese choosing patches already occcupied by conspecifiecs In the case of white-fronted geese, an ABM explored possible positive interactions among the geese ( Amano et al., 2006 ). The model tested the hypothesis that geese foraging in patches usually have no prior information on patch quality before making a choice but can benefit from choosing patches already occupied by conspecifics; i.e., the presence of conspecifics in the patch has the potential positive effect of decreasing the need for vigilance , although it also causes prey to be depleted faster. The authors tested the hypothesis and found that it was better than alternatives foraging models at describing empirical data, indicating that the assumptions are realistic bases of foraging decisions. 05.P06 \u00b6 tradeoff: starvation vs. predation EM (expected maturity) : maximize growth & survival In the classical DSVM theory mentioned earlier, tradeoffs are assumed between risks of starvation and predation. Many ABM foraging models also incorporate these risks. Railsback and Harvey (2002) used ABM to predict empirical patterns of habitat selection in drift-feeding juvenile trout foraging in a stream, based on assumptions concerning their decisions to choose habitat to forage in, given a range of empirical factors; e.g., water depth, velocity, available food, in the stream environment. The risks involved in deciding to move to a given location were starvation, predation by terrestrial animals, predation by fish, high water velocity (which requires energy to stay in place), stranding , and competition. A bioenergetics model was used to follow daily growth, and daily predation mortality depended on the size of the fish and water depth at its location, which gave the current \u201cstate\u201d of the individual. Three foraging strategies were compared, based on the agent's state and prediction of conditions over the near term (thus the authors label their approach \u201cstate and prediction theory\u201d). The strategy that compared well with all observed patterns involved daily decisions that maximized, over a longer term, the \u201cexpected maturity\u201d (EM) , or the product of predicted survival from starvation and other mortality risks and the fraction of reproductive size reached over the period of the 75-day simulation. The EM strategy, in which the fish agent maximizes both growth and survival, gives better results than other maximization criteria, such as maximizing only growth or only survival probability, used in many classical models. 05.P07 \u00b6 These studies all illustrate the ability of ABMs not only to incorporate high amounts of the complexity of real ecological systems and their ability to compare output with empirically observed patterns, but also to derive more realistic pictures of the decision-making process . 05.P08 \u00b6 importance of behavioral decisions of the predator Lima (2002) noted the importance of the behavioral decisions of the predator when studying predator-prey interactions, rather than treating predators as \u201cunresponsive black boxes.\u201d Accordingly, ABMs have also been used to extend the focus of predation risk beyond merely the prey strategy to include feedbacks from the predator or from competitors. A detailed investigation of tradeoffs of dusky dolphins choosing between feeding and avoiding predation by killer whales is given in a spatially explicit ABM of Srinivasan et al. (2010) . The authors explored the fitness costs and benefits of various escape strategies potentially used by dusky dolphins, including the time spent hiding in a refuge from predation and degree of vigilance when feeding. The model included counter strategies of the amount of time a killer whale would wait for a dolphin to emerge from a refuge , so the model contains a version of game theory. Another example of feedback effects, in this case coming from competition, is illustrated by a model of Peacor et al. (2007) . Individual foragers and a common resource for which they compete were simulated, both with and without the presence of a predator. The density of competing foragers affected the tradeoff in foraging. Under low density of competitors, the forager spent 40% of its time eating, but the time eating decreased to 7% when the forager density was high. This outcome resulted from the decreased benefits of foraging, as the resource was reduced by competitors. Such an outcome could not easily be anticipated or incorporated in simple models. A species' response to predation risk can affect whole food chains. An illustration of how behaviors of herbivores can modify top-down effects of predators on plants is the ABM of Schmitz and Booth (1997) for a tri-trophic chain (spiders, grasshoppers, and a food base of two plant types, grass and a herbaceous plant) of individual organisms on a spatial lattice. The spiders could affect the grasshoppers directly, by predation, and indirectly, by causing grasshoppers within a certain radius to move from grass to the safer herb sites. The model allowed the relative effects of direct predation and the predator avoidance by grasshoppers to be examined individually. The model predicted long-term field observations of the predator's top-down effect and showed the importance of including the behavioral response of the herbivore. 06. Social Interactions in Populations \u00b6 06.P01 \u00b6 reproduction parental care territorial / home range defense Social behavior requires decisions involving, among other things, reproduction, parental care, and territorial or home range defense. These, plus a wider range of activities, have been represented in ABMs. 06.P02 \u00b6 population dynamics is influenced by landscaoe structure & life history trait ( Ye (2014) ) Several studies have addressed the question of when to reproduce, incorporating external factors such as population density and the availability and quality of resources in an area ( Stewart et al., 2005 ; Brouwer et al., 2015 ). When deciding to reproduce, individual agents within an ABM often consider information of the surrounding environment. An individual agent's decision to reproduce is frequently represented in a logistical or probabilistic manner, in which agents may decide to reproduce if they meet some requirement or threshold. For example, Ye et al. (2014) used a grid-based spatially explicit ABM to gain insight on how the population dynamics of a virtual species is influenced by landscape structure and various life history traits. In the ABM, the resource share of an individual reflects the number of individuals in a spatial cell and the cell's underlying habitat quality. Only individual agents with a resource fraction over a given threshold may reproduce in a given time step with a given probability, reflecting the importance of competition and the quality of the surrounding habitat in driving reproductive success. 06.P03 \u00b6 The agent's internal state also influences its decision to reproduce. Hancock et al. (2005) developed an ABM to predict the population dynamics of Bornean bearded pigs, tracking the \u201cfatness index\u201d of each pig agent in the model. The reproduction status of each agent varied according to their respective fatness index, which in turn depended on the available food supply and the density of the pig population. A pig agent decides to reproduce if its fatness index is above a specific threshold, such that agents in a large population with low food supply are unlikely to reproduce due to low fatness indices (Hancock et al., 2005). Fish agents within another ABM also took their internal state into consideration in deciding when to spawn. The ratio of actual weight to expected weight of a fish of a given length (a \u201ccondition index\u201d) is tracked for each fish agent, and only agents with ratios over 0.75 were able to spawn (Rashleigh and Grossman, 2005). 06.P04 \u00b6 After reproduction, decisions must be made on parental care. Different strategies of parental investment reflect variation in the decision-making process of individuals, and ultimately have consequences for the dynamics of the population. Decisions involved in provisioning of offspring by bluebirds were studied by Davis et al. (1999), from the point of view of survival of the offspring under different conditions of food availability in the environment. The authors performed model simulations for different levels of food availability by assuming four different strategies on feeding decisions: (1) Feed the smallest first, (2) feed the largest first, or (3) feed the hungriest first, in each case going to the next larger, smaller, or less hungry, respectively, if the offspring representing the first choice was full. These choices were compared to random feeding (strategy 4). Environmental conditions determined which strategy maximizes the nest success, in terms of total biomass of surviving fledglings, with strategies (1), (2), and (3) performing best, successively, with increasing food availability. The random strategy was never the best. 06.P05 \u00b6 The formation and maintenance of territories and home ranges occurs in many taxa. ABMs can simulate the spatiotemporal changes in resource availability throughout the environment and interactions between various agents through time, potentially elucidating the mechanisms underlying territory and home range dynamics. In order to aid conservation planning for the tiger (Panthera tigris) by providing an estimate of population number, location and size of territories, Carter et al. (2015), modeled territory formation of both females and males using a spatially explicit ABM. Females at 3 years of age were assumed to move to a site within 33 km of their natal site, and at least 2 km away from any other female, and to establish the center of her territory where prey density was highest. If the female failed to find such a site, she lowered the limit on neighboring females from 2 to 1 km. A female was assumed to be able to sense the total prey in her territory and in neighboring spatial cells. She could try to add habitat cells to their territories based on a few decision rules, including prey availability and the status of the neighboring female tiger's hierarchical status (avoiding cells near a female of higher status, which is correlated with age). When possible, she adds habitat cells to her territory until the total amount of prey reaches a certain threshold. Another spatially explicit model of home range dynamics is that of Wang and Grimm (2007) for the common shrew (Sorex araneus). Shrews were assumed to continually adjust their territories by sensing the amount of food in a cell of the habitat and the presence of other individuals. Individuals tried to optimize their home range by preferentially selecting cells with high food sources and avoiding cells occupied by other individuals. Acquisition and release of cells followed an optimization procedure, in which the shrew is assumed capable of ranking all the cells in its territory and releasing the worst ones in favor of adding new ones. These simple model rules predicted realistic territories for both tigers and shrews. 07. Developments in the Modeling of Decisions in Population Models \u00b6 07.P01 \u00b6 The papers reviewed above show that ABM is an important departure from earlier mathematical approaches, which, collectively, have been critical theoretical frameworks over the past few decades. In many earlier approaches, the individual was assumed to make a sequence of decisions through time that would result in optimal fitness at some end time. These approaches often assume knowledge of future conditions and ignore the feedbacks on the individuals through changes in the environment created by the individual's own decisions. To incorporate the feedbacks and uncertainty that are present in real ecological systems, a trend in ABMs has been to simulate how proximate decisions are made based on the individual's current state and short-term predictions. This is modeled differently by different modeling groups. As noted earlier, Railsback and Harvey (2002) (see also Railsback et al., 1999) use a \u201cstate and prediction theory\u201d strategy for individuals, which estimate an \u201cexpected survival\u201d over a specified time horizon over which little appreciable change in the environment is expected, considering the risks of predation, starvation, etc., and choosing the behavior that achieves the best fitness over that time. Giske and his group (e.g., Eliassen et al., 2016) find the adaptive behaviors for situations encountered through selection in genetic algorithms (GA) and artificial neural networks (ANNs). Other ABM modeling approaches include the computational system Digital Organisms in a Virtual Ecosystem (DOVE), in which a GA is used to allow phenotypic plasticity to evolve and interact with a dynamic environment (Peacor et al., 2007). 07.P02 \u00b6 Borrowing concepts from machine learning and artificial life studies, modelers such as those noted above have utilized ANN and GA (together referred to as individual-based neural-network-genetic algorithm, or ING techniques) to represent the decision-making processes and consequent behaviors of organisms. Instead of fixed rules governing the way an organism makes decisions, ING techniques are flexible approaches that use principles of neurobiology and natural selection to solve optimization problems, resulting in individuals making adaptive decisions (Huse et al., 1999; Hamblin, 2013). ABMs that use genetic algorithms to govern decisions initialize a population of individuals with different solutions to a given optimization problem. Optimization problems may include finding the decision that will maximize fitness during foraging, or the probability of survival when selecting a patch (Hamblin, 2013). The individuals with the best solutions to the problem are allowed to reproduce through various commonly recognized selection operators, and reproduction continues for many generations. Trebitz (1991) was an early user of a GA-type approach to estimate the optimal spawning time for largemouth bass. In the model a female that spawned too early in spring risked the loss of eggs or larvae due to random cold snaps, while if it spawned too late it risked zooplankton being depleted by the offspring of earlier spawning females. 07.P03 \u00b6 ANNs \u201ctrain\u201d the weights of input data by continuously modifying these weights until the resulting decisions and subsequent behaviors of individuals reach a specific fitness measure. In this way, ANNs and the way they capture the decision-making process of individuals mirror brain functions by \u201clearning\u201d from the outputs of the various input data weight modifications. Once an ANN is trained, it can be used on new input data to determine the decisions and behaviors that satisfy the specific fitness measure (Huse et al., 1999; Lek and Gu\u00e9gan, 1999). In GAs and ANNs, as in the more fixed logical and probabilistic decision-making rules within ABMs discussed earlier, the decision-making processes of individuals are generally geared to optimize some sort of fitness measure, but the decision-making strategy evolves through selection. 07.P04 \u00b6 ANNs have been increasingly used to represent the decision-making processes of organisms, particularly with respect to movement through space. As the decisions of real-world individuals are driven by neuronal responses to internal and external stimuli, neural networks are well-suited to represent decision-making in an individual agent (Eliassen et al., 2016). Once a fitness criterion is identified for a given ABM, the ANN can be trained to find the input weights that correspond to outputs best fitting the fitness criterion (Huse et al., 1999). ANN training can occur in several ways, but here we focus on training with GAs, an approach common within ABMs utilizing ANNs. GAs are optimization tools that utilize the principles of crossing over and mutation to essentially \u201cevolve\u201d the decision-making processes of individuals, without the need for probabilistic or logistical rules. 07.P05 \u00b6 For example, Okunishi et al. (2009) developed an ABM to simulate the growth and large-scale movements of Japanese sardines with the goal of exploring the effects of climate change on the population dynamics of the species, primarily its distribution and production in the North Pacific. The authors utilized an ANN to represent the way fish decide on movement directions when migrating to spawn. The inputs to the ANN were environmental factors known to influence the migration of sardines and included ocean current speed, the distance from land, and the experienced temperature change during subsequent days. Back propagation (a training method where weights are assigned to the various inputs based on a training data set) was one method used to train the ANN on spawning migration trajectories from actual sardines. The authors also used a GA to find the sardine offspring's input weights that would produce optimal outputs through crossing over and mutation of the parent sardine's weights. In training the ANN with a GA, the sardines' decisions representing swimming directions were allowed to evolve through many simulations; subsequently, the authors were able to find optimal combinations of weights that produce realistic migration trajectories. They found that combining back propagation with a GA to determine input weights produced the most realistic migration trajectories, indicating that utilizing the principles underlying the GA (in tandem with observed movement data) adequately captures the decision-making process of migrating sardines (Okunishi et al., 2009). 07.P06 \u00b6 Morales et al. (2005) utilized several ANNs, each in combination with a GA, to determine efficient movement decisions for elk within a spatially explicit ABM. Movement decisions to be made by the elk included when to switch behaviors from foraging to exploring, where to move if foraging or exploring, and which type of plant to consume at a given time if foraging. The fitness measures that characterized the efficiency of elk agent decisions were energy gain for fat reserves and survival probability associated with predation risks. Percent body fat, forage biomass, and local predation risks were a few of the ANN inputs representing both internal and external movement stimuli. Rather than setting specific rules to guide the decision-making processes, the authors allowed the process to evolve over many generations. 07.P07 \u00b6 ANNs were also used to model movement in Mueller and Fagan (2008), who noted three basic types of pattern resulting from movement (or lack thereof): sedentary, migratory, and nomadism. The authors assumed that landscape structure drove individual-level movement types and that there were four gradients in hierarchical order; resource abundance, spatial configuration of resources, temporal variability of resource locations, and temporal predictability of resources. Mueller et al. (2011) followed up by using ANNs to evolutionarily train model organisms to use and combine different types of information representing the different movement behaviors (memory, oriented, and non-oriented movements). 240 individual animals were simulated moving across landscape and making choices of patches and, in doing so, depleting exhaustible resources. Different movement strategies involving combinations of these movement types worked better on different landscape types. After 5,000 generations with inheritance, survivors using unique movement strategies had optimized fitness on particular landscapes. 07.P08 \u00b6 In incorporating optimization of fitness, classic analytic models omit consideration of the immediate, or proximate, complexities that organisms encounter. This allows the integration of individual strategies with ultimate fitness, but the proximate mechanisms through which organisms solve problems are largely ignored (Sih et al., 2004; Fawcett et al., 2013; Eliassen et al., 2016). Animals need to be able to respond quickly and adequately in situations they have never experienced before; that is, the world is too complex for evolution to produce rules for every possible circumstance. It is likely that animals will evolve rules that perform well on average in their natural environment (McNamara and Houston, 2009). One of the basic advantages of ABMs is their ability to incorporate such heuristic, or rule of thumb, decision-making to perform well in complex environments. The GA approach allows one to simulate how such rules evolve. 07.P09 \u00b6 Toward this end Giske et al. (2013) formulated a model based on recent insights from a range of empirical disciplines that sheds light on the processes involved with decision making. In vertebrates, multipurpose rules are arbitrated through an \u201cemotion system,\u201d which describes the integration of information, motivation, and physiological state in determining physiological and behavioral outcomes. These outcomes affect the survival, growth, development, space use, and life history of the organism. Giske et al. (2013) modeled fish, where the fish had the choice of moving among different depths. Fish at deeper depths were generally safer from predation, but food was more limited, though uncertainty was entered into the model by letting both food levels and predation risk vary stochastically from fish generation to generation. The authors employed the survival-circuit concept (LeDoux, 2012), in which emotions form the basis for decisions, and they contribute to the survival or fitness of the organism. The first half of the survival-circuit is \u201cemotional appraisal.\u201d It starts with sensory input, considers motivational impact related to developmental stage, and may potentially activate the organism into a \u201cglobal organismic state,\u201d such that the whole organism is focusing on the situation. The second half of the survival circuit; the \u201cemotional response,\u201d consists of physiological responses and behavior. Physiological activation enables the organism to focus its sensory attention, brain activity, and potentially also bodily functions, such as heartbeat and muscle tension, toward the present situation. Fear and hunger are the emotions, and the ABM includes sensory inputs. The options for a fish are to stay at its current depth or move a short distance upward or downward, as determined by equations incorporating hunger and fear. On the basis of its global organismic state, the motivated fish behaves in a way that maximizes its net neuronal response. 07.P10 \u00b6 The model of Giske et al. (2013) predicted the same general type of spatial distribution patterns that classic optimization and game models produce, such as diel vertical migration with some extension around the average. But the model of Giske et al. (2013) differed from the classic models by allowing the fish to respond on immediate timescales to food and perceived risk, as opposed to optimization criteria involving only long-term goals. For example, danger is avoided because of an evolved proximate preference to stay with others or in darker waters when afraid, while danger is largely ignored when hungry. This is important in a fluctuating environment, where simple rules of thumb couple proximate constraints in determining behavior, with long-term adaptive value. 08. Prospectus \u00b6 08.P01 \u00b6 In his book \u201cSociobiology,\u201d Wilson (1975) foresaw the gradual merger of population biology and behavioral ecology and the growing importance of neurophysiology to the latter (Wilson's Figure 1.2). These developments have been accelerated through ABM and ANN, particularly through using ABM as a means through which population and community dynamics emerge from the adaptive traits of individuals, including \u201chow individuals make decisions in response to other individuals, the environment, or changes in themselves\u201d (Grimm and Railsback, 2005). 08.P02 \u00b6 Future development of ABM in modeling decision-making in ecology will be influenced by the continued refinement of modeling methodology and increases in needed data for parameterizing ABMs. Two of the modeling methodologies, the \u201cstate and predict\u201d (Railsback and Harvey, 2002) and ING (Eliassen et al., 2016) continue to be developed. In particular, a cognitive architecture based on the survival-circuit concept (LeDoux 2012) forms a general modeling framework for linking decision-making to neurobiological mechanisms (Bach and Dayan, 2017; Landsr\u00f8d, 2017; Budaev et al., 2018). Other approaches to providing rules of thumb for individuals, such as Fuzzy Cognitive Maps (FCM), based on Fuzzy Set Theory (Berkes and Berkes, 2009), are also being used in evolving predator-prey systems. In addition, off the shelf ABM modeling programs such as NetLogo and Ecobeaker are making development and use of ABM easier (e.g., Railsback and Grimm, 2011). Concerning relevant data, ABMs generally require a large amount of data at the individual level and site level. Such data are generally only available for ecological systems that have been intensively studied. But technology is rapidly increasing data collection capabilities, and the availability of remote sensing data and the ease with which these data are integrated within the ABM framework facilitates the development of decision rules based on a plethora of environmental attributes that potentially drive animal decisions. This may facilitate wider use of ABM, though this will require more coordinated efforts in making data broadly available to the modeling community (Hampton et al., 2013). 08.P03 \u00b6 Recent advancements in technological tools should facilitate the parameterization of future ABMs developed for simulating dispersal, migration, and local-scale movements of animal groups. In particular, developments in GPS telemetry (and satellite telemetry in general) and biotelemetry devices have allowed for remote tracking of an individual's movement, physiological state, and behavior over long periods of time at relatively fine temporal resolutions (Cooke et al., 2004). This is especially useful considering the challenges ecologists face when attempting to directly observe and study free-ranging animals in a non-invasive manner. Basic movement parameters, including step length and turning angle distributions, can be derived from a time series of relocation data obtained from GPS and satellite telemetry and together shed light onto the limits of an individual's motion capacity. Subsequently, parameters driving agent movement within an ABM can be made more accurate. Bioenergetic models used to track agents' growth and reproduction within ABMs can be further parametrized with information gained from biotelemetry monitoring of undisturbed animals in their natural environments. The wealth of data generated for many species thus far should allow ecologists to more accurately model how individuals decide on an action in response to dynamic internal and external variables. What will most define the future success of ABM will be its ability to address complex questions that are difficult for traditional approaches. These questions include, among many others, the origin of collective actions, the interactions of tradeoffs at the individual level and the whole ecological system, the dynamics of complex social systems, and applications to conservation and management issues. 08.P04 \u00b6 How collective actions such as migration are initiated, usually by a small fraction of leaders, has been discussed as resulting from differences in experience within populations (Ferno et al., 1998; Krause et al., 2000). Collective actions that benefit the whole group, but which may have negative fitness consequences for individuals, have long attracted attention, but are still not well-understood and may profit from use of ABM. An example is \u201cmobbing action\u201d by birds against nest threats. Unrelated birds and even birds of different species may be involved in these aggressive actions against a threat to nests, such as hawks or owls. While there is clear benefit to each individual in protecting its nest, there are also costs, such as becoming a victim of the threat or revealing the location of one's nest. Wheatcroft and Price (2018), in an empirical/theoretical study, proposed that collective action in this case can build up among birds whose nest locations are distributed across various distances from the threat. Birds nearest the nest respond first, and others that have nests farther and farther away decide to join as the risk to any individual declines with the growing size of the mob. The authors used a mathematical model with two individuals to analyze the amount of investment in each in aggressiveness to the threat. While the mathematical model provides insight, this is the type of problem where an ABM, which can simulate the actions of many birds with individual variation, could provide output directly comparable with observations of mobbing behavior. 08.P05 \u00b6 Social interactions of at least moderate complexity already occur in several of the models noted above, in which social hierarchies constrain actions, but extension to much more complex interactions is possible using ABM. For example, in many primate troops the type and level of interaction between individuals, whether positive (cooperation) or negative (e.g., fighting) depends on genetic relatedness. Because there is kin-recognition, any interaction, such as a fight between two unrelated individuals, can give rise to a larger complex of interactions involving their relatives (Cheney and Seyfarth, 1992). Modeling the behavior of such societies may require ABM to represent a network of relevant interactions. 08.P06 \u00b6 Behavioral ecologists and modelers like to think that their work can have some useful applications in conservation and wildlife management. A review by Caro (2007) of application of ecological theory found little evidence that many of the applications proposed so far by behavioral ecology, based on hypothetical conditions, would be effective in practice. However, ABM may be able to bridge the gap between theory and real conservation issues (Wood et al., 2015). For example, to elucidate whether specific predation strategies influenced the nesting success of waterfowl, Ringelman (2014) simulated the movements of different predators in a landscape with clumped and dispersed nests. The results of the ABM suggested that managers concerned with nesting success of waterfowl should consider the various behavioral strategies used by nest predators when creating and managing potential nesting habitat. The results highlight the practical applications of ABMs, particularly when utilized as tools for wildlife ecology investigations (some examples of which are detailed in McLane et al., 2011). 08.P07 \u00b6 The above are all areas in which ABM incorporating decision-making has room to expand. ABM is still a young field, and its potential for contributing to the understanding of how decisions are made by animals and how they affect whole ecosystems has yet to be fully exploited. \u00b6 img{width: 51%; float: right;}","title":"190410 DeAngelis D., Diaz S., 2019"},{"location":"190410_DeAngelisD_DiazS_2019/#toc","text":"00. Abstract 01. Introduction 02. Decisions in Classical Population Models 03. Agent-based Modeling in Ecology 04. Movement Decisions and Their Consequences 04.01. When to Move? 04.02. Where to Move? 04.03. Collective Movement Behavior 05. Foraging Decisions and Population Interactions 06. Social Interactions in Populations 07. Developments in the Modeling of Decisions in Population Models 08. Prospectus","title":"ToC"},{"location":"190410_DeAngelisD_DiazS_2019/#00_abstract","text":"","title":"00. Abstract"},{"location":"190410_DeAngelisD_DiazS_2019/#00p01","text":"All basic processes of ecological populations involve decisions; when and where to move, when and what to eat, and whether to fight or flee. Yet decisions and the underlying principles of decision-making have been difficult to integrate into the classical population-level models of ecology. Certainly, there is a long history of modeling individuals' searching behavior, diet selection, or conflict dynamics within social interactions. When all the individuals are given certain simple rules to govern their decision-making processes, the resultant population\u2013level models have yielded important generalizations and theory. But it is also recognized that such models do not represent the way real individuals decide on actions. Factors that influence a decision include the organism's environment with its dynamic rewards and risks , the complex internal state of the organism , and its imperfect knowledge of the environment . In the case of animals, it may also involve complex social factors, and experience and learning , which vary among individuals. The way that all factors are weighed and processed to lead to decisions is a major area of behavioral theory.","title":"00.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#00p02","text":"Individual- / Agent-based model (IBM / ABM) While classic population-level modeling is limited in its ability to integrate decision-making in its actual complexity, the development of individual- or agent-based models (IBM/ABMs) (we use ABM throughout to designate both \u201cagent-based modeling\u201d and an \u201cagent-based model\u201d) has opened the possibility of describing the way that decisions are made, and their effects, in minute detail. Over the years, these models have increased in size and complexity. Current ABMs can simulate thousands of individuals in realistic environments, and with highly detailed internal physiology, perception and ability to process the perceptions and make decisions based on those and their internal states . The implementation of decision-making in ABMs ranges from fairly simple to highly complex; the process of an individual deciding on an action can occur through the use of logical and simple (if-then) rules to more sophisticated neural networks and genetic algorithms . The purpose of this paper is to give an overview of the ways in which decisions are integrated into a variety of ABMs and to give a prospectus on the future of modeling of decisions in ABMs.","title":"00.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#01_introduction","text":"","title":"01. Introduction"},{"location":"190410_DeAngelisD_DiazS_2019/#01p01","text":"classical models: randomly moving w/ growth, reproducitin, mortality, interaction w/ env & other organisms e.g., logistric population model Lotka-Volterra predator-prey and competition model reaction-diffusion partial differential equation (PDE) model decision-making by individuals is unimportant? decision continually made e.g., single-celled animals (paramecia) ( 2009_Wagner ) social ants ( 2006_Deneubourg_Detrain ) The role of decision-making by individual organisms is largely ignored in the classical mathematical models of ecology, such as the logistic population models , Lotka-Volterra predator-prey and competition models , and their many variations. These models, as well as their extensions to space, as reaction-diffusion partial differential equation (PDE) models , treat organisms as randomly moving atoms, with added features of growth, reproduction, mortality, and interactions with their environment and other organisms. Because the classical models of ecological populations have been successful in revealing much about ecological systems, one might ask if decision-making by individuals is unimportant enough that it can be ignored at the population level, at least for simple organisms. But something like decisions are continually made, even by organisms perceived as simple. Bray (2009) notes that single-celled animals, like swimming paramecia, \u201ccontinually encounter different situations\u2026 and have to evaluate their options and assign priorities.\u201d Wagner (2009) asks \u201cDoes the bacterium choose to change direction?\u201d and concludes that this may be a matter of perspective. In more complex organisms, such as social ants, \u201ceach individual is a sensitive unit which can process a lot of information\u201d ( Detrain and Deneubourg, 2006 ). These actions are programmed into the organism's DNA, and are unconscious, but clearly decisions are being made, and we will follow Ydenberg (2010) (cited in Rypstra et al., 2015 ) in calling a decision \u201cwherever one or two (or more) options is/are selected .\u201d","title":"01.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#01p02","text":"How important these decisions are at the population level and above is therefore an important question, even for organisms of low cognitive ability. Our perspective in this review is from that of population and community ecology and how modeling helps link individual behaviors to phenomena at the level of collections of individuals. We begin with a brief overview of how decisions have been incorporated in some classical analytic models of ecology. Then we introduce agent-based modeling (ABM) and describe how it has been used to simulate decision-making in individual movement, foraging behavior, population interactions, and social interactions within populations. This is not intended to be comprehensive, but to touch on a variety of ways ABM is used. Finally, we discuss more recent developments in modeling decisions within population models and present a prospectus for future directions.","title":"01.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#02_decisions_in_classical_population_models","text":"","title":"02. Decisions in Classical Population Models"},{"location":"190410_DeAngelisD_DiazS_2019/#02p01","text":"individual decision \u21d2 population < individual fitness \u21d0 ecological context The use of modeling to address the question of the effect of individual decisions on population level dynamics developed more slowly than modeling of the converse question of how ecological context influences the effects of decisions on individual fitness. The latter has been explored by behavioral ecologists using classical population models for several decades, focusing especially on decisions regarding foraging movement and its effects on the fitness of individuals. Additionally, some models have been able to incorporate decision-making when modeling collections of individuals, whole populations, and even multiple populations.","title":"02.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#02p02","text":"models kinesis model (decisions on when to speed up/slow down/turn in response to detected local cond) restricted are search patch DSVM (Dynamic State Variable Modeling) Movement of animals toward favorable conditions could be decomposed into simple decisions on directed movement, or taxis, in relation to light, temperature, or resource gradients. Because it may be difficult for organisms to detect gradients, but possible to assess conditions at a current location, many models focused on kinesis, which involves decisions on when to speed up, slow down, or turn in response to detected local conditions (e.g., Gunn and Fraenkel, 1961 ; Sch\u00f6ne, 1984 ; Bell, 1991 ; Gr\u00fcnbaum, 1999 ; Gautestad, 2016 ). Other models used \u201crestricted area search\u201d in which organisms evaluate conditions within a limited area before making a movement choice (e.g., Humston et al., 2004 ). Movement decisions can be combined with decisions on settling at a place or leaving it, for which a variety of modeling approaches are used ( Lima and Zollner, 1996 ). At one extreme, animals may simply move in one direction until they find a spot to settle, or they may use spatial memory and learning to gain knowledge of the landscape such that they can choose the nearest detectable habitat patch ( Fahrig, 1988 ). Decisions on leaving a patch may increase as the level of resources is depleted. Mathematical theory has been applied to predict the optimal time to leave a patch, depending on its resource level relative to other patches and travel costs ( Charnov, 1976 ), or to predict what succession of patches, with varying risks and rewards, to choose in order to maximize fitness over longer times ( Mangel and Clark, 1988 ; Houston and McNamara, 1999 ; Clark and Mangel, 2000 ), an approach referred to as Dynamic State Variable Modeling (DSVM) .","title":"02.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#02p03","text":"advective-dffusion model ( Skalski and Gilliam (2000) ) e.g., fish formation purposeful kinesis population density gradients ( Flierl (1999) ) The step from modeling individuals to modeling collections of individuals could in some cases be done using mathematical models, in which all individuals follow the same basic rules that could be incorporated into PDEs. For example, Skalski and Gilliam (2000) used an advective -diffusion model to simulate patterns of fish formation in which the only decisions involved swimming fast or slow and having an upstream directional bias rather than pure random movement. However, many observed movement patterns of collectives, such as of flocks of birds, schools of fish, swarms of insects, and patterns formed by herding mammals, are more complex. Modeling these patterns requires more than movement decisions based on abiotic conditions, but they can be approximated when the PDEs also incorporate terms that represent decisions to move up or down population density gradients . Such individual movement behavior differs from random walk and can result in various patterns of collections of organisms ( Patterson et al., 2008 ). \u201c Purposeful kinesis \u201d can alter diffusive patterns ( Gorban and \u00c7abukoglu, 2018 ), leading to positive density-dependent diffusion, or \u201c super-diffusion ,\u201d and other variations on diffusion through dependence on population density ( Topaz and Bertozzi, 2004 ; Lutscher, 2008 ; Almeida et al., 2015 ; Tilles and Petrovskii, 2016 ). For example, the phenomenon of clustering, in insect swarms and fish shoals, can occur when individuals accelerate in the direction of a positive density gradient ( Tyutyunov et al., 2004 ). Flierl et al. (1999) provide a general review of mathematical modeling of collective behavior.","title":"02.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#02p04","text":"IFD (Ideal free distribution) e.g., predator-induced defenses Holling type 2 functional responses The influence of individual decisions on the level of whole populations and multi-population systems can also be studied when decision-making is incorporated into models of classical ecology, if the decisions are limited to simple rules, such as optimization of fitness in foraging ( MacArthur and Pianka, 1966 ; Charnov, 1976 ), or in diet selection ( Pulliam, 1974 ) and life history ( Roff, 1992 ). For example, if all individuals foraging on a spatial environment of habitat patches with different resource levels are assumed to move among them until no further movement would increase their fitness, the population would reach what is called an Ideal Free Distribution (IFD) ( Fretwell and Lucas, 1969 ). The IFD concept applies to a wider range of decisions, such as the choice that organisms in a population have in allocating resources in different proportions to foraging, defense, reproduction, etc. A particular example is that of predator-induced defenses , which are known to exist in many ecological systems and may involve changes in both morphology and behavior. The defended individuals are still edible but less so than undefended prey and, as a tradeoff, have lower resource intake rates than undefended prey. Recently it has been shown that the existence of inducible defenses in species at the bottom or middle of the food chain can affect the stability of the food chain, as well as the ability of the top predator to exert top-down control on the system ( Vos et al., 2004 ). In DeAngelis et al. (2007) , a system containing a predator, a prey with an inducible defense, and a resource of the prey, was studied using a differential equation model, with Holling type 2 functional responses describing the trophic interactions. The prey could choose between allocation to induced defense, with the tradeoff of lower resource uptake. The prey population evolved to switching between defense and non-defense, leading to a balance in which certain proportions of prey were in the undefended state and the rest in the defended state, the proportions depending on predator density. The prey in each state had equal fitness, so this was an IFD. This strategy of the prey influenced the populations of the whole food chain.","title":"02.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#02p05","text":"game theory Mathematical models have also represented simple decisions on allocation of time and energy to foraging vs. avoiding or defending against predators ( Abrams, 1982 , 1993 ; Lima and Dill, 1990 ; Abrams and Matsuda, 1993 ; Lima and Zollner, 1996 ; Werner and Peacor, 2003 ). Other models, using differential equations, describe a forager in an environment of several prey, in which the forager could choose which prey to feed on, based on a maximization of long-term food intake (e.g., Feng et al., 2009 ). Another approach of classical mathematical theory in ecology is game theory , which can be used to determine the optimal strategy of an individual when the expected pay-off of a decision (e.g., to \u201cfight or flee\u201d when in a confrontation) depends on the decisions made by other individuals ( Riechert and Hammerstein, 1983 ). In all these cases models showed that optimal decisions taken by members of the populations can have large ecosystem consequences.","title":"02.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#03_agent-based_modeling_in_ecology","text":"","title":"03. Agent-based Modeling in Ecology"},{"location":"190410_DeAngelisD_DiazS_2019/#03p01","text":"2 trends: variability w/i populations in behaviors e.g., behavioral tendencies, personality ABM (Agent-based modeling) The fact that mathematical or analytic models based on a few simple decision rules can explain even complex patterns is remarkable, and such modeling is still a lively and important area of theory. But behaviorists recognized that the capacity of these simple models to represent the real decisions of organisms, determined by a multitude of inputs, was limited, and that inclusion of decision-making was important ( Dill, 1987 ). Two trends in ecology that are relevant to that problem have accelerated over the past two decades. One trend is the increasing recognition of marked variability within populations, not just in age, size, or stage, but also in behaviors of individuals within given classes. In fact, individuals across many taxa appear to have their own personalities, or temporally consistent \u201c behavioral tendencies \u201d ( Biro and Stamps, 2008 ; Beekman and Jordan, 2017 ). The prevalence of behavioral differences, or different personalities, that exists across animal taxa was reviewed in a pivotal paper ( Bolnick et al., 2003 ). Personality differences such as \u201cbold\u201d vs. \u201cconservative\u201d behavior in fish (e.g., Blake et al., 2018 ) exist and involve various aspects of behavior, such as responses to intra-and inter-specific competition ( Ara\u00fajo et al., 2011 ; Dall et al., 2012 ), and tradeoffs between growth and mortality ( Biro and Stamps, 2008 ) and early and late reproduction ( Wolf et al., 2011 ). Bolnick et al. (2011) discuss several ways in which this individual-level specialization can affect community dynamics, which is an impetus to including individual differences in models. Variation in individuals, and therefore in their possible decision-making, casts further doubt on the capability of analytic models to adequately represent real populations. The second trend is the rapid spread of individual- or agent-based modeling (ABM) , which has become an established approach with numerous modeling platforms and is encompassed by a vast literature. The latter development may provide a solution to the impasse of creating models of sophistication comparable to what is known about decision-making.","title":"03.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#03p02","text":"ABM simulate autonomous \"agents\" state var representing internal states (behavioral states) unique history of interactions w/ env & other agents ABMs are ideally suited to accounting for individual differences in organisms. First applied to tree communities ( Botkin et al., 1972 ), in the last few decades ABMs have become well established in all areas of ecology. ABMs simulate the interactions of autonomous \u201cagents,\u201d generally representing individual organisms or other real-world entities, with other agents and with the external environment ( DeAngelis and Mooij, 2005 ). In ABMs, every individual of a population can, in principle, be simulated to almost any level of detail. Each agent may have state variables representing internal states, including behavioral states, and each can have a unique history of interactions with its environment and other agents ( DeAngelis and Grimm, 2014 ). Agent-based modeling attempts to capture the variation among individuals that is relevant to the questions being addressed. In particular, it can incorporate what is known about individual decision-making to explore the consequences in population and community models ( Parunak et al., 1998 ; Railsback, 2001 ; Vincenot, 2018 ).","title":"03.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#03p03","text":"deterministic probabilistic An ABM can be used where decisions are complex and/or are in a setting of populations or communities. The simplest and most straightforward way to represent individual decision-making in an ABM is to utilize logical rules following the \u201cif-then\u201d structure. The behavior of an individual can be modeled when the \u201cif\u201d part contains a condition, and the proceeding \u201cthen\u201d part presents the individual's response. The rules governing decision-making processes can be set up in various ways. Strictly logical, deterministic rules would assign only one possible behavior to an individual in a particular circumstance ( Grimm and Railsback, 2005 ). Alternatively, a rule may be probabilistic , with a different probability for each choice in an array of possible actions in response to some stimulus. Rules may also be a combination of probabilistic and deterministic. For many animals, however, decision-making processes may be more complex, and can be influenced by many variables including the internal state of the individual, that may vary among individuals. As will be discussed later, ABMs are increasingly being used in situations where organisms are assumed to have incomplete knowledge of the surrounding environment, and differences in perception and navigation capacities, but are able to employ cognitive skills to make choices that are good proximate decisions, albeit less than optimal. In this way, ABMs are able to encapsulate the basic underlying principles of the decision-making process in a more realistic way than classical models, and may more effectively represent the way individuals actually decide on actions and how the resulting behaviors shape population-level processes ( Figure 1 ).","title":"03.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#figure_1","text":"individual complexity proximate decision-making, uncerteainty, emergence Figure 1. Schematic representation of the models and methodologies utilized to model decision-making in ecology. Along the horizontal axis, these models and methodologies are able to introduce greater individual complexity and represent more complex interactions. Along the vertical axis, the models' entities can be characterized by more proximate decision-making . There is more uncertainty introduced along this axis, but also a greater level of emergence , or higher-level behaviors resulting from lower-level interactions.","title":"Figure 1"},{"location":"190410_DeAngelisD_DiazS_2019/#04_movement_decisions_and_their_consequences","text":"","title":"04. Movement Decisions and Their Consequences"},{"location":"190410_DeAngelisD_DiazS_2019/#04p01","text":"compoponents: internal state physiological psychological external env spatially-explicit navigational capacity motion capacity ABMs have been used extensively to simulate the movement behavior of both terrestrial and marine organisms, due in large part to their flexibility in incorporating the various components that influence an individual's movement through space. These components include an individual's internal state, external environment, and navigational and motion capabilities ( Tang and Bennett, 2010 ; Figure 2 ). ABMs can represent a variety of dynamic physiological and psychological state variables comprising an organism's internal state, including commonly used bioenergetic variables (Tang and Bennett, 2010). When simulating movement through space, the external environment is generally spatially-explicit , often represented as grid cells, such that the location of agents and environmental attributes and the spatial relationship between them is explicitly defined ( Duning, 1995 ; Bauer and Klaassen, 2013 ).","title":"04.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#figure_2","text":"Figure 2. Schematic illustrating the major components influencing animal movement, all of which can be represented within an ABM ( Baguette et al., 2014 ).","title":"Figure 2"},{"location":"190410_DeAngelisD_DiazS_2019/#04p02","text":"2 important decisions: when to move where to move By portraying an individual's dynamic environment and internal state in detail, ABMs can capture the two important decisions that an individual in motion must continually make: when to move , and where to move . The models can also be used to derive the consequences of many organisms moving and interacting with each other at the same time, which gives rise to spatial patterns.","title":"04.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0401_when_to_move","text":"","title":"04.01. When to Move?"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p01","text":"internal state cond of the current area presence of competition & predation temporal changes prompt agents moement e.g., trout, bear ABMs for movement generally incorporate rules that dictate when an individual agent decides to move from its current location. For real-world organisms, the onset of movement at fine scales may depend on the individual's current internal state, including its physiological and psychological conditions, the condition of the current area that the individual is in, and the presence of competition and predation ( Semeniuk et al., 2011 ; Martin et al., 2013 ; Doherty and Driscoll, 2017 ). As such, ABMs simulating the fine-scale movement of individuals often keep track of temporal changes in the individual agent's internal state and its local surroundings . These changes generally prompt agent movement if they somehow increase (or at the very least, not decrease) the agent's fitness. For example, brown and rainbow trout agents decide to move from their position if the previous day's calculated ratio of mortality risk to growth is greater than expected. Thus, at a given time step, fish agents might move to maximize the ratio of growth to risk of mortality ( Van Winkle et al., 1998 ). Bear agents in an ABM simulating human-bear interactions decide to move to a new location if their current location has a low amount of food, or if they are threatened by human activity ( Marley et al., 2017 ).","title":"04.01.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p02","text":"changes of many individuals through time influence movement e.g., green woodhoopoe An individual's decision on when to move may be influenced by the individual's life cycle and social factors. ABMs can track important biological changes of many individuals through time and be utilized to explore how these changes influence movement when placed within a social context. For example, Neuert et al. (1995) developed a model of the territorial group-living green woodhoopoe ( Phoeniculus purpureus ) to address the question of when a subdominant (and thus non-breeding) individual should decide to leave the group and scout for a territory on which it could breed, vs. waiting around to become high enough in status to breed at its natal site. In the model, the decision was based on its own age and rank, where the rank is correlated with age . The simple decision trait of higher propensity to go on scouting forays with increasing age provided the best agreement with empirical data.","title":"04.01.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p03","text":"temporal changes in recoucese availability other factors e.g., timing of pink-footed geese migration are influenced by factors: minimal body stores maximal stores date temperature plant phenology fixed duration of stay Large scale movement, such as migration, may be triggered by temporal changes in resource availability ( Van Moorter et al., 2013 ) along with a number of other factors, and ABMs have been utilized to explore the potential decision-rules that may dictate the timing of such behavior for various species. For example, ( Duriez et al., 2009 ) assumed the following factors to be important for the timing of pink-footed geese migration; (1) having minimal body stores, (2) having maximal stores, (3) date, (4) temperature, (5) plant phenology, and (6) fixed duration of stay. The authors found that decision-rules related to food resources were important for dictating the onset of migration, but later in the season, decision-rules related to the geese agents' internal clocks and date are likely used.","title":"04.01.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p04","text":"range expansion of population 3 phases initial probability of offspring dispersing from natal cell depending on population density transfer probability dependind on landscape composition of the cell & neghboring animals settlement probability depending on habitat suitability, presence of potential mate, density of conspeceifics Movement is also involved in the range expansion of a population , and Bocedi et al. (2014) modeled range expansion by considering three phases. Within the ABM, there is an initial probability of offspring dispersing from a natal cell , which can depend on population density. Then there is \u201c transfer probability ,\u201d the direction of which is weighted by the costs of moving to each adjacent cell, which depends on landscape composition of the cell and neighboring animals. In the final phase, four alternative strategies were compared to determine which best described settlement probability , and each of which was based on a combination of several factors, including habitat suitability, presence of a potential mate, and density of conspecifics .","title":"04.01.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#0402_where_to_move","text":"","title":"04.02. Where to Move?"},{"location":"190410_DeAngelisD_DiazS_2019/#0402p01","text":"navigation capacity: links internal state and ecternal var Many organisms can process information about their environment and make movement decisions to satisfy internal desires. Changes in the internal state of an organism may result in changes in the organism's goals, movement decisions, and subsequent movement behavior ( Tang and Bennett, 2010 ). When deciding where to move, mobile animals rely on their navigation capacity, which links the animals' internal states and external variables and manifests itself as either non-oriented, oriented, or memory-driven movement ( Nathan et al., 2008 ; Doherty and Driscoll, 2017 ). The ABM framework lends itself to representing dynamic environmental cues, particularly when the spatiotemporal relationships between the agent and the environment is explicitly represented, as in spatially explicit ABMs. The internal states of individuals can be represented as dynamic state variables and integrated with the cognitive capabilities of individuals, which allows model agents to assess various movement decisions within complex landscapes and ultimately decide on their next destination ( Tang and Bennett, 2010 ). Additionally, some stochasticity in selecting an area to move to is incorporated within many ABMs by using a combination of probabilistic and logical rules, reflecting imperfect knowledge of the environment and perception capabilities.","title":"04.02.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#0402p02","text":"e.g., caribous destination cell influenced by daily energetic state reproductive energy requirement predation risk jaguars energy reserves (internal state) hippopotamus Many ABMs simulating animal movement explicitly represent and track various components of an individual agent's internal state in detail, often resulting in movement characteristics that closely mimic those of real-world organisms. For example, Semeniuk et al. (2012) explored potential habitat-selection strategies employed by woodland caribou in response to industrial features in the landscape and represented the caribou's internal states by primarily tracking an individual's energy gain and loss. The caribou's decision on selecting a destination cell was influenced by its daily energetic state, reproductive energy requirement, and predation risk. The way that the internal state of the agent influenced movement varied for each alternative habitat-selection strategy. The authors found that the behavioral strategy concerned with balancing daily energy intake, conserving energy for reproduction, and minimizing predation risk agreed with real-world data better than the other strategies ( Semeniuk et al., 2012 ). Watkins et al. (2015) kept track of energy reserves of jaguar agents within a model landscape representing central Belize . The landscape cells were characterized by attributes including food availability, the presence of marks from other jaguars, and the presence of roads. When agents decide to move, their decision-making process concerning cell selection depends on the habitat attributes underlying a specific cell and the agent's internal state, namely, energy reserves . The individual jaguar's energy reserve levels modulate the preference of different attributes; consequently, agents with high energy reserve levels may decide to move to a cell that does not necessarily have high food availability; see also Lewison and Carter (2004) for an ABM simulating hippopotamus foraging behavior.","title":"04.02.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0402p03","text":"resistance cost When information on the dynamics of an individual's internal state is lacking, it may be appropriate to simulate movement by using simple decision rules based on empirical observations of the resistance that different habitat types may confer to the movement of real-world individuals. For example, Aben et al. (2014) developed and explored the effectiveness of an ABM in simulating forest bird movement, in which a bird agent's selection of a cell (habitat area) to move to at any given time step was partially determined by the land-cover class that characterized a given cell. Land-cover classes conferring more resistance to movement were given higher \u201ccost\u201d values . Spatial cells characterized as having a low cost to birds that are moving through the cells had a higher probability of being selected than cells characterized as having a high cost. Similarly, simple decisions rules governing the selection of destination cells may be based on the quality of the surrounding environment, such that agents generally move toward preferred or favorable areas. In an ABM developed to estimate landscape connectivity for bighorn sheep, each cell in the landscape is represented by landscape attributes including its proximity to escape terrain and the presence of roads ( Allen et al., 2016 ). Bighorn sheep agents have a higher probability of moving to cells closer to escape terrain and away from roads as these cells represent more favorable habitats to real-world bighorn sheep. The characteristics of the surrounding environment also played a major role for agents deciding on a destination in ABMs simulating movement for tiger ( Kanagaraj et al., 2013 ), tortoise ( Anad\u00f3n et al., 2012 ), and capercaillie ( Graf et al., 2007 ).","title":"04.02.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#0403_collective_movement_behavior","text":"","title":"04.03. Collective Movement Behavior"},{"location":"190410_DeAngelisD_DiazS_2019/#0403p01","text":"single-agent multi-agent key to understanding collective behavior: principles of the bahavioral algorithms followed by individual how info flows between animals Many of the above models of animal movement are single-agent ABMs, as opposed to multi-agent ABMs used to simulate collections of interacting individuals. One of the enigmas of natural history is the striking coordinated collective movements of various taxa (birds, insects, fish, mammals, etc.). Investigation of these phenomena is based on the recognition that the movement characteristics of individuals are often influenced by the spatial position and movement of conspecifics, which results in collective movement ( Krause et al., 2010 ). Individuals must make decisions on the initialization and direction of movement, and these decisions somehow incorporate the states of their neighbors. As Sumpter (2006) notes, the key to understanding collective behavior lies in identifying the principles of the behavioral algorithms followed by individual animals and how information flows between the animals . The decision-making process of individuals involved in collective movement has been represented by a spectrum of simple to complex rules within ABMs.","title":"04.03.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#0403p02","text":"fish school ( Huth and Wissel (1992) ) rupulsion () attraction () Couzin 2002 : repulsion (move away from nearby neighbors) alignment (adopt the same direction) attractin (avoid becoming isolated) Both Sumpter (2006) and Couzin and Krause (2003) provide reviews of the use of ABM in describing collective behaviors of organisms based on relatively small sets of decision rules. In an early use of a spatially explicit ABM, Huth and Wissel (1992) showed that the complex movement of fish schools could be described by a few rules; the fish tend to keep a certain distance between themselves and a number of its close neighbors and move parallel to other fish when they are within a certain range. The fish agents decide to move away from neighbors if they are too close (repulsion) or align themselves with their neighbors if they are too far (attraction). Size sorting also occurs in fish schools, which may be represented by simple rules, where individuals tend to actively stay with individuals of similar size and avoid those of a different size ( Hemelrijk and Kunz, 2005 ). Couzin et al. (2002) proposed a similarly simple model in which individual animals followed three rules of thumb; (1) move away from very nearby neighbors, (2) adopt the same direction as those that are close by, and (3) avoid becoming isolated; thus, the authors utilized the simple rules of repulsion, alignment, and attraction. In taking into account the location of an individual's neighbors at all times, the authors' model was able to reproduce collective behavior often seen in nature, including swarm ing and torus behavior.","title":"04.03.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0403p03","text":"Gueron 1996 : stress zone attraction zone neutral zone rear zone Hoare 2004 : \"zones of interactions\" influence the size of groups Dynamics of a small herd of mammal browsers were described by a more complex decision tree than those used by Huth and Wissel (1992) , and Couzin et al. (2002) . Gueron et al. (1996) integrated a combination of \u201cstress zones,\u201d \u201cattraction zones,\u201d \u201cneutral zones,\u201d and \u201c rear zones\u201d in the decision-making process of individuals moving within a herd. The various zones corresponded to specific distances extending from each individual, and each individual could invade the zones of other individuals. Probabilities and directions of movement depended on the type of zone an individual invaded. For example, if an individual sensed another individual within its \u201cstress zone,\u201d the individual slowed down to avoid collision. If neighbors were all on one side of the individual, the individual moved toward neighbors. Even though this sort of collective behavior is a much looser kind of group cohesion than fish schooling, the ABM showed that by simply integrating information about an individual's neighbors into the decision rules, many patterns of collective movement could emerge. Hoare et al. (2004) used a similar model to explain the group size distributions of fish, where dynamic \u201czones of interactions\u201d strongly influence the size of groups in simulations.","title":"04.03.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#05_foraging_decisions_and_population_interactions","text":"","title":"05. Foraging Decisions and Population Interactions"},{"location":"190410_DeAngelisD_DiazS_2019/#05p01","text":"temporal / spatial patterns The use of ABM in modeling foraging behavior changes the emphasis from predicting the fitness of the individual based on its decisions to predicting temporal and spatial patterns formed by large collections of individuals foraging and competing for resources (though sometimes interacting positively).","title":"05.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#05p02","text":"observations of real population in real locations A common, though not universal, characteristic of ABMs is that they are based on observations of real populations in real locations. Thus, the assumptions built into the models constitute hypotheses that can be tested against observational data. A few models will be mentioned here: Barnacle geese (Branta leucopsis) in Helgeland, Norway, white-fronted geese (Anser albifrons) in Lake Miyajimanuma, Japan, oystercatchers (Haematopus ostralegus) in the Exe estuary, England, salmonids in Little Jones Creek, California, USA, and dusky dolphins (Lagenorrhyncus obscurus) and killer whales (Orcinus orca) near Kaikoura, New Zealand.","title":"05.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#05p03","text":"geese Bioenergetics model Kanarek et al. (2008) modeled barnacle geese that return to the same group of islands each year as a migration stop. The geese are assumed to vary in age, energy reserves, genetic disposition, and spatial memory of previously visited locations. A goose chooses a specific island based on a combination of factors; constraint due to rank in the dominance hierarchy, memories of previously visited sites and past reproductive success, inherited genetic influences toward site faithfulness, and knowledge of the available biomass density. Within the island the goose chooses to forage on a particular patch until its intake rate drops below a certain amount. Once a goose decides to leave a patch, it uses its knowledge to choose where to go next, assigning a score to a patch based on several factors. A bioenergetics model keeps track of the body mass of the goose, and the goose leaves the staging area either when a threshold of the amount of energy stored or when the end of the stopover period is reached. Each year that a goose returns to the same patch, its familiarity and ability to locate food increases as well as its hierarchical rank.","title":"05.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#05p04","text":"dominance hierarchy A dominance hierarchy is also incorporated into the ABM of a migratory bird, the oystercatcher , feeding on mussels in a tidal estuary in England during the winter ( Stillman et al., 1997 ). A population of individual oystercatchers was modeled on a two-dimensional patch, and the individuals were assigned to places in a dominance hierarchy. When two individuals come within a certain distance, and one is handling prey, the other might attack to attempt to steal the prey, prompting the other to fight back, with the dominant individual always winning. The alternative behavior is avoidance, an action taken with higher probability by less dominant individuals. Avoidance subtracts less time from foraging than fighting. The authors assumed that individuals could calculate the costs vs. benefits of a given action and decide accordingly. The results of the model show, in accord with observations, much less incidence of interference than equivalent analytic models, because, given the differences in rank that are incorporated in the ABM, dominants wasted less time avoiding subdominants, and subdominants avoided fighting.","title":"05.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#05p05","text":"white-fronted geese choosing patches already occcupied by conspecifiecs In the case of white-fronted geese, an ABM explored possible positive interactions among the geese ( Amano et al., 2006 ). The model tested the hypothesis that geese foraging in patches usually have no prior information on patch quality before making a choice but can benefit from choosing patches already occupied by conspecifics; i.e., the presence of conspecifics in the patch has the potential positive effect of decreasing the need for vigilance , although it also causes prey to be depleted faster. The authors tested the hypothesis and found that it was better than alternatives foraging models at describing empirical data, indicating that the assumptions are realistic bases of foraging decisions.","title":"05.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#05p06","text":"tradeoff: starvation vs. predation EM (expected maturity) : maximize growth & survival In the classical DSVM theory mentioned earlier, tradeoffs are assumed between risks of starvation and predation. Many ABM foraging models also incorporate these risks. Railsback and Harvey (2002) used ABM to predict empirical patterns of habitat selection in drift-feeding juvenile trout foraging in a stream, based on assumptions concerning their decisions to choose habitat to forage in, given a range of empirical factors; e.g., water depth, velocity, available food, in the stream environment. The risks involved in deciding to move to a given location were starvation, predation by terrestrial animals, predation by fish, high water velocity (which requires energy to stay in place), stranding , and competition. A bioenergetics model was used to follow daily growth, and daily predation mortality depended on the size of the fish and water depth at its location, which gave the current \u201cstate\u201d of the individual. Three foraging strategies were compared, based on the agent's state and prediction of conditions over the near term (thus the authors label their approach \u201cstate and prediction theory\u201d). The strategy that compared well with all observed patterns involved daily decisions that maximized, over a longer term, the \u201cexpected maturity\u201d (EM) , or the product of predicted survival from starvation and other mortality risks and the fraction of reproductive size reached over the period of the 75-day simulation. The EM strategy, in which the fish agent maximizes both growth and survival, gives better results than other maximization criteria, such as maximizing only growth or only survival probability, used in many classical models.","title":"05.P06"},{"location":"190410_DeAngelisD_DiazS_2019/#05p07","text":"These studies all illustrate the ability of ABMs not only to incorporate high amounts of the complexity of real ecological systems and their ability to compare output with empirically observed patterns, but also to derive more realistic pictures of the decision-making process .","title":"05.P07"},{"location":"190410_DeAngelisD_DiazS_2019/#05p08","text":"importance of behavioral decisions of the predator Lima (2002) noted the importance of the behavioral decisions of the predator when studying predator-prey interactions, rather than treating predators as \u201cunresponsive black boxes.\u201d Accordingly, ABMs have also been used to extend the focus of predation risk beyond merely the prey strategy to include feedbacks from the predator or from competitors. A detailed investigation of tradeoffs of dusky dolphins choosing between feeding and avoiding predation by killer whales is given in a spatially explicit ABM of Srinivasan et al. (2010) . The authors explored the fitness costs and benefits of various escape strategies potentially used by dusky dolphins, including the time spent hiding in a refuge from predation and degree of vigilance when feeding. The model included counter strategies of the amount of time a killer whale would wait for a dolphin to emerge from a refuge , so the model contains a version of game theory. Another example of feedback effects, in this case coming from competition, is illustrated by a model of Peacor et al. (2007) . Individual foragers and a common resource for which they compete were simulated, both with and without the presence of a predator. The density of competing foragers affected the tradeoff in foraging. Under low density of competitors, the forager spent 40% of its time eating, but the time eating decreased to 7% when the forager density was high. This outcome resulted from the decreased benefits of foraging, as the resource was reduced by competitors. Such an outcome could not easily be anticipated or incorporated in simple models. A species' response to predation risk can affect whole food chains. An illustration of how behaviors of herbivores can modify top-down effects of predators on plants is the ABM of Schmitz and Booth (1997) for a tri-trophic chain (spiders, grasshoppers, and a food base of two plant types, grass and a herbaceous plant) of individual organisms on a spatial lattice. The spiders could affect the grasshoppers directly, by predation, and indirectly, by causing grasshoppers within a certain radius to move from grass to the safer herb sites. The model allowed the relative effects of direct predation and the predator avoidance by grasshoppers to be examined individually. The model predicted long-term field observations of the predator's top-down effect and showed the importance of including the behavioral response of the herbivore.","title":"05.P08"},{"location":"190410_DeAngelisD_DiazS_2019/#06_social_interactions_in_populations","text":"","title":"06. Social Interactions in Populations"},{"location":"190410_DeAngelisD_DiazS_2019/#06p01","text":"reproduction parental care territorial / home range defense Social behavior requires decisions involving, among other things, reproduction, parental care, and territorial or home range defense. These, plus a wider range of activities, have been represented in ABMs.","title":"06.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#06p02","text":"population dynamics is influenced by landscaoe structure & life history trait ( Ye (2014) ) Several studies have addressed the question of when to reproduce, incorporating external factors such as population density and the availability and quality of resources in an area ( Stewart et al., 2005 ; Brouwer et al., 2015 ). When deciding to reproduce, individual agents within an ABM often consider information of the surrounding environment. An individual agent's decision to reproduce is frequently represented in a logistical or probabilistic manner, in which agents may decide to reproduce if they meet some requirement or threshold. For example, Ye et al. (2014) used a grid-based spatially explicit ABM to gain insight on how the population dynamics of a virtual species is influenced by landscape structure and various life history traits. In the ABM, the resource share of an individual reflects the number of individuals in a spatial cell and the cell's underlying habitat quality. Only individual agents with a resource fraction over a given threshold may reproduce in a given time step with a given probability, reflecting the importance of competition and the quality of the surrounding habitat in driving reproductive success.","title":"06.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#06p03","text":"The agent's internal state also influences its decision to reproduce. Hancock et al. (2005) developed an ABM to predict the population dynamics of Bornean bearded pigs, tracking the \u201cfatness index\u201d of each pig agent in the model. The reproduction status of each agent varied according to their respective fatness index, which in turn depended on the available food supply and the density of the pig population. A pig agent decides to reproduce if its fatness index is above a specific threshold, such that agents in a large population with low food supply are unlikely to reproduce due to low fatness indices (Hancock et al., 2005). Fish agents within another ABM also took their internal state into consideration in deciding when to spawn. The ratio of actual weight to expected weight of a fish of a given length (a \u201ccondition index\u201d) is tracked for each fish agent, and only agents with ratios over 0.75 were able to spawn (Rashleigh and Grossman, 2005).","title":"06.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#06p04","text":"After reproduction, decisions must be made on parental care. Different strategies of parental investment reflect variation in the decision-making process of individuals, and ultimately have consequences for the dynamics of the population. Decisions involved in provisioning of offspring by bluebirds were studied by Davis et al. (1999), from the point of view of survival of the offspring under different conditions of food availability in the environment. The authors performed model simulations for different levels of food availability by assuming four different strategies on feeding decisions: (1) Feed the smallest first, (2) feed the largest first, or (3) feed the hungriest first, in each case going to the next larger, smaller, or less hungry, respectively, if the offspring representing the first choice was full. These choices were compared to random feeding (strategy 4). Environmental conditions determined which strategy maximizes the nest success, in terms of total biomass of surviving fledglings, with strategies (1), (2), and (3) performing best, successively, with increasing food availability. The random strategy was never the best.","title":"06.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#06p05","text":"The formation and maintenance of territories and home ranges occurs in many taxa. ABMs can simulate the spatiotemporal changes in resource availability throughout the environment and interactions between various agents through time, potentially elucidating the mechanisms underlying territory and home range dynamics. In order to aid conservation planning for the tiger (Panthera tigris) by providing an estimate of population number, location and size of territories, Carter et al. (2015), modeled territory formation of both females and males using a spatially explicit ABM. Females at 3 years of age were assumed to move to a site within 33 km of their natal site, and at least 2 km away from any other female, and to establish the center of her territory where prey density was highest. If the female failed to find such a site, she lowered the limit on neighboring females from 2 to 1 km. A female was assumed to be able to sense the total prey in her territory and in neighboring spatial cells. She could try to add habitat cells to their territories based on a few decision rules, including prey availability and the status of the neighboring female tiger's hierarchical status (avoiding cells near a female of higher status, which is correlated with age). When possible, she adds habitat cells to her territory until the total amount of prey reaches a certain threshold. Another spatially explicit model of home range dynamics is that of Wang and Grimm (2007) for the common shrew (Sorex araneus). Shrews were assumed to continually adjust their territories by sensing the amount of food in a cell of the habitat and the presence of other individuals. Individuals tried to optimize their home range by preferentially selecting cells with high food sources and avoiding cells occupied by other individuals. Acquisition and release of cells followed an optimization procedure, in which the shrew is assumed capable of ranking all the cells in its territory and releasing the worst ones in favor of adding new ones. These simple model rules predicted realistic territories for both tigers and shrews.","title":"06.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#07_developments_in_the_modeling_of_decisions_in_population_models","text":"","title":"07. Developments in the Modeling of Decisions in Population Models"},{"location":"190410_DeAngelisD_DiazS_2019/#07p01","text":"The papers reviewed above show that ABM is an important departure from earlier mathematical approaches, which, collectively, have been critical theoretical frameworks over the past few decades. In many earlier approaches, the individual was assumed to make a sequence of decisions through time that would result in optimal fitness at some end time. These approaches often assume knowledge of future conditions and ignore the feedbacks on the individuals through changes in the environment created by the individual's own decisions. To incorporate the feedbacks and uncertainty that are present in real ecological systems, a trend in ABMs has been to simulate how proximate decisions are made based on the individual's current state and short-term predictions. This is modeled differently by different modeling groups. As noted earlier, Railsback and Harvey (2002) (see also Railsback et al., 1999) use a \u201cstate and prediction theory\u201d strategy for individuals, which estimate an \u201cexpected survival\u201d over a specified time horizon over which little appreciable change in the environment is expected, considering the risks of predation, starvation, etc., and choosing the behavior that achieves the best fitness over that time. Giske and his group (e.g., Eliassen et al., 2016) find the adaptive behaviors for situations encountered through selection in genetic algorithms (GA) and artificial neural networks (ANNs). Other ABM modeling approaches include the computational system Digital Organisms in a Virtual Ecosystem (DOVE), in which a GA is used to allow phenotypic plasticity to evolve and interact with a dynamic environment (Peacor et al., 2007).","title":"07.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#07p02","text":"Borrowing concepts from machine learning and artificial life studies, modelers such as those noted above have utilized ANN and GA (together referred to as individual-based neural-network-genetic algorithm, or ING techniques) to represent the decision-making processes and consequent behaviors of organisms. Instead of fixed rules governing the way an organism makes decisions, ING techniques are flexible approaches that use principles of neurobiology and natural selection to solve optimization problems, resulting in individuals making adaptive decisions (Huse et al., 1999; Hamblin, 2013). ABMs that use genetic algorithms to govern decisions initialize a population of individuals with different solutions to a given optimization problem. Optimization problems may include finding the decision that will maximize fitness during foraging, or the probability of survival when selecting a patch (Hamblin, 2013). The individuals with the best solutions to the problem are allowed to reproduce through various commonly recognized selection operators, and reproduction continues for many generations. Trebitz (1991) was an early user of a GA-type approach to estimate the optimal spawning time for largemouth bass. In the model a female that spawned too early in spring risked the loss of eggs or larvae due to random cold snaps, while if it spawned too late it risked zooplankton being depleted by the offspring of earlier spawning females.","title":"07.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#07p03","text":"ANNs \u201ctrain\u201d the weights of input data by continuously modifying these weights until the resulting decisions and subsequent behaviors of individuals reach a specific fitness measure. In this way, ANNs and the way they capture the decision-making process of individuals mirror brain functions by \u201clearning\u201d from the outputs of the various input data weight modifications. Once an ANN is trained, it can be used on new input data to determine the decisions and behaviors that satisfy the specific fitness measure (Huse et al., 1999; Lek and Gu\u00e9gan, 1999). In GAs and ANNs, as in the more fixed logical and probabilistic decision-making rules within ABMs discussed earlier, the decision-making processes of individuals are generally geared to optimize some sort of fitness measure, but the decision-making strategy evolves through selection.","title":"07.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#07p04","text":"ANNs have been increasingly used to represent the decision-making processes of organisms, particularly with respect to movement through space. As the decisions of real-world individuals are driven by neuronal responses to internal and external stimuli, neural networks are well-suited to represent decision-making in an individual agent (Eliassen et al., 2016). Once a fitness criterion is identified for a given ABM, the ANN can be trained to find the input weights that correspond to outputs best fitting the fitness criterion (Huse et al., 1999). ANN training can occur in several ways, but here we focus on training with GAs, an approach common within ABMs utilizing ANNs. GAs are optimization tools that utilize the principles of crossing over and mutation to essentially \u201cevolve\u201d the decision-making processes of individuals, without the need for probabilistic or logistical rules.","title":"07.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#07p05","text":"For example, Okunishi et al. (2009) developed an ABM to simulate the growth and large-scale movements of Japanese sardines with the goal of exploring the effects of climate change on the population dynamics of the species, primarily its distribution and production in the North Pacific. The authors utilized an ANN to represent the way fish decide on movement directions when migrating to spawn. The inputs to the ANN were environmental factors known to influence the migration of sardines and included ocean current speed, the distance from land, and the experienced temperature change during subsequent days. Back propagation (a training method where weights are assigned to the various inputs based on a training data set) was one method used to train the ANN on spawning migration trajectories from actual sardines. The authors also used a GA to find the sardine offspring's input weights that would produce optimal outputs through crossing over and mutation of the parent sardine's weights. In training the ANN with a GA, the sardines' decisions representing swimming directions were allowed to evolve through many simulations; subsequently, the authors were able to find optimal combinations of weights that produce realistic migration trajectories. They found that combining back propagation with a GA to determine input weights produced the most realistic migration trajectories, indicating that utilizing the principles underlying the GA (in tandem with observed movement data) adequately captures the decision-making process of migrating sardines (Okunishi et al., 2009).","title":"07.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#07p06","text":"Morales et al. (2005) utilized several ANNs, each in combination with a GA, to determine efficient movement decisions for elk within a spatially explicit ABM. Movement decisions to be made by the elk included when to switch behaviors from foraging to exploring, where to move if foraging or exploring, and which type of plant to consume at a given time if foraging. The fitness measures that characterized the efficiency of elk agent decisions were energy gain for fat reserves and survival probability associated with predation risks. Percent body fat, forage biomass, and local predation risks were a few of the ANN inputs representing both internal and external movement stimuli. Rather than setting specific rules to guide the decision-making processes, the authors allowed the process to evolve over many generations.","title":"07.P06"},{"location":"190410_DeAngelisD_DiazS_2019/#07p07","text":"ANNs were also used to model movement in Mueller and Fagan (2008), who noted three basic types of pattern resulting from movement (or lack thereof): sedentary, migratory, and nomadism. The authors assumed that landscape structure drove individual-level movement types and that there were four gradients in hierarchical order; resource abundance, spatial configuration of resources, temporal variability of resource locations, and temporal predictability of resources. Mueller et al. (2011) followed up by using ANNs to evolutionarily train model organisms to use and combine different types of information representing the different movement behaviors (memory, oriented, and non-oriented movements). 240 individual animals were simulated moving across landscape and making choices of patches and, in doing so, depleting exhaustible resources. Different movement strategies involving combinations of these movement types worked better on different landscape types. After 5,000 generations with inheritance, survivors using unique movement strategies had optimized fitness on particular landscapes.","title":"07.P07"},{"location":"190410_DeAngelisD_DiazS_2019/#07p08","text":"In incorporating optimization of fitness, classic analytic models omit consideration of the immediate, or proximate, complexities that organisms encounter. This allows the integration of individual strategies with ultimate fitness, but the proximate mechanisms through which organisms solve problems are largely ignored (Sih et al., 2004; Fawcett et al., 2013; Eliassen et al., 2016). Animals need to be able to respond quickly and adequately in situations they have never experienced before; that is, the world is too complex for evolution to produce rules for every possible circumstance. It is likely that animals will evolve rules that perform well on average in their natural environment (McNamara and Houston, 2009). One of the basic advantages of ABMs is their ability to incorporate such heuristic, or rule of thumb, decision-making to perform well in complex environments. The GA approach allows one to simulate how such rules evolve.","title":"07.P08"},{"location":"190410_DeAngelisD_DiazS_2019/#07p09","text":"Toward this end Giske et al. (2013) formulated a model based on recent insights from a range of empirical disciplines that sheds light on the processes involved with decision making. In vertebrates, multipurpose rules are arbitrated through an \u201cemotion system,\u201d which describes the integration of information, motivation, and physiological state in determining physiological and behavioral outcomes. These outcomes affect the survival, growth, development, space use, and life history of the organism. Giske et al. (2013) modeled fish, where the fish had the choice of moving among different depths. Fish at deeper depths were generally safer from predation, but food was more limited, though uncertainty was entered into the model by letting both food levels and predation risk vary stochastically from fish generation to generation. The authors employed the survival-circuit concept (LeDoux, 2012), in which emotions form the basis for decisions, and they contribute to the survival or fitness of the organism. The first half of the survival-circuit is \u201cemotional appraisal.\u201d It starts with sensory input, considers motivational impact related to developmental stage, and may potentially activate the organism into a \u201cglobal organismic state,\u201d such that the whole organism is focusing on the situation. The second half of the survival circuit; the \u201cemotional response,\u201d consists of physiological responses and behavior. Physiological activation enables the organism to focus its sensory attention, brain activity, and potentially also bodily functions, such as heartbeat and muscle tension, toward the present situation. Fear and hunger are the emotions, and the ABM includes sensory inputs. The options for a fish are to stay at its current depth or move a short distance upward or downward, as determined by equations incorporating hunger and fear. On the basis of its global organismic state, the motivated fish behaves in a way that maximizes its net neuronal response.","title":"07.P09"},{"location":"190410_DeAngelisD_DiazS_2019/#07p10","text":"The model of Giske et al. (2013) predicted the same general type of spatial distribution patterns that classic optimization and game models produce, such as diel vertical migration with some extension around the average. But the model of Giske et al. (2013) differed from the classic models by allowing the fish to respond on immediate timescales to food and perceived risk, as opposed to optimization criteria involving only long-term goals. For example, danger is avoided because of an evolved proximate preference to stay with others or in darker waters when afraid, while danger is largely ignored when hungry. This is important in a fluctuating environment, where simple rules of thumb couple proximate constraints in determining behavior, with long-term adaptive value.","title":"07.P10"},{"location":"190410_DeAngelisD_DiazS_2019/#08_prospectus","text":"","title":"08. Prospectus"},{"location":"190410_DeAngelisD_DiazS_2019/#08p01","text":"In his book \u201cSociobiology,\u201d Wilson (1975) foresaw the gradual merger of population biology and behavioral ecology and the growing importance of neurophysiology to the latter (Wilson's Figure 1.2). These developments have been accelerated through ABM and ANN, particularly through using ABM as a means through which population and community dynamics emerge from the adaptive traits of individuals, including \u201chow individuals make decisions in response to other individuals, the environment, or changes in themselves\u201d (Grimm and Railsback, 2005).","title":"08.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#08p02","text":"Future development of ABM in modeling decision-making in ecology will be influenced by the continued refinement of modeling methodology and increases in needed data for parameterizing ABMs. Two of the modeling methodologies, the \u201cstate and predict\u201d (Railsback and Harvey, 2002) and ING (Eliassen et al., 2016) continue to be developed. In particular, a cognitive architecture based on the survival-circuit concept (LeDoux 2012) forms a general modeling framework for linking decision-making to neurobiological mechanisms (Bach and Dayan, 2017; Landsr\u00f8d, 2017; Budaev et al., 2018). Other approaches to providing rules of thumb for individuals, such as Fuzzy Cognitive Maps (FCM), based on Fuzzy Set Theory (Berkes and Berkes, 2009), are also being used in evolving predator-prey systems. In addition, off the shelf ABM modeling programs such as NetLogo and Ecobeaker are making development and use of ABM easier (e.g., Railsback and Grimm, 2011). Concerning relevant data, ABMs generally require a large amount of data at the individual level and site level. Such data are generally only available for ecological systems that have been intensively studied. But technology is rapidly increasing data collection capabilities, and the availability of remote sensing data and the ease with which these data are integrated within the ABM framework facilitates the development of decision rules based on a plethora of environmental attributes that potentially drive animal decisions. This may facilitate wider use of ABM, though this will require more coordinated efforts in making data broadly available to the modeling community (Hampton et al., 2013).","title":"08.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#08p03","text":"Recent advancements in technological tools should facilitate the parameterization of future ABMs developed for simulating dispersal, migration, and local-scale movements of animal groups. In particular, developments in GPS telemetry (and satellite telemetry in general) and biotelemetry devices have allowed for remote tracking of an individual's movement, physiological state, and behavior over long periods of time at relatively fine temporal resolutions (Cooke et al., 2004). This is especially useful considering the challenges ecologists face when attempting to directly observe and study free-ranging animals in a non-invasive manner. Basic movement parameters, including step length and turning angle distributions, can be derived from a time series of relocation data obtained from GPS and satellite telemetry and together shed light onto the limits of an individual's motion capacity. Subsequently, parameters driving agent movement within an ABM can be made more accurate. Bioenergetic models used to track agents' growth and reproduction within ABMs can be further parametrized with information gained from biotelemetry monitoring of undisturbed animals in their natural environments. The wealth of data generated for many species thus far should allow ecologists to more accurately model how individuals decide on an action in response to dynamic internal and external variables. What will most define the future success of ABM will be its ability to address complex questions that are difficult for traditional approaches. These questions include, among many others, the origin of collective actions, the interactions of tradeoffs at the individual level and the whole ecological system, the dynamics of complex social systems, and applications to conservation and management issues.","title":"08.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#08p04","text":"How collective actions such as migration are initiated, usually by a small fraction of leaders, has been discussed as resulting from differences in experience within populations (Ferno et al., 1998; Krause et al., 2000). Collective actions that benefit the whole group, but which may have negative fitness consequences for individuals, have long attracted attention, but are still not well-understood and may profit from use of ABM. An example is \u201cmobbing action\u201d by birds against nest threats. Unrelated birds and even birds of different species may be involved in these aggressive actions against a threat to nests, such as hawks or owls. While there is clear benefit to each individual in protecting its nest, there are also costs, such as becoming a victim of the threat or revealing the location of one's nest. Wheatcroft and Price (2018), in an empirical/theoretical study, proposed that collective action in this case can build up among birds whose nest locations are distributed across various distances from the threat. Birds nearest the nest respond first, and others that have nests farther and farther away decide to join as the risk to any individual declines with the growing size of the mob. The authors used a mathematical model with two individuals to analyze the amount of investment in each in aggressiveness to the threat. While the mathematical model provides insight, this is the type of problem where an ABM, which can simulate the actions of many birds with individual variation, could provide output directly comparable with observations of mobbing behavior.","title":"08.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#08p05","text":"Social interactions of at least moderate complexity already occur in several of the models noted above, in which social hierarchies constrain actions, but extension to much more complex interactions is possible using ABM. For example, in many primate troops the type and level of interaction between individuals, whether positive (cooperation) or negative (e.g., fighting) depends on genetic relatedness. Because there is kin-recognition, any interaction, such as a fight between two unrelated individuals, can give rise to a larger complex of interactions involving their relatives (Cheney and Seyfarth, 1992). Modeling the behavior of such societies may require ABM to represent a network of relevant interactions.","title":"08.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#08p06","text":"Behavioral ecologists and modelers like to think that their work can have some useful applications in conservation and wildlife management. A review by Caro (2007) of application of ecological theory found little evidence that many of the applications proposed so far by behavioral ecology, based on hypothetical conditions, would be effective in practice. However, ABM may be able to bridge the gap between theory and real conservation issues (Wood et al., 2015). For example, to elucidate whether specific predation strategies influenced the nesting success of waterfowl, Ringelman (2014) simulated the movements of different predators in a landscape with clumped and dispersed nests. The results of the ABM suggested that managers concerned with nesting success of waterfowl should consider the various behavioral strategies used by nest predators when creating and managing potential nesting habitat. The results highlight the practical applications of ABMs, particularly when utilized as tools for wildlife ecology investigations (some examples of which are detailed in McLane et al., 2011).","title":"08.P06"},{"location":"190410_DeAngelisD_DiazS_2019/#08p07","text":"The above are all areas in which ABM incorporating decision-making has room to expand. ABM is still a young field, and its potential for contributing to the understanding of how decisions are made by animals and how they affect whole ecosystems has yet to be fully exploited.","title":"08.P07"},{"location":"190427_JaafraY_LaurentJL_2018/","text":"19-04-27 \u00b6 Original | Mendeley 00. Abstract \u00b6 Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are gen- erally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. De- signing such architectures requires significant human expertise, substantial computation time and doesnt always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search. 01. Introduction \u00b6 01.P01 \u00b6 \u201dA neuron is nothing more than a switch with information input and output. The switch will be activated if there are enough stimuli of other neurons hitting the information input. Then, at the information output, a pulse is sent to, for example, other neurons \u201d [1] . Brain-inspired machine learning imitates in a simplified manner the hierarchical operating mode of biological neurons [2] . The concept of artificial neural networks (ANN) achieved a huge progress from its first theoretical proposal in the 1950s until the recent considerable outcomes of deep learning. In computer vision and more specifically in classification tasks, CNN, which we will examine in this review, are among the most popular deep learning techniques since they are outperforming humans in some vision complex tasks [3] . 01.P02 \u00b6 advances: backpropagation algorithm large training datasets / computational resources CNN differ in that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer The origin of CNN that were initially established by [4] goes back to the 1950s with the advent of \u201dperceptron\u201d, the first neural network prototyped by Frank Rosenblatt. However, neural network models were not extensively used until recently, after researchers overcame certain limits. Among these advances we can mention the generalization of perceptrons to many layers [5], the emergence of backpropagation algorithm as an appropriate training method for such architectures [6] and, mainly, the availability of large training datasets and computational resources to learn millions of parameters. CNN differ from classical neural networks in the fact that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer . This selective connection endow the network with the ability to operate, implicitly, hierarchical features extraction. For an image classification case, the first hidden layer can visualize edges, the second a specific shape and so on until the final layer that will identify the object. 01.P03 \u00b6 convolusion pooling fully connected multiple choices: num / order of layers hyperparameter (receptive fild size, stride) in this paper overview of deep learning history of CNN architectures several methods for automating CNN design according to search optimizaiton architecture design methods search acceleration techniques future works CNN architecture consists of several types of layers including convolution, pooling, and fully connected. The network expert has to make multiple choices while designing a CNN such as the number and ordering of layers , the hyperparameters for each type of layer ( receptive field size , stride , etc.). Thus, selecting the appropriate architecture and related hyperparameters requires a trial and error manual search process mainly directed by intuition and experience. Additionally, the number of available choices makes the selection space of CNN architectures extremely wide and impossible for an exhaustive manual exploration. Many research effort in meta-modeling tries to minimize human intervention in designing neural network architectures. In this paper, we first give a general overview and define the field of deep learning. We then briefly survey the history of CNN architectures. In the following section we review several methods for automating CNN design according to three dimensions: search optimization , architecture design methods (plain or modular) and search acceleration techniques . Finally, we conclude the article with a discussion of future works. 02. Background \u00b6 02.P01 \u00b6 Before embarking with CNN, we will introduce in this section some basic generalities about artificial networks and deep learning. 02.01. Artificial Neural Networks \u00b6 02.01.P01 \u00b6 ANN are a major field of artificial intelligence that attempts to replicate human brain processing. Three types of neural layers distinguish an ANN: input, output and hidden layers. The latter operate transitional representa- tions of the input data evolving from low level features (lines and edges) to higher ones (complex patterns) as far as deeper layers are reached. Figure 1 provide an example of ANN involving classical fully-connected layers where every neuron is connected to all ones of the previous layer. Figure 1 \u00b6 [![Fig.1][fig_01]][fig_01] Figure 1: Artificial neural network, containing an input layer, an output layer and two hidden layers. 02.01.P02 \u00b6 weight bias During training, an ANN aims at learning two types of parameters that will condition its predictive performance. First, connection weights that assess to which extent a neuron result will impact the output of higher level neuron. Second, the bias which is a global estimator of a feature presence across all inputs. Hence, a neuron output can be formalized through a linear combination of weighted inputs and associated bias: \\text{output} = \\Big( \\sum_i{ \\text{input}_i \u2217 \\text{weight}_i} \\Big) + \\text{bias} 02.01.P03 \u00b6 activation fn ReLU ( Rectified Linear Unit ) In order to allow the network operating non-linear transformations, an activation function is applied to the previous output. Equation 1 presents an example of such transformation using one of the most common and efficient activation function which is the Rectified Linear Unit ( ReLU ) [7]: f(x) = \\max(x, 0) \\tag{1} 02.02. Deep learning \u00b6 02.02.P01 \u00b6 Deep learning: ML processing w/i multi-layer ANN loss function evalation backpropagation The concept of deep learning refers to machine learning processing within multi-layer ANN [8]. The training of these networks relies on a loss function evaluation . For example, in supervised learning the loss is assimilated to the matching accuracy between ANN predictions and real expected outputs. An iterative update procedure is implemented to adjust network parameters according to loss function computed gradient. This procedure is called Backpropagation since parameters updates are spread from final layers to initial ones. Deep learning implies a certain number of challenges such as vanish- ing/exploding gradient and overfitting. The solutions to these problems will be discussed when developing CNN design architectures in next sections. 03. CNN Layers \u00b6 03.P01 \u00b6 3 characteristics local receptive fields reflect image data shared weights \u21d2 param reduction grid-structured image \u21d2 pooling operations CNN are widely used in a great number of pattern and image recognition problems. Three main characteristics are making this deep learning technique successful and suitable to visual data. First, local receptive fields perfectly reflect image data specificity to be correlated locally and uncorrelated in global segments . Second, shared weights allows a substantial parameter re- duction without altering image processing since the convolution is applicable to the whole image. Last, grid-structured image enable pooling operations that simplify data without losing useful information [9]. 03.01. Convolutional Layer \u00b6 03.01.P01 \u00b6 The convolutional layer is the basic CNN unit that has been inspired by physiological research evidence of hierarchical processing in the visual cor- tex of mammals 10 . Simple cells detect primitive attributes while more compound structures are subsequently extracted by complex cells. Thus, convolutional layer consists of a set of feature maps issued from convolving different filters (kernels) with an input image or previous layer output 11 . The 2-dimensional maps are stacked together to produce the resulting vol- ume of the convolutional layer. This process reduces drastically the network complexity since the neurons of a same feature map share the same weights and bias maintaining a low number of parameters to learn 12 . 03.01.P02 \u00b6 depth F num of filters stride S filter movement from receptive field to the next zero padding P The hyperparameters characterizing a convolutional layer are the depth F (number of filters), the stride S (filter movement from a receptive field to the next one) and the zero padding P to control input size 13 . Assuming that the filter size (\\text{height}, \\text{width}, \\text{depth}) = (h, w, F) , the dimensions of the feature maps generated can be obtained according to: ( H_1, W_1, F ) = \\bigg( \\frac{(H + 2P \u2212 h)}{S} + 1, \\frac{(W + 2P \u2212 w)}{S} + 1, F \\bigg) Where (H, W, F) is the size (\\text{height}, \\text{width}, \\text{depth}) of the input image. 03.02. Pooling Layer \u00b6 03.02.P01 \u00b6 CNN architectures generally alternate convolution and pooling layers. The latter have the purpose of reducing network complexity and avoid the problem of overfitting. At biological level, pooling is assimilated to the behav- ior of cortical complex cells that reveal a certain degree of position invariance. A pooling layer neuron is connected to a region of the previous layer by per- forming a non-parameterized function. Thus it differs from convolution as it doesn\u2019t have learnable weights or bias and additionally, it keeps the same depth of the previous layer. Max pooling [14] is one of the most common type of pooling that consists in retaining the maximum value of a neurons cluster. It means that max pooling is detecting if a given feature has been identified in a receptive field without recording the exact location [9]. 03.03. Fully connected Layer \u00b6 The convolution layers identify local features in the input data such as edges and shapes. The Fully connected layer operates the high level reasoning (classification for image case) by combining information from all the previous layers. As in a regular ANN, neurons at this level are fully connected to all ones in the previous layer. A softmax loss layer is then used to compute the probability distribution of the CNN final outputs. 04. CNN Architecture History \u00b6 This section presents the most influential hand-crafted CNN architectures that have impacted the recent work on automatic architecture design. Most of them won at least one of the \u201dImageNet Large Scale Visual Recognition Competition\u201d (ILSVRC) challenges [3]. 04.01. LeNet \u00b6 As mentioned previously, LeNet [7] was the innovative work that intro- duced convolutional networks. The model was experimented successfully to classify handwritten digits without any preprocessing of the input image (of size 32 \u2217 32 pixels). LeNet architecture is illustrated in figure 2. It consists of an input and an output layers of respective sizes 32 \u2217 32 and 10 as well as 6 hidden Layers. The basic idea of this design is to operate multiple con- volutions (3) with pooling in-between (2) then transmitting the final signal via a fully-connected layer toward the output layer. Unfortunately, due to the lack of adequate training data and computing power, it wasn\u2019t possible to extend this architecture to more complex applications. [![fig.2][fig_02]][fig_02] Figure 2: Architecture of LeNet-5 [7]. 04.02. AlexNet \u00b6 AlexNet [11] is one of the most influential deep CNN that won the ILSVRC (Imagenet Large Scale Visual Recognition Challenge) competitions in 2012. As shown in figure 3, it is not much different from LeNet. Never- theless, the corresponding architecture is deeper with 8 layers in total, 5 con- volutional and 3 fully connected. The effective contribution of AlexNet lies in several design and training specificities. First, it introduced the Rectified Linear Unit (ReLU) nonlinearity which helped to overcome the problem of vanishing gradient and boosted a faster training. Furthermore, AlexNet im- plements a dropout step [15] that consists in setting to zero a predefined per- centage of layers\u2019 parameters. This technique decreases learned parameters and controls neurons correlation in order to limit overfitting impact. Third, training process convergence is accelerated with momentum and conditional learning rate decrease (e.g. when learning stagnates). Finally, training data volume is increased artificially by generating variations of the original images that are shifted randomly. Thus the network learning is enhanced with the use of invariant representations of the data. [![fig.3][fig_03]][fig_03] Figure 3: An illustration of AlexNet [11]. 04.03. VGGNet \u00b6 Submitted for the ILSVRC 2014, VGGNet [16] won the second place and demonstrated that deeper architectures achieve better results. Indeed, with its 19 hidden layers, it was much deeper than previous convolutional networks. In order to allow an increase in depth without an exponential growth of the parameters number, smaller convolution filters (3\u22173) were used in all layers (e.g. lower size than the 11 \u2217 11 filters adopted in AlexNet). An additional advantage of using smaller filters consists in reducing overlapping scanned pixels which results in feature maps with more local details [17]. 04.04. GoogLeNet \u00b6 Since it has been demonstrated that a CNN architecture size is positively correlated to its performance, recent efforts focus on how to increase the depth of a CNN while keeping an acceptable number of parameters. Winner of ILSVRC 2014, GoogLeNet [18] innovated network design by replacing the classical strategy of alternating convolutional and pooling layers with stacked Inception Modules depicted in figure 4. Despite being deeper than VGGNet with 22 hidden layers, GoogLeNet requires outstandingly fewer parameters due to this sparse connection technique. Within an inception module, several convolutions with different scales and pooling are performed in parallel then concatenated in one single layer. This enables the CNN to detect patterns of various sizes within the same layer and avoid heavy parameters redundancies [18]. GoogLeNet hidden layers consist of 3 convolutions, 9 inception blocks (2 layers deep each one) and one fully connected. [![Fig.4][fig_04]][fig_04] Figure 4: Inception module [18]. 04.05. ResNet \u00b6 Deep Residual Network (ResNet) [19], was the first neural network to ex- ceed human-level accuracy in ImageNet Challenge (ILSVRC 2015). Thanks to residual connections, such kind of architecture went deeper and was im- plemented with multiple versions of 34, 50, 101 and 152 layers. Indeed, one of the difficulties with very deep networks training is the vanishing gradi- ent during error backpropagation which penalizes the appropriate update of earlier layers weights. ResNet main contribution consists in dividing con- volutional layers into residual blocks. Each block is bypassed by a residual (skip) connection that forwards the block input using an identity mapping. The final output is the summation of the block output and the mapped input as illustrated in figure 5. By adding skip connections, backpropagation can be operated without any interference with previous layers which allows to prevent vanishing gra- dient and train very deep architectures. ResNet-101 consists of one convolu- tional layer followed by 33 residual blocks (3 layers deep each one), and one fully connected layer. [![Fig.5][fig_05]][fig_05] Figure 5: Residual learning: a building block [19] 04.06. More Networks \u00b6 After ResNet [19] success, which exceeded human-level accuracy in (ILSVRC 2015), the so-called modern hand-crafted CNN are still being designed on the basis of previous models looking for more efficiency and lower training time. Inception-v4 [20] is a new release of GoogLeNet that involved many more layers than the initial version. Inception-ResNet [20] is built as a combi- nation of an Inception network and a ResNet, joining inception blocks and residual connections. The last example of this section is DenseNet (Dense Convolutional Networks) [21] where each dense block layer is connected via skip connections to all subsequent ones allowing the learning of new features. 05. Meta-modeling for CNN automatic architecture design \u00b6 Meta-modeling for neural network architectures design aims at reduc- ing the intervention of human expertise in this process. The earliest meta- modeling methods were based on genetic algorithms and Bayesian optimiza- tion then more recently, reinforcement learning became among the most im- plemented approaches [22]. 05.01. Context of automation \u00b6 The performance of a neural network and particularly a CNN mainly depends on the setting of the model structure, the training process, and the data representation. All of these variables are controlled through a number of hyperparameters and impact the learning process to a large extent. In order to achieve an optimal performance of CNN, these hyperparameters including the depth of the network, learning rates, layer type, number of units per layer, dropout rates, etc., should be then carefully tuned. On the other hand, the advent of deeper and more complex modern architectures (see section 4) is increasing the number and the types of hyperparameters. Hence, tuning step and more generally CNN architectures search become very expensive and heavy for an expert trial-and-error procedure. Additionally, CNN parameters setting is considered as a black-box [23] optimization problem because of the unknown nature of the mapping between the architecture, the performance, and the learning task. In this context, au- tomatic design solutions are highly required and instigates a large volume of research. The task of CNN hyperparameters tuning has been handled through meta-modeling that consists in applying machine learning models for designing CNN architectures. Three meta-modeling approaches are gen- erally used in the literature of architecture search and will be described in the next paragraph: bayesian optimization, evolutionary algorithms and re- inforcement learning. 05.02. Meta-controllers \u00b6 Meta-modeling approaches perform iterative selection from the hyperpa- rameters space and build associated architectures that are then trained and evaluated. Accuracies records are fed to meta-modeling controllers (meta- controllers) to guide next architectures sampling. Meta-controllers for CNN design are mainly based on bayesian optimization ([24], [25]), evolutionary algorithms ([26], [27]) or more recently on reinforcement learning ([28], [29]). Bayesian optimization is an efficient way to optimize black-box objective functions f : X \\rightarrow R that are slow to evaluate [30]. It aims at finding an input x = \\arg \\min_{x \\in X} f(x) that globally minimizes f where in the context of a machine learning algorithm, x refers to the set of hyperparameters to optimize. The problem with this kind of optimization is that evaluating the objective function is very costly due to the great number of hyperparameters and the complex nature of models like deep neural networks. In order to overcome this problem, bayesian approaches propose probabilistic surrogate reconstruction of the objective function p(f|D) where D is a set of past observations. The evaluation of the empirical function is much cheaper than the true objective function [31]. Some of the most used probabilistic surrogate (regression) models are gaussian processes [32], random forests [33] and tree- structured Parzen estimator [24]. Briefly, the processing of a bayesian optimization consists in building an empirical (probabilistic) model of the objective function. Then, iteratively, the model identifies a set of optimal hyperparameters for which the objective function returns corresponding results (e.g. loss values). Each feedback al- lows the update of the surrogate model and the guidance of hyperparameters predictions until the process reaches a termination condition. Evolutionary algorithms present another strategy of hyperparameters op- timization that modifies a set of candidate solutions (population) on the basis of a number of rules (operators). Following an iterative procedure of muta- tion, crossover and selection [34], an evolutionary algorithm initializes, in a first step, a set of N random networks to create a primary population. The second step consists in introducing a fitness function to score each net- work through its classification accuracy and keep the top ranked networks to construct the next generation. The evolutionary process continues until a termination criteria is met, which is generally defined as the maximum num- ber of allowed generations. One of the advantage of evolutionary algorithms is the adaptation to complex combination of discrete (layer type) and contin- uous (learning rate) hyperparameters which is suitable to neuronal network optimization models [35]. An important approach for goal-oriented optimization is reinforcement learning (RL) inspired from behaviorist psychology [36]. The frame of RL is an agent learning through interaction with its environment (figure 6). Thus the agent adapts its behavior (transition to a state s_{t+1} ) on the basis of observed consequences (rewards) of an action at taken in state s_t . The agent purpose is to learn a policy \\pi that is able to identify the optimal sequence of actions maximizing the expected cumulative rewards. The environment return reinforces the agent to select new actions to improve learning process, hence the name of reinforcement learning. [![Fig.6][fig_06]][fig_06] Figure 6: Illustration of the RL process. The methods developed to resolve reinforcement tasks are based on value functions, policy search or a combination of both strategies (actor-critic methods) [37]. Value function methods consist in estimating the expected reward value R when reaching a given state s and following a policy \\pi : V ^\\pi(s) = \\mathbb{E} [\\mathfrak{R}|s, \\pi] A recursive form of this function is particularly used in recent Q-learning [38] models assigned to CNN architecture design ([39], [40]): Q(s_t, a_t) = Q(s_t, a_t) + \\alpha[r_{t + 1} + \\gamma \\text{max}_a Q(s_{t+1}, a) \u2212 Q(s_t, a_t)] Where s_t is a current state, a_t is a current action, \\alpha is the learning rate, r_{t+1} is the reward earned when transitioning from time t to the next and \\gamma is the discount rate. In contrast to value function methods, policy search methods do not implement a value function and apply, instead, a gradient-based procedure to identify directly an optimal policy \\pi^\u2217 . In this context, deep reinforcement learning is achieved when deep neural networks are used to approximate one of the reinforcement learning components : value function, policy or reward function [41]. Among the active fields of designing CNN architectures through deep reinforcement learning, recurrent neural networks (RNN) arise as a valuable model that handles a set of tasks such as hyperparameters prediction ([28], [29]). In fact, a RNN operates sequentially involving hidden units to store processing history, which allows the reinforcement learning to profit from past observations. Long short term memory networks (LSTM), a variant of RNN, offers a more efficient way of evolving conditionally on the basis of previous elements. 06. Neural Architecture Search \u00b6 Various strategies have been developed to operate CNN architectures de- sign for the majority of which reinforcement learning has been selected as meta-controller. This section is assigned to review in detail most recent promising automatic search approaches differentiated according to search spaces specificities and complexity level. 06.01. Plain Architecture Design \u00b6 Some architecture search approaches focus on designing plain CNN which consists exclusively of conventional layers, mainly convolution, pooling and fully-connected. The resulting research space is relatively simple and the approaches contribution lies almost entirely in the design strategy. 06.01.01. MetaQNN \u00b6 MetaQNN model [39] relies on Q-learning, a type of reinforcement learn- ing (refer to previous section for more details), to sequentially select network layers and their parameters among a finite space. This method implies, first, the definition of each learning agent state as a layer with all associated relevant parameters. As an example, 5 layers are depicted in figure 7: convo- lution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM). [![Fig.7][fig_07]][fig_07] Figure 7: State space possible parameters [39]. Second, the agent action space is assimilated to the possible layers the agent may move to given a certain number of constraints set intentionally, for the majority, to enable faster convergence. Figure 8 illustrates a set of state and action spaces and an eventual agent path to design a CNN architecture. MetaQNN was evaluated competitive with similar and different hand-crafted CNN architectures as with existing automated network design methods. [![Fig.8][fig_08]][fig_08] Figure 8: An illustration of the full state and action space (a) and a path that the agent has chosen (b) [39]. 06.01.02. NAS \u00b6 Using reinforcement learning, [28] train a recurrent neural network to generate convolutional architectures. Figure 9 shows a RNN controller gener- ating sequentially CNN parameters associated to convolutional layers. Every sequence output is predicted by a softmax classifier then used as input of the next sequence. The parameters set consists of filter height and width, stride height and width and the number of filters per layer. The design of an ar- chitecture takes an end once the number of layers reaches a predefined value that increases all along training. The accuracy of the designed architecture is fed as a reward to train the RNN controller through reinforcement learning in order to maximize the expected validation accuracy of the next architectures. The experimentation of the global approach achieved competitive results on CIFAR-10 and Penn Treebank datasets. [![Fig.9][fig_09]][fig_09] Figure 9: Illustration of the way the agent used to select hyperparameters [28]. 06.01.03. EAS \u00b6 In their very recent work Efficient Architecture Search, [42] implement network transformation techniques that allow reusing pre-existing models and efficiently exploring search space for automatic architecture design. This novel approach differs from the previous ones in the definition of reinforce- ment learning states and actions. The state is the current network archi- tecture while the action involves network transformation operations such as adding, enlarging and deleting layers. Starting point architectures used in ex- periments are plain CNN which only consist of convolutional, fully-connected and pooling layers. EAS approach is inspired from Net2Net technique intro- duced in [43] and based on the idea of building deeper student network to reproduce the same processing of an associated teacher network. As shown in figure 10, an encoder network implemented with bidirectional recurrent neural network [44] feeds actors network with given architectures. The se- lected actor networks performs 2 types of transformation: widening layers in terms of units and filters and inserting new layers. EAS outperforms sim- ilar state-of-the-art models designed either manually or automatically with the attractive advantage of using relatively much smaller computational re- sources. [![Fig.10][fig_10]][fig_10] Figure 10: A meta-controller operation for network transformation [42]. 06.02. Modular Architecture Design \u00b6 Most of recent work on neural architecture search is based on more com- plex modular (multi-branch) structures inspired by modern architectures pre- sented in section 4. Rather than operating the tedious search over entire networks, this second set of approaches focus on finding building blocks sim- ilarly to the ones used in, e.g. GoogLeNet and ResNet models. These multi- branch elements are then stacked repetitively involving skip connections to build the final deep architecture. As we will see through the models detailed in this section, \u201dblock-wise\u201d architecture design reduces drastically search space speeding up search process, enhances generated networks performance and gives them more transferable ability through minor adaptation. 06.02.01. BlockQNN \u00b6 One of the first approaches implementing block-wise architecture search, BlockQNN [40] automatically builds convolutional neworks using Q-Learning reinforcement technique [45] with epsilon-greedy as exploration strategy [46]. The block structure is similar to ResNet and Inception (GoogLeNet) modern networks since it contains shortcut connections and multi-branch layer com- binations. The search space of the approach is reduced given that the focus is switched to explore network blocks rather than designing the entire network. The block search space is detailed in figure 11 and consists of 5 parameters: a layer index (its position in the block), an operation type (selected among 7 types commonly used), a kernel size and 2 predecessors layers indexes. Fig- ure 12 depicts 2 different samples of blocks, one with multi-branch structure and the second showing a skip connection. As described in previous sections, the Q-learning model includes an agent, states and actions, where the state represents the current layer of the agent and the action refers to the transi- tion to the next layer. On the basis of defined blocks, the complete network is constructed by stacking them sequentially N times. [![Fig.11][fig_11]][fig_11] Figure 11: Network structure code space [40]. 06.02.02. PNAS \u00b6 Progressive neural architecture search [47] proposes to explore the space of modular structures starting from simple models then evolving to more complex ones, discarding underperforming structures as learning progresses. The modular structure in this approach is called a cell and consists of a fixed number of blocks. Each block is a combination of 2 operators among 8 selected ones such as identity, pooling and convolution. A cell structure is learned first then it\u2019s stacked N times in order to build the resulting CNN. The main contribution of PNAS lies in the optimization of the search process by avoiding direct search in the entire space of cells. This was made possible with the use of a sequential model-based optimization (SMBO) strategy. [![Fig.12][fig_12]][fig_12] Figure 12: Representative block exemplars with their network structure codes [40]. The initial step consists in building, training and evaluating all possible 1- block cells. Then the cell is expanded to 2-block size exploding the number of total combinations. The innovation brought by PNAS is to predict the performance of the second level cells by training a RNN (predictor) on the performance of previous level ones. Only the K best cells (i.e. most promising ones) are transferred to the next step of cell size expansion. This process is repeated until the maximum allowed blocks number is reached. With an accuracy comparable to NAS [28] approach, PNAS is up to 5 times faster using a cell maximum size of 5 blocks and K equal to 256. This result is due to the fact that performance prediction takes much less time than full training of designed cells. The best cell architecture is shown in figure 13. 06.02.03. ENAS \u00b6 Efficient neural architecture search [29] comes in the continuity of previous work NAS [28] and PNAS [47]. It explores a cell-based search space through a controller RNN trained with reinforcement learning. The cell structure is similar to PNAS model where block concept is replaced with a node that consists of 2 operations and two skip connections. The controller RNN man- ages thus 2 types of decisions at each node. First it identifies 2 previous nodes to connect to, allowing the cell to set skip connections. Second, the controller selects 2 operations to implement among a set of 1 identity, 2 depth [![Fig.13][fig_13]][fig_13] wise-separable convolutions of filter sizes 3 \u2217 3 and 5 \u2217 5 [48], max pooling and average pooling both of size 3 \u2217 3. Within each node, the operations results are added in order to constitute an input for the next node. Figure 14 illustrates the design of a 4-node cell. At the end, the entire CNN is built by stacking N times convolutional cells. Another contribution of ENAS consists in sampling mini-batches from validation dataset to train designed models. The models with the best ac- curacy are then trained on the entire validation dataset. Additionally, the approach efficiency is greatly improved by implementing a weight sharing strategy. Each node has its own parameters (used when involved operations are activated) that are shared through inheritance by the generated child models. The latter are hence not trained from scratch saving a considerable processing time. ENAS provides competitive results on CIFAR-10 and Penn Treebank datasets. It specifically takes much less time to build the convolu- tion cells than previous approaches that adopt the same strategy of designing modular structures then stack them to obtain a final CNN. 06.02.04. EAS With Path Level Transformation \u00b6 A developed version of EAS [42] which adopts network transformation for efficient CNN architecture search is presented in [49]. The new ap- proach tackle the constraint of only performing plain architecture modifi- cation (layer-level), e.g. adding (removing) units, filters and layers, by using path-level transformation operations. The proposed model is similar to ([42]) [![Fig.14][fig_14]][fig_14] where the reinforcement learning meta-controller samples network transfor- mation actions to build new architectures. The latter are then trained and re- sulting accuracies are used as reward to update the meta-controller. However, certain changes have been implemented in order to adapt search methods to the tree-structured architecture space: using a tree-structured LSTM, ([50]) as meta-controller, defining a new action space consisting of feature maps allocation schemes (replication, skip), merge schemes (add, concatenation, none) and primitive operations (convolution, identity, depthwise-separable convolution, etc.). Figure 15 presents an example of transformation decisions operated by the meta-controller. Experimenting with ResNet and DenseNet architectures as base input, the path level transformation approach achieves competitive performance with state-of-the-art models maintaining low com- putational resources comparable to EAS approach ones. [![Fig.15][fig_15]][fig_15] Figure 15: Path-level transformation: from a single layer to a tree-structured motif [49]. 06.03. Architecture search accelerators \u00b6 Reinforcement learning methods have been applied successfully to design neural networks. Although multi-branch structures and skip connections improves the efficiency of architectures automatic search, the latter is still computationally expensive (hundreds of GPU hours), time consuming and requires further acceleration of learning process. Thus, in addition to the methods assigned to architectural search optimization and complex compo- nent building, some techniques are developed to speed up learning and are depicted in the current section. Early stopping strategy proposed in [40] enables fast convergence of the learning agent while maintaining an acceptable level of efficiency. This is pos- sible by taking into account intermediate rewards ignored in previous works (set to zero delaying reinforcement learning convergence [36]. In such case, the agent stops searching in an early training phase as the accuracy rewards reach higher levels in fewer iterations. The reward function is redefined in order to include designed block complexity and density and avoid possible poor accuracy resulting from training early stopping. A second technique is presented in [40] which consists of a distributed asynchronous framework assembling 3 nodes with different functions. The master node is the place where block structures are sampled by agent. Then, in the controller node, the entire network is built from generated blocks and transmitted to multiple compute nodes for training. The framework is a kind of simplified parameter-server [51] and allows the parallel training of designed networks in each compute nodes. Hence, the whole design and learning processing is operated in multiple machines and GPUs. [28] uses the same parameter server scheme with replication of controllers in order to train various architectures in parallel. As seen previously, reinforcement learning policies use explored architec- tures performance as a guiding reward for controllers updates. Training and evaluating every sampled architecture (among hundreds) on validation data is responsible for most of computational load. Extracting architecture perfor- mance was consequently subject to several estimation attempts. A number of approaches focus on performance prediction on the basis of past observa- tions. Most of such techniques are based on learning curve extrapolation [25] and surrogate models using RNN predictor [52] that aim at predicting and eliminating poor architectures before full training. Another idea to estimate performance and rank designed architectures is to use simplified (proxy) met- rics for training such as data subsets (mini-batches) [29] and down-sampled data (like images with lower resolution) [53]. Network transformation is one of the more recent techniques assigned to accelerate neural architecture search ([49], [54]). It consists in train- ing explored architectures reusing previously trained or existing networks. This modeling feature allows to address a limitation of reinforcement learn- ing approaches where training is performed with a random initialization of weights. Thus, extending network morphisms [55] to initiate architecture search through the transfer of experience and knowledge reflected by reused weights enables the framework to scrutinize the search space efficiently. Although the techniques presented above have saved substantial compu- tational resources for neural architecture search, there is still more effort needed to examine the extent of bias impact of such techniques on the search process. Indeed, it\u2019s crucial to assure that modifications brought through re-sampled data, discarded cases and early convergence do not influence the models original predictions. Further studies are thus required to verify that learning accelerators do not have amplified effect on approaches predictions and validation accuracies. 07. Conclusion \u00b6 The review of recent work trend on automatic design of CNN architectures raised some methodological options that are adopted by the majority of built approaches. Despite some attempts to use design meta-controllers based on evolutionary algorithms ([26], [27]) and Bayesian optimization ([25], [56]), reinforcement learning has shown promising empirical results and stands as the preferred strategy to train design controllers [57]. Another common conception option is the introduction of multi-branch (modular) structures as an elementary component of the entire network which restricts the search space to block/cell level. The plain network design is generally kept as a first step of proposed approaches application ([28], [42]) given that it leads to simple networks and allows to focus on the method itself before switching to more complex structures with modular design ([29], [49]). A third option used in design approaches at a lower scale is the prediction of explored architectures rewards before full training the most promising ones ([25], [29]). This training acceleration technique is implemented for performance improvement purpose and requires further attention to control possible bias impact on the models behavior. The success of current reinforcement-learning-based approaches to design CNN architectures is widely proven especially for image classification tasks. However, it is achieved at the cost of high computational resources despite the acceleration attempts of most of recent models. Such fact is preventing indi- vidual researchers and small research entities (companies and laboratories) from fully access to this innovative technology [42]. Hence, deeper and more revolutionary optimizing methods are required to practically operate CNN automatic design. Transformation approaches based on extended network morphisms [49] are among the first attempts in this direction that achieved drastic decrease in computational cost and demonstrated generalization ca- pacity. Additional future directions to control automatic design complexity is to develop methods for multi-task problems [58] and weights sharing [59] in order to benefit from knowledge transfer contributions. References \u00b6 [D. Kriesel, A Brief Introduction to Neural Networks, 2007.][2007_KrieselD] [V. Sze, Y. Chen, T. Yang, J. S. Emer, Efficient processing of deep neural networks: A tutorial and survey, Proceedings of the IEEE 105 (12) (2017) 2295\u20132329.] [O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei, Imagenet large scale visual recognition challenge, Int. J. Comput. Vision 115 (3) (2015) 211\u2013252.] [Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, L. D. Jackel, Handwritten digit recognition with a back- propagation network, in: D. S. Touretzky (Ed.), Advances in Neural Information Processing Systems 2, Morgan-Kaufmann, 1990, pp. 396\u2013 404.] [M. Minsky, S. Papert, Perceptrons: An Introduction to Computational Geometry, MIT Press, Cambridge, MA, USA, 1969.] [D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning representations by back-propagating errors, Nature 323 (1986) 533\u2013536.] [Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (11) (1998) 2278\u20132324.] [K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best multi-stage architecture for object recognition?, in: 2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 2146\u20132153.] [M. A. Nielsen, Neural Networks and Deep Learning, Determination Press, 2018.] D. Hubel, T. Wiesel, Receptive fields, binocular interaction, and func- tional architecture in the cat\u2019s visual cortex, Journal of Physiology 160 (1962) 106\u2013154. A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1097\u20131105. H. Wu, X. Gu, Max-pooling dropout for regularization of convolutional neural networks, in: Neural Information Processing - 22nd International Conference, ICONIP 2015, Istanbul, Turkey, November 9-12, 2015, Pro- ceedings, Part I, 2015, pp. 46\u201354. I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016. [M. Zeiler, R. Fergus, Stochastic pooling for regularization of deep con- volutional neural networks, in: Proceedings of the International Confer- ence on Learning Representations (ICLR), 2013.] [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, J. Mach. Learn. Res. 15 (1) (2014) 1929\u20131958.] [K. Simonyan, A. Zisserman, Very deep convolutional networks for large- scale image recognition, in: Proceedings of the International Conference on Learning Representations (ICLR), 2015.] [M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Com- puter Vision \u2013 ECCV 2014, Springer International Publishing, Cham, 2014, pp. 818\u2013833.] [C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.] [K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.] [C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi, Inception-v4, inception- resnet and the impact of residual connections on learning, in: S. P. Singh, S. Markovitch (Eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., AAAI Press, 2017, pp. 4278\u20134284.] [G. Huang, Z. Liu, L. v. d. Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261\u20132269.] [B. Baker, O. Gupta, R. Raskar, N. Naik, Accelerating neural architec- ture search using performance prediction, in: International Conference on Learning Representations, Workshop, 2018.] [Y. Bengio, A. Courville, P. Vincent, Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell. 35 (8) (2013) 1798\u20131828.] [J. S. Bergstra, R. Bardenet, Y. Bengio, B. K \u0301egl, Algorithms for hyper- parameter optimization, in: Advances in Neural Information Processing Systems 24, Curran Associates, Inc., 2011, pp. 2546\u20132554.] [T. Domhan, J. T. Springenberg, F. Hutter, Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves, in: Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI\u201915, AAAI Press, 2015, pp. 3460\u20133468.] [K. O. Stanley, D. B. D\u2019Ambrosio, J. Gauci, A hypercube-based encoding for evolving large-scale neural networks, Artif. Life 15 (2) (2009) 185\u2013 212.] [M. Suganuma, S. Shirakawa, T. Nagao, A genetic programming ap- proach to designing convolutional neural network architectures, in: Pro- ceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201917, ACM, New York, NY, USA, 2017, pp. 497\u2013504.] [B. Zoph, Q. V. Le, Neural architecture search with reinforcement learn- ing, in: Proceedings of the International Conference on Learning Rep- resentations (ICLR), 2017.] [H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, Efficient neural architec- ture search via parameters sharing, in: J. Dy, A. Krause (Eds.), Proceed- ings of the 35th International Conference on Machine Learning, Vol. 80, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 4095\u20134104.] [E. Brochu, T. Brochu, N. de Freitas, A bayesian interactive optimization approach to procedural animation design, in: Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association, Goslar Germany, Germany, 2010, pp. 103\u2013 112.] [A. Klein, S. Falkner, S. Bartels, P. Hennig, F. Hutter, Fast bayesian optimization of machine learning hyperparameters on large datasets, in: Proceedings of the 20th International Conference on Artificial Intelli- gence and Statistics (AISTATS 2017), Vol. 54 of Proceedings of Machine Learning Research, PMLR, 2017, pp. 528\u2013536.] [C. E. Rasmussen, C. K. I. Williams, Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning), The MIT Press, 2005.] [L. Breiman, Random forests, Mach. Learn. 45 (1) (2001) 5\u201332.] [A. E. Eiben, J. E. Smith, Introduction to Evolutionary Computing, 2nd Edition, Springer Publishing Company, Incorporated, 2015.] [E. Dufourq, B. A. Bassett, Eden: Evolutionary deep networks for ef- ficient machine learning, in: 2017 Pattern Recognition Association of South Africa and Robotics and Mechatronics (PRASA-RobMech), 2017, pp. 110\u2013115.] [R. S. Sutton, A. G. Barto, Reinforcement learning - an introduction, Adaptive computation and machine learning, MIT Press, 1998.] [K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, Deep reinforcement learning: A brief survey, IEEE Signal Processing Maga- zine 34 (6) (2017) 26\u201338.] [C. J. C. H. Watkins, P. Dayan, Q-learning, Machine Learning 8 (3) (1992) 279\u2013292.] [B. Baker, O. Gupta, N. Naik, R. Raskar, Designing neural network architectures using reinforcement learning, in: Proceedings of the Inter- national Conference on Learning Representations (ICLR), 2017.] [Z. Zhong, J. Yan, W. Wu, J. Shao, C.-L. Liu, Practical block-wise neural network architecture generation, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.] [F. Tan, P. Yan, X. Guan, Deep reinforcement learning: From q-learning to deep q-learning, in: D. Liu, S. Xie, Y. Li, D. Zhao, E.-S. M. El-Alfy (Eds.), Neural Information Processing, Springer International Publish- ing, Cham, 2017, pp. 475\u2013483.] [H. Cai, T. Chen, W. Zhang, Y. Yu, J. Wang, Efficient architecture search by network transformation, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innova- tive Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI- 18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2787\u20132794.] [T. Chen, I. Goodfellow, J. Shlens, Net2Net: Accelerating learning via knowledge transfer, in: International Conference on Learning Represen- tations (ICLR), 2016.] [M. Schuster, K. Paliwal, Bidirectional recurrent neural networks, Trans. Sig. Proc. 45 (11) (1997) 2673\u20132681.] [C. J. C. H. Watkins, Learning from delayed rewards, Ph.D. thesis, King\u2019s College, Cambridge, UK (1989).] [V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (2015) 529\u2013533.] [C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, K. Murphy, Progressive neural architecture search, in: V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (Eds.), Computer Vision \u2013 ECCV 2018, Springer International Publishing, Cham, 2018, pp. 19\u201335.] [F. Chollet, Xception: Deep learning with depthwise separable convo- lutions, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1800\u20131807.] [H. Cai, J. Yang, W. Zhang, S. Han, Y. Yu, Path-level network trans- formation for efficient architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stock- holmsmssan, Stockholm Sweden, 2018, pp. 678\u2013687.] [K. S. Tai, R. Socher, C. D. Manning, Improved semantic representations from tree-structured long short-term memory networks, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, 2015, pp. 1556\u20131566.] [J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng, Large scale distributed deep networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1223\u20131231.] [H. Liu, K. Simonyan, O. Vinyals, C. Fernando, K. Kavukcuoglu, Hierar- chical representations for efficient architecture search, in: International Conference on Learning Representations (ICLR), 2018.] [T. Hinz, N. Navarro-Guerrero, S. Magg, S. Wermter, Speeding up the Hyperparameter Optimization of Deep Convolutional Neural Networks, International Journal of Computational Intelligence and Applications 17 (2).] [F. H. Thomas Elsken, Jan Hendrik Metzen, Simple and efficient archi- tecture search for convolutional neural networks, in: Proceedings of the International Conference on Learning Representations (ICLR), 2018.] [T. Wei, C. Wang, C. W. Chen, Modularized morphing of neural net- works, in: International Conference on Learning Representations, Work- shop, 2017.] [H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, F. Hutter, To- wards automatically-tuned neural networks, in: F. Hutter, L. Kotthoff, J. Vanschoren (Eds.), Proceedings of the Workshop on Automatic Ma- chine Learning, Vol. 64, PMLR, New York, New York, USA, 2016, pp. 58\u201365.] [J. Perez-Rua, M. Baccouche, S. Pateux, Efficient progressive neural ar- chitecture search, in: British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, BMVA Press, 2018, p. 150.] [J. Liang, E. Meyerson, R. Miikkulainen, Evolutionary architecture search for deep multitask networks, in: Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201918, ACM, New York, NY, USA, 2018, pp. 466\u2013473.] [G. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, Q. Le, Un- derstanding and simplifying one-shot architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 550\u2013559.] \u00b6 img{width: 50%; float: right;}","title":"190427 Jaafra, Y., Laurent, J.L. 2018"},{"location":"190427_JaafraY_LaurentJL_2018/#00_abstract","text":"Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are gen- erally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. De- signing such architectures requires significant human expertise, substantial computation time and doesnt always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.","title":"00. Abstract"},{"location":"190427_JaafraY_LaurentJL_2018/#01_introduction","text":"","title":"01. Introduction"},{"location":"190427_JaafraY_LaurentJL_2018/#01p01","text":"\u201dA neuron is nothing more than a switch with information input and output. The switch will be activated if there are enough stimuli of other neurons hitting the information input. Then, at the information output, a pulse is sent to, for example, other neurons \u201d [1] . Brain-inspired machine learning imitates in a simplified manner the hierarchical operating mode of biological neurons [2] . The concept of artificial neural networks (ANN) achieved a huge progress from its first theoretical proposal in the 1950s until the recent considerable outcomes of deep learning. In computer vision and more specifically in classification tasks, CNN, which we will examine in this review, are among the most popular deep learning techniques since they are outperforming humans in some vision complex tasks [3] .","title":"01.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#01p02","text":"advances: backpropagation algorithm large training datasets / computational resources CNN differ in that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer The origin of CNN that were initially established by [4] goes back to the 1950s with the advent of \u201dperceptron\u201d, the first neural network prototyped by Frank Rosenblatt. However, neural network models were not extensively used until recently, after researchers overcame certain limits. Among these advances we can mention the generalization of perceptrons to many layers [5], the emergence of backpropagation algorithm as an appropriate training method for such architectures [6] and, mainly, the availability of large training datasets and computational resources to learn millions of parameters. CNN differ from classical neural networks in the fact that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer . This selective connection endow the network with the ability to operate, implicitly, hierarchical features extraction. For an image classification case, the first hidden layer can visualize edges, the second a specific shape and so on until the final layer that will identify the object.","title":"01.P02"},{"location":"190427_JaafraY_LaurentJL_2018/#01p03","text":"convolusion pooling fully connected multiple choices: num / order of layers hyperparameter (receptive fild size, stride) in this paper overview of deep learning history of CNN architectures several methods for automating CNN design according to search optimizaiton architecture design methods search acceleration techniques future works CNN architecture consists of several types of layers including convolution, pooling, and fully connected. The network expert has to make multiple choices while designing a CNN such as the number and ordering of layers , the hyperparameters for each type of layer ( receptive field size , stride , etc.). Thus, selecting the appropriate architecture and related hyperparameters requires a trial and error manual search process mainly directed by intuition and experience. Additionally, the number of available choices makes the selection space of CNN architectures extremely wide and impossible for an exhaustive manual exploration. Many research effort in meta-modeling tries to minimize human intervention in designing neural network architectures. In this paper, we first give a general overview and define the field of deep learning. We then briefly survey the history of CNN architectures. In the following section we review several methods for automating CNN design according to three dimensions: search optimization , architecture design methods (plain or modular) and search acceleration techniques . Finally, we conclude the article with a discussion of future works.","title":"01.P03"},{"location":"190427_JaafraY_LaurentJL_2018/#02_background","text":"","title":"02. Background"},{"location":"190427_JaafraY_LaurentJL_2018/#02p01","text":"Before embarking with CNN, we will introduce in this section some basic generalities about artificial networks and deep learning.","title":"02.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#0201_artificial_neural_networks","text":"","title":"02.01. Artificial Neural Networks"},{"location":"190427_JaafraY_LaurentJL_2018/#0201p01","text":"ANN are a major field of artificial intelligence that attempts to replicate human brain processing. Three types of neural layers distinguish an ANN: input, output and hidden layers. The latter operate transitional representa- tions of the input data evolving from low level features (lines and edges) to higher ones (complex patterns) as far as deeper layers are reached. Figure 1 provide an example of ANN involving classical fully-connected layers where every neuron is connected to all ones of the previous layer.","title":"02.01.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#figure_1","text":"[![Fig.1][fig_01]][fig_01] Figure 1: Artificial neural network, containing an input layer, an output layer and two hidden layers.","title":"Figure 1"},{"location":"190427_JaafraY_LaurentJL_2018/#0201p02","text":"weight bias During training, an ANN aims at learning two types of parameters that will condition its predictive performance. First, connection weights that assess to which extent a neuron result will impact the output of higher level neuron. Second, the bias which is a global estimator of a feature presence across all inputs. Hence, a neuron output can be formalized through a linear combination of weighted inputs and associated bias: \\text{output} = \\Big( \\sum_i{ \\text{input}_i \u2217 \\text{weight}_i} \\Big) + \\text{bias}","title":"02.01.P02"},{"location":"190427_JaafraY_LaurentJL_2018/#0201p03","text":"activation fn ReLU ( Rectified Linear Unit ) In order to allow the network operating non-linear transformations, an activation function is applied to the previous output. Equation 1 presents an example of such transformation using one of the most common and efficient activation function which is the Rectified Linear Unit ( ReLU ) [7]: f(x) = \\max(x, 0) \\tag{1}","title":"02.01.P03"},{"location":"190427_JaafraY_LaurentJL_2018/#0202_deep_learning","text":"","title":"02.02. Deep learning"},{"location":"190427_JaafraY_LaurentJL_2018/#0202p01","text":"Deep learning: ML processing w/i multi-layer ANN loss function evalation backpropagation The concept of deep learning refers to machine learning processing within multi-layer ANN [8]. The training of these networks relies on a loss function evaluation . For example, in supervised learning the loss is assimilated to the matching accuracy between ANN predictions and real expected outputs. An iterative update procedure is implemented to adjust network parameters according to loss function computed gradient. This procedure is called Backpropagation since parameters updates are spread from final layers to initial ones. Deep learning implies a certain number of challenges such as vanish- ing/exploding gradient and overfitting. The solutions to these problems will be discussed when developing CNN design architectures in next sections.","title":"02.02.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#03_cnn_layers","text":"","title":"03. CNN Layers"},{"location":"190427_JaafraY_LaurentJL_2018/#03p01","text":"3 characteristics local receptive fields reflect image data shared weights \u21d2 param reduction grid-structured image \u21d2 pooling operations CNN are widely used in a great number of pattern and image recognition problems. Three main characteristics are making this deep learning technique successful and suitable to visual data. First, local receptive fields perfectly reflect image data specificity to be correlated locally and uncorrelated in global segments . Second, shared weights allows a substantial parameter re- duction without altering image processing since the convolution is applicable to the whole image. Last, grid-structured image enable pooling operations that simplify data without losing useful information [9].","title":"03.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#0301_convolutional_layer","text":"","title":"03.01. Convolutional Layer"},{"location":"190427_JaafraY_LaurentJL_2018/#0301p01","text":"The convolutional layer is the basic CNN unit that has been inspired by physiological research evidence of hierarchical processing in the visual cor- tex of mammals 10 . Simple cells detect primitive attributes while more compound structures are subsequently extracted by complex cells. Thus, convolutional layer consists of a set of feature maps issued from convolving different filters (kernels) with an input image or previous layer output 11 . The 2-dimensional maps are stacked together to produce the resulting vol- ume of the convolutional layer. This process reduces drastically the network complexity since the neurons of a same feature map share the same weights and bias maintaining a low number of parameters to learn 12 .","title":"03.01.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#0301p02","text":"depth F num of filters stride S filter movement from receptive field to the next zero padding P The hyperparameters characterizing a convolutional layer are the depth F (number of filters), the stride S (filter movement from a receptive field to the next one) and the zero padding P to control input size 13 . Assuming that the filter size (\\text{height}, \\text{width}, \\text{depth}) = (h, w, F) , the dimensions of the feature maps generated can be obtained according to: ( H_1, W_1, F ) = \\bigg( \\frac{(H + 2P \u2212 h)}{S} + 1, \\frac{(W + 2P \u2212 w)}{S} + 1, F \\bigg) Where (H, W, F) is the size (\\text{height}, \\text{width}, \\text{depth}) of the input image.","title":"03.01.P02"},{"location":"190427_JaafraY_LaurentJL_2018/#0302_pooling_layer","text":"","title":"03.02. Pooling Layer"},{"location":"190427_JaafraY_LaurentJL_2018/#0302p01","text":"CNN architectures generally alternate convolution and pooling layers. The latter have the purpose of reducing network complexity and avoid the problem of overfitting. At biological level, pooling is assimilated to the behav- ior of cortical complex cells that reveal a certain degree of position invariance. A pooling layer neuron is connected to a region of the previous layer by per- forming a non-parameterized function. Thus it differs from convolution as it doesn\u2019t have learnable weights or bias and additionally, it keeps the same depth of the previous layer. Max pooling [14] is one of the most common type of pooling that consists in retaining the maximum value of a neurons cluster. It means that max pooling is detecting if a given feature has been identified in a receptive field without recording the exact location [9].","title":"03.02.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#0303_fully_connected_layer","text":"The convolution layers identify local features in the input data such as edges and shapes. The Fully connected layer operates the high level reasoning (classification for image case) by combining information from all the previous layers. As in a regular ANN, neurons at this level are fully connected to all ones in the previous layer. A softmax loss layer is then used to compute the probability distribution of the CNN final outputs.","title":"03.03. Fully connected Layer"},{"location":"190427_JaafraY_LaurentJL_2018/#04_cnn_architecture_history","text":"This section presents the most influential hand-crafted CNN architectures that have impacted the recent work on automatic architecture design. Most of them won at least one of the \u201dImageNet Large Scale Visual Recognition Competition\u201d (ILSVRC) challenges [3].","title":"04. CNN Architecture History"},{"location":"190427_JaafraY_LaurentJL_2018/#0401_lenet","text":"As mentioned previously, LeNet [7] was the innovative work that intro- duced convolutional networks. The model was experimented successfully to classify handwritten digits without any preprocessing of the input image (of size 32 \u2217 32 pixels). LeNet architecture is illustrated in figure 2. It consists of an input and an output layers of respective sizes 32 \u2217 32 and 10 as well as 6 hidden Layers. The basic idea of this design is to operate multiple con- volutions (3) with pooling in-between (2) then transmitting the final signal via a fully-connected layer toward the output layer. Unfortunately, due to the lack of adequate training data and computing power, it wasn\u2019t possible to extend this architecture to more complex applications. [![fig.2][fig_02]][fig_02] Figure 2: Architecture of LeNet-5 [7].","title":"04.01. LeNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0402_alexnet","text":"AlexNet [11] is one of the most influential deep CNN that won the ILSVRC (Imagenet Large Scale Visual Recognition Challenge) competitions in 2012. As shown in figure 3, it is not much different from LeNet. Never- theless, the corresponding architecture is deeper with 8 layers in total, 5 con- volutional and 3 fully connected. The effective contribution of AlexNet lies in several design and training specificities. First, it introduced the Rectified Linear Unit (ReLU) nonlinearity which helped to overcome the problem of vanishing gradient and boosted a faster training. Furthermore, AlexNet im- plements a dropout step [15] that consists in setting to zero a predefined per- centage of layers\u2019 parameters. This technique decreases learned parameters and controls neurons correlation in order to limit overfitting impact. Third, training process convergence is accelerated with momentum and conditional learning rate decrease (e.g. when learning stagnates). Finally, training data volume is increased artificially by generating variations of the original images that are shifted randomly. Thus the network learning is enhanced with the use of invariant representations of the data. [![fig.3][fig_03]][fig_03] Figure 3: An illustration of AlexNet [11].","title":"04.02. AlexNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0403_vggnet","text":"Submitted for the ILSVRC 2014, VGGNet [16] won the second place and demonstrated that deeper architectures achieve better results. Indeed, with its 19 hidden layers, it was much deeper than previous convolutional networks. In order to allow an increase in depth without an exponential growth of the parameters number, smaller convolution filters (3\u22173) were used in all layers (e.g. lower size than the 11 \u2217 11 filters adopted in AlexNet). An additional advantage of using smaller filters consists in reducing overlapping scanned pixels which results in feature maps with more local details [17].","title":"04.03. VGGNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0404_googlenet","text":"Since it has been demonstrated that a CNN architecture size is positively correlated to its performance, recent efforts focus on how to increase the depth of a CNN while keeping an acceptable number of parameters. Winner of ILSVRC 2014, GoogLeNet [18] innovated network design by replacing the classical strategy of alternating convolutional and pooling layers with stacked Inception Modules depicted in figure 4. Despite being deeper than VGGNet with 22 hidden layers, GoogLeNet requires outstandingly fewer parameters due to this sparse connection technique. Within an inception module, several convolutions with different scales and pooling are performed in parallel then concatenated in one single layer. This enables the CNN to detect patterns of various sizes within the same layer and avoid heavy parameters redundancies [18]. GoogLeNet hidden layers consist of 3 convolutions, 9 inception blocks (2 layers deep each one) and one fully connected. [![Fig.4][fig_04]][fig_04] Figure 4: Inception module [18].","title":"04.04. GoogLeNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0405_resnet","text":"Deep Residual Network (ResNet) [19], was the first neural network to ex- ceed human-level accuracy in ImageNet Challenge (ILSVRC 2015). Thanks to residual connections, such kind of architecture went deeper and was im- plemented with multiple versions of 34, 50, 101 and 152 layers. Indeed, one of the difficulties with very deep networks training is the vanishing gradi- ent during error backpropagation which penalizes the appropriate update of earlier layers weights. ResNet main contribution consists in dividing con- volutional layers into residual blocks. Each block is bypassed by a residual (skip) connection that forwards the block input using an identity mapping. The final output is the summation of the block output and the mapped input as illustrated in figure 5. By adding skip connections, backpropagation can be operated without any interference with previous layers which allows to prevent vanishing gra- dient and train very deep architectures. ResNet-101 consists of one convolu- tional layer followed by 33 residual blocks (3 layers deep each one), and one fully connected layer. [![Fig.5][fig_05]][fig_05] Figure 5: Residual learning: a building block [19]","title":"04.05. ResNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0406_more_networks","text":"After ResNet [19] success, which exceeded human-level accuracy in (ILSVRC 2015), the so-called modern hand-crafted CNN are still being designed on the basis of previous models looking for more efficiency and lower training time. Inception-v4 [20] is a new release of GoogLeNet that involved many more layers than the initial version. Inception-ResNet [20] is built as a combi- nation of an Inception network and a ResNet, joining inception blocks and residual connections. The last example of this section is DenseNet (Dense Convolutional Networks) [21] where each dense block layer is connected via skip connections to all subsequent ones allowing the learning of new features.","title":"04.06. More Networks"},{"location":"190427_JaafraY_LaurentJL_2018/#05_meta-modeling_for_cnn_automatic_architecture_design","text":"Meta-modeling for neural network architectures design aims at reduc- ing the intervention of human expertise in this process. The earliest meta- modeling methods were based on genetic algorithms and Bayesian optimiza- tion then more recently, reinforcement learning became among the most im- plemented approaches [22].","title":"05. Meta-modeling for CNN automatic architecture design"},{"location":"190427_JaafraY_LaurentJL_2018/#0501_context_of_automation","text":"The performance of a neural network and particularly a CNN mainly depends on the setting of the model structure, the training process, and the data representation. All of these variables are controlled through a number of hyperparameters and impact the learning process to a large extent. In order to achieve an optimal performance of CNN, these hyperparameters including the depth of the network, learning rates, layer type, number of units per layer, dropout rates, etc., should be then carefully tuned. On the other hand, the advent of deeper and more complex modern architectures (see section 4) is increasing the number and the types of hyperparameters. Hence, tuning step and more generally CNN architectures search become very expensive and heavy for an expert trial-and-error procedure. Additionally, CNN parameters setting is considered as a black-box [23] optimization problem because of the unknown nature of the mapping between the architecture, the performance, and the learning task. In this context, au- tomatic design solutions are highly required and instigates a large volume of research. The task of CNN hyperparameters tuning has been handled through meta-modeling that consists in applying machine learning models for designing CNN architectures. Three meta-modeling approaches are gen- erally used in the literature of architecture search and will be described in the next paragraph: bayesian optimization, evolutionary algorithms and re- inforcement learning.","title":"05.01. Context of automation"},{"location":"190427_JaafraY_LaurentJL_2018/#0502_meta-controllers","text":"Meta-modeling approaches perform iterative selection from the hyperpa- rameters space and build associated architectures that are then trained and evaluated. Accuracies records are fed to meta-modeling controllers (meta- controllers) to guide next architectures sampling. Meta-controllers for CNN design are mainly based on bayesian optimization ([24], [25]), evolutionary algorithms ([26], [27]) or more recently on reinforcement learning ([28], [29]). Bayesian optimization is an efficient way to optimize black-box objective functions f : X \\rightarrow R that are slow to evaluate [30]. It aims at finding an input x = \\arg \\min_{x \\in X} f(x) that globally minimizes f where in the context of a machine learning algorithm, x refers to the set of hyperparameters to optimize. The problem with this kind of optimization is that evaluating the objective function is very costly due to the great number of hyperparameters and the complex nature of models like deep neural networks. In order to overcome this problem, bayesian approaches propose probabilistic surrogate reconstruction of the objective function p(f|D) where D is a set of past observations. The evaluation of the empirical function is much cheaper than the true objective function [31]. Some of the most used probabilistic surrogate (regression) models are gaussian processes [32], random forests [33] and tree- structured Parzen estimator [24]. Briefly, the processing of a bayesian optimization consists in building an empirical (probabilistic) model of the objective function. Then, iteratively, the model identifies a set of optimal hyperparameters for which the objective function returns corresponding results (e.g. loss values). Each feedback al- lows the update of the surrogate model and the guidance of hyperparameters predictions until the process reaches a termination condition. Evolutionary algorithms present another strategy of hyperparameters op- timization that modifies a set of candidate solutions (population) on the basis of a number of rules (operators). Following an iterative procedure of muta- tion, crossover and selection [34], an evolutionary algorithm initializes, in a first step, a set of N random networks to create a primary population. The second step consists in introducing a fitness function to score each net- work through its classification accuracy and keep the top ranked networks to construct the next generation. The evolutionary process continues until a termination criteria is met, which is generally defined as the maximum num- ber of allowed generations. One of the advantage of evolutionary algorithms is the adaptation to complex combination of discrete (layer type) and contin- uous (learning rate) hyperparameters which is suitable to neuronal network optimization models [35]. An important approach for goal-oriented optimization is reinforcement learning (RL) inspired from behaviorist psychology [36]. The frame of RL is an agent learning through interaction with its environment (figure 6). Thus the agent adapts its behavior (transition to a state s_{t+1} ) on the basis of observed consequences (rewards) of an action at taken in state s_t . The agent purpose is to learn a policy \\pi that is able to identify the optimal sequence of actions maximizing the expected cumulative rewards. The environment return reinforces the agent to select new actions to improve learning process, hence the name of reinforcement learning. [![Fig.6][fig_06]][fig_06] Figure 6: Illustration of the RL process. The methods developed to resolve reinforcement tasks are based on value functions, policy search or a combination of both strategies (actor-critic methods) [37]. Value function methods consist in estimating the expected reward value R when reaching a given state s and following a policy \\pi : V ^\\pi(s) = \\mathbb{E} [\\mathfrak{R}|s, \\pi] A recursive form of this function is particularly used in recent Q-learning [38] models assigned to CNN architecture design ([39], [40]): Q(s_t, a_t) = Q(s_t, a_t) + \\alpha[r_{t + 1} + \\gamma \\text{max}_a Q(s_{t+1}, a) \u2212 Q(s_t, a_t)] Where s_t is a current state, a_t is a current action, \\alpha is the learning rate, r_{t+1} is the reward earned when transitioning from time t to the next and \\gamma is the discount rate. In contrast to value function methods, policy search methods do not implement a value function and apply, instead, a gradient-based procedure to identify directly an optimal policy \\pi^\u2217 . In this context, deep reinforcement learning is achieved when deep neural networks are used to approximate one of the reinforcement learning components : value function, policy or reward function [41]. Among the active fields of designing CNN architectures through deep reinforcement learning, recurrent neural networks (RNN) arise as a valuable model that handles a set of tasks such as hyperparameters prediction ([28], [29]). In fact, a RNN operates sequentially involving hidden units to store processing history, which allows the reinforcement learning to profit from past observations. Long short term memory networks (LSTM), a variant of RNN, offers a more efficient way of evolving conditionally on the basis of previous elements.","title":"05.02. Meta-controllers"},{"location":"190427_JaafraY_LaurentJL_2018/#06_neural_architecture_search","text":"Various strategies have been developed to operate CNN architectures de- sign for the majority of which reinforcement learning has been selected as meta-controller. This section is assigned to review in detail most recent promising automatic search approaches differentiated according to search spaces specificities and complexity level.","title":"06. Neural Architecture Search"},{"location":"190427_JaafraY_LaurentJL_2018/#0601_plain_architecture_design","text":"Some architecture search approaches focus on designing plain CNN which consists exclusively of conventional layers, mainly convolution, pooling and fully-connected. The resulting research space is relatively simple and the approaches contribution lies almost entirely in the design strategy.","title":"06.01. Plain Architecture Design"},{"location":"190427_JaafraY_LaurentJL_2018/#060101_metaqnn","text":"MetaQNN model [39] relies on Q-learning, a type of reinforcement learn- ing (refer to previous section for more details), to sequentially select network layers and their parameters among a finite space. This method implies, first, the definition of each learning agent state as a layer with all associated relevant parameters. As an example, 5 layers are depicted in figure 7: convo- lution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM). [![Fig.7][fig_07]][fig_07] Figure 7: State space possible parameters [39]. Second, the agent action space is assimilated to the possible layers the agent may move to given a certain number of constraints set intentionally, for the majority, to enable faster convergence. Figure 8 illustrates a set of state and action spaces and an eventual agent path to design a CNN architecture. MetaQNN was evaluated competitive with similar and different hand-crafted CNN architectures as with existing automated network design methods. [![Fig.8][fig_08]][fig_08] Figure 8: An illustration of the full state and action space (a) and a path that the agent has chosen (b) [39].","title":"06.01.01. MetaQNN"},{"location":"190427_JaafraY_LaurentJL_2018/#060102_nas","text":"Using reinforcement learning, [28] train a recurrent neural network to generate convolutional architectures. Figure 9 shows a RNN controller gener- ating sequentially CNN parameters associated to convolutional layers. Every sequence output is predicted by a softmax classifier then used as input of the next sequence. The parameters set consists of filter height and width, stride height and width and the number of filters per layer. The design of an ar- chitecture takes an end once the number of layers reaches a predefined value that increases all along training. The accuracy of the designed architecture is fed as a reward to train the RNN controller through reinforcement learning in order to maximize the expected validation accuracy of the next architectures. The experimentation of the global approach achieved competitive results on CIFAR-10 and Penn Treebank datasets. [![Fig.9][fig_09]][fig_09] Figure 9: Illustration of the way the agent used to select hyperparameters [28].","title":"06.01.02. NAS"},{"location":"190427_JaafraY_LaurentJL_2018/#060103_eas","text":"In their very recent work Efficient Architecture Search, [42] implement network transformation techniques that allow reusing pre-existing models and efficiently exploring search space for automatic architecture design. This novel approach differs from the previous ones in the definition of reinforce- ment learning states and actions. The state is the current network archi- tecture while the action involves network transformation operations such as adding, enlarging and deleting layers. Starting point architectures used in ex- periments are plain CNN which only consist of convolutional, fully-connected and pooling layers. EAS approach is inspired from Net2Net technique intro- duced in [43] and based on the idea of building deeper student network to reproduce the same processing of an associated teacher network. As shown in figure 10, an encoder network implemented with bidirectional recurrent neural network [44] feeds actors network with given architectures. The se- lected actor networks performs 2 types of transformation: widening layers in terms of units and filters and inserting new layers. EAS outperforms sim- ilar state-of-the-art models designed either manually or automatically with the attractive advantage of using relatively much smaller computational re- sources. [![Fig.10][fig_10]][fig_10] Figure 10: A meta-controller operation for network transformation [42].","title":"06.01.03. EAS"},{"location":"190427_JaafraY_LaurentJL_2018/#0602_modular_architecture_design","text":"Most of recent work on neural architecture search is based on more com- plex modular (multi-branch) structures inspired by modern architectures pre- sented in section 4. Rather than operating the tedious search over entire networks, this second set of approaches focus on finding building blocks sim- ilarly to the ones used in, e.g. GoogLeNet and ResNet models. These multi- branch elements are then stacked repetitively involving skip connections to build the final deep architecture. As we will see through the models detailed in this section, \u201dblock-wise\u201d architecture design reduces drastically search space speeding up search process, enhances generated networks performance and gives them more transferable ability through minor adaptation.","title":"06.02. Modular Architecture Design"},{"location":"190427_JaafraY_LaurentJL_2018/#060201_blockqnn","text":"One of the first approaches implementing block-wise architecture search, BlockQNN [40] automatically builds convolutional neworks using Q-Learning reinforcement technique [45] with epsilon-greedy as exploration strategy [46]. The block structure is similar to ResNet and Inception (GoogLeNet) modern networks since it contains shortcut connections and multi-branch layer com- binations. The search space of the approach is reduced given that the focus is switched to explore network blocks rather than designing the entire network. The block search space is detailed in figure 11 and consists of 5 parameters: a layer index (its position in the block), an operation type (selected among 7 types commonly used), a kernel size and 2 predecessors layers indexes. Fig- ure 12 depicts 2 different samples of blocks, one with multi-branch structure and the second showing a skip connection. As described in previous sections, the Q-learning model includes an agent, states and actions, where the state represents the current layer of the agent and the action refers to the transi- tion to the next layer. On the basis of defined blocks, the complete network is constructed by stacking them sequentially N times. [![Fig.11][fig_11]][fig_11] Figure 11: Network structure code space [40].","title":"06.02.01. BlockQNN"},{"location":"190427_JaafraY_LaurentJL_2018/#060202_pnas","text":"Progressive neural architecture search [47] proposes to explore the space of modular structures starting from simple models then evolving to more complex ones, discarding underperforming structures as learning progresses. The modular structure in this approach is called a cell and consists of a fixed number of blocks. Each block is a combination of 2 operators among 8 selected ones such as identity, pooling and convolution. A cell structure is learned first then it\u2019s stacked N times in order to build the resulting CNN. The main contribution of PNAS lies in the optimization of the search process by avoiding direct search in the entire space of cells. This was made possible with the use of a sequential model-based optimization (SMBO) strategy. [![Fig.12][fig_12]][fig_12] Figure 12: Representative block exemplars with their network structure codes [40]. The initial step consists in building, training and evaluating all possible 1- block cells. Then the cell is expanded to 2-block size exploding the number of total combinations. The innovation brought by PNAS is to predict the performance of the second level cells by training a RNN (predictor) on the performance of previous level ones. Only the K best cells (i.e. most promising ones) are transferred to the next step of cell size expansion. This process is repeated until the maximum allowed blocks number is reached. With an accuracy comparable to NAS [28] approach, PNAS is up to 5 times faster using a cell maximum size of 5 blocks and K equal to 256. This result is due to the fact that performance prediction takes much less time than full training of designed cells. The best cell architecture is shown in figure 13.","title":"06.02.02. PNAS"},{"location":"190427_JaafraY_LaurentJL_2018/#060203_enas","text":"Efficient neural architecture search [29] comes in the continuity of previous work NAS [28] and PNAS [47]. It explores a cell-based search space through a controller RNN trained with reinforcement learning. The cell structure is similar to PNAS model where block concept is replaced with a node that consists of 2 operations and two skip connections. The controller RNN man- ages thus 2 types of decisions at each node. First it identifies 2 previous nodes to connect to, allowing the cell to set skip connections. Second, the controller selects 2 operations to implement among a set of 1 identity, 2 depth [![Fig.13][fig_13]][fig_13] wise-separable convolutions of filter sizes 3 \u2217 3 and 5 \u2217 5 [48], max pooling and average pooling both of size 3 \u2217 3. Within each node, the operations results are added in order to constitute an input for the next node. Figure 14 illustrates the design of a 4-node cell. At the end, the entire CNN is built by stacking N times convolutional cells. Another contribution of ENAS consists in sampling mini-batches from validation dataset to train designed models. The models with the best ac- curacy are then trained on the entire validation dataset. Additionally, the approach efficiency is greatly improved by implementing a weight sharing strategy. Each node has its own parameters (used when involved operations are activated) that are shared through inheritance by the generated child models. The latter are hence not trained from scratch saving a considerable processing time. ENAS provides competitive results on CIFAR-10 and Penn Treebank datasets. It specifically takes much less time to build the convolu- tion cells than previous approaches that adopt the same strategy of designing modular structures then stack them to obtain a final CNN.","title":"06.02.03. ENAS"},{"location":"190427_JaafraY_LaurentJL_2018/#060204_eas_with_path_level_transformation","text":"A developed version of EAS [42] which adopts network transformation for efficient CNN architecture search is presented in [49]. The new ap- proach tackle the constraint of only performing plain architecture modifi- cation (layer-level), e.g. adding (removing) units, filters and layers, by using path-level transformation operations. The proposed model is similar to ([42]) [![Fig.14][fig_14]][fig_14] where the reinforcement learning meta-controller samples network transfor- mation actions to build new architectures. The latter are then trained and re- sulting accuracies are used as reward to update the meta-controller. However, certain changes have been implemented in order to adapt search methods to the tree-structured architecture space: using a tree-structured LSTM, ([50]) as meta-controller, defining a new action space consisting of feature maps allocation schemes (replication, skip), merge schemes (add, concatenation, none) and primitive operations (convolution, identity, depthwise-separable convolution, etc.). Figure 15 presents an example of transformation decisions operated by the meta-controller. Experimenting with ResNet and DenseNet architectures as base input, the path level transformation approach achieves competitive performance with state-of-the-art models maintaining low com- putational resources comparable to EAS approach ones. [![Fig.15][fig_15]][fig_15] Figure 15: Path-level transformation: from a single layer to a tree-structured motif [49].","title":"06.02.04. EAS With Path Level Transformation"},{"location":"190427_JaafraY_LaurentJL_2018/#0603_architecture_search_accelerators","text":"Reinforcement learning methods have been applied successfully to design neural networks. Although multi-branch structures and skip connections improves the efficiency of architectures automatic search, the latter is still computationally expensive (hundreds of GPU hours), time consuming and requires further acceleration of learning process. Thus, in addition to the methods assigned to architectural search optimization and complex compo- nent building, some techniques are developed to speed up learning and are depicted in the current section. Early stopping strategy proposed in [40] enables fast convergence of the learning agent while maintaining an acceptable level of efficiency. This is pos- sible by taking into account intermediate rewards ignored in previous works (set to zero delaying reinforcement learning convergence [36]. In such case, the agent stops searching in an early training phase as the accuracy rewards reach higher levels in fewer iterations. The reward function is redefined in order to include designed block complexity and density and avoid possible poor accuracy resulting from training early stopping. A second technique is presented in [40] which consists of a distributed asynchronous framework assembling 3 nodes with different functions. The master node is the place where block structures are sampled by agent. Then, in the controller node, the entire network is built from generated blocks and transmitted to multiple compute nodes for training. The framework is a kind of simplified parameter-server [51] and allows the parallel training of designed networks in each compute nodes. Hence, the whole design and learning processing is operated in multiple machines and GPUs. [28] uses the same parameter server scheme with replication of controllers in order to train various architectures in parallel. As seen previously, reinforcement learning policies use explored architec- tures performance as a guiding reward for controllers updates. Training and evaluating every sampled architecture (among hundreds) on validation data is responsible for most of computational load. Extracting architecture perfor- mance was consequently subject to several estimation attempts. A number of approaches focus on performance prediction on the basis of past observa- tions. Most of such techniques are based on learning curve extrapolation [25] and surrogate models using RNN predictor [52] that aim at predicting and eliminating poor architectures before full training. Another idea to estimate performance and rank designed architectures is to use simplified (proxy) met- rics for training such as data subsets (mini-batches) [29] and down-sampled data (like images with lower resolution) [53]. Network transformation is one of the more recent techniques assigned to accelerate neural architecture search ([49], [54]). It consists in train- ing explored architectures reusing previously trained or existing networks. This modeling feature allows to address a limitation of reinforcement learn- ing approaches where training is performed with a random initialization of weights. Thus, extending network morphisms [55] to initiate architecture search through the transfer of experience and knowledge reflected by reused weights enables the framework to scrutinize the search space efficiently. Although the techniques presented above have saved substantial compu- tational resources for neural architecture search, there is still more effort needed to examine the extent of bias impact of such techniques on the search process. Indeed, it\u2019s crucial to assure that modifications brought through re-sampled data, discarded cases and early convergence do not influence the models original predictions. Further studies are thus required to verify that learning accelerators do not have amplified effect on approaches predictions and validation accuracies.","title":"06.03. Architecture search accelerators"},{"location":"190427_JaafraY_LaurentJL_2018/#07_conclusion","text":"The review of recent work trend on automatic design of CNN architectures raised some methodological options that are adopted by the majority of built approaches. Despite some attempts to use design meta-controllers based on evolutionary algorithms ([26], [27]) and Bayesian optimization ([25], [56]), reinforcement learning has shown promising empirical results and stands as the preferred strategy to train design controllers [57]. Another common conception option is the introduction of multi-branch (modular) structures as an elementary component of the entire network which restricts the search space to block/cell level. The plain network design is generally kept as a first step of proposed approaches application ([28], [42]) given that it leads to simple networks and allows to focus on the method itself before switching to more complex structures with modular design ([29], [49]). A third option used in design approaches at a lower scale is the prediction of explored architectures rewards before full training the most promising ones ([25], [29]). This training acceleration technique is implemented for performance improvement purpose and requires further attention to control possible bias impact on the models behavior. The success of current reinforcement-learning-based approaches to design CNN architectures is widely proven especially for image classification tasks. However, it is achieved at the cost of high computational resources despite the acceleration attempts of most of recent models. Such fact is preventing indi- vidual researchers and small research entities (companies and laboratories) from fully access to this innovative technology [42]. Hence, deeper and more revolutionary optimizing methods are required to practically operate CNN automatic design. Transformation approaches based on extended network morphisms [49] are among the first attempts in this direction that achieved drastic decrease in computational cost and demonstrated generalization ca- pacity. Additional future directions to control automatic design complexity is to develop methods for multi-task problems [58] and weights sharing [59] in order to benefit from knowledge transfer contributions.","title":"07. Conclusion"},{"location":"190427_JaafraY_LaurentJL_2018/#references","text":"[D. Kriesel, A Brief Introduction to Neural Networks, 2007.][2007_KrieselD] [V. Sze, Y. Chen, T. Yang, J. S. Emer, Efficient processing of deep neural networks: A tutorial and survey, Proceedings of the IEEE 105 (12) (2017) 2295\u20132329.] [O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei, Imagenet large scale visual recognition challenge, Int. J. Comput. Vision 115 (3) (2015) 211\u2013252.] [Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, L. D. Jackel, Handwritten digit recognition with a back- propagation network, in: D. S. Touretzky (Ed.), Advances in Neural Information Processing Systems 2, Morgan-Kaufmann, 1990, pp. 396\u2013 404.] [M. Minsky, S. Papert, Perceptrons: An Introduction to Computational Geometry, MIT Press, Cambridge, MA, USA, 1969.] [D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning representations by back-propagating errors, Nature 323 (1986) 533\u2013536.] [Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (11) (1998) 2278\u20132324.] [K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best multi-stage architecture for object recognition?, in: 2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 2146\u20132153.] [M. A. Nielsen, Neural Networks and Deep Learning, Determination Press, 2018.] D. Hubel, T. Wiesel, Receptive fields, binocular interaction, and func- tional architecture in the cat\u2019s visual cortex, Journal of Physiology 160 (1962) 106\u2013154. A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1097\u20131105. H. Wu, X. Gu, Max-pooling dropout for regularization of convolutional neural networks, in: Neural Information Processing - 22nd International Conference, ICONIP 2015, Istanbul, Turkey, November 9-12, 2015, Pro- ceedings, Part I, 2015, pp. 46\u201354. I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016. [M. Zeiler, R. Fergus, Stochastic pooling for regularization of deep con- volutional neural networks, in: Proceedings of the International Confer- ence on Learning Representations (ICLR), 2013.] [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, J. Mach. Learn. Res. 15 (1) (2014) 1929\u20131958.] [K. Simonyan, A. Zisserman, Very deep convolutional networks for large- scale image recognition, in: Proceedings of the International Conference on Learning Representations (ICLR), 2015.] [M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Com- puter Vision \u2013 ECCV 2014, Springer International Publishing, Cham, 2014, pp. 818\u2013833.] [C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.] [K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.] [C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi, Inception-v4, inception- resnet and the impact of residual connections on learning, in: S. P. Singh, S. Markovitch (Eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., AAAI Press, 2017, pp. 4278\u20134284.] [G. Huang, Z. Liu, L. v. d. Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261\u20132269.] [B. Baker, O. Gupta, R. Raskar, N. Naik, Accelerating neural architec- ture search using performance prediction, in: International Conference on Learning Representations, Workshop, 2018.] [Y. Bengio, A. Courville, P. Vincent, Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell. 35 (8) (2013) 1798\u20131828.] [J. S. Bergstra, R. Bardenet, Y. Bengio, B. K \u0301egl, Algorithms for hyper- parameter optimization, in: Advances in Neural Information Processing Systems 24, Curran Associates, Inc., 2011, pp. 2546\u20132554.] [T. Domhan, J. T. Springenberg, F. Hutter, Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves, in: Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI\u201915, AAAI Press, 2015, pp. 3460\u20133468.] [K. O. Stanley, D. B. D\u2019Ambrosio, J. Gauci, A hypercube-based encoding for evolving large-scale neural networks, Artif. Life 15 (2) (2009) 185\u2013 212.] [M. Suganuma, S. Shirakawa, T. Nagao, A genetic programming ap- proach to designing convolutional neural network architectures, in: Pro- ceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201917, ACM, New York, NY, USA, 2017, pp. 497\u2013504.] [B. Zoph, Q. V. Le, Neural architecture search with reinforcement learn- ing, in: Proceedings of the International Conference on Learning Rep- resentations (ICLR), 2017.] [H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, Efficient neural architec- ture search via parameters sharing, in: J. Dy, A. Krause (Eds.), Proceed- ings of the 35th International Conference on Machine Learning, Vol. 80, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 4095\u20134104.] [E. Brochu, T. Brochu, N. de Freitas, A bayesian interactive optimization approach to procedural animation design, in: Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association, Goslar Germany, Germany, 2010, pp. 103\u2013 112.] [A. Klein, S. Falkner, S. Bartels, P. Hennig, F. Hutter, Fast bayesian optimization of machine learning hyperparameters on large datasets, in: Proceedings of the 20th International Conference on Artificial Intelli- gence and Statistics (AISTATS 2017), Vol. 54 of Proceedings of Machine Learning Research, PMLR, 2017, pp. 528\u2013536.] [C. E. Rasmussen, C. K. I. Williams, Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning), The MIT Press, 2005.] [L. Breiman, Random forests, Mach. Learn. 45 (1) (2001) 5\u201332.] [A. E. Eiben, J. E. Smith, Introduction to Evolutionary Computing, 2nd Edition, Springer Publishing Company, Incorporated, 2015.] [E. Dufourq, B. A. Bassett, Eden: Evolutionary deep networks for ef- ficient machine learning, in: 2017 Pattern Recognition Association of South Africa and Robotics and Mechatronics (PRASA-RobMech), 2017, pp. 110\u2013115.] [R. S. Sutton, A. G. Barto, Reinforcement learning - an introduction, Adaptive computation and machine learning, MIT Press, 1998.] [K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, Deep reinforcement learning: A brief survey, IEEE Signal Processing Maga- zine 34 (6) (2017) 26\u201338.] [C. J. C. H. Watkins, P. Dayan, Q-learning, Machine Learning 8 (3) (1992) 279\u2013292.] [B. Baker, O. Gupta, N. Naik, R. Raskar, Designing neural network architectures using reinforcement learning, in: Proceedings of the Inter- national Conference on Learning Representations (ICLR), 2017.] [Z. Zhong, J. Yan, W. Wu, J. Shao, C.-L. Liu, Practical block-wise neural network architecture generation, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.] [F. Tan, P. Yan, X. Guan, Deep reinforcement learning: From q-learning to deep q-learning, in: D. Liu, S. Xie, Y. Li, D. Zhao, E.-S. M. El-Alfy (Eds.), Neural Information Processing, Springer International Publish- ing, Cham, 2017, pp. 475\u2013483.] [H. Cai, T. Chen, W. Zhang, Y. Yu, J. Wang, Efficient architecture search by network transformation, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innova- tive Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI- 18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2787\u20132794.] [T. Chen, I. Goodfellow, J. Shlens, Net2Net: Accelerating learning via knowledge transfer, in: International Conference on Learning Represen- tations (ICLR), 2016.] [M. Schuster, K. Paliwal, Bidirectional recurrent neural networks, Trans. Sig. Proc. 45 (11) (1997) 2673\u20132681.] [C. J. C. H. Watkins, Learning from delayed rewards, Ph.D. thesis, King\u2019s College, Cambridge, UK (1989).] [V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (2015) 529\u2013533.] [C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, K. Murphy, Progressive neural architecture search, in: V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (Eds.), Computer Vision \u2013 ECCV 2018, Springer International Publishing, Cham, 2018, pp. 19\u201335.] [F. Chollet, Xception: Deep learning with depthwise separable convo- lutions, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1800\u20131807.] [H. Cai, J. Yang, W. Zhang, S. Han, Y. Yu, Path-level network trans- formation for efficient architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stock- holmsmssan, Stockholm Sweden, 2018, pp. 678\u2013687.] [K. S. Tai, R. Socher, C. D. Manning, Improved semantic representations from tree-structured long short-term memory networks, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, 2015, pp. 1556\u20131566.] [J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng, Large scale distributed deep networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1223\u20131231.] [H. Liu, K. Simonyan, O. Vinyals, C. Fernando, K. Kavukcuoglu, Hierar- chical representations for efficient architecture search, in: International Conference on Learning Representations (ICLR), 2018.] [T. Hinz, N. Navarro-Guerrero, S. Magg, S. Wermter, Speeding up the Hyperparameter Optimization of Deep Convolutional Neural Networks, International Journal of Computational Intelligence and Applications 17 (2).] [F. H. Thomas Elsken, Jan Hendrik Metzen, Simple and efficient archi- tecture search for convolutional neural networks, in: Proceedings of the International Conference on Learning Representations (ICLR), 2018.] [T. Wei, C. Wang, C. W. Chen, Modularized morphing of neural net- works, in: International Conference on Learning Representations, Work- shop, 2017.] [H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, F. Hutter, To- wards automatically-tuned neural networks, in: F. Hutter, L. Kotthoff, J. Vanschoren (Eds.), Proceedings of the Workshop on Automatic Ma- chine Learning, Vol. 64, PMLR, New York, New York, USA, 2016, pp. 58\u201365.] [J. Perez-Rua, M. Baccouche, S. Pateux, Efficient progressive neural ar- chitecture search, in: British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, BMVA Press, 2018, p. 150.] [J. Liang, E. Meyerson, R. Miikkulainen, Evolutionary architecture search for deep multitask networks, in: Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201918, ACM, New York, NY, USA, 2018, pp. 466\u2013473.] [G. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, Q. Le, Un- derstanding and simplifying one-shot architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 550\u2013559.]","title":"References"},{"location":"190507_NelsonPeterT2019/","text":"19-05-07 Limbic-predominant age-related TDP-43 encephalopathy (LATE): consensus working group report \u00b6 Original | Mendeley ToC \u00b6 00. Abstract 01. Introduction 02. Background 02.01. LATE neuropathological changes 02.02. LATE MRI studies 02.03. Recommendations for routine autopsy evaluation and classification of LATE-NC 02.04. Clinical and neurocognitive features of LATE 02.05. Public health impact of LATE 02.06. Genetics of LATE 02.07. LATE biomarkers 02.08. Implications for Alzheimer\u2019s disease and LATE clinical trials 03. Conclusions and future directions 00. Abstract \u00b6 LATE ( limbic-predominant age-related TDP-43 encephalopathy ) LATE-NC (LATE neuropathological change) We describe a recently recognized disease entity, limbic-predominant age-related TDP-43 encephalopathy (LATE). LATE neuropathological change (LATE-NC) is defined by a stereotypical TDP-43 proteinopathy in older adults, with or without coexisting hippocampal sclerosis pathology. LATE-NC is a common TDP-43 proteinopathy, associated with an amnestic dementia syndrome that mimicked Alzheimer\u2019s-type dementia in retrospective autopsy studies. LATE is distinguished from frontotemporal lobar degeneration with TDP-43 pathology based on its epidemiology (LATE generally affects older subjects), and relatively restricted neuroanatomical distribution of TDP-43 proteinopathy. In community-based autopsy cohorts, \u223c25% of brains had sufficient burden of LATE-NC to be associated with discernible cognitive impairment. Many subjects with LATE-NC have comorbid brain pathologies, often including amyloid-\u03b2 plaques and tauopathy. Given that the \u2018oldest-old\u2019 are at greatest risk for LATE-NC, and subjects of advanced age constitute a rapidly growing demographic group in many countries, LATE has an expanding but under-recognized impact on public health. For these reasons, a working group was convened to develop diagnostic criteria for LATE, aiming both to stimulate research and to promote awareness of this pathway to dementia. We report consensus-based recommendations including guidelines for diagnosis and staging of LATE-NC. For routine autopsy workup of LATE-NC, an anatomically-based preliminary staging scheme is proposed with TDP-43 immunohistochemistry on tissue from three brain areas, reflecting a hierarchical pattern of brain involvement: amygdala, hippocampus, and middle frontal gyrus. LATE-NC appears to affect the medial temporal lobe structures preferentially, but other areas also are impacted. Neuroimaging studies demonstrated that subjects with LATE-NC also had atrophy in the medial temporal lobes, frontal cortex, and other brain regions. Genetic studies have thus far indicated five genes with risk alleles for LATE-NC: GRN , TMEM106B , ABCC9 , KCNMB2 , and APOE . The discovery of these genetic risk variants indicate that LATE shares pathogenetic mechanisms with both frontotemporal lobar degeneration and Alzheimer\u2019s disease, but also suggests disease-specific underlying mechanisms. Large gaps remain in our understanding of LATE. For advances in prevention, diagnosis, and treatment, there is an urgent need for research focused on LATE, including in vitro and animal models. An obstacle to clinical progress is lack of diagnostic tools, such as biofluid or neuroimaging biomarkers, for ante-mortem detection of LATE. Development of a disease biomarker would augment observational studies seeking to further define the risk factors, natural history, and clinical features of LATE, as well as eventual subject recruitment for targeted therapies in clinical trials. 01. Introduction \u00b6 Transactive response DNA binding protein of 43 kDa (TDP-43) proteinopathy in limbic brain structures is commonly observed in subjects past 80 years of age. This proteinopathy has been associated with substantial cognitive impairment that mimicked Alzheimer\u2019s disease clinical syndrome in retrospective studies. Despite evidence from many sources attest ing to the public health impact of age-related TDP-43 proteinopathy, there is as yet no consensus-based nomenclature . To address this problem, we propose new terminology: limbic-predominant age-related TDP-43 encephalopathy (LATE). Guidelines are suggested for the autopsy evaluation and staging of LATE neuropathological change (LATE-NC). We review the medical literature pertaining to LATE, including cognitive manifestations, neuroimaging, public health impact, and genetics. The importance of LATE as a contributing factor in neurodegeneration is stressed, as are the needs for specific LATE biomarker development, TDP-43 focused drug discovery, and eventual clinical trials. We conclude by highlighting important knowledge gaps and potential future directions for research on LATE. Summary points are presented in Box 1. Box 1 \u00b6 LATE and LATE-NC summary points LATE-NC features A sampling and staging system for routine autopsy diagnosis is proposed to characterize the anatomical distribution of TDP-43 proteinopathy Stage 1: amygdala only Stage 2: +hippocampus Stage 3: +middle frontal gyrus Hippocampal sclerosis pathology may be observed (and should be reported), but is neither necessary nor sufficient for diagnosis of LATE-NC LATE-NC is present in >20% (up to 50%) of individuals past age 80 years according to large community-based autopsy series LATE is associated with substantial disease-specific cognitive impairment, usually an amnestic dementia syndrome (\u2018dementia of the Alzheimer\u2019s type\u2019) The overall public health impact of LATE is on the same order of magnitude as Alzheimer\u2019s disease neuropathological changes; the diseases are often comorbid, but which pathology is more severe varies greatly between individuals Genetic risk factors for LATE have some overlap with FTLD-TDP and with Alzheimer\u2019s disease There is no molecule-specific biomarker for LATE. This is an important area of need for use in clinical trials (including as a potential exclusion criterion for Alzheimer\u2019s disease clinical trials) and longitudinal studies of the clinical and pathological progression of LATE 02. Background \u00b6 02.P01 \u00b6 There is growing awareness that Alzheimer\u2019s disease neuropathological change ( ADNC ) is only one of multiple neuropathological substrates associated with amnestic mild cognitive impairment and the Alzheimer\u2019s clinical syndrome in the aged population ( Korczyn, 2002 ; Zekry et al., 2002 ; Bennett et al., 2006 ; Jellinger and Attems, 2007 ; Schneider et al., 2007 ; Crary et al., 2014 ; Murray et al., 2014 ; Rahimi and Kovacs, 2014 ; James et al., 2016 ). Recent studies have gathered rich clinical data from large groups of subjects across a spectrum of cognitive states, correlated these clinical findings with new pathological markers at autopsy, and then analysed the data using powerful statistical methods. These studies have indicated that the diseases of aged human brains are complex: multiple comorbid pathologies are the norm, and there is substantial interindividual variation in neuropathological phenotypes ( Neuropathology Group. Medical Research Council Cognitive and Aging, 2001 ; Brayne et al., 2009 ; Kovacs et al., 2013 ; Murray et al., 2014 ; Rahimi and Kovacs, 2014 ; White et al., 2016 ; Abner et al., 2017 ; Kapasi et al., 2017 ; [Suemoto et al., 2017][2017_Suemoto]; [Tanskanen et al., 2017][2017_Tanskanen]; [Robinson et al., 2018b][2018_Robinson], [c][2018_Robinson]). While there is a strong association between severe ADNC and cognitive impairment in all age groups ([Nelson et al., 2009][2009_Nelson]; [Abner et al., 2011][2011_Abner]), subjects who die after 80 years of age often have exhibited cognitive decline exceeding expectation given the severity of ADNC ([Kawas and Corrada, 2006][2006_Corrada_Kawas]; [Savva et al., 2009][2009_Savva]; [Nelson et al., 2012][2012_Nelson]). LATE-NC is an important contributor to this apparent clinicopathological mismatch (see below). 02.P02 \u00b6 TDP-43 ( Neumann et al., 2006 ) TDP-43 w/i nuclei \u27f6 phospharylted w/i cytoplasm Historically, the first-recognized pathological manifestation of LATE was profound hippocampal neuron loss and gliosis, collectively termed hippocampal sclerosis. In a landmark study, Dickson et al. (1994) identified 13 elderly subjects with dementia and hippocampal sclerosis, yet who lacked substantial ADNC. Other larger autopsy series that included subjects with dementia and hippocampal sclerosis were later reported ([Crystal et al., 2000][2000_Crystal]; Barker et al., 2002; Leverenz et al., 2002; White et al., 2002; Zarow et al., 2005; Attems and Jellinger, 2006; Brayne et al., 2009). In 2006, phosphorylated TDP-43 was discovered as the disease protein in the ubiquitylated inclusions that are signatures of amyotrophic lateral sclerosis (ALS) and most cases of frontotemporal lobar degeneration (FTLD), known as FTLD-TDP (Neumann et al., 2006; Cairns et al., 2007a). TDP-43 protein, encoded by the TARDBP gene (Ou et al., 1995), is a protein that binds to RNA and DNA as well as to other proteins, and serves multiple functions in gene expression regulation at the levels of both transcription and translation (Cohen et al., 2011; Guo and Shorter, 2017). Expressed in most human tissues and cell types, TDP-43 is predominantly non-phosphorylated and localized mostly within nuclei, while in disease states the protein is phosphorylated and often translocated to the cytoplasm ( Neumann et al., 2006 ). 02.P03 \u00b6 LATE TDP-43 discovered in > 80yo w/o FTLD / ALS also comorbid hipocampal sclerosis &/ ADNC advanced age \u2191 TDP-43 proteinopathy / hippocampal sclerosis pathology / amnestic dementia \u2193 severe ADNC Following the detection of TDP-43 proteinopathy in FTLD-TDP and in the large majority of ALS cases (Mackenzie et al., 2007), TDP-43 proteinopathy was also discovered in the brains of subjects over age 80 years without FTLD or ALS, but often with comorbid hippocampal sclerosis and/or ADNC ( Amador-Ortiz et al., 2007a , b ). In subjects with ADNC, LATE-NC represents a common comorbid lesion that lowers the threshold for developing dementia ([Josephs et al., 2014b], [2015]). In retrospective studies, age-related TDP-43 proteinopathy has been associated with a progressive amnestic syndrome that mimicked the Alzheimer\u2019s clinical syndrome ([Pao et al., 2011]; [Brenowitz et al., 2014]). TDP-43 proteinopathy, hippocampal sclerosis pathology, and the associated amnestic dementia increases with advanced age, while the prevalence of severe ADNC decreases in extreme old age ([Nelson et al., 2011a], [b], [2013]; Brenowitz et al., 2014 ). The presence of pathological TDP-43 in these cases suggests a novel disease mechanism in older adults. As there is currently no universally agreed upon terminology or staging system for common age-related TDP-43 proteinopathy, this condition is under-studied and not well recognized even among investigators in the field of dementia research. The promotion of research and increasing awareness of this disease are the primary motivations for developing the new term LATE, and for the recommendations that follow. 02.01. LATE neuropathological changes \u00b6 02.01.P01 \u00b6 LATE encompoass hippocampal sclerosis hippocampal sclerosis of aging hippocampal sclerosis dementia cerebral age-related TDP-43 w/ sclerosis TDP-43 pathology in the elderly LATE-NC is a TDP-43 proteinopathy of advanced age, especially in subjects older than age 80. Following the convention proposed by a working group for the neuropathological criteria of Alzheimer\u2019s disease ([Montine et al., 2012]), we use LATE to refer to the disease, and LATE-NC as the term to indicate LATE neuropathological changes. The term LATE is intended to encompass several previously used designations related to TDP-43 proteinopathy that may be associated with cognitive impairment, including hippocampal sclerosis, hippocampal sclerosis of aging, hippocampal sclerosis dementia, cerebral age-related TDP-43 with sclerosis, and TDP-43 pathologies in the elderly (for reviews see [Kuslansky et al., 2004]; [Lippa and Dickson, 2004]; [Nelson et al., 2013], 2016b ; [Dutra et al., 2015]). 02.01.P02 \u00b6 TDP-43 inclusion body in cytoplasm abnormal accumulation in nuclei & neurite type A FTLD-TDP similar/different TDP-43 in LATE HCP olfactory bulb neocortex basal ganglia brainstem \u2018TDP-43 proteinopathy\u2019 refers to loss of normal nuclear TDP-43 immunoreactivity, with TDP-43 protein \u2018inclusion bodies\u2019 in the neuronal cytoplasm, as well as abnormal TDP-43 accumulation (much of it phosphorylated) in nuclei and cell processes (neurites) of neurons and in oligodendroglia and astrocytes. For representative examples of TDP-43 proteinopathy, see Fig. 1 . Mislocalized and phosphorylated TDP-43 is a necessary feature of LATE-NC and sometimes has characteristics similar to those seen in type A FTLD-TDP ([Lin et al., 2009][2009_Lin]; [Mackenzie et al., 2011][2011_Mackenzie]; Murray et al., 2014 ; Aoki et al., 2015 ), but often the features do not fit cleanly into an established FTLD-TDP subtype. Indeed, a recent study described features of LATE-NC including both similarities and differences from type A FTLD-TDP ( Josephs et al., 2019 ). In addition to limbic structures, TDP-43 proteinopathy in LATE has also been described in the olfactory bulb, neocortex, basal ganglia, and less frequently in brainstem ([Josephs et al., 2008][2008_Josephs]; [Geser et al., 2010][2010_Geser]; [Josephs and Dickson, 2016][2016_Dickson_Josephs]; [Nelson et al., 2018][2018_Nelson]). Immunoelectron microscopy showed that the TDP-43 inclusions have a fibrillary ultrastructure composed of bundled 10\u201320-nm diameter straight filaments ([Lin and Dickson, 2008][2008_Dickson_Lin]; [Lin et al., 2009][2009_Lin]), often accompanied by electron dense granules ([Cairns et al., 2007b][2007_Cairns]; [Robinson et al., 2013][2013_Robinson]). Figure 1 \u00b6 LATE neuropathological changes (LATE-NC). (A\u2013E) Coronally sectioned human hippocampi stained using haematoxylin and eosin (H&E). Note that the photomicrographs in A\u2013C are presented at the same magnification. (A) LATE-NC with hippocampal sclerosis (HS). The hippocampus is atrophic and the neuropil rarefied. (D) Higher magnification in CA1 subfield, with lack of normal cellular architecture and with extensive gliosis. (C) Control age-matched hippocampus. (E) CA1 of the control hippocampus to demonstrate the normal cellular architecture and intact eosinophilic neuropil (asterisk). The hippocampus shown in B is less atrophic, with less obvious neuropil disruption, in comparison to the case in A at low magnification; however, an adjacent section revealed TDP-43 proteinopathy. Hippocampal fields are labelled in B: dg = dentate granule layer; Sub = subiculum. TDP-43 proteinopathy can be recognized using antibodies raised against either non-phosphorylated or phosphorylated TDP-43 epitopes. (F) Dentate granule cells in a case lacking TDP-43 pathology. Note that cell nuclei are normally immunopositive for non-phosphorylated TDP-43 protein. In a case with LATE-NC (G), by contrast, an antibody against phosphorylated TDP-43 protein recognizes only the pathological inclusions in the nucleus (green arrow) and cytoplasm (red arrow). Unlike the antibody against non-phosphorylated TDP-43, the antibody against phosphorylated TDP-43 is negative in non-affected cells. Most cells in G are visualized with the counterstain, haematoxylin, which stains cell nuclei blue. The Venn diagram in H illustrates schematically the imperfect overlap between cases with TDP-43 proteinopathy, hippocampal sclerosis, and LATE-NC. A subset of cases with TDP-43 pathology have comorbid hippocampal sclerosis pathology; the change zone between non-hippocampal sclerosis and hippocampal sclerosis cases is indistinct because many cases seem to be in transition with incipient hippocampal neuron loss and gliosis. Importantly, cases with hippocampal sclerosis pathology but no TDP-43 proteinopathy (e.g. hippocampal sclerosis pathology associated with anoxia or epilepsy) are not classified as LATE-NC. (I) Phospho-TDP-43 proteinopathy in two neurons in hippocampal CA1, along with phospho-TDP-43 immunoreactive dystrophic neurites. (J) Tangle-like phospho-TDP-43 immunoreactive cytoplasmic inclusions in amygdala (red arrows) with fewer phospho-TDP-43 immunoreactive neurites in the background. (K) An intraneuronal phospho-TDP-43 inclusion (red arrow) and a phospho-TDP-43 deposit (green arrow) surrounding a capillary (shown with blue arrows); these TDP-43 immunoreactive structures have been demonstrated to exist within astrocyte end-feet (Lin et al., 2009). Note also the presence of a cell with cytoplasmic puncta (green arrow), perhaps in an early phase of phosphorylated TDP-43 proteinopathy. Scale bar in A = 4 mm for A\u2013C; D = 200 \u03bcm; E = 100 \u03bcm; F = 30 \u03bcm; G = 35 \u03bcm; I = 30 \u03bcm; and K = 25 \u03bcm. 02.01.P03 \u00b6 neuronal dropoout & astrocytosis in LATE-NC HPC subiculum entorhinal cortex amygdala In brains with LATE-NC, haematoxylin and eosin stains may reveal neuronal dropout and astrocytosis in the CA1 sector of the hippocampus, as well as in the subiculum , entorhinal cortex, and amygdala ([Amador-Ortiz and Dickson, 2008][2008_Dickson_Amador-Ortiz]). Atrophy can be marked in these areas (Fig. 1A\u2013C). In severely affected hippocampi, the neuropil becomes rarefied and loss of neuronal components is accompanied by reactive astrocytosis ([Amador-Ortiz et al., 2007a][2007_Amador-Ortiz]). Pronounced leucocyte infiltrates or perivascular cuffing are not typically seen, but hypertrophic microglia can be numerous ([Bachstetter et al., 2015][2015_Bachstetter]). The neuronal cell loss is segmental in some subjects, observed in some but not all sections from the same brain area ([Ighodaro et al., 2015][2015_Ighodaro]). Hippocampal sclerosis pathology is unilateral in \u223c40\u201350% of cases in which both sides were evaluated ([Nelson et al., 2011b][2011_Nelson]; [Zarow et al., 2012][2012_Zarow]; [Kero et al., 2018][2018_Kero]), not unlike FTLD-TDP ([Irwin et al., 2018][2018_Irwin]). 02.01.P04 \u00b6 TDP-43 \u2013 /HS + : NOT represent LATE-NC Hippocampal sclerosis is present in a subset of cases with severe LATE-NC, and was the first characteristic pathological feature that distinguished it from ADNC ( Dickson et al., 1994 ). Nevertheless, hippocampal sclerosis is neither specific to LATE-NC nor sufficient for the diagnosis of LATE. The neuropathological diagnosis of hippocampal sclerosis is fraught with difficulty. The most recent consensus guidelines for ADNC and related disorders stated that hippocampal sclerosis pathology is \u2018defined by severe pyramidal cell loss and gliosis in CA1 and subiculum of the hippocampal formation that is out of proportion to AD neuropathologic change in the same structures\u2019 ([Montine et al., 2012][2012_Montine]). There is, however, significant topographic and phenotypic heterogeneity in hippocampal degeneration, creating difficulties in establishing strict criteria for widespread use. Moreover, hippocampal sclerosis is a pathological endpoint associated with various underlying disease processes, including epilepsy, hypoxia, hypoglycaemia, certain infections, and numerous neurodegenerative conditions ([Josephs et al., 2007][2007_Josephs]; [Thom et al., 2009][2009_Thom]; [Yokota et al., 2010][2010_Yokota]; [Malek-Ahmadi et al., 2013][2013_Malek-Ahmadi]; [Murray et al., 2013][2013_Murray]; [Ling et al., 2017][2017_Ling]; [Popkirov et al., 2017][2017_Popkirov]; [Sen et al., 2018][2018_Sen]). Having originated in a 19th century study of epilepsy by Wilhelm Sommer ([Sommer, 1880][1880_Sommer]; [Thom, 2009][2009_Thom]), the term hippocampal sclerosis is still used widely by radiologists and pathologists in the context of seizure disorders ([Isnard and Bourdillon, 2015][2015_Bourdillon_Isnard]; [Thom and Sisodiya, 2015][2015_Sisodiya_Thom]). Detailed discussions of histopathological features and subtypes of hippocampal sclerosis can be found elsewhere ([Probst et al., 2007][2007_Probst]; [Rauramaa et al., 2013][2013_Rauramaa]; [Hatanpaa et al., 2014][2014_Hatanpaa]; [Dutra et al., 2015][2015_Dutra]; [Thom and Sisodiya, 2015][2015_Sisodiya_Thom]; [Cykowski et al., 2017][2017_Cykowski]). Brains with hippocampal sclerosis, but lacking TDP-43 pathology (TDP-43\u2212/HS+), do not represent LATE-NC. For example, brains with hippocampal sclerosis caused by acute hypoxia or associated with epilepsy are negative for TDP-43 proteinopathy ([Amador-Ortiz et al., 2007b][2017_Amador-Ortiz]; [Lee and Lee, 2008][2008_Lee_Lee]; [Nelson et al., 2011b][2011_Nelson]) and do not fulfil criteria for LATE-NC (Fig. 1H). In summary, TDP-43 proteinopathy is a necessary feature of LATE-NC that may or may not be accompanied by hippocampal sclerosis. 02.01.P05 \u00b6 TDP-43 + /HS \u2013 \u2192 TDP-43 + /HS + As has been the case in other neurodegenerative diseases ([Braak et al., 1993][1993_Braak], [2006][2006_Braak]; [Thal et al., 2000][2000_Thal]; [Zaccai et al., 2008][2008_Zaccai]; [Alafuzoff et al., 2009][2009_Alafuzoff]), careful assessments of autopsy data, from both longitudinal studies of clinic-based research subjects as in the NIA-funded Alzheimer\u2019s Disease Centers, and from community-based studies, have expanded our understanding of LATE. While subjects with advanced age and hippocampal sclerosis often have TDP-43 proteinopathy ([Amador-Ortiz et al., 2007b][2007_Amador-Ortiz]; [Nelson et al., 2011b][2011_Nelson]; [Robinson et al., 2014][2014_Robinson]; [Nag et al., 2015][2015_Nag], 2018 ), TDP-43 proteinopathy in limbic structures is more prevalent than hippocampal sclerosis (Kovacs et al., 2013; Josephs et al., 2014b; Keage et al., 2014; Murray et al., 2014; Rahimi and Kovacs, 2014; Aoki et al., 2015; Nag et al., 2015, 2017; Hokkanen et al., 2018; Robinson et al., 2018b). The TDP-43-positive (+) and hippocampal sclerosis-negative (HS\u2212) cases are a subset of LATE-NC that represent 5\u201340% of research subjects in autopsy series. Prior researchers have used terms for brains with TDP-43 proteinopathy and with some degree of cell dropout and gliosis, but lacking frank hippocampal sclerosis, as a \u2018precursor to HS\u2019, \u2018pre-HpScl,\u2019 or \u2018pre-HS-Aging\u2019 ([Hatanpaa et al., 2008][2008_Hatanpaa]; [Aoki et al., 2015][2015_Aoki]; [Hokkanen et al., 2018][2018_Hokkanen]). As suggested by the terminology, TDP-43 + /HS \u2212 brains may represent an early or transitional phase on the same disease continuum as TDP-43 + /HS + cases. There are other clues about LATE that were gathered from autopsy cohort studies. For example, even when hippocampal sclerosis was unilateral or segmental, the TDP-43 proteinopathy was almost always bilateral ([Nelson et al., 2011b][2011_Nelson]; [Ighodaro et al., 2015][2015_Ighodaro]). These observations have implications about how LATE evolves, which remains an important and open question. 02.01.P06 \u00b6 3 hypothesis subset of TDP-43 proteinopathy develop HS TDP-43 associated independently w dementia pathogenetic mechanisms associted w/ ADNC also associate w/ TDP-43 proteinopathy Data gathered in large autopsy series have been analysed to test hypotheses about progression of LATE. Multivariable regression-based assessment can be used to generate models to test whether cross-sectional data align with proposed sequential pathways of neuropathological changes. Results of one such pathway analysis, from the Rush University community-based autopsy studies, are shown in Fig. 2. These analyses were performed as described previously ( Power et al., 2018 ) and the findings are compatible with at least three hypotheses: (i) a subset of cases with TDP-43 proteinopathy develop hippocampal sclerosis caused or exacerbated by overlapping process(s) that promoted the TDP-43 proteinopathy, or directly by the TDP-43 proteinopathy itself; (ii) TDP-43 proteinopathy is associated independently with dementia, even in cases lacking hippocampal sclerosis; and (iii) pathogenetic mechanisms associated with ADNC (in Fig. 2, data are provided on neuritic amyloid-\u03b2 plaques) are also associated with increased TDP-43 proteinopathy. Current rodent models of TDP-43 proteinopathy with hippocampal sclerosis-like pathology are few ([Ke et al., 2015][2015_Ke]). TDP-43 proteinopathy was shown to be transmissible in mouse models similar to pathological tau and amyloid-\u03b2 from Alzheimer\u2019s disease brains ([Porta et al., 2018][2018_Porta]), but the published TDP-43 models are thought to be more directly relevant to FTLD-TDP than LATE. For now, the lack of adequate longitudinal biomarker data and the limitations of current animal models hamper our study of disease mechanism(s) and further investigations are needed. Figure 2 \u00b6 Statistical analyses on data related to LATE from the Rush University community-based autopsy cohort depicting the results of pathway analyses. Data were analysed from research volunteers (total n = 1309) in two clinical\u2010pathological studies of ageing from Rush University as described previously (Power et al., 2018). In this sample, the mean age of death was 89.7 years [standard deviation (SD) 6.5 years, range 65\u2013108 years]. These analyses incorporated age, density of amyloid-\u03b2 neuritic amyloid plaques (to factor in ADNC), TDP-43 proteinopathy, hippocampal sclerosis pathology, and the endpoint of Alzheimer\u2019s-type clinical dementia. The components of the pathway analyses most strongly associated with LATE-NC are shown in red. The numbers are path coefficients with standard error in parentheses (shown in purple). These numbers help to quantify the effects of individual pathways. For instance, the data are compatible with there being two pathways from TDP-43 proteinopathy to dementia, one direct pathway (TDP-43 proteinopathy\u2192dementia) and the other indirect pathway that includes hippocampal sclerosis pathology (TDP-43 proteinopathy\u2192hippocampal sclerosis\u2192dementia): in the statistical model, the TDP-43 proteinopathy is independently associated with both hippocampal sclerosis pathology and clinical dementia status. Further, the data indicate that a subset of TDP-43 proteinopathy is \u2018downstream\u2019 of ADNC-type neuritic amyloid plaque pathology. In a practical sense, this means that brains with more neuritic amyloid plaques are more likely to have TDP-43 proteinopathy, with all other known factors being the same. A\u03b2 = amyloid-\u03b2. 02.02. LATE MRI studies \u00b6 02.02.P01 \u00b6 MRI studies have provided a complementary window into brain changes in LATE, highlighting brain atrophy both within and outside of the medial temporal lobes of brains with autopsy-verified LATE-NC. Prior studies featured research volunteers who underwent MRI with autopsy follow-up. Several of these studies focused on the subset of cases with hippocampal sclerosis (i.e. presumed severe LATE-NC), therefore, most of the published data were lacking information about less severely affected cases. With that caveat in mind, a common finding in MRI studies is that hippocampal atrophy is greater in cases with LATE-NC than in those with pure Alzheimer\u2019s disease ([Jagust et al., 2008][2008_Jagust]; [Josephs et al., 2008][2008_Josephs], [2017a][2017_Josephs]; [Dawe et al., 2011][2011_Dawe]; [Kaur et al., 2014][2014_Kaur]; [Dallaire-Theroux et al., 2017][2017_Dallaire-Theroux]; [Hanko et al., 2019][2019_Hanko]). [Barkhof et al. (2007)][2007_Barkhof] found that many subjects with medial temporal atrophy lacked primary underlying ADNC. In this study cohort, the sensitivity and specificity of severe atrophy for ADNC was 63% and 69%, respectively, consistent with prior findings ([Jack et al., 2002][2002_Jack]). [Josephs et al. (2008)][2008_Josephs] reported that subjects with neuropathology consistent with LATE-NC tended to be older, with more cognitive impairment, and with more pronounced hippocampal atrophy than TDP-43\u2212 subjects. [Zarow et al. (2011)][2011_Zarow] also described atrophy and deformation of the hippocampus considerably greater in those with hippocampal sclerosis and LATE-NC than in those with only ADNC ([Zarow et al., 2011][2011_Zarow]). In hippocampal sclerosis associated with LATE-NC, hippocampal atrophy was often asymmetric, and it tended to progress in a rostral-caudal gradient in the hippocampus. Using post-mortem MRI, [Dawe et al. (2011)][2011_Dawe] reported stronger correlations between hippocampal atrophy and LATE-NC (with hippocampal sclerosis pathology) than between hippocampal atrophy and ADNC, and subjects with both ADNC and LATE-NC had greater hippocampal atrophy than those with only ADNC. A recent study found that the volume and shape of the amygdala is associated with underlying LATE-NC and that these structural changes are indicative of cognitive decline beyond what can be explained with other pathological indices ([Makkinejad et al., 2019][2019_Makkinejad]). 02.02.P02 \u00b6 brain atrophy med tem l inf fro ant tem ins correponds to TDP-43 proteinopathy pattern Post-mortem MRI research has also provided strong evidence that LATE-NC is associated with substantial brain atrophy outside the medial temporal lobes ([Kotrotsou et al., 2015][2015_Kotrotsou]). Figure 3A shows updated data from the Rush University autopsy cohort. After controlling for demographics, ADNC and other age-related pathologies, LATE-NC was related to not only the mesial temporal lobe atrophy, but also to atrophy in the inferior frontal, anterior temporal, and insular cortices. It is noteworthy that this regional atrophy pattern corresponds with the distribution of TDP-43 proteinopathy at autopsy ( Josephs et al., 2016 ; Nag et al., 2018 ) ([Fig. 3B][fig_03]). These data are in agreement with pathological studies of LATE-NC, as well as neuroimaging in subjects with LATE-NC risk genotypes, showing widespread brain involvement ([Neltner et al., 2014][2014_Neltner]; [Cykowski et al., 2016][2016_Cykowski]; Josephs et al., 2016 ; [Nelson et al., 2016a][2016_Nelson]; [Nho et al., 2016][2016_Nho]). Figure 3 \u00b6 Brain regions that are affected in LATE. (A) Post-mortem MRI with autopsy confirmation allows discrimination of regions of brain atrophy associated with LATE-NC. These data indicate grey matter regions inside and outside of the medial temporal lobe with atrophy in cases with autopsy-confirmed LATE-NC from a community-based autopsy sample. The figure was prepared similarly to the methods used in [Kotrotsou et al. (2015)][2015_Kotrotsou], with some modifications. Cerebral hemispheres from 539 participants of two cohort studies of ageing (Rush Memory and Aging Project and Religious Orders Study) were imaged with MRI ex vivo and also underwent detailed neuropathological characterization. The cortical and subcortical grey matter were segmented into 41 regions. Linear regression was used to investigate the association of regional volumes (normalized by height) with the score of LATE-NC at autopsy (scores: 0 = no TDP-43 inclusions, or inclusions in amygdala only; 1 = TDP-43 inclusions in amygdala as well as entorhinal cortex or hippocampus CA1, and neocortex; 2 = TDP-43 inclusions in amygdala, entorhinal cortex or hippocampus CA1, and neocortex, and hippocampal sclerosis pathology) controlling for amyloid plaques and neurofibrillary tangles, Lewy bodies, gross and microscopic infarcts, atherosclerosis, arteriolosclerosis, cerebral amyloid angiopathy, as well as age, sex, years of education, post-mortem interval to fixation and to imaging, and scanners. Unique colours have been assigned to different model estimates (units: mm2) for grey matter regions with significant negative correlation between their volumes and LATE pathology (P < 0.05, false discovery rate-corrected); darker colours indicate greater brain atrophy in that region. Results are overlaid on both hemispheres of the T1-weighted template of the IIT Human Brain Atlas (v.4.2). Lateral, medial and inferior to superior 3D views of the results are also shown. (B) Classification of LATE-NC according to anatomical region(s) affected by TDP-43 proteinopathy. The present working group recommended a simplified staging scheme for routine assessment of LATE-NC. This requires sampling and TDP-43 immunohistochemical staining of amygdala, hippocampus, and middle frontal gyrus. More detailed TDP-43 immunohistochemical staging schemes that are directly relevant to LATE-NC were previously published by [Josephs et al. (2014a][2014_Josephs], 2016 ) and Nag et al. (2018) . MFG = middle frontal gyrus. 02.03. Recommendations for routine autopsy evaluation and classification of LATE-NC \u00b6 02.03.P01 \u00b6 stage 1: amygdala early stage 2: hippocampus pathological change is associated w/ cogni imp stage 3: mid fro g It is recommended that TDP-43 immunohistochemistry be performed as part of the neuropathological evaluation in all older subjects. At a minimum, immunohistochemical staining for TDP-43 is recommended in three brain areas: amygdala, mid-level hippocampus, and middle frontal gyrus. We recommend evaluating these regions as they are commonly obtained at autopsy of aged subjects and capture presumed progression of LATE-NC in the brain. This sampling includes the brain area affected early in the disease course (amygdala, Stage 1), an intermediate stage where the pathological change is robustly associated with cognitive impairment (hippocampus, Stage 2), and a region affected at more advanced stages (middle frontal gyrus, Stage 3) ( Nag et al., 2018 ). Any detected TDP-43 proteinopathy is sufficient to define an anatomical region-based stage: for example, a minute amount of detected TDP-43 proteinopathy in the hippocampus indicates at least Stage 2. We emphasize that the proposed sampling for LATE-NC autopsy screening is a minimal evaluation, whereas more detailed sampling and staging should be considered for specific research settings (Josephs et al., 2014a, 2016; Uchino et al., 2015; Nag et al., 2017, 2018; Zhang et al., 2019). Figure 3B depicts staging schemes for LATE-NC, including sampling recommended for neuropathological evaluation of brain of older adults. This does not address regions that would be assessed in separate TDP-43 pathological staging schemes developed for ALS or FTLD-TDP ([Brettschneider et al., 2013][2013_Brettschneider]; [Fatima et al., 2015][2015_Fatima]; [Tan et al., 2015][2015_Tan]; [Verde et al., 2017][2017_Verde]; [Neumann and Mackenzie, 2019][2019_Mackenzie_Neumann]). 02.03.P02 \u00b6 Practical questions arise in relation to diagnostic \u2018boundary zones\u2019 between LATE-NC, FTLD-TDP, and ADNC. While both LATE-NC and FTLD-TDP may affect neocortical areas and may be comorbid with hippocampal sclerosis, LATE-NC usually has a later age of onset, an amnestic dementia, and limbic predominance of pathological change (Nelson et al., 2011b). On the other hand, recommendations for LATE-NC do not stipulate any age cut-offs, because the exact age ranges of disease susceptibility for FTLD-TDP or LATE-NC are not yet fully understood. For prior pathology-based comparisons between subtypes of TDP-43 proteinopathies (not related to age of onset), previous studies should be consulted (Amador-Ortiz et al., 2007a; Tan et al., 2015). More widespread and severe cortical atrophy is typically present in advanced FTLD-TDP than LATE-NC. There may indeed be features that could definitively distinguish LATE-NC cases (histopathologically or molecularly) from subtypes of FTLD-TDP (Arai et al., 2010; Hasegawa et al., 2011; Tsuji et al., 2012; Laferriere et al., 2019); however, more work is needed in this area. For now, definitive criteria to differentiate severe LATE-NC from FTLD-TDP await discovery of specific features that discriminate among various TDP-43 proteinopathies (Tan et al., 2017a). Although LATE-NC and ADNC are recognized by differing neuropathological hallmarks, they may share upstream risk factors and disease mechanisms. Genetic variants predisposing to one protein misfolding disorder may also cause or exacerbate others (see below), and there may be interactions between the misfolded proteins themselves (Trojanowski and Lee, 2000; Higashi et al., 2007; Hu et al., 2008; Uryu et al., 2008; Kadokura et al., 2009; Davis et al., 2017; Spires-Jones et al., 2017; Tan et al., 2017b; Nelson et al., 2018). Brains that harbour ADNC, including some subjects with early-onset familial Alzheimer\u2019s disease or Down syndrome, tend to also contain TDP-43 proteinopathy at rates higher than those lacking ADNC (Ala et al., 2000; Jellinger, 2000; Lippa et al., 2009; Davidson et al., 2011; Zarow et al., 2012). Individual neurons with both tau neurofibrillary tangle pathology and TDP-43 inclusions have been described, particularly in the amygdala, entorhinal cortex, and dentate gyrus of the hippocampus (Amador-Ortiz et al., 2007b; Kadokura et al., 2009; Smith et al., 2017; Robinson et al., 2018c; Josephs et al., 2019). Several published accounts have evaluated the connections between primary age-related tauopathy (PART) and age-related TDP-43 proteinopathy (Josephs et al., 2017b; Smith et al., 2017; Zhang et al., 2019), and TDP-43 proteinopathy has also been described in brains with coexisting argyrophilic grains or glial tauopathy (Fujishiro et al., 2009; Yokota et al., 2010; Arnold et al., 2013; Kertesz et al., 2015). The implications of comorbid amyloid-\u03b2 and various tau pathologies in the context of LATE-NC are still incompletely understood, so further studies are required. There is also evidence that Lewy body disease and TDP-43 proteinopathy may coexist (Nakashima-Yasuda et al., 2007; McAleese et al., 2017; Miki et al., 2018; Trieu et al., 2018). On the other hand, many cases with \u2018end-stage\u2019 ADNC or Lewy body disease lack TDP-43 proteinopathy, so we recommend reporting the presence or absence of LATE-NC as a separate diagnostic entity, even when there are comorbid amyloid-\u03b2, tau and/or \u03b1-synuclein proteinopathies. Additional research is required to guide future consensus-based recommendations in this evolving field. In terms of immunohistochemical reagents used to detect TDP-43 proteinopathy, there is no current consensus that a specific antibody can be recommended. Many neuropathologists use sensitive phospho-TDP-43 antibodies (Hasegawa et al., 2008; Alafuzoff et al., 2015); small aggregates can be readily seen using these reagents. Others use antibodies against non-phosphorylated epitopes, especially for detecting early changes (pathological nuclear to cytoplasmic redistribution) that may precede inclusion body formation (Vatsavayai et al., 2016; Braak et al., 2017; Braak and Del Tredici, 2018; Nana et al., 2019). It is unclear whether the absence of nuclear TDP-43 is reversible, but animal studies using inducible pathogenetic systems would suggest so (Ke et al., 2015). Further, there is some evidence that TDP-43 antigenicity can be vulnerable to fixation artefacts, and epitope retrieval methodology can influence results (Hatanpaa et al., 2008). Additional practice guidelines for studying LATE-NC need formal blinded cross validation studies as has been done for amyloid-\u03b2, tau and \u03b1-synuclein pathological biomarkers. Future studies will be needed to validate and refine systems for staging LATE-NC, and grading local pathological severity, as they relate to clinical and neuroimaging outcomes, especially since at least three staging schemes have been proposed as summarized in Fig. 3B. 02.04. Clinical and neurocognitive features of LATE \u00b6 The clinical course of subjects with autopsy-proven LATE-NC has been characterized as an amnestic cognitive syndrome that can evolve to incorporate multiple cognitive domains and ultimately to impair activities of daily living, i.e. the dementia syndrome (Nelson et al., 2010; Nag et al., 2015; Robinson et al., 2018a, b). The cognitive impairment is greater than can be accounted for by ADNC or other pathologies (Gold et al., 2000; Kawas and Corrada, 2006; Imhof et al., 2007; Giannakopoulos et al., 2008; Nelson et al., 2011b; Kravitz et al., 2012; Boyle et al., 2013; Erten-Lyons et al., 2013). Initial reports on subjects with LATE-NC were focused on subjects with severe pathology (Dickson et al., 1994; Snowdon et al., 1997; Crystal et al., 2000; Vinters et al., 2000; Leverenz et al., 2002; Kuslansky et al., 2004; Zarow et al., 2005, 2008; Attems and Jellinger, 2006; Chui et al., 2006; Leverenz and Lipton, 2008), which helped to show that LATE-NC can be associated with dementia. More recent autopsy series, with both large sample sizes and broad ranges of clinical and pathological findings, have enabled statistical approaches to model the likely relative impact of each disease type. With these methods, LATE-NC was associated with substantial cognitive impairment that was independent of other coexisting pathologies (Nelson et al., 2010; Keage et al., 2014; Murray et al., 2014; Josephs et al., 2015; Nag et al., 2017). Table 1 shows primary data on the relationship between LATE-NC (stratified by the recommended three-stage system) and cognition. The neurological features associated with LATE-NC were different from the behavioural or aphasic clinical syndromes seen in FTLD-TDP cases (Nelson et al., 2011b; Jung et al., 2014; Wilson et al., 2019). While TDP-43 proteinopathy has been documented in some cognitively unimpaired subjects (Arnold et al., 2013; Keage et al., 2014; Uchino et al., 2015; Elobeid et al., 2016; Nascimento et al., 2016; Nag et al., 2018; Nascimento et al., 2018), it is likely that this represents preclinical disease in subjects dying before onset of clinical symptoms; such clinical resilience to pathological changes has been described in many disorders (Perkins et al., 2003; Shojania et al., 2003; Roulson et al., 2005; Latimer et al., 2017; Robinson et al., 2018b). Table 1 \u00b6 Characteristics 0 1 2 3 P-value n 666 263 258 189 - Age at death (SD) 87.9 (6.8) 89.9 (6.2) 91.8 (5.6) 91.9 (5.4) <0.001 % Female 65.3 67.7 74 72.5 0.040 Clinical diagnosis <0.001 % Normal 41.8 33.5 18.9 7.6 % MCI or dementia 58.2 66.5 81.1 92.4 % with comorbid HS pathology 1.7 3.5 13.6 42.9 <0.001 Cognitive function tests proximate to death, mean (SD) MMSE score 22.8 (8.1) 21.2 (8.9) 18.2 (9.8) 14.0 (10.0) <0.001 Episodic memory score \u22120.60 (1.28) \u22120.76 (1.31) \u22121.36 (1.34) \u22122.06 (1.23) <0.001 \u00b6 Although there is overlap in clinical features of autopsy-confirmed LATE-NC and ADNC (Pao et al., 2011; Brenowitz et al., 2014; Murray et al., 2014; Nag et al., 2017), careful analyses may identify distinctive neurocognitive features. Preliminary evidence suggests that subjects with relatively \u2018pure\u2019 LATE-NC (lacking severe comorbid pathologies) tend to have a more gradual clinical decline compared to those with \u2018pure\u2019 ADNC (Murray et al., 2014; Boyle et al., 2017). In contrast, those with comorbid ADNC and LATE-NC showed faster decline and more severe cognitive impairment than those with either ADNC or LATE-NC alone (Josephs et al., 2014b, 2015; Nag et al., 2017). In studies with both detailed longitudinal cognitive testing and comprehensive neuropathological evaluations, subjects with LATE-NC had prominent impairment in episodic memory (Table 1), but other cognitive domains and global cognitive status were also commonly affected especially in the later disease stages (Nag et al., 2015, 2017, 2018; Wilson et al., 2019). Correlative studies indicate that certain neurocognitive assessments, such as verbal fluency measures, are not independently associated with hippocampal volume, but are instead correlated with neocortical grey matter volumes (Ajilore et al., 2015; Pelletier et al., 2017). Correspondingly, subjects with relatively preserved verbal fluency (cortically-dependent), despite profound deficiency in word list delayed recall (hippocampal-dependent), have been shown to be at risk for LATE-NC (Nelson et al., 2011b). This pattern of neurocognitive test scores in LATE differs from that seen in subjects with ADNC alone (Nelson et al., 2011b) or FTLD-TDP (Brenowitz et al., 2014). Neuropsychiatric disturbances have been reported in some subjects with LATE-NC (Ighodaro et al., 2015), and a retrospective, cross-sectional, multicentre study found evidence of increased risk of \u2018agitation/aggression\u2019 symptoms in subjects with ADNC and comorbid TDP-43 proteinopathy in comparison to subjects with ADNC lacking TDP-43 proteinopathy (Sennik et al., 2017). However, not all prior studies found that LATE-NC was associated with non-amnestic manifestations (Velakoulis et al., 2009; Nelson et al., 2011b; Vatsavayi et al., 2014; Sahoo et al., 2018). Future investigations are warranted to test for specific neuropsychiatric, motor, or autonomic signs that distinguish LATE from other degenerative disorders. 02.05. Public health impact of LATE \u00b6 The public health impact of LATE is likely to be quite significant. Two basic study design elements that influence recognition of LATE-NC in autopsy cohorts are the age range in the cohort, and the date of the study. Researchers were unaware of TDP-43 proteinopathy prior to 2006, so studies prior to this time could not assess the specific impact of LATE. LATE-NC is mostly seen in the oldest-old, whereas in early clinical-pathological correlation studies of dementia (Roth et al., 1966; Blessed et al., 1968), the research subjects had died in their early 70s. LATE-NC needs to be assessed in population studies that include all age ranges. More recent clinical studies have demonstrated biomarker evidence of \u2018suspected non-Alzheimer\u2019s disease pathophysiology\u2019 (SNAP) causing amnestic type cognitive impairment with substantial hippocampal atrophy but lacking detectable amyloid-\u03b2 amyloidosis (Caroli et al., 2015; Burnham et al., 2016; Jack et al., 2016, 2017; Abner et al., 2017; Wisse et al., 2018). For example, the evaluation of 1535 participants in the Mayo Clinic Study of Aging showed significantly greater prevalence of SNAP compared with preclinical Alzheimer\u2019s disease, and multimorbidity was increased in SNAP (odds ratio 2.16) (Vassilaki et al., 2018). LATE is probably an important contributor in this group of subjects (see below). Among subjects autopsied past 80 years of age, most studies indicate that >20% of brains had pathological features consistent with LATE-NC (Fig. 4). It is noteworthy that the majority of these cases had additional comorbid pathologies, so the measured clinical-pathological correlation (relative contribution of each pathology to cognitive impairment) depends on how the investigators defined diagnostic thresholds and cut-points. The frequency of LATE-NC in autopsy series have varied, ranging from 5% to 50% of brains that were evaluated using TDP-43 immunohistochemistry, approximately twice the frequencies that were detected in prior studies that could only assess hippocampal sclerosis pathology (Leverenz et al., 2002; Lippa and Dickson, 2004; Arai et al., 2009; Nelson et al., 2011b; Rauramaa et al., 2011; Tremblay et al., 2011; Corrada et al., 2012; Zarow et al., 2012; Malek-Ahmadi et al., 2013; Keage et al., 2014; Jellinger and Attems, 2015; Uchino et al., 2015; Takao et al., 2016; Latimer et al., 2017; McAleese et al., 2017; Hokkanen et al., 2018; Kero et al., 2018; Robinson et al., 2018a). Differences in study design, including the application of various criteria for defining pathological abnormalities, pathological methods, recruitment strategy, and cohort demographics, all contribute to the variability in the reported frequency of LATE-NC. Figure 4 \u00b6 Different neurodegenerative disease conditions stratified by age: LATE-NC, severe ADNC, and FTD. FTD/FTLD cases were not present in data shown in A\u2013D. Note that published studies to estimate disease prevalence for the various diseases have used importantly different study designs\u2014thus, E is a clinical (no autopsy) study because population-based autopsy cohorts lack substantial numbers of FTD/FTLD cases. (A and B) Data from a community-based autopsy cohort\u2014the Rush University ROS-MAP cohort (overall n = 1376). The TDP-43 pathology is operationalized using standard methods as described previously (Nag et al., 2018) and then the current paper\u2019s suggested simplified staging system was applied; sample sizes for each age group (in years) are: <75 (n = 34); 75\u201380 (n = 82); 80\u201385 (n = 192); 85\u201390 (n = 375); 90\u201395 (n = 407); 95\u2013100 (n = 222); and >100 (n = 64). Note that in this community-based sample, the proportion of cases with advanced ADNC is <50% in all age groups. (C and D) Data from the National Alzheimer\u2019s Coordinating Center (NACC), which derives from 27 different research centres, as described previously (Besser et al., 2018; Katsumata et al., 2018). Overall sample size is n = 806, stratified thus by age groups (in years): <75 (n = 155); 75\u201380 (n = 118); 80\u201385 (n = 165); 85\u201390 (n = 170); 90\u201395 (n = 122); 95\u2013100 (n = 57); and >100 (n = 19). The NACC research subjects were more likely to come to autopsy after being followed in dementia clinics, and the sample includes a higher percentage of subjects with severe ADNC. The percentage of subjects with LATE-NC is still >20% in each age group. Note that in both the community-based cohort (A and B) and clinic-based cohort (C and D), the proportion of subjects with severe ADNC decreased in advanced old age, while in the same cases the proportion of subjects with LATE-NC increased. (E) Epidemiological data on FTD syndromes for comparison to LATE. Data are provided about crude prevalence rates for FTD syndromes that have been associated with FTLD-TDP. Several of these clinical syndromes are likely to have considerable numbers of cases with FTLD-tau (bvFTD and nfvPPA) or ADNC (other PPA) rather than FTLD-TDP, so the actual prevalence of FTLD-TDP pathology is probably lower than this data suggests. Note that the clinical syndromes associated with FTLD-TDP have a prevalence that are several orders of magnitude lower than LATE-NC. These data, described in detail previously (Coyle-Gilchrist et al., 2016), derive from multisource referral over 2 years, which identified all diagnosed or suspected cases of FTD subtypes in two UK counties comprising the PiPPIN (Pick\u2019s Disease and Progressive Supranuclear Palsy: Prevalence and Incidence) catchment area in the East of England. Two cities in the PiPPIN catchment area were Norfolk and Cambridge. Diagnostic confirmation used current consensus diagnostic criteria after interview and re-examination. Total sample size was n = 986 483 subjects. Shown are crude prevalence rates for the major FTLD-TDP associated syndromes by age and syndrome. bvFTD = behavioural variant frontotemporal dementia; nfvPPA = non-fluent agrammatic variant primary progressive aphasia; svPPA = semantic variant PPA. Note that subjects between ages 55 and 80 are at greatest risk for FTD, and, the FTLD-TDP associated FTD syndrome prevalence is <30 per 100 000 (E), in sharp contrast to the data shown in A\u2013D. \u00b6 One approach that can be used to assess the public health impact of a disease is the evaluation of attributable risk (Porta, 2014). Although generally used to study the impact of risk factors on disease prevalence in a population (Bruzzi et al., 1985), this statistical method can be applied to neuropathological studies to indicate the relative impact of different neuropathologies on clinical dementia. More specifically, the assessment of attributable risk can query how the frequency of LATE-NC, in relation to other common brain lesions detected at autopsy, is associated with the probability of a dementia diagnosis (Boyle et al., 2019). This analytical approach theoretically makes it possible to estimate the proportion of dementia that might be prevented if LATE-NC could be eliminated, and to compare that with other neuropathologies. The results of an analysis of attributable risk in the Rush University Religious Orders Study is shown in Table 2; methods have been described in detail previously (Boyle et al., 2019). These data are compatible with the hypothesis that a significant (\u223c15\u201320%) proportion of clinically diagnosed Alzheimer\u2019s disease dementia (i.e. the Alzheimer\u2019s clinical syndrome) in advanced age is attributable to LATE-NC; the impact is about half the magnitude of ADNC in this group of older subjects, and the impact is similar to the combined effects of vascular neuropathologies. Table 2 \u00b6 Neuropathological indices Fraction attributable % (95% CI) [a] Alzheimer\u2019s disease (ADNC) 39.4 (31.5\u201347.4) Vascular disease pathology [b] 24.8 (17.3\u201332.1) LATE-NC 17.3 (13.1\u201322.0) \u03b1-Synucleinopathy/Lewy body pathology 11.9 (8.4\u201315.6) \u00b6 Also pertinent to the current and future public health impact of LATE is the age range of subjects with highest risk for the disease. The tendency for LATE-NC to occur among the oldest-old has been appreciated for decades, since the groundbreaking studies on age-related hippocampal sclerosis (Crystal et al., 1993; Dickson et al., 1994), a pathological manifestation later shown to be associated with LATE-NC. In multiple subsequent large autopsy samples, LATE-NC was observed with increasing frequency in each year of life after age 85 (Nelson et al., 2013; Keage et al., 2014; Uchino et al., 2015; Hokkanen et al., 2018) (Fig. 4). This is in contrast to amyloid-\u03b2 plaques, which are common (seen in >50% of subjects) in all elderly age groups, but are not universal and not more frequently seen at autopsy with every year of advanced old age (Braak et al., 2011; Nelson et al., 2011a, 2013; Brenowitz et al., 2014; Neltner et al., 2016). LATE appears to be \u223c100-fold more prevalent than FTD syndromes, which tend to affect younger subjects (Knopman and Roberts, 2011; Coyle-Gilchrist et al., 2016) (Fig. 4E; note the y-axis scale). Females are generally more likely to survive to advanced old age than males (Neltner et al., 2016), which places them at increased lifetime risk for LATE. Otherwise, there is no compelling evidence to date of strong sex-related or ethnoracial differences in susceptibility to LATE (Brenowitz et al., 2014; Murray et al., 2014; Latimer et al., 2017; Oveisgharan et al., 2018), but further studies in diverse populations are needed. Since most relevant current data were derived from autopsy cohorts, the prevalence of LATE-NC may be higher in younger subjects than currently recognized if there is a survival bias (those with LATE-NC live to older ages), underscoring the need for more longitudinal studies that incorporate clinical biomarkers. Since the demographic group made up of subjects past 85 years of age is predicted to greatly expand in the coming decades (Gardner et al., 2013; Nelson et al., 2013), LATE is likely to become a far greater public health burden in the future unless preventative or therapeutic strategies are developed. 02.06. Genetics of LATE \u00b6 Genetic studies provide insights into disease-related mechanisms and, potentially, future therapeutic targets. The following five genes (in the chronological order in which they were identified) have been reported to harbour risk alleles associated with pathological manifestations we refer to as LATE-NC: granulin (GRN) on chromosome 17q, transmembrane protein 106B (TMEM106B) on chromosome 7p, ATP-binding cassette sub-family member 9 (ABCC9) on chromosome 12p, potassium channel subfamily M regulatory beta subunit 2 (KCNMB2) on chromosome 3q, and apolipoprotein E (APOE) on chromosome 19q (Dickson et al., 2010; Pao et al., 2011; Beecham et al., 2014; Murray et al., 2014; Nelson et al., 2014, 2015b; Aoki et al., 2015; Katsumata et al., 2017; Yang et al., 2018). See Supplementary Table 1 for summary information on these genes and their associated phenotypes. For this discussion, we include the endophenotype that was used in the published research (usually hippocampal sclerosis) rather than LATE-NC. Gene variants in GRN and TMEM106B were shown to be associated with hippocampal sclerosis and TDP-43 proteinopathy risk using allele tests, based on the known relationship of those two genes to FTLD-TDP (Baker et al., 2006; Boeve et al., 2006; Cruts et al., 2006; Van Deerlin et al., 2010). These gene variants have now been most consistently associated with risk of LATE-NC. For the association between the GRN and hippocampal sclerosis, Dickson et al. showed that hippocampal sclerosis in aged subjects was associated with the T-allele of the GRN single nucleotide polymorphism (SNP) rs5848 (Dickson et al., 2010; Murray et al., 2014). Aoki and colleagues reported that the frequency of the C-allele of TMEM106B rs1990622 in hippocampal sclerosis was lower than that in non-hippocampal sclerosis controls (Aoki et al., 2015). Following the initial studies, the findings were replicated of increased risk for hippocampal sclerosis associated with each copy of the T-allele of TMEM106B rs1990622 (Nelson et al., 2014, 2015b; Dickson et al., 2015; Yu et al., 2015). Since GRN and TMEM106B were both implicated in FTLD-TDP, their strong association with LATE-NC provides compelling evidence for pathogenetic overlap between FTLD-TDP and LATE. From a mechanistic perspective, the cognate proteins for these genes have been shown to play important roles in endosomal/lysosomal biology, and there is experimental evidence for interaction of these gene products (Chen-Plotkin et al., 2012; Nicholson and Rademakers, 2016; Klein et al., 2017; Zhou et al., 2017; Paushter et al., 2018). The TMEM106B gene appears to be pleiotropic for multiple diseases (Gallagher et al., 2014; Ou et al., 2015; Hsiao et al., 2017; Cherry et al., 2018; Chornenkyy et al., 2019), and the LATE-NC risk allele in TMEM106B may influence healthy brain ageing (Rhinn and Abeliovich, 2017; Ren et al., 2018). Separate studies have found that GRN gene products (granulins) play roles in inflammation and wound repair (Ahmed et al., 2007; Miller et al., 2013). Notably, the GRN risk variant rs5848 has been associated with increased inflammatory mediators in CSF (e.g. AXL and CLU) (Fardo et al., 2017). More work is required to enable better understanding of how molecular pathways relevant to FTLD-TDP are involved in LATE. An important recent finding by several different groups is that the APOE \u025b4 allele, which is a risk factor for ADNC and Lewy body disease, is also associated with increased risk for TDP-43 proteinopathy in the elderly (Robinson et al., 2018c; Wennberg et al., 2018; Yang et al., 2018). Other studies did not find an association between APOE genotypes and risk for hippocampal sclerosis (Troncoso et al., 1996; Leverenz et al., 2002; Nelson et al., 2011b; Pao et al., 2011; Brenowitz et al., 2014; Hall et al., 2019; but see Farfel et al., 2016). Few subjects with the APOE \u025b4 allele survive into advanced old age without any amyloid-\u03b2 plaques (Saunders et al., 1993; Schmechel et al., 1993), and it remains to be seen exactly how the APOE \u025b4 protein influences TDP-43 proteinopathy. Nevertheless, recent studies from large research cohorts have provided additional insights into the presence of pathogenetic mechanisms that are shared between neurodegenerative diseases. Since the presence or absence of risk alleles in TMEM106B, GRN, and APOE cannot by themselves or in combination confidently predict the risk for LATE-NC in a given subjects (Katsumata et al., 2017; Nelson et al., 2019), there must be other factors that influence the disease phenotype. The connections of the ABCC9 and KCNMB2 genes with risk of LATE-NC were discovered via genome-wide association studies (GWAS), which are neither helped nor biased by prior mechanistic hypotheses. The finding of the associations between ABCC9 gene variants and LATE-NC (Nelson et al., 2015b), and brain atrophy detected with MRI (Nho et al., 2016), were reported in separate samples from the initial GWAS (Nelson et al., 2014). Neither ABCC9 nor KCNMB2 gene variants were associated with LATE-NC in cohorts other than those described above. ABCC9 and KCNMB2 are genes coding for proteins that serve to regulate potassium channels (Zarei et al., 2007; Nelson et al., 2015a). The ABCC9 risk genotype also implicates thyroid hormone dysregulation in LATE-NC; the locus was found to be associated with altered brain expression of genes induced by thyroid hormone (Nelson et al., 2016a). Thyroid hormones have been found to be dysregulated in subjects with autopsy-confirmed LATE-NC in recent studies (Trieu et al., 2018; Nelson et al., 2019), and high thyrotropin was associated with reduced hippocampal volume in a population-based study (Ittermann et al., 2018). A gene variant near ABCC9, which lies within both the SLCO1A2 and IAPP genes, was also found in a GWAS study to be associated with neurodegeneration disproportional to amyloid-\u03b2 accumulation (Roostaei et al., 2016), which may indicate LATE in those cases. The KCNMB2 gene has been associated with suicidal ideation in US military veterans (Kimbrel et al., 2018) and may be related to depression, which is common in the elderly. Further, when KCNMB2 is overexpressed in the hippocampus of mice, it rescues memory deficits (Yu et al., 2018). More work is required to enable better understanding and identification of the molecular pathways involved in LATE. Prior genetic studies on TDP-43 proteinopathy and hippocampal sclerosis have varied in important ways, including patient inclusion/exclusion criteria, disease definitions, and age composition, which may explain their differing findings with regard to genotype/phenotype associations. The prospects for successful future genetic discoveries will be improved by the development of specific and standardized LATE-NC endophenotypes. FTLD-TDP provides an example in which pathological subtyping of patients has been beneficial for genetic correlation studies: there are, for example, strong associations between TMEM106B and GRN gene variants with FTLD-TDP type A pathology (Rademakers et al., 2008; Aoki et al., 2015). Preliminary studies suggest that distinguishing morphology of TDP-43 pathology in LATE-NC may also be relevant to genetic risk (Josephs et al., 2019). We speculate that genetic profiling may eventually become a key consideration for recruitment to clinical trials, and possible future precision medicine approaches, since some genotypes may be differentially responsive to specific interventions. 02.07. LATE biomarkers \u00b6 Optimal biomarkers for LATE, including biofluids or PET ligands, would be specific for the disease-defining feature, namely TDP-43 proteinopathy (Steinacker et al., 2018). At this time, no biofluid or PET biomarker satisfies this essential criterion of molecular specificity. Nor do PET ligands for LATE seem to be on the near-term horizon. The problems of intracellular location and small pathological burden of TDP-43 proteinopathy are obstacles that limit signal-to-noise ratio for biomarkers. The NIA-AA Research Framework group recommended a system for classifying subjects based on amyloid-\u03b2 amyloid (A), tau (T) and neurodegeneration/neuronal injury (N) biomarkers, which is termed AT(N) (Burnham et al., 2016; Jack et al., 2016). Each biomarker category can be binarized as positive (+) or negative (\u2212) resulting in eight possible biomarker profiles. Certain AT(N) profiles indicate increased likelihood that LATE-NC might be present. The \u2018N\u2019 in AT(N) is in parentheses to indicate that it represents cumulative brain injury/neurodegeneration from all aetiologies and is not specific for any one aetiology. An assumption is that in Alzheimer\u2019s disease, neurodegeneration is associated with tauopathy, and therefore in an A+T\u2212(N)+ subject, the N+ is likely due to a comorbid non-Alzheimer\u2019s disease pathophysiological process(es). If (N)+ is ascertained by an imaging measure that captures neurodegeneration as reflected medial temporal atrophy or hypometabolism, then this implicates LATE (often with hippocampal sclerosis) as a likely non-Alzheimer\u2019s disease comorbidity. Similar logic applies to subjects with an A\u2212T\u2212(N)+ profile, the N+ is presumably due to a non-Alzheimer\u2019s disease pathological process(es), and if the (N)+ measure is hippocampal atrophy, or medial temporal hypometabolism, then LATE is implicated (Fig. 5). Figure 5 \u00b6 Biomarkers are currently not specific to LATE-NC. (A) Radiological scans from an 86-year-old female who suffered amnestic cognitive impairment compatible with \u2018Probable Alzheimer\u2019s disease\u2019 diagnosis. However, the amyloid-\u03b2 PET scan was negative, tau PET scan was also negative, and the MRI showed appreciable atrophy of the medial temporal lobes bilaterally. This combination is considered \u2018A\u2212T\u2212N+\u2019 and was diagnosed during life as \u2018suspected non-Alzheimer\u2019s pathology\u2019 (SNAP). Autopsy within a year of the brain scans confirmed the presence of TDP-43 pathology and hippocampal sclerosis, which now is diagnosable as LATE-NC. (B) Another common biomarker combination, in the brain of a 91-year-old male with dementia. In this subject, the amyloid PET scan was positive, yet the tau PET scan was negative. The MRI again showed atrophy of the medial temporal lobes. The combination of pathologies\u2014in this case presumed early ADNC and comorbid LATE-NC\u2014is common, especially in the brains of subjects in advanced age. \u00b6 SNAP is a non-specific biomarker-defined category that affects \u223c15\u201330% of subjects in prior clinical series (Jack et al., 2012; Vos et al., 2013; Wisse et al., 2015; Burnham et al., 2016; Gordon et al., 2016), and includes a variety of non-Alzheimer\u2019s disease aetiologies, but prominent among these is LATE. Autopsy studies indicate that LATE-NC can exist without other brain pathologies, but commonly co-occurs with ADNC (Jellinger, 2000; Attems and Jellinger, 2006; Josephs et al., 2014b, 2015, 2016); biomarker studies are consistent with those results. Botha et al. (2018) have shown that tau-PET-negative dementia can mimic Alzheimer\u2019s disease clinically, suggesting that LATE is probably a common cause of tau-negative dementia. Further, a fluorodeoxyglucose (FDG) PET measure (the ratio of inferior to medial temporal metabolism) was elevated in autopsy proven LATE with hippocampal sclerosis compared to autopsy proven Alzheimer\u2019s disease cases lacking LATE-NC (Botha et al., 2018). LATE-NC was confirmed at autopsy in two subjects with tau-PET-negative scans who both had elevated inferior to medial temporal FDG metabolism ratios. These data are compatible with the hypothesis that cognitively impaired tau-PET-negative subjects with marked medial temporal hypometabolism are likely to have LATE-NC. Other studies correlating autopsy findings with post-mortem magnetic resonance measures of regional tissue T2 relaxation times (Dawe et al., 2014), hippocampal shape (Dawe et al., 2011), and regional cortical volume measures (Kotrotsou et al., 2015) indicate that neuroimaging signatures of ADNC and LATE-NC may differ (see above). The shape differences in the medial temporal lobes associated with LATE-NC versus ADNC provide potential imaging biomarkers of LATE (Dawe et al., 2011; Makkinejad et al., 2019), whereas the evaluation of multiple brain regions is warranted as demonstrated in Fig. 3. A non-specific biomarker of cumulative brain injury (N) may be useful in the context of LATE. If a biomarker for LATE is not forthcoming, then a quantitative in vivo indicator might remain the difference between the magnitude of an observed (N) biomarker minus the predicted (N) value given the results of all other known biomarkers. A predictive (but not diagnostic) LATE measure could be envisioned as the \u2018residual of the regression\u2019 of expected on observed medial/basal temporal neurodegeneration, given all knowable information about other pathological processes. Recent studies indicate that neurofilament light chain (NfL) might be a preferred biofluid (N) biomarker (Zetterberg, 2016; Kortvelyessy et al., 2018) but perturbation of NfL in LATE (plasma or CSF) remains to be tested. Moreover, elevated levels of NfL occur in many different causes of brain injury so NfL lacks disease specificity. Although no specific LATE biomarker exists at present, the AT(N) system was designed explicitly to enable expansion to incorporate new biomarkers in categories beyond AT(N) (Jack et al., 2018). If or when a biomarker of LATE is validated, AT(N) could be expanded to ATL(N), where \u2018L\u2019 stands for LATE. The ultimate objective would be comprehensive characterization of many relevant brain pathologies in vivo using combinations of biomarkers. Future diagnostic biomarkers may be less centred on ADNC, and able to incorporate the common combinations of diseases that occur in ageing brain better. This concept is illustrated in Supplementary Table 2. If a specific biomarker for LATE is developed, this may complement ongoing efforts to develop an optimal neuropathological assessment. LATE-NC may in the future be assessed along three dimensions: stage (i.e. anatomical distribution); subtype (i.e. differing histopathological patterns in a given region); and grade (i.e. severity or pathological load). While systems for subtyping or grading LATE-NC have yet to be validated, these may in the future be useful for early diagnosis, improved clinical prognosis, and development of new strategies to treat or prevent the disease. 02.08. Implications for Alzheimer\u2019s disease and LATE clinical trials \u00b6 Formalization of LATE diagnostic criteria and increased awareness of this disease should help guide the design and interpretation of Alzheimer\u2019s disease clinical trials. Comorbid ADNC and LATE-NC becomes increasingly more prevalent with advancing age, and the mechanisms underlying each of these common lesions have independent effects on cognitive performance (Nelson et al., 2010). LATE-NC, when coexisting with ADNC, will have the potential to obscure the effects of a potential disease modifying agent on cognitive assessment results in living subjects. The primary outcome measures in disease-modifying Alzheimer\u2019s disease clinical trials will remain cognitive or functional scales for the foreseeable future (Cummings et al., 2016; Register, 2018). Thus, the presence of LATE-NC will complicate interpretation of Alzheimer\u2019s disease-specific treatment effects that are inferred from observed cognitive outcomes. Until there are biomarkers for LATE, clinical trials should be powered to account for TDP-43 proteinopathy. LATE is among the common age-related diseases that can mimic the amnestic presentation of Alzheimer\u2019s disease (Nelson et al., 2013), and it is one of many reasons why biological rather than clinical disease definitions are important in the era of disease modifying clinical trials (Jack et al., 2018). Biomarkers have roles for both inclusion and exclusion. It will be important, at recruitment of subjects into future disease-modifying Alzheimer\u2019s disease clinical trials, to stratify according to major known predictors, including clinical features, genetics, and known biomarkers. This stratification will enable enrichment for subjects on the ADNC continuum (Sevigny et al., 2016) while excluding subjects likely to have high risk for LATE-NC (Botha et al., 2018). Even with best efforts at baseline, the multiplicity of diseases that occur in brains of older subjects will still require analyses according to subgroups. This is another reason why clinical trials in dementing diseases of ageing will require large sample sizes. Research into Alzheimer\u2019s disease has provided additional topical caveats (Gulisano et al., 2018; Hunter et al., 2018; Morris et al., 2018). For example, there is a danger that we fundamentally misunderstand the nature and complexity of processes related to TDP-43 proteinopathy, and this could lead to significant biases in the ways that we approach clinical diagnosis and clinical trials of LATE. For now, as with Alzheimer\u2019s disease, the misfolded proteins provide a disease marker and a potential target for therapies. Clinical trials directed at preventing or treating LATE, in isolation or in concert with other brain diseases, should be a major direction for future research. Performing such trials optimally will first require development of a specific LATE biomarker. For now, five alternative, but not mutually exclusive, approaches exist for developing disease-modifying therapies: (i) focus on pathways and gene products such as APOE \u025b4 that seem to be in common between Alzheimer\u2019s disease, Lewy body disease, and LATE; (ii) focus on pathways and gene products such as TMEM106B and GRN that are shared between FTLD-TDP and LATE; (iii) focus on pathways and gene products such as ABCC9 and KCNMB2 that have been implicated by GWAS; (iv) focus on potential research subjects with the A\u2212T\u2212(N+) biomarker profile, who are now excluded from many Alzheimer\u2019s disease-related clinical trials; and/or (v) focus on strategies to eliminate TDP-43 aggregates or to prevent the formation of these aggregates. 03. Conclusions and future directions \u00b6 A key goal of this working group effort was to catalyse future research on LATE, an under-recognized condition that affects many older subjects. It is important to promote awareness in multiple scientific areas and to focus on translational and interdisciplinary approaches. Development of specific LATE biomarker(s) should be a high scientific priority. While a sensitive and specific biomarker using neuroimaging or biofluids would be ideal, other disease markers could capitalize on existing metrics such as the AT(N) research guidelines with or without imaging or biofluid risk profiling. Developing biomarkers or other criteria to identify subjects with LATE would augment observational studies that seek to unravel the natural history of LATE, and its coevolution with other ageing-related diseases. With sufficient longitudinal observations, cause and effect inferences may become possible, and clinical trials implemented. Further pathology studies will also be necessary. The consensus pathological classification scheme that we propose should be considered preliminary because much remains to be learned about LATE. The application of pathological subtyping has been useful in the context of FTLD-TDP (Lee et al., 2017; Mackenzie and Neumann, 2017; Pottier et al., 2018), and pathological subtyping may help refine LATE-NC endophenotypes for diagnostic and genetic studies (Josephs et al., 2019). At this point, there is no consensus about how or whether to apply such criteria for LATE-NC. A detailed characterization of the molecular pathology of TDP-43 is required for different cell types across brain regions in large population-representative samples. This should include characterization of various phosphorylation states, cleavage fragments, and other post-translational modifications of TDP-43. Further, each anti-TDP-43 antibody used should be assessed for potential cross-reactivity with other proteins or LATE-NC features in situ. It will also be important to determine the prevalence of all co-pathologies associated with LATE-NC, the impact of the molecular conformations and modifications of TDP-43, the cellular types involved, and the natural history of the disease. These advances will also assist in developing animal models. Additional epidemiological, clinical, neuroimaging, and genetic studies will be important to better characterize the public health impact and clinical phenotypes for LATE. Further, LATE must be studied in more diverse populations and cohorts. Careful clinical assessments over time and into the oldest age groups is essential, along with detailed biological measures and autopsy, so that the complexity of ageing changes can be assessed (Brayne, 1993). In vivo and ex vivo imaging studies to determine the focal and more diffuse changes in the brains of subjects with LATE will also be important. Future studies may generate better insights into the clinical indices and cognitive features that are associated with increased probability of LATE-NC. Risk factors, protective influences, and other correlates could thus be identified to help prevent or predict LATE. For example, autoimmune disease may play a role in TDP-43 proteinopathy and LATE-NC in particular (Miller et al., 2013; Trieu et al., 2018). Optimally, future studies will complement traditional GWAS and gene-focused analyses with multi-omics studies to capture a greater appreciation of the complex mechanisms and diagnostic or therapeutic opportunities in the study of LATE. Animal models and basic science research into LATE are imperative, with the caveat that the aged human brain is challenging to model accurately. Functional studies, including transmission animal models that use TDP-43 fibrils (Porta et al., 2018) or extracts from brains with LATE-NC injected into animals or cell cultures (Laferriere et al., 2019), can be combined with genetic studies to test hypotheses and to add statistical power for preclinical and hypothesis-testing experiments. Molecular studies that focus on TDP-43 and the upstream triggers and downstream molecular consequences are necessary to elucidate mechanisms of disease. Models that account for co-pathologies are rare at present, but have the potential to be highly informative. Ultimately, it is hoped that these collective research efforts will one day result in successful preventative and therapeutic strategies. References \u00b6 \u00b6 img.fig1{width: 50%; float: right;}","title":"190507 Nelson, Peter T, 2019"},{"location":"190507_NelsonPeterT2019/#toc","text":"00. Abstract 01. Introduction 02. Background 02.01. LATE neuropathological changes 02.02. LATE MRI studies 02.03. Recommendations for routine autopsy evaluation and classification of LATE-NC 02.04. Clinical and neurocognitive features of LATE 02.05. Public health impact of LATE 02.06. Genetics of LATE 02.07. LATE biomarkers 02.08. Implications for Alzheimer\u2019s disease and LATE clinical trials 03. Conclusions and future directions","title":"ToC"},{"location":"190507_NelsonPeterT2019/#00_abstract","text":"LATE ( limbic-predominant age-related TDP-43 encephalopathy ) LATE-NC (LATE neuropathological change) We describe a recently recognized disease entity, limbic-predominant age-related TDP-43 encephalopathy (LATE). LATE neuropathological change (LATE-NC) is defined by a stereotypical TDP-43 proteinopathy in older adults, with or without coexisting hippocampal sclerosis pathology. LATE-NC is a common TDP-43 proteinopathy, associated with an amnestic dementia syndrome that mimicked Alzheimer\u2019s-type dementia in retrospective autopsy studies. LATE is distinguished from frontotemporal lobar degeneration with TDP-43 pathology based on its epidemiology (LATE generally affects older subjects), and relatively restricted neuroanatomical distribution of TDP-43 proteinopathy. In community-based autopsy cohorts, \u223c25% of brains had sufficient burden of LATE-NC to be associated with discernible cognitive impairment. Many subjects with LATE-NC have comorbid brain pathologies, often including amyloid-\u03b2 plaques and tauopathy. Given that the \u2018oldest-old\u2019 are at greatest risk for LATE-NC, and subjects of advanced age constitute a rapidly growing demographic group in many countries, LATE has an expanding but under-recognized impact on public health. For these reasons, a working group was convened to develop diagnostic criteria for LATE, aiming both to stimulate research and to promote awareness of this pathway to dementia. We report consensus-based recommendations including guidelines for diagnosis and staging of LATE-NC. For routine autopsy workup of LATE-NC, an anatomically-based preliminary staging scheme is proposed with TDP-43 immunohistochemistry on tissue from three brain areas, reflecting a hierarchical pattern of brain involvement: amygdala, hippocampus, and middle frontal gyrus. LATE-NC appears to affect the medial temporal lobe structures preferentially, but other areas also are impacted. Neuroimaging studies demonstrated that subjects with LATE-NC also had atrophy in the medial temporal lobes, frontal cortex, and other brain regions. Genetic studies have thus far indicated five genes with risk alleles for LATE-NC: GRN , TMEM106B , ABCC9 , KCNMB2 , and APOE . The discovery of these genetic risk variants indicate that LATE shares pathogenetic mechanisms with both frontotemporal lobar degeneration and Alzheimer\u2019s disease, but also suggests disease-specific underlying mechanisms. Large gaps remain in our understanding of LATE. For advances in prevention, diagnosis, and treatment, there is an urgent need for research focused on LATE, including in vitro and animal models. An obstacle to clinical progress is lack of diagnostic tools, such as biofluid or neuroimaging biomarkers, for ante-mortem detection of LATE. Development of a disease biomarker would augment observational studies seeking to further define the risk factors, natural history, and clinical features of LATE, as well as eventual subject recruitment for targeted therapies in clinical trials.","title":"00. Abstract"},{"location":"190507_NelsonPeterT2019/#01_introduction","text":"Transactive response DNA binding protein of 43 kDa (TDP-43) proteinopathy in limbic brain structures is commonly observed in subjects past 80 years of age. This proteinopathy has been associated with substantial cognitive impairment that mimicked Alzheimer\u2019s disease clinical syndrome in retrospective studies. Despite evidence from many sources attest ing to the public health impact of age-related TDP-43 proteinopathy, there is as yet no consensus-based nomenclature . To address this problem, we propose new terminology: limbic-predominant age-related TDP-43 encephalopathy (LATE). Guidelines are suggested for the autopsy evaluation and staging of LATE neuropathological change (LATE-NC). We review the medical literature pertaining to LATE, including cognitive manifestations, neuroimaging, public health impact, and genetics. The importance of LATE as a contributing factor in neurodegeneration is stressed, as are the needs for specific LATE biomarker development, TDP-43 focused drug discovery, and eventual clinical trials. We conclude by highlighting important knowledge gaps and potential future directions for research on LATE. Summary points are presented in Box 1.","title":"01. Introduction"},{"location":"190507_NelsonPeterT2019/#box_1","text":"LATE and LATE-NC summary points LATE-NC features A sampling and staging system for routine autopsy diagnosis is proposed to characterize the anatomical distribution of TDP-43 proteinopathy Stage 1: amygdala only Stage 2: +hippocampus Stage 3: +middle frontal gyrus Hippocampal sclerosis pathology may be observed (and should be reported), but is neither necessary nor sufficient for diagnosis of LATE-NC LATE-NC is present in >20% (up to 50%) of individuals past age 80 years according to large community-based autopsy series LATE is associated with substantial disease-specific cognitive impairment, usually an amnestic dementia syndrome (\u2018dementia of the Alzheimer\u2019s type\u2019) The overall public health impact of LATE is on the same order of magnitude as Alzheimer\u2019s disease neuropathological changes; the diseases are often comorbid, but which pathology is more severe varies greatly between individuals Genetic risk factors for LATE have some overlap with FTLD-TDP and with Alzheimer\u2019s disease There is no molecule-specific biomarker for LATE. This is an important area of need for use in clinical trials (including as a potential exclusion criterion for Alzheimer\u2019s disease clinical trials) and longitudinal studies of the clinical and pathological progression of LATE","title":"Box 1"},{"location":"190507_NelsonPeterT2019/#02_background","text":"","title":"02. Background"},{"location":"190507_NelsonPeterT2019/#02p01","text":"There is growing awareness that Alzheimer\u2019s disease neuropathological change ( ADNC ) is only one of multiple neuropathological substrates associated with amnestic mild cognitive impairment and the Alzheimer\u2019s clinical syndrome in the aged population ( Korczyn, 2002 ; Zekry et al., 2002 ; Bennett et al., 2006 ; Jellinger and Attems, 2007 ; Schneider et al., 2007 ; Crary et al., 2014 ; Murray et al., 2014 ; Rahimi and Kovacs, 2014 ; James et al., 2016 ). Recent studies have gathered rich clinical data from large groups of subjects across a spectrum of cognitive states, correlated these clinical findings with new pathological markers at autopsy, and then analysed the data using powerful statistical methods. These studies have indicated that the diseases of aged human brains are complex: multiple comorbid pathologies are the norm, and there is substantial interindividual variation in neuropathological phenotypes ( Neuropathology Group. Medical Research Council Cognitive and Aging, 2001 ; Brayne et al., 2009 ; Kovacs et al., 2013 ; Murray et al., 2014 ; Rahimi and Kovacs, 2014 ; White et al., 2016 ; Abner et al., 2017 ; Kapasi et al., 2017 ; [Suemoto et al., 2017][2017_Suemoto]; [Tanskanen et al., 2017][2017_Tanskanen]; [Robinson et al., 2018b][2018_Robinson], [c][2018_Robinson]). While there is a strong association between severe ADNC and cognitive impairment in all age groups ([Nelson et al., 2009][2009_Nelson]; [Abner et al., 2011][2011_Abner]), subjects who die after 80 years of age often have exhibited cognitive decline exceeding expectation given the severity of ADNC ([Kawas and Corrada, 2006][2006_Corrada_Kawas]; [Savva et al., 2009][2009_Savva]; [Nelson et al., 2012][2012_Nelson]). LATE-NC is an important contributor to this apparent clinicopathological mismatch (see below).","title":"02.P01"},{"location":"190507_NelsonPeterT2019/#02p02","text":"TDP-43 ( Neumann et al., 2006 ) TDP-43 w/i nuclei \u27f6 phospharylted w/i cytoplasm Historically, the first-recognized pathological manifestation of LATE was profound hippocampal neuron loss and gliosis, collectively termed hippocampal sclerosis. In a landmark study, Dickson et al. (1994) identified 13 elderly subjects with dementia and hippocampal sclerosis, yet who lacked substantial ADNC. Other larger autopsy series that included subjects with dementia and hippocampal sclerosis were later reported ([Crystal et al., 2000][2000_Crystal]; Barker et al., 2002; Leverenz et al., 2002; White et al., 2002; Zarow et al., 2005; Attems and Jellinger, 2006; Brayne et al., 2009). In 2006, phosphorylated TDP-43 was discovered as the disease protein in the ubiquitylated inclusions that are signatures of amyotrophic lateral sclerosis (ALS) and most cases of frontotemporal lobar degeneration (FTLD), known as FTLD-TDP (Neumann et al., 2006; Cairns et al., 2007a). TDP-43 protein, encoded by the TARDBP gene (Ou et al., 1995), is a protein that binds to RNA and DNA as well as to other proteins, and serves multiple functions in gene expression regulation at the levels of both transcription and translation (Cohen et al., 2011; Guo and Shorter, 2017). Expressed in most human tissues and cell types, TDP-43 is predominantly non-phosphorylated and localized mostly within nuclei, while in disease states the protein is phosphorylated and often translocated to the cytoplasm ( Neumann et al., 2006 ).","title":"02.P02"},{"location":"190507_NelsonPeterT2019/#02p03","text":"LATE TDP-43 discovered in > 80yo w/o FTLD / ALS also comorbid hipocampal sclerosis &/ ADNC advanced age \u2191 TDP-43 proteinopathy / hippocampal sclerosis pathology / amnestic dementia \u2193 severe ADNC Following the detection of TDP-43 proteinopathy in FTLD-TDP and in the large majority of ALS cases (Mackenzie et al., 2007), TDP-43 proteinopathy was also discovered in the brains of subjects over age 80 years without FTLD or ALS, but often with comorbid hippocampal sclerosis and/or ADNC ( Amador-Ortiz et al., 2007a , b ). In subjects with ADNC, LATE-NC represents a common comorbid lesion that lowers the threshold for developing dementia ([Josephs et al., 2014b], [2015]). In retrospective studies, age-related TDP-43 proteinopathy has been associated with a progressive amnestic syndrome that mimicked the Alzheimer\u2019s clinical syndrome ([Pao et al., 2011]; [Brenowitz et al., 2014]). TDP-43 proteinopathy, hippocampal sclerosis pathology, and the associated amnestic dementia increases with advanced age, while the prevalence of severe ADNC decreases in extreme old age ([Nelson et al., 2011a], [b], [2013]; Brenowitz et al., 2014 ). The presence of pathological TDP-43 in these cases suggests a novel disease mechanism in older adults. As there is currently no universally agreed upon terminology or staging system for common age-related TDP-43 proteinopathy, this condition is under-studied and not well recognized even among investigators in the field of dementia research. The promotion of research and increasing awareness of this disease are the primary motivations for developing the new term LATE, and for the recommendations that follow.","title":"02.P03"},{"location":"190507_NelsonPeterT2019/#0201_late_neuropathological_changes","text":"","title":"02.01. LATE neuropathological changes"},{"location":"190507_NelsonPeterT2019/#0201p01","text":"LATE encompoass hippocampal sclerosis hippocampal sclerosis of aging hippocampal sclerosis dementia cerebral age-related TDP-43 w/ sclerosis TDP-43 pathology in the elderly LATE-NC is a TDP-43 proteinopathy of advanced age, especially in subjects older than age 80. Following the convention proposed by a working group for the neuropathological criteria of Alzheimer\u2019s disease ([Montine et al., 2012]), we use LATE to refer to the disease, and LATE-NC as the term to indicate LATE neuropathological changes. The term LATE is intended to encompass several previously used designations related to TDP-43 proteinopathy that may be associated with cognitive impairment, including hippocampal sclerosis, hippocampal sclerosis of aging, hippocampal sclerosis dementia, cerebral age-related TDP-43 with sclerosis, and TDP-43 pathologies in the elderly (for reviews see [Kuslansky et al., 2004]; [Lippa and Dickson, 2004]; [Nelson et al., 2013], 2016b ; [Dutra et al., 2015]).","title":"02.01.P01"},{"location":"190507_NelsonPeterT2019/#0201p02","text":"TDP-43 inclusion body in cytoplasm abnormal accumulation in nuclei & neurite type A FTLD-TDP similar/different TDP-43 in LATE HCP olfactory bulb neocortex basal ganglia brainstem \u2018TDP-43 proteinopathy\u2019 refers to loss of normal nuclear TDP-43 immunoreactivity, with TDP-43 protein \u2018inclusion bodies\u2019 in the neuronal cytoplasm, as well as abnormal TDP-43 accumulation (much of it phosphorylated) in nuclei and cell processes (neurites) of neurons and in oligodendroglia and astrocytes. For representative examples of TDP-43 proteinopathy, see Fig. 1 . Mislocalized and phosphorylated TDP-43 is a necessary feature of LATE-NC and sometimes has characteristics similar to those seen in type A FTLD-TDP ([Lin et al., 2009][2009_Lin]; [Mackenzie et al., 2011][2011_Mackenzie]; Murray et al., 2014 ; Aoki et al., 2015 ), but often the features do not fit cleanly into an established FTLD-TDP subtype. Indeed, a recent study described features of LATE-NC including both similarities and differences from type A FTLD-TDP ( Josephs et al., 2019 ). In addition to limbic structures, TDP-43 proteinopathy in LATE has also been described in the olfactory bulb, neocortex, basal ganglia, and less frequently in brainstem ([Josephs et al., 2008][2008_Josephs]; [Geser et al., 2010][2010_Geser]; [Josephs and Dickson, 2016][2016_Dickson_Josephs]; [Nelson et al., 2018][2018_Nelson]). Immunoelectron microscopy showed that the TDP-43 inclusions have a fibrillary ultrastructure composed of bundled 10\u201320-nm diameter straight filaments ([Lin and Dickson, 2008][2008_Dickson_Lin]; [Lin et al., 2009][2009_Lin]), often accompanied by electron dense granules ([Cairns et al., 2007b][2007_Cairns]; [Robinson et al., 2013][2013_Robinson]).","title":"02.01.P02"},{"location":"190507_NelsonPeterT2019/#figure_1","text":"LATE neuropathological changes (LATE-NC). (A\u2013E) Coronally sectioned human hippocampi stained using haematoxylin and eosin (H&E). Note that the photomicrographs in A\u2013C are presented at the same magnification. (A) LATE-NC with hippocampal sclerosis (HS). The hippocampus is atrophic and the neuropil rarefied. (D) Higher magnification in CA1 subfield, with lack of normal cellular architecture and with extensive gliosis. (C) Control age-matched hippocampus. (E) CA1 of the control hippocampus to demonstrate the normal cellular architecture and intact eosinophilic neuropil (asterisk). The hippocampus shown in B is less atrophic, with less obvious neuropil disruption, in comparison to the case in A at low magnification; however, an adjacent section revealed TDP-43 proteinopathy. Hippocampal fields are labelled in B: dg = dentate granule layer; Sub = subiculum. TDP-43 proteinopathy can be recognized using antibodies raised against either non-phosphorylated or phosphorylated TDP-43 epitopes. (F) Dentate granule cells in a case lacking TDP-43 pathology. Note that cell nuclei are normally immunopositive for non-phosphorylated TDP-43 protein. In a case with LATE-NC (G), by contrast, an antibody against phosphorylated TDP-43 protein recognizes only the pathological inclusions in the nucleus (green arrow) and cytoplasm (red arrow). Unlike the antibody against non-phosphorylated TDP-43, the antibody against phosphorylated TDP-43 is negative in non-affected cells. Most cells in G are visualized with the counterstain, haematoxylin, which stains cell nuclei blue. The Venn diagram in H illustrates schematically the imperfect overlap between cases with TDP-43 proteinopathy, hippocampal sclerosis, and LATE-NC. A subset of cases with TDP-43 pathology have comorbid hippocampal sclerosis pathology; the change zone between non-hippocampal sclerosis and hippocampal sclerosis cases is indistinct because many cases seem to be in transition with incipient hippocampal neuron loss and gliosis. Importantly, cases with hippocampal sclerosis pathology but no TDP-43 proteinopathy (e.g. hippocampal sclerosis pathology associated with anoxia or epilepsy) are not classified as LATE-NC. (I) Phospho-TDP-43 proteinopathy in two neurons in hippocampal CA1, along with phospho-TDP-43 immunoreactive dystrophic neurites. (J) Tangle-like phospho-TDP-43 immunoreactive cytoplasmic inclusions in amygdala (red arrows) with fewer phospho-TDP-43 immunoreactive neurites in the background. (K) An intraneuronal phospho-TDP-43 inclusion (red arrow) and a phospho-TDP-43 deposit (green arrow) surrounding a capillary (shown with blue arrows); these TDP-43 immunoreactive structures have been demonstrated to exist within astrocyte end-feet (Lin et al., 2009). Note also the presence of a cell with cytoplasmic puncta (green arrow), perhaps in an early phase of phosphorylated TDP-43 proteinopathy. Scale bar in A = 4 mm for A\u2013C; D = 200 \u03bcm; E = 100 \u03bcm; F = 30 \u03bcm; G = 35 \u03bcm; I = 30 \u03bcm; and K = 25 \u03bcm.","title":"Figure 1"},{"location":"190507_NelsonPeterT2019/#0201p03","text":"neuronal dropoout & astrocytosis in LATE-NC HPC subiculum entorhinal cortex amygdala In brains with LATE-NC, haematoxylin and eosin stains may reveal neuronal dropout and astrocytosis in the CA1 sector of the hippocampus, as well as in the subiculum , entorhinal cortex, and amygdala ([Amador-Ortiz and Dickson, 2008][2008_Dickson_Amador-Ortiz]). Atrophy can be marked in these areas (Fig. 1A\u2013C). In severely affected hippocampi, the neuropil becomes rarefied and loss of neuronal components is accompanied by reactive astrocytosis ([Amador-Ortiz et al., 2007a][2007_Amador-Ortiz]). Pronounced leucocyte infiltrates or perivascular cuffing are not typically seen, but hypertrophic microglia can be numerous ([Bachstetter et al., 2015][2015_Bachstetter]). The neuronal cell loss is segmental in some subjects, observed in some but not all sections from the same brain area ([Ighodaro et al., 2015][2015_Ighodaro]). Hippocampal sclerosis pathology is unilateral in \u223c40\u201350% of cases in which both sides were evaluated ([Nelson et al., 2011b][2011_Nelson]; [Zarow et al., 2012][2012_Zarow]; [Kero et al., 2018][2018_Kero]), not unlike FTLD-TDP ([Irwin et al., 2018][2018_Irwin]).","title":"02.01.P03"},{"location":"190507_NelsonPeterT2019/#0201p04","text":"TDP-43 \u2013 /HS + : NOT represent LATE-NC Hippocampal sclerosis is present in a subset of cases with severe LATE-NC, and was the first characteristic pathological feature that distinguished it from ADNC ( Dickson et al., 1994 ). Nevertheless, hippocampal sclerosis is neither specific to LATE-NC nor sufficient for the diagnosis of LATE. The neuropathological diagnosis of hippocampal sclerosis is fraught with difficulty. The most recent consensus guidelines for ADNC and related disorders stated that hippocampal sclerosis pathology is \u2018defined by severe pyramidal cell loss and gliosis in CA1 and subiculum of the hippocampal formation that is out of proportion to AD neuropathologic change in the same structures\u2019 ([Montine et al., 2012][2012_Montine]). There is, however, significant topographic and phenotypic heterogeneity in hippocampal degeneration, creating difficulties in establishing strict criteria for widespread use. Moreover, hippocampal sclerosis is a pathological endpoint associated with various underlying disease processes, including epilepsy, hypoxia, hypoglycaemia, certain infections, and numerous neurodegenerative conditions ([Josephs et al., 2007][2007_Josephs]; [Thom et al., 2009][2009_Thom]; [Yokota et al., 2010][2010_Yokota]; [Malek-Ahmadi et al., 2013][2013_Malek-Ahmadi]; [Murray et al., 2013][2013_Murray]; [Ling et al., 2017][2017_Ling]; [Popkirov et al., 2017][2017_Popkirov]; [Sen et al., 2018][2018_Sen]). Having originated in a 19th century study of epilepsy by Wilhelm Sommer ([Sommer, 1880][1880_Sommer]; [Thom, 2009][2009_Thom]), the term hippocampal sclerosis is still used widely by radiologists and pathologists in the context of seizure disorders ([Isnard and Bourdillon, 2015][2015_Bourdillon_Isnard]; [Thom and Sisodiya, 2015][2015_Sisodiya_Thom]). Detailed discussions of histopathological features and subtypes of hippocampal sclerosis can be found elsewhere ([Probst et al., 2007][2007_Probst]; [Rauramaa et al., 2013][2013_Rauramaa]; [Hatanpaa et al., 2014][2014_Hatanpaa]; [Dutra et al., 2015][2015_Dutra]; [Thom and Sisodiya, 2015][2015_Sisodiya_Thom]; [Cykowski et al., 2017][2017_Cykowski]). Brains with hippocampal sclerosis, but lacking TDP-43 pathology (TDP-43\u2212/HS+), do not represent LATE-NC. For example, brains with hippocampal sclerosis caused by acute hypoxia or associated with epilepsy are negative for TDP-43 proteinopathy ([Amador-Ortiz et al., 2007b][2017_Amador-Ortiz]; [Lee and Lee, 2008][2008_Lee_Lee]; [Nelson et al., 2011b][2011_Nelson]) and do not fulfil criteria for LATE-NC (Fig. 1H). In summary, TDP-43 proteinopathy is a necessary feature of LATE-NC that may or may not be accompanied by hippocampal sclerosis.","title":"02.01.P04"},{"location":"190507_NelsonPeterT2019/#0201p05","text":"TDP-43 + /HS \u2013 \u2192 TDP-43 + /HS + As has been the case in other neurodegenerative diseases ([Braak et al., 1993][1993_Braak], [2006][2006_Braak]; [Thal et al., 2000][2000_Thal]; [Zaccai et al., 2008][2008_Zaccai]; [Alafuzoff et al., 2009][2009_Alafuzoff]), careful assessments of autopsy data, from both longitudinal studies of clinic-based research subjects as in the NIA-funded Alzheimer\u2019s Disease Centers, and from community-based studies, have expanded our understanding of LATE. While subjects with advanced age and hippocampal sclerosis often have TDP-43 proteinopathy ([Amador-Ortiz et al., 2007b][2007_Amador-Ortiz]; [Nelson et al., 2011b][2011_Nelson]; [Robinson et al., 2014][2014_Robinson]; [Nag et al., 2015][2015_Nag], 2018 ), TDP-43 proteinopathy in limbic structures is more prevalent than hippocampal sclerosis (Kovacs et al., 2013; Josephs et al., 2014b; Keage et al., 2014; Murray et al., 2014; Rahimi and Kovacs, 2014; Aoki et al., 2015; Nag et al., 2015, 2017; Hokkanen et al., 2018; Robinson et al., 2018b). The TDP-43-positive (+) and hippocampal sclerosis-negative (HS\u2212) cases are a subset of LATE-NC that represent 5\u201340% of research subjects in autopsy series. Prior researchers have used terms for brains with TDP-43 proteinopathy and with some degree of cell dropout and gliosis, but lacking frank hippocampal sclerosis, as a \u2018precursor to HS\u2019, \u2018pre-HpScl,\u2019 or \u2018pre-HS-Aging\u2019 ([Hatanpaa et al., 2008][2008_Hatanpaa]; [Aoki et al., 2015][2015_Aoki]; [Hokkanen et al., 2018][2018_Hokkanen]). As suggested by the terminology, TDP-43 + /HS \u2212 brains may represent an early or transitional phase on the same disease continuum as TDP-43 + /HS + cases. There are other clues about LATE that were gathered from autopsy cohort studies. For example, even when hippocampal sclerosis was unilateral or segmental, the TDP-43 proteinopathy was almost always bilateral ([Nelson et al., 2011b][2011_Nelson]; [Ighodaro et al., 2015][2015_Ighodaro]). These observations have implications about how LATE evolves, which remains an important and open question.","title":"02.01.P05"},{"location":"190507_NelsonPeterT2019/#0201p06","text":"3 hypothesis subset of TDP-43 proteinopathy develop HS TDP-43 associated independently w dementia pathogenetic mechanisms associted w/ ADNC also associate w/ TDP-43 proteinopathy Data gathered in large autopsy series have been analysed to test hypotheses about progression of LATE. Multivariable regression-based assessment can be used to generate models to test whether cross-sectional data align with proposed sequential pathways of neuropathological changes. Results of one such pathway analysis, from the Rush University community-based autopsy studies, are shown in Fig. 2. These analyses were performed as described previously ( Power et al., 2018 ) and the findings are compatible with at least three hypotheses: (i) a subset of cases with TDP-43 proteinopathy develop hippocampal sclerosis caused or exacerbated by overlapping process(s) that promoted the TDP-43 proteinopathy, or directly by the TDP-43 proteinopathy itself; (ii) TDP-43 proteinopathy is associated independently with dementia, even in cases lacking hippocampal sclerosis; and (iii) pathogenetic mechanisms associated with ADNC (in Fig. 2, data are provided on neuritic amyloid-\u03b2 plaques) are also associated with increased TDP-43 proteinopathy. Current rodent models of TDP-43 proteinopathy with hippocampal sclerosis-like pathology are few ([Ke et al., 2015][2015_Ke]). TDP-43 proteinopathy was shown to be transmissible in mouse models similar to pathological tau and amyloid-\u03b2 from Alzheimer\u2019s disease brains ([Porta et al., 2018][2018_Porta]), but the published TDP-43 models are thought to be more directly relevant to FTLD-TDP than LATE. For now, the lack of adequate longitudinal biomarker data and the limitations of current animal models hamper our study of disease mechanism(s) and further investigations are needed.","title":"02.01.P06"},{"location":"190507_NelsonPeterT2019/#figure_2","text":"Statistical analyses on data related to LATE from the Rush University community-based autopsy cohort depicting the results of pathway analyses. Data were analysed from research volunteers (total n = 1309) in two clinical\u2010pathological studies of ageing from Rush University as described previously (Power et al., 2018). In this sample, the mean age of death was 89.7 years [standard deviation (SD) 6.5 years, range 65\u2013108 years]. These analyses incorporated age, density of amyloid-\u03b2 neuritic amyloid plaques (to factor in ADNC), TDP-43 proteinopathy, hippocampal sclerosis pathology, and the endpoint of Alzheimer\u2019s-type clinical dementia. The components of the pathway analyses most strongly associated with LATE-NC are shown in red. The numbers are path coefficients with standard error in parentheses (shown in purple). These numbers help to quantify the effects of individual pathways. For instance, the data are compatible with there being two pathways from TDP-43 proteinopathy to dementia, one direct pathway (TDP-43 proteinopathy\u2192dementia) and the other indirect pathway that includes hippocampal sclerosis pathology (TDP-43 proteinopathy\u2192hippocampal sclerosis\u2192dementia): in the statistical model, the TDP-43 proteinopathy is independently associated with both hippocampal sclerosis pathology and clinical dementia status. Further, the data indicate that a subset of TDP-43 proteinopathy is \u2018downstream\u2019 of ADNC-type neuritic amyloid plaque pathology. In a practical sense, this means that brains with more neuritic amyloid plaques are more likely to have TDP-43 proteinopathy, with all other known factors being the same. A\u03b2 = amyloid-\u03b2.","title":"Figure 2"},{"location":"190507_NelsonPeterT2019/#0202_late_mri_studies","text":"","title":"02.02. LATE MRI studies"},{"location":"190507_NelsonPeterT2019/#0202p01","text":"MRI studies have provided a complementary window into brain changes in LATE, highlighting brain atrophy both within and outside of the medial temporal lobes of brains with autopsy-verified LATE-NC. Prior studies featured research volunteers who underwent MRI with autopsy follow-up. Several of these studies focused on the subset of cases with hippocampal sclerosis (i.e. presumed severe LATE-NC), therefore, most of the published data were lacking information about less severely affected cases. With that caveat in mind, a common finding in MRI studies is that hippocampal atrophy is greater in cases with LATE-NC than in those with pure Alzheimer\u2019s disease ([Jagust et al., 2008][2008_Jagust]; [Josephs et al., 2008][2008_Josephs], [2017a][2017_Josephs]; [Dawe et al., 2011][2011_Dawe]; [Kaur et al., 2014][2014_Kaur]; [Dallaire-Theroux et al., 2017][2017_Dallaire-Theroux]; [Hanko et al., 2019][2019_Hanko]). [Barkhof et al. (2007)][2007_Barkhof] found that many subjects with medial temporal atrophy lacked primary underlying ADNC. In this study cohort, the sensitivity and specificity of severe atrophy for ADNC was 63% and 69%, respectively, consistent with prior findings ([Jack et al., 2002][2002_Jack]). [Josephs et al. (2008)][2008_Josephs] reported that subjects with neuropathology consistent with LATE-NC tended to be older, with more cognitive impairment, and with more pronounced hippocampal atrophy than TDP-43\u2212 subjects. [Zarow et al. (2011)][2011_Zarow] also described atrophy and deformation of the hippocampus considerably greater in those with hippocampal sclerosis and LATE-NC than in those with only ADNC ([Zarow et al., 2011][2011_Zarow]). In hippocampal sclerosis associated with LATE-NC, hippocampal atrophy was often asymmetric, and it tended to progress in a rostral-caudal gradient in the hippocampus. Using post-mortem MRI, [Dawe et al. (2011)][2011_Dawe] reported stronger correlations between hippocampal atrophy and LATE-NC (with hippocampal sclerosis pathology) than between hippocampal atrophy and ADNC, and subjects with both ADNC and LATE-NC had greater hippocampal atrophy than those with only ADNC. A recent study found that the volume and shape of the amygdala is associated with underlying LATE-NC and that these structural changes are indicative of cognitive decline beyond what can be explained with other pathological indices ([Makkinejad et al., 2019][2019_Makkinejad]).","title":"02.02.P01"},{"location":"190507_NelsonPeterT2019/#0202p02","text":"brain atrophy med tem l inf fro ant tem ins correponds to TDP-43 proteinopathy pattern Post-mortem MRI research has also provided strong evidence that LATE-NC is associated with substantial brain atrophy outside the medial temporal lobes ([Kotrotsou et al., 2015][2015_Kotrotsou]). Figure 3A shows updated data from the Rush University autopsy cohort. After controlling for demographics, ADNC and other age-related pathologies, LATE-NC was related to not only the mesial temporal lobe atrophy, but also to atrophy in the inferior frontal, anterior temporal, and insular cortices. It is noteworthy that this regional atrophy pattern corresponds with the distribution of TDP-43 proteinopathy at autopsy ( Josephs et al., 2016 ; Nag et al., 2018 ) ([Fig. 3B][fig_03]). These data are in agreement with pathological studies of LATE-NC, as well as neuroimaging in subjects with LATE-NC risk genotypes, showing widespread brain involvement ([Neltner et al., 2014][2014_Neltner]; [Cykowski et al., 2016][2016_Cykowski]; Josephs et al., 2016 ; [Nelson et al., 2016a][2016_Nelson]; [Nho et al., 2016][2016_Nho]).","title":"02.02.P02"},{"location":"190507_NelsonPeterT2019/#figure_3","text":"Brain regions that are affected in LATE. (A) Post-mortem MRI with autopsy confirmation allows discrimination of regions of brain atrophy associated with LATE-NC. These data indicate grey matter regions inside and outside of the medial temporal lobe with atrophy in cases with autopsy-confirmed LATE-NC from a community-based autopsy sample. The figure was prepared similarly to the methods used in [Kotrotsou et al. (2015)][2015_Kotrotsou], with some modifications. Cerebral hemispheres from 539 participants of two cohort studies of ageing (Rush Memory and Aging Project and Religious Orders Study) were imaged with MRI ex vivo and also underwent detailed neuropathological characterization. The cortical and subcortical grey matter were segmented into 41 regions. Linear regression was used to investigate the association of regional volumes (normalized by height) with the score of LATE-NC at autopsy (scores: 0 = no TDP-43 inclusions, or inclusions in amygdala only; 1 = TDP-43 inclusions in amygdala as well as entorhinal cortex or hippocampus CA1, and neocortex; 2 = TDP-43 inclusions in amygdala, entorhinal cortex or hippocampus CA1, and neocortex, and hippocampal sclerosis pathology) controlling for amyloid plaques and neurofibrillary tangles, Lewy bodies, gross and microscopic infarcts, atherosclerosis, arteriolosclerosis, cerebral amyloid angiopathy, as well as age, sex, years of education, post-mortem interval to fixation and to imaging, and scanners. Unique colours have been assigned to different model estimates (units: mm2) for grey matter regions with significant negative correlation between their volumes and LATE pathology (P < 0.05, false discovery rate-corrected); darker colours indicate greater brain atrophy in that region. Results are overlaid on both hemispheres of the T1-weighted template of the IIT Human Brain Atlas (v.4.2). Lateral, medial and inferior to superior 3D views of the results are also shown. (B) Classification of LATE-NC according to anatomical region(s) affected by TDP-43 proteinopathy. The present working group recommended a simplified staging scheme for routine assessment of LATE-NC. This requires sampling and TDP-43 immunohistochemical staining of amygdala, hippocampus, and middle frontal gyrus. More detailed TDP-43 immunohistochemical staging schemes that are directly relevant to LATE-NC were previously published by [Josephs et al. (2014a][2014_Josephs], 2016 ) and Nag et al. (2018) . MFG = middle frontal gyrus.","title":"Figure 3"},{"location":"190507_NelsonPeterT2019/#0203_recommendations_for_routine_autopsy_evaluation_and_classification_of_late-nc","text":"","title":"02.03. Recommendations for routine autopsy evaluation and classification of LATE-NC"},{"location":"190507_NelsonPeterT2019/#0203p01","text":"stage 1: amygdala early stage 2: hippocampus pathological change is associated w/ cogni imp stage 3: mid fro g It is recommended that TDP-43 immunohistochemistry be performed as part of the neuropathological evaluation in all older subjects. At a minimum, immunohistochemical staining for TDP-43 is recommended in three brain areas: amygdala, mid-level hippocampus, and middle frontal gyrus. We recommend evaluating these regions as they are commonly obtained at autopsy of aged subjects and capture presumed progression of LATE-NC in the brain. This sampling includes the brain area affected early in the disease course (amygdala, Stage 1), an intermediate stage where the pathological change is robustly associated with cognitive impairment (hippocampus, Stage 2), and a region affected at more advanced stages (middle frontal gyrus, Stage 3) ( Nag et al., 2018 ). Any detected TDP-43 proteinopathy is sufficient to define an anatomical region-based stage: for example, a minute amount of detected TDP-43 proteinopathy in the hippocampus indicates at least Stage 2. We emphasize that the proposed sampling for LATE-NC autopsy screening is a minimal evaluation, whereas more detailed sampling and staging should be considered for specific research settings (Josephs et al., 2014a, 2016; Uchino et al., 2015; Nag et al., 2017, 2018; Zhang et al., 2019). Figure 3B depicts staging schemes for LATE-NC, including sampling recommended for neuropathological evaluation of brain of older adults. This does not address regions that would be assessed in separate TDP-43 pathological staging schemes developed for ALS or FTLD-TDP ([Brettschneider et al., 2013][2013_Brettschneider]; [Fatima et al., 2015][2015_Fatima]; [Tan et al., 2015][2015_Tan]; [Verde et al., 2017][2017_Verde]; [Neumann and Mackenzie, 2019][2019_Mackenzie_Neumann]).","title":"02.03.P01"},{"location":"190507_NelsonPeterT2019/#0203p02","text":"Practical questions arise in relation to diagnostic \u2018boundary zones\u2019 between LATE-NC, FTLD-TDP, and ADNC. While both LATE-NC and FTLD-TDP may affect neocortical areas and may be comorbid with hippocampal sclerosis, LATE-NC usually has a later age of onset, an amnestic dementia, and limbic predominance of pathological change (Nelson et al., 2011b). On the other hand, recommendations for LATE-NC do not stipulate any age cut-offs, because the exact age ranges of disease susceptibility for FTLD-TDP or LATE-NC are not yet fully understood. For prior pathology-based comparisons between subtypes of TDP-43 proteinopathies (not related to age of onset), previous studies should be consulted (Amador-Ortiz et al., 2007a; Tan et al., 2015). More widespread and severe cortical atrophy is typically present in advanced FTLD-TDP than LATE-NC. There may indeed be features that could definitively distinguish LATE-NC cases (histopathologically or molecularly) from subtypes of FTLD-TDP (Arai et al., 2010; Hasegawa et al., 2011; Tsuji et al., 2012; Laferriere et al., 2019); however, more work is needed in this area. For now, definitive criteria to differentiate severe LATE-NC from FTLD-TDP await discovery of specific features that discriminate among various TDP-43 proteinopathies (Tan et al., 2017a). Although LATE-NC and ADNC are recognized by differing neuropathological hallmarks, they may share upstream risk factors and disease mechanisms. Genetic variants predisposing to one protein misfolding disorder may also cause or exacerbate others (see below), and there may be interactions between the misfolded proteins themselves (Trojanowski and Lee, 2000; Higashi et al., 2007; Hu et al., 2008; Uryu et al., 2008; Kadokura et al., 2009; Davis et al., 2017; Spires-Jones et al., 2017; Tan et al., 2017b; Nelson et al., 2018). Brains that harbour ADNC, including some subjects with early-onset familial Alzheimer\u2019s disease or Down syndrome, tend to also contain TDP-43 proteinopathy at rates higher than those lacking ADNC (Ala et al., 2000; Jellinger, 2000; Lippa et al., 2009; Davidson et al., 2011; Zarow et al., 2012). Individual neurons with both tau neurofibrillary tangle pathology and TDP-43 inclusions have been described, particularly in the amygdala, entorhinal cortex, and dentate gyrus of the hippocampus (Amador-Ortiz et al., 2007b; Kadokura et al., 2009; Smith et al., 2017; Robinson et al., 2018c; Josephs et al., 2019). Several published accounts have evaluated the connections between primary age-related tauopathy (PART) and age-related TDP-43 proteinopathy (Josephs et al., 2017b; Smith et al., 2017; Zhang et al., 2019), and TDP-43 proteinopathy has also been described in brains with coexisting argyrophilic grains or glial tauopathy (Fujishiro et al., 2009; Yokota et al., 2010; Arnold et al., 2013; Kertesz et al., 2015). The implications of comorbid amyloid-\u03b2 and various tau pathologies in the context of LATE-NC are still incompletely understood, so further studies are required. There is also evidence that Lewy body disease and TDP-43 proteinopathy may coexist (Nakashima-Yasuda et al., 2007; McAleese et al., 2017; Miki et al., 2018; Trieu et al., 2018). On the other hand, many cases with \u2018end-stage\u2019 ADNC or Lewy body disease lack TDP-43 proteinopathy, so we recommend reporting the presence or absence of LATE-NC as a separate diagnostic entity, even when there are comorbid amyloid-\u03b2, tau and/or \u03b1-synuclein proteinopathies. Additional research is required to guide future consensus-based recommendations in this evolving field. In terms of immunohistochemical reagents used to detect TDP-43 proteinopathy, there is no current consensus that a specific antibody can be recommended. Many neuropathologists use sensitive phospho-TDP-43 antibodies (Hasegawa et al., 2008; Alafuzoff et al., 2015); small aggregates can be readily seen using these reagents. Others use antibodies against non-phosphorylated epitopes, especially for detecting early changes (pathological nuclear to cytoplasmic redistribution) that may precede inclusion body formation (Vatsavayai et al., 2016; Braak et al., 2017; Braak and Del Tredici, 2018; Nana et al., 2019). It is unclear whether the absence of nuclear TDP-43 is reversible, but animal studies using inducible pathogenetic systems would suggest so (Ke et al., 2015). Further, there is some evidence that TDP-43 antigenicity can be vulnerable to fixation artefacts, and epitope retrieval methodology can influence results (Hatanpaa et al., 2008). Additional practice guidelines for studying LATE-NC need formal blinded cross validation studies as has been done for amyloid-\u03b2, tau and \u03b1-synuclein pathological biomarkers. Future studies will be needed to validate and refine systems for staging LATE-NC, and grading local pathological severity, as they relate to clinical and neuroimaging outcomes, especially since at least three staging schemes have been proposed as summarized in Fig. 3B.","title":"02.03.P02"},{"location":"190507_NelsonPeterT2019/#0204_clinical_and_neurocognitive_features_of_late","text":"The clinical course of subjects with autopsy-proven LATE-NC has been characterized as an amnestic cognitive syndrome that can evolve to incorporate multiple cognitive domains and ultimately to impair activities of daily living, i.e. the dementia syndrome (Nelson et al., 2010; Nag et al., 2015; Robinson et al., 2018a, b). The cognitive impairment is greater than can be accounted for by ADNC or other pathologies (Gold et al., 2000; Kawas and Corrada, 2006; Imhof et al., 2007; Giannakopoulos et al., 2008; Nelson et al., 2011b; Kravitz et al., 2012; Boyle et al., 2013; Erten-Lyons et al., 2013). Initial reports on subjects with LATE-NC were focused on subjects with severe pathology (Dickson et al., 1994; Snowdon et al., 1997; Crystal et al., 2000; Vinters et al., 2000; Leverenz et al., 2002; Kuslansky et al., 2004; Zarow et al., 2005, 2008; Attems and Jellinger, 2006; Chui et al., 2006; Leverenz and Lipton, 2008), which helped to show that LATE-NC can be associated with dementia. More recent autopsy series, with both large sample sizes and broad ranges of clinical and pathological findings, have enabled statistical approaches to model the likely relative impact of each disease type. With these methods, LATE-NC was associated with substantial cognitive impairment that was independent of other coexisting pathologies (Nelson et al., 2010; Keage et al., 2014; Murray et al., 2014; Josephs et al., 2015; Nag et al., 2017). Table 1 shows primary data on the relationship between LATE-NC (stratified by the recommended three-stage system) and cognition. The neurological features associated with LATE-NC were different from the behavioural or aphasic clinical syndromes seen in FTLD-TDP cases (Nelson et al., 2011b; Jung et al., 2014; Wilson et al., 2019). While TDP-43 proteinopathy has been documented in some cognitively unimpaired subjects (Arnold et al., 2013; Keage et al., 2014; Uchino et al., 2015; Elobeid et al., 2016; Nascimento et al., 2016; Nag et al., 2018; Nascimento et al., 2018), it is likely that this represents preclinical disease in subjects dying before onset of clinical symptoms; such clinical resilience to pathological changes has been described in many disorders (Perkins et al., 2003; Shojania et al., 2003; Roulson et al., 2005; Latimer et al., 2017; Robinson et al., 2018b).","title":"02.04. Clinical and neurocognitive features of LATE"},{"location":"190507_NelsonPeterT2019/#table_1","text":"Characteristics 0 1 2 3 P-value n 666 263 258 189 - Age at death (SD) 87.9 (6.8) 89.9 (6.2) 91.8 (5.6) 91.9 (5.4) <0.001 % Female 65.3 67.7 74 72.5 0.040 Clinical diagnosis <0.001 % Normal 41.8 33.5 18.9 7.6 % MCI or dementia 58.2 66.5 81.1 92.4 % with comorbid HS pathology 1.7 3.5 13.6 42.9 <0.001 Cognitive function tests proximate to death, mean (SD) MMSE score 22.8 (8.1) 21.2 (8.9) 18.2 (9.8) 14.0 (10.0) <0.001 Episodic memory score \u22120.60 (1.28) \u22120.76 (1.31) \u22121.36 (1.34) \u22122.06 (1.23) <0.001","title":"Table 1"},{"location":"190507_NelsonPeterT2019/#0205_public_health_impact_of_late","text":"The public health impact of LATE is likely to be quite significant. Two basic study design elements that influence recognition of LATE-NC in autopsy cohorts are the age range in the cohort, and the date of the study. Researchers were unaware of TDP-43 proteinopathy prior to 2006, so studies prior to this time could not assess the specific impact of LATE. LATE-NC is mostly seen in the oldest-old, whereas in early clinical-pathological correlation studies of dementia (Roth et al., 1966; Blessed et al., 1968), the research subjects had died in their early 70s. LATE-NC needs to be assessed in population studies that include all age ranges. More recent clinical studies have demonstrated biomarker evidence of \u2018suspected non-Alzheimer\u2019s disease pathophysiology\u2019 (SNAP) causing amnestic type cognitive impairment with substantial hippocampal atrophy but lacking detectable amyloid-\u03b2 amyloidosis (Caroli et al., 2015; Burnham et al., 2016; Jack et al., 2016, 2017; Abner et al., 2017; Wisse et al., 2018). For example, the evaluation of 1535 participants in the Mayo Clinic Study of Aging showed significantly greater prevalence of SNAP compared with preclinical Alzheimer\u2019s disease, and multimorbidity was increased in SNAP (odds ratio 2.16) (Vassilaki et al., 2018). LATE is probably an important contributor in this group of subjects (see below). Among subjects autopsied past 80 years of age, most studies indicate that >20% of brains had pathological features consistent with LATE-NC (Fig. 4). It is noteworthy that the majority of these cases had additional comorbid pathologies, so the measured clinical-pathological correlation (relative contribution of each pathology to cognitive impairment) depends on how the investigators defined diagnostic thresholds and cut-points. The frequency of LATE-NC in autopsy series have varied, ranging from 5% to 50% of brains that were evaluated using TDP-43 immunohistochemistry, approximately twice the frequencies that were detected in prior studies that could only assess hippocampal sclerosis pathology (Leverenz et al., 2002; Lippa and Dickson, 2004; Arai et al., 2009; Nelson et al., 2011b; Rauramaa et al., 2011; Tremblay et al., 2011; Corrada et al., 2012; Zarow et al., 2012; Malek-Ahmadi et al., 2013; Keage et al., 2014; Jellinger and Attems, 2015; Uchino et al., 2015; Takao et al., 2016; Latimer et al., 2017; McAleese et al., 2017; Hokkanen et al., 2018; Kero et al., 2018; Robinson et al., 2018a). Differences in study design, including the application of various criteria for defining pathological abnormalities, pathological methods, recruitment strategy, and cohort demographics, all contribute to the variability in the reported frequency of LATE-NC.","title":"02.05. Public health impact of LATE"},{"location":"190507_NelsonPeterT2019/#figure_4","text":"Different neurodegenerative disease conditions stratified by age: LATE-NC, severe ADNC, and FTD. FTD/FTLD cases were not present in data shown in A\u2013D. Note that published studies to estimate disease prevalence for the various diseases have used importantly different study designs\u2014thus, E is a clinical (no autopsy) study because population-based autopsy cohorts lack substantial numbers of FTD/FTLD cases. (A and B) Data from a community-based autopsy cohort\u2014the Rush University ROS-MAP cohort (overall n = 1376). The TDP-43 pathology is operationalized using standard methods as described previously (Nag et al., 2018) and then the current paper\u2019s suggested simplified staging system was applied; sample sizes for each age group (in years) are: <75 (n = 34); 75\u201380 (n = 82); 80\u201385 (n = 192); 85\u201390 (n = 375); 90\u201395 (n = 407); 95\u2013100 (n = 222); and >100 (n = 64). Note that in this community-based sample, the proportion of cases with advanced ADNC is <50% in all age groups. (C and D) Data from the National Alzheimer\u2019s Coordinating Center (NACC), which derives from 27 different research centres, as described previously (Besser et al., 2018; Katsumata et al., 2018). Overall sample size is n = 806, stratified thus by age groups (in years): <75 (n = 155); 75\u201380 (n = 118); 80\u201385 (n = 165); 85\u201390 (n = 170); 90\u201395 (n = 122); 95\u2013100 (n = 57); and >100 (n = 19). The NACC research subjects were more likely to come to autopsy after being followed in dementia clinics, and the sample includes a higher percentage of subjects with severe ADNC. The percentage of subjects with LATE-NC is still >20% in each age group. Note that in both the community-based cohort (A and B) and clinic-based cohort (C and D), the proportion of subjects with severe ADNC decreased in advanced old age, while in the same cases the proportion of subjects with LATE-NC increased. (E) Epidemiological data on FTD syndromes for comparison to LATE. Data are provided about crude prevalence rates for FTD syndromes that have been associated with FTLD-TDP. Several of these clinical syndromes are likely to have considerable numbers of cases with FTLD-tau (bvFTD and nfvPPA) or ADNC (other PPA) rather than FTLD-TDP, so the actual prevalence of FTLD-TDP pathology is probably lower than this data suggests. Note that the clinical syndromes associated with FTLD-TDP have a prevalence that are several orders of magnitude lower than LATE-NC. These data, described in detail previously (Coyle-Gilchrist et al., 2016), derive from multisource referral over 2 years, which identified all diagnosed or suspected cases of FTD subtypes in two UK counties comprising the PiPPIN (Pick\u2019s Disease and Progressive Supranuclear Palsy: Prevalence and Incidence) catchment area in the East of England. Two cities in the PiPPIN catchment area were Norfolk and Cambridge. Diagnostic confirmation used current consensus diagnostic criteria after interview and re-examination. Total sample size was n = 986 483 subjects. Shown are crude prevalence rates for the major FTLD-TDP associated syndromes by age and syndrome. bvFTD = behavioural variant frontotemporal dementia; nfvPPA = non-fluent agrammatic variant primary progressive aphasia; svPPA = semantic variant PPA. Note that subjects between ages 55 and 80 are at greatest risk for FTD, and, the FTLD-TDP associated FTD syndrome prevalence is <30 per 100 000 (E), in sharp contrast to the data shown in A\u2013D.","title":"Figure 4"},{"location":"190507_NelsonPeterT2019/#table_2","text":"Neuropathological indices Fraction attributable % (95% CI) [a] Alzheimer\u2019s disease (ADNC) 39.4 (31.5\u201347.4) Vascular disease pathology [b] 24.8 (17.3\u201332.1) LATE-NC 17.3 (13.1\u201322.0) \u03b1-Synucleinopathy/Lewy body pathology 11.9 (8.4\u201315.6)","title":"Table 2"},{"location":"190507_NelsonPeterT2019/#0206_genetics_of_late","text":"Genetic studies provide insights into disease-related mechanisms and, potentially, future therapeutic targets. The following five genes (in the chronological order in which they were identified) have been reported to harbour risk alleles associated with pathological manifestations we refer to as LATE-NC: granulin (GRN) on chromosome 17q, transmembrane protein 106B (TMEM106B) on chromosome 7p, ATP-binding cassette sub-family member 9 (ABCC9) on chromosome 12p, potassium channel subfamily M regulatory beta subunit 2 (KCNMB2) on chromosome 3q, and apolipoprotein E (APOE) on chromosome 19q (Dickson et al., 2010; Pao et al., 2011; Beecham et al., 2014; Murray et al., 2014; Nelson et al., 2014, 2015b; Aoki et al., 2015; Katsumata et al., 2017; Yang et al., 2018). See Supplementary Table 1 for summary information on these genes and their associated phenotypes. For this discussion, we include the endophenotype that was used in the published research (usually hippocampal sclerosis) rather than LATE-NC. Gene variants in GRN and TMEM106B were shown to be associated with hippocampal sclerosis and TDP-43 proteinopathy risk using allele tests, based on the known relationship of those two genes to FTLD-TDP (Baker et al., 2006; Boeve et al., 2006; Cruts et al., 2006; Van Deerlin et al., 2010). These gene variants have now been most consistently associated with risk of LATE-NC. For the association between the GRN and hippocampal sclerosis, Dickson et al. showed that hippocampal sclerosis in aged subjects was associated with the T-allele of the GRN single nucleotide polymorphism (SNP) rs5848 (Dickson et al., 2010; Murray et al., 2014). Aoki and colleagues reported that the frequency of the C-allele of TMEM106B rs1990622 in hippocampal sclerosis was lower than that in non-hippocampal sclerosis controls (Aoki et al., 2015). Following the initial studies, the findings were replicated of increased risk for hippocampal sclerosis associated with each copy of the T-allele of TMEM106B rs1990622 (Nelson et al., 2014, 2015b; Dickson et al., 2015; Yu et al., 2015). Since GRN and TMEM106B were both implicated in FTLD-TDP, their strong association with LATE-NC provides compelling evidence for pathogenetic overlap between FTLD-TDP and LATE. From a mechanistic perspective, the cognate proteins for these genes have been shown to play important roles in endosomal/lysosomal biology, and there is experimental evidence for interaction of these gene products (Chen-Plotkin et al., 2012; Nicholson and Rademakers, 2016; Klein et al., 2017; Zhou et al., 2017; Paushter et al., 2018). The TMEM106B gene appears to be pleiotropic for multiple diseases (Gallagher et al., 2014; Ou et al., 2015; Hsiao et al., 2017; Cherry et al., 2018; Chornenkyy et al., 2019), and the LATE-NC risk allele in TMEM106B may influence healthy brain ageing (Rhinn and Abeliovich, 2017; Ren et al., 2018). Separate studies have found that GRN gene products (granulins) play roles in inflammation and wound repair (Ahmed et al., 2007; Miller et al., 2013). Notably, the GRN risk variant rs5848 has been associated with increased inflammatory mediators in CSF (e.g. AXL and CLU) (Fardo et al., 2017). More work is required to enable better understanding of how molecular pathways relevant to FTLD-TDP are involved in LATE. An important recent finding by several different groups is that the APOE \u025b4 allele, which is a risk factor for ADNC and Lewy body disease, is also associated with increased risk for TDP-43 proteinopathy in the elderly (Robinson et al., 2018c; Wennberg et al., 2018; Yang et al., 2018). Other studies did not find an association between APOE genotypes and risk for hippocampal sclerosis (Troncoso et al., 1996; Leverenz et al., 2002; Nelson et al., 2011b; Pao et al., 2011; Brenowitz et al., 2014; Hall et al., 2019; but see Farfel et al., 2016). Few subjects with the APOE \u025b4 allele survive into advanced old age without any amyloid-\u03b2 plaques (Saunders et al., 1993; Schmechel et al., 1993), and it remains to be seen exactly how the APOE \u025b4 protein influences TDP-43 proteinopathy. Nevertheless, recent studies from large research cohorts have provided additional insights into the presence of pathogenetic mechanisms that are shared between neurodegenerative diseases. Since the presence or absence of risk alleles in TMEM106B, GRN, and APOE cannot by themselves or in combination confidently predict the risk for LATE-NC in a given subjects (Katsumata et al., 2017; Nelson et al., 2019), there must be other factors that influence the disease phenotype. The connections of the ABCC9 and KCNMB2 genes with risk of LATE-NC were discovered via genome-wide association studies (GWAS), which are neither helped nor biased by prior mechanistic hypotheses. The finding of the associations between ABCC9 gene variants and LATE-NC (Nelson et al., 2015b), and brain atrophy detected with MRI (Nho et al., 2016), were reported in separate samples from the initial GWAS (Nelson et al., 2014). Neither ABCC9 nor KCNMB2 gene variants were associated with LATE-NC in cohorts other than those described above. ABCC9 and KCNMB2 are genes coding for proteins that serve to regulate potassium channels (Zarei et al., 2007; Nelson et al., 2015a). The ABCC9 risk genotype also implicates thyroid hormone dysregulation in LATE-NC; the locus was found to be associated with altered brain expression of genes induced by thyroid hormone (Nelson et al., 2016a). Thyroid hormones have been found to be dysregulated in subjects with autopsy-confirmed LATE-NC in recent studies (Trieu et al., 2018; Nelson et al., 2019), and high thyrotropin was associated with reduced hippocampal volume in a population-based study (Ittermann et al., 2018). A gene variant near ABCC9, which lies within both the SLCO1A2 and IAPP genes, was also found in a GWAS study to be associated with neurodegeneration disproportional to amyloid-\u03b2 accumulation (Roostaei et al., 2016), which may indicate LATE in those cases. The KCNMB2 gene has been associated with suicidal ideation in US military veterans (Kimbrel et al., 2018) and may be related to depression, which is common in the elderly. Further, when KCNMB2 is overexpressed in the hippocampus of mice, it rescues memory deficits (Yu et al., 2018). More work is required to enable better understanding and identification of the molecular pathways involved in LATE. Prior genetic studies on TDP-43 proteinopathy and hippocampal sclerosis have varied in important ways, including patient inclusion/exclusion criteria, disease definitions, and age composition, which may explain their differing findings with regard to genotype/phenotype associations. The prospects for successful future genetic discoveries will be improved by the development of specific and standardized LATE-NC endophenotypes. FTLD-TDP provides an example in which pathological subtyping of patients has been beneficial for genetic correlation studies: there are, for example, strong associations between TMEM106B and GRN gene variants with FTLD-TDP type A pathology (Rademakers et al., 2008; Aoki et al., 2015). Preliminary studies suggest that distinguishing morphology of TDP-43 pathology in LATE-NC may also be relevant to genetic risk (Josephs et al., 2019). We speculate that genetic profiling may eventually become a key consideration for recruitment to clinical trials, and possible future precision medicine approaches, since some genotypes may be differentially responsive to specific interventions.","title":"02.06. Genetics of LATE"},{"location":"190507_NelsonPeterT2019/#0207_late_biomarkers","text":"Optimal biomarkers for LATE, including biofluids or PET ligands, would be specific for the disease-defining feature, namely TDP-43 proteinopathy (Steinacker et al., 2018). At this time, no biofluid or PET biomarker satisfies this essential criterion of molecular specificity. Nor do PET ligands for LATE seem to be on the near-term horizon. The problems of intracellular location and small pathological burden of TDP-43 proteinopathy are obstacles that limit signal-to-noise ratio for biomarkers. The NIA-AA Research Framework group recommended a system for classifying subjects based on amyloid-\u03b2 amyloid (A), tau (T) and neurodegeneration/neuronal injury (N) biomarkers, which is termed AT(N) (Burnham et al., 2016; Jack et al., 2016). Each biomarker category can be binarized as positive (+) or negative (\u2212) resulting in eight possible biomarker profiles. Certain AT(N) profiles indicate increased likelihood that LATE-NC might be present. The \u2018N\u2019 in AT(N) is in parentheses to indicate that it represents cumulative brain injury/neurodegeneration from all aetiologies and is not specific for any one aetiology. An assumption is that in Alzheimer\u2019s disease, neurodegeneration is associated with tauopathy, and therefore in an A+T\u2212(N)+ subject, the N+ is likely due to a comorbid non-Alzheimer\u2019s disease pathophysiological process(es). If (N)+ is ascertained by an imaging measure that captures neurodegeneration as reflected medial temporal atrophy or hypometabolism, then this implicates LATE (often with hippocampal sclerosis) as a likely non-Alzheimer\u2019s disease comorbidity. Similar logic applies to subjects with an A\u2212T\u2212(N)+ profile, the N+ is presumably due to a non-Alzheimer\u2019s disease pathological process(es), and if the (N)+ measure is hippocampal atrophy, or medial temporal hypometabolism, then LATE is implicated (Fig. 5).","title":"02.07. LATE biomarkers"},{"location":"190507_NelsonPeterT2019/#figure_5","text":"Biomarkers are currently not specific to LATE-NC. (A) Radiological scans from an 86-year-old female who suffered amnestic cognitive impairment compatible with \u2018Probable Alzheimer\u2019s disease\u2019 diagnosis. However, the amyloid-\u03b2 PET scan was negative, tau PET scan was also negative, and the MRI showed appreciable atrophy of the medial temporal lobes bilaterally. This combination is considered \u2018A\u2212T\u2212N+\u2019 and was diagnosed during life as \u2018suspected non-Alzheimer\u2019s pathology\u2019 (SNAP). Autopsy within a year of the brain scans confirmed the presence of TDP-43 pathology and hippocampal sclerosis, which now is diagnosable as LATE-NC. (B) Another common biomarker combination, in the brain of a 91-year-old male with dementia. In this subject, the amyloid PET scan was positive, yet the tau PET scan was negative. The MRI again showed atrophy of the medial temporal lobes. The combination of pathologies\u2014in this case presumed early ADNC and comorbid LATE-NC\u2014is common, especially in the brains of subjects in advanced age.","title":"Figure 5"},{"location":"190507_NelsonPeterT2019/#0208_implications_for_alzheimers_disease_and_late_clinical_trials","text":"Formalization of LATE diagnostic criteria and increased awareness of this disease should help guide the design and interpretation of Alzheimer\u2019s disease clinical trials. Comorbid ADNC and LATE-NC becomes increasingly more prevalent with advancing age, and the mechanisms underlying each of these common lesions have independent effects on cognitive performance (Nelson et al., 2010). LATE-NC, when coexisting with ADNC, will have the potential to obscure the effects of a potential disease modifying agent on cognitive assessment results in living subjects. The primary outcome measures in disease-modifying Alzheimer\u2019s disease clinical trials will remain cognitive or functional scales for the foreseeable future (Cummings et al., 2016; Register, 2018). Thus, the presence of LATE-NC will complicate interpretation of Alzheimer\u2019s disease-specific treatment effects that are inferred from observed cognitive outcomes. Until there are biomarkers for LATE, clinical trials should be powered to account for TDP-43 proteinopathy. LATE is among the common age-related diseases that can mimic the amnestic presentation of Alzheimer\u2019s disease (Nelson et al., 2013), and it is one of many reasons why biological rather than clinical disease definitions are important in the era of disease modifying clinical trials (Jack et al., 2018). Biomarkers have roles for both inclusion and exclusion. It will be important, at recruitment of subjects into future disease-modifying Alzheimer\u2019s disease clinical trials, to stratify according to major known predictors, including clinical features, genetics, and known biomarkers. This stratification will enable enrichment for subjects on the ADNC continuum (Sevigny et al., 2016) while excluding subjects likely to have high risk for LATE-NC (Botha et al., 2018). Even with best efforts at baseline, the multiplicity of diseases that occur in brains of older subjects will still require analyses according to subgroups. This is another reason why clinical trials in dementing diseases of ageing will require large sample sizes. Research into Alzheimer\u2019s disease has provided additional topical caveats (Gulisano et al., 2018; Hunter et al., 2018; Morris et al., 2018). For example, there is a danger that we fundamentally misunderstand the nature and complexity of processes related to TDP-43 proteinopathy, and this could lead to significant biases in the ways that we approach clinical diagnosis and clinical trials of LATE. For now, as with Alzheimer\u2019s disease, the misfolded proteins provide a disease marker and a potential target for therapies. Clinical trials directed at preventing or treating LATE, in isolation or in concert with other brain diseases, should be a major direction for future research. Performing such trials optimally will first require development of a specific LATE biomarker. For now, five alternative, but not mutually exclusive, approaches exist for developing disease-modifying therapies: (i) focus on pathways and gene products such as APOE \u025b4 that seem to be in common between Alzheimer\u2019s disease, Lewy body disease, and LATE; (ii) focus on pathways and gene products such as TMEM106B and GRN that are shared between FTLD-TDP and LATE; (iii) focus on pathways and gene products such as ABCC9 and KCNMB2 that have been implicated by GWAS; (iv) focus on potential research subjects with the A\u2212T\u2212(N+) biomarker profile, who are now excluded from many Alzheimer\u2019s disease-related clinical trials; and/or (v) focus on strategies to eliminate TDP-43 aggregates or to prevent the formation of these aggregates.","title":"02.08. Implications for Alzheimer\u2019s disease and LATE clinical trials"},{"location":"190507_NelsonPeterT2019/#03_conclusions_and_future_directions","text":"A key goal of this working group effort was to catalyse future research on LATE, an under-recognized condition that affects many older subjects. It is important to promote awareness in multiple scientific areas and to focus on translational and interdisciplinary approaches. Development of specific LATE biomarker(s) should be a high scientific priority. While a sensitive and specific biomarker using neuroimaging or biofluids would be ideal, other disease markers could capitalize on existing metrics such as the AT(N) research guidelines with or without imaging or biofluid risk profiling. Developing biomarkers or other criteria to identify subjects with LATE would augment observational studies that seek to unravel the natural history of LATE, and its coevolution with other ageing-related diseases. With sufficient longitudinal observations, cause and effect inferences may become possible, and clinical trials implemented. Further pathology studies will also be necessary. The consensus pathological classification scheme that we propose should be considered preliminary because much remains to be learned about LATE. The application of pathological subtyping has been useful in the context of FTLD-TDP (Lee et al., 2017; Mackenzie and Neumann, 2017; Pottier et al., 2018), and pathological subtyping may help refine LATE-NC endophenotypes for diagnostic and genetic studies (Josephs et al., 2019). At this point, there is no consensus about how or whether to apply such criteria for LATE-NC. A detailed characterization of the molecular pathology of TDP-43 is required for different cell types across brain regions in large population-representative samples. This should include characterization of various phosphorylation states, cleavage fragments, and other post-translational modifications of TDP-43. Further, each anti-TDP-43 antibody used should be assessed for potential cross-reactivity with other proteins or LATE-NC features in situ. It will also be important to determine the prevalence of all co-pathologies associated with LATE-NC, the impact of the molecular conformations and modifications of TDP-43, the cellular types involved, and the natural history of the disease. These advances will also assist in developing animal models. Additional epidemiological, clinical, neuroimaging, and genetic studies will be important to better characterize the public health impact and clinical phenotypes for LATE. Further, LATE must be studied in more diverse populations and cohorts. Careful clinical assessments over time and into the oldest age groups is essential, along with detailed biological measures and autopsy, so that the complexity of ageing changes can be assessed (Brayne, 1993). In vivo and ex vivo imaging studies to determine the focal and more diffuse changes in the brains of subjects with LATE will also be important. Future studies may generate better insights into the clinical indices and cognitive features that are associated with increased probability of LATE-NC. Risk factors, protective influences, and other correlates could thus be identified to help prevent or predict LATE. For example, autoimmune disease may play a role in TDP-43 proteinopathy and LATE-NC in particular (Miller et al., 2013; Trieu et al., 2018). Optimally, future studies will complement traditional GWAS and gene-focused analyses with multi-omics studies to capture a greater appreciation of the complex mechanisms and diagnostic or therapeutic opportunities in the study of LATE. Animal models and basic science research into LATE are imperative, with the caveat that the aged human brain is challenging to model accurately. Functional studies, including transmission animal models that use TDP-43 fibrils (Porta et al., 2018) or extracts from brains with LATE-NC injected into animals or cell cultures (Laferriere et al., 2019), can be combined with genetic studies to test hypotheses and to add statistical power for preclinical and hypothesis-testing experiments. Molecular studies that focus on TDP-43 and the upstream triggers and downstream molecular consequences are necessary to elucidate mechanisms of disease. Models that account for co-pathologies are rare at present, but have the potential to be highly informative. Ultimately, it is hoped that these collective research efforts will one day result in successful preventative and therapeutic strategies.","title":"03. Conclusions and future directions"},{"location":"190507_NelsonPeterT2019/#references","text":"","title":"References"},{"location":"190517_Avena-KoenigsbergerA_2019/","text":"19-05-17 A spectrum of routing strategies for brain networks \u00b6 Original | Mendeley ToC \u00b6 00. Abstract 01. Introduction 01.01. A continuous spectrum of routing strategies combining local and global information 01.02. The cost of reshaping the system\u2019s dynamics 01.03. Brain networks are more efficient within an intermediate region of the communication spectrum 01.04. Source vs. target communication cost 01.05. Routing strategies for privileged nodes 01.06. A communication cost trade-off within subjects 02. Discussion 03. Matrials and methods 04. Supporting infromation 00. Abstract \u00b6 01. Introduction \u00b6 01.01. A continuous spectrum of routing strategies combining local and global information \u00b6 01.02. The cost of reshaping the system\u2019s dynamics \u00b6 01.03. Brain networks are more efficient within an intermediate region of the communication spectrum \u00b6 01.04. Source vs. target communication cost \u00b6 01.05. Routing strategies for privileged nodes \u00b6 01.06. A communication cost trade-off within subjects \u00b6 02. Discussion \u00b6 03. Matrials and methods \u00b6 04. Supporting infromation \u00b6 References \u00b6 \u00b6 img{width: 51%; float: right;}","title":"190517 Avena-Koenigsberger, A., 2019"},{"location":"190517_Avena-KoenigsbergerA_2019/#toc","text":"00. Abstract 01. Introduction 01.01. A continuous spectrum of routing strategies combining local and global information 01.02. The cost of reshaping the system\u2019s dynamics 01.03. Brain networks are more efficient within an intermediate region of the communication spectrum 01.04. Source vs. target communication cost 01.05. Routing strategies for privileged nodes 01.06. A communication cost trade-off within subjects 02. Discussion 03. Matrials and methods 04. Supporting infromation","title":"ToC"},{"location":"190517_Avena-KoenigsbergerA_2019/#00_abstract","text":"","title":"00. Abstract"},{"location":"190517_Avena-KoenigsbergerA_2019/#01_introduction","text":"","title":"01. Introduction"},{"location":"190517_Avena-KoenigsbergerA_2019/#0101_a_continuous_spectrum_of_routing_strategies_combining_local_and_global_information","text":"","title":"01.01. A continuous spectrum of routing strategies combining local and global information"},{"location":"190517_Avena-KoenigsbergerA_2019/#0102_the_cost_of_reshaping_the_systems_dynamics","text":"","title":"01.02. The cost of reshaping the system\u2019s dynamics"},{"location":"190517_Avena-KoenigsbergerA_2019/#0103_brain_networks_are_more_efficient_within_an_intermediate_region_of_the_communication_spectrum","text":"","title":"01.03. Brain networks are more efficient within an intermediate region of the communication spectrum"},{"location":"190517_Avena-KoenigsbergerA_2019/#0104_source_vs_target_communication_cost","text":"","title":"01.04. Source vs. target communication cost"},{"location":"190517_Avena-KoenigsbergerA_2019/#0105_routing_strategies_for_privileged_nodes","text":"","title":"01.05. Routing strategies for privileged nodes"},{"location":"190517_Avena-KoenigsbergerA_2019/#0106_a_communication_cost_trade-off_within_subjects","text":"","title":"01.06. A communication cost trade-off within subjects"},{"location":"190517_Avena-KoenigsbergerA_2019/#02_discussion","text":"","title":"02. Discussion"},{"location":"190517_Avena-KoenigsbergerA_2019/#03_matrials_and_methods","text":"","title":"03. Matrials and methods"},{"location":"190517_Avena-KoenigsbergerA_2019/#04_supporting_infromation","text":"","title":"04. Supporting infromation"},{"location":"190517_Avena-KoenigsbergerA_2019/#references","text":"","title":"References"},{"location":"190517_LynnChristopherW_2019/","text":"19-05-17 The physics of brain network structure, function and control \u00b6 Lynn, C. W., & Bassett, D. S. (2019). The physics of brain network structure, function and control. Nature Reviews Physics, 1. @article{lynn2019physics, title={The physics of brain network structure, function and control}, author={Lynn, Christopher W and Bassett, Danielle S}, journal={Nature Reviews Physics}, pages={1}, year={2019}, publisher={Nature Publishing Group} } Original | Arxiv | Mendeley ToC \u00b6 00. Abstract 01. Introduction 02. The physics of brain network structure Box 1 02.01. Measureing brain netrork structure 02.02. Modeling brain network structure 02.02.01. Random structure 02.02.02. Community structure 02.02.03. Small-world structure 02.02.04. Hub structure 02.02.05. Spatial structure 02.02.06. Competition between structural properties 02.03. The future of brain network structure Box 2 03. The physics of brain network function 03.01. Measuring brain network function 03.02. Modeling brain network function 03.02.01. Artificial models 03.02.02. Biophysical models 03.03. The future of brain network function Box 3 04. Perteration experiments and the physics of brain network control 04.01. Trageted perturbations and clinical interventions 04.02. Network control in the brain Box 4 04.03. The future of brain network control 05. Conclusions and future directions in the neurophysics of brain networks Summary \u00b6 Further \u00b6 Terminology \u00b6 Original \u00b6 00. Abstract \u00b6 The brain is characterized by heterogeneous patterns of structural connections supporting unparalleled feat s of cognition and a wide range of behaviours. New non-invasive imaging techniques now allow comprehensive mapping of these patterns. However, a fundamental challenge remains to understand how the brain\u2019s structural wiring supports cognitive processes, with major implications for personalized mental health treatments. Here, we review recent efforts to meet this challenge, drawing on physics intuitions, models and theories, spanning the domains of statistical mechanics, information theory, dynamical systems and control. We first describe the organizing principles of brain network architecture instantiate d in structural wiring under constraints of spatial embedding and energy minimization. We then survey models of brain network function that stipulate how neural activity propagates along structural connections. Finally, we discuss perturbative experiments and models for brain network control; these use the physics of signal transmission along structural connections to infer intrinsic control processes that support goal-directed behaviour and to inform stimulation-based therapies for neurological and psychiatric disease. Throughout, we highlight open questions that invite the creative efforts of pioneering physicists. 00.01. Key points \u00b6 From the first measurement of the nerve impulse by Hermann von Helmholtz in 1849 to the cutting-edge superconducting devices used in magnetoencephalography, physics and neuroscience have always been inextricably linked. Today, network neuroscience \u2014 the study of the brain as a complex web of interacting components \u2014 draws intuitions and techniques from nearly every branch of physics. The architecture of structural connections between neurons or brain regions is constrained by requirements of energy minimization and efficient information transfer. The materialization of long-range correlations and synchronization from the collective firing of individual neurons conjures notions of emergence and criticality from statistical mechanics. Together, these investigations of brain network structure and function guide targeted treatments for cognitive disorders using theories of network control. Now more than ever, understanding the complexities of the mind lies at the feet of curious and pioneering physicists. 01. Introduction \u00b6 It is our good fortune as physicists to seek to understand the nature of the observable world around us. In this inquiry, we need not reach to contemporary science to appreciate the fact that our perception of the world around us is inextricably linked to the world within us: the mind. Indeed, even Aristotle c. 350 B.C. noted that it is by mapping the structure of the world that the human comes to understand their own mind 1 . \u201cMind thinks itself because it shares the nature of the object of thought; for it becomes an object of thought in coming into contact with and thinking its objects, so that mind and object of thought are the same\u201d 2 . Over the ensuing 2000-plus years, it has not completely escaped notice that the mappers of the world have unique contributions to offer the mapping of the mind (from Thales of Miletus, c. 624\u2013546 B.C., to Leonardo Da Vinci, 1452\u20131519). More recently, it is notable that nearly all famous physicists of the early 20th century \u2013 Albert Einstein, Niels Bohr, Erwin Schroedinger, Werner Heisenberg, Max Born \u2013 considered the philosophical implications of their observations and theories 3 . In the post-war era, philosophical musings turned to particularly conspicuous empirical contributions at the intersection of neuroscience and artificial intelligence, spanning polymath John von Neumann\u2019s work enhancing our understanding of computational architectures 4 and physicist John Hopfield\u2019s invention of the associative neural network, which revolutionized our understanding of collective computation 5 . In the contemporary study of the mind and its fundamental organ \u2013 the brain \u2013 nearly all of the domains of physics, perhaps with the exception of relativity, are not only relevant but truly essential, motivating the early coinage of the term neurophysics some four decades ago 6 . The fundamentals of electricity and magnetism prove critical for building theoretical models of neurons and the transmission of action potentials 7 . These theories are being increasingly informed by mechanics to understand how force-generating and load-bearing proteins bend, curl, kink, buckle, constrict, and stretch to mediate neuronal signaling and plasticity 8 . Principles from thermodynamics come into play when predicting how the brain samples the environment (action) or shifts the distribution of information that it encodes (perception) 9 . Collectively, theories of brain function are either buttress ed or dismantle d by imaging, with common tools including magnetic resonance imaging 10 and magnetoencephalography 11 , the latter being built on superconducting quantum interference devices and next-generation quantum sensors that can be embedded into a system that can be worn like a helmet, revolutionizing our ability to measure brain function while allowing free and natural movement 12 . Moreover, recent developments in nanoscale analysis tools and in the design and synthesis of nanomaterials have generated optical, electrical, and chemical methods to explore brain function by enabling simultaneous measurement and manipulation of the activity of thousands or even millions of neurons 13 . Beyond its relevance for continued imaging advance- ments 14 , optics has come to the fore of neuroscience over the last decade with the development of optogenetics, an approach that uses light to alter neural processing at the level of single spikes and synaptic events, offering reliable, millisecond-timescale control of excitatory and inhibitory synaptic transmission 15 . Such astounding advances, enabled by the intersection of physics and neuroscience, have motivated the construction of a National Brain Observatory at the Argonne National Laboratory (Director: Peter Littlewood, previously of Cavendish Laboratories) funded by the National Sci- ence Foundation, as well as frequent media coverage including titles in the APS News such as \u201cPhysicists, the Brain is Calling You.\u201d16 And as physicists answer the call, our understanding of the brain deepens and our ability to mark and measure its component parts expands. Yet alongside this growing systematization and archivation, we have begun to face an increasing realization that it is the interactions between hundreds or thousands of neurons that generate the mind\u2019s functional states 13. Indeed, from interactions among neural components emerge computation 17, commu- nication 18, and information propagation 19. We can confidently state of neuroscience what Henri Poincare, the French mathematician, theoretical physicist, and philosopher of science, states of sci- ence generally: \u201cThe aim of science is not things themselves, as the dogmatists in their simplicity imagine, but the relations among things; outside these relations there is no reality knowable.\u201d20 The overarching goal of mapping these interactions in neural systems has motivated multibillion-dollar investments across the United States (the Brain Initiative generally, and the Human Connectome Project specifically 21), the European Union (the Blue Brain Project 22), China (the China Brain Project 23), and Japan (Japan\u2019s Brain/MINDS project 24). While it is clear that interactions are paramount, exactly how the functions of the mind arise from these interactions remains one of the fundamental open questions of brain science 25. To the physicist, such a question appears to exist naturally within the purview of statistical mechanics 26, with one major caveat: the interaction patterns observed in the brain are far from regular, such as those observed in crystalline structures, and are also far from random, such as those observed in fully disordered systems 27. Indeed, the observed heterogeneity of interaction patterns in neural systems \u2013 across a range of spatial and temporal scales \u2013 generally limits the utility of basic contin- uum models or mean-field theories, which would otherwise comprise our natural first approaches. Fortunately, similar observations of interaction heterogeneity have been made in other technologi- cal, social, and biological systems, leading to concerted efforts to develop a statistical mechanics of complex networks 28. The resultant area of inquiry includes criteria for building a network model of a complex system 29, statistics to quantify the architecture of that network 30, models to stipulate the dynamics that can occur both in and on a network 31\u201333, and theories of network function and control 34,35. Here, we provide a brief review for the curious physicist, spanning the network-based ap- proaches, statistics, models, and theories that have recently been used to understand the brain. Importantly, the interpretations that can be rationally drawn from all such efforts depend upon the nature of the network representation 29, including its descriptive, explanatory, and predictive valid- ity \u2013 topics that are treated with some philosophical rigor elsewhere 36. Following a simple primer on the nature of network models, we discuss the physics of brain network structure, beginning with an exposition regarding measurement before turning to an exposition regarding modeling. In a parallel line of discourse, we then discuss the physics of brain network function, followed by a description of perturbation experiments and brain network control. In each section we separate our remarks into the known and the unknown, the past and the future, the fact and the speculation. Our goal is to provide an accessible introduction to the field, and to inspire the younger generation of physicists to courageously tackle some of the most pressing open questions surrounding the inner workings of the mind. 02. The physics of brain network structure \u00b6 Box 1: A simple primer on networks \u00b6 02.01. Measureing brain netrork structure \u00b6 02.02. Modeling brain network structure \u00b6 02.02.01. Random structure \u00b6 02.02.02. Community structure \u00b6 02.02.03. Small-world structure \u00b6 02.02.04. Hub structure \u00b6 02.02.05. Spatial structure \u00b6 02.02.06. Competition between structural properties \u00b6 02.03. The future of brain network structure \u00b6 Box 2: Bridging spatiotemporal scales \u00b6 03. The physics of brain network function \u00b6 03.01. Measuring brain network function \u00b6 03.02. Modeling brain network function \u00b6 03.02.01. Artificial models \u00b6 03.02.02. Biophysical models \u00b6 03.03. The future of brain network function \u00b6 Box 3: Information theory and network neuroscience \u00b6 04. Perteration experiments and the physics of brain network control \u00b6 04.01. Trageted perturbations and clinical interventions \u00b6 04.02. Network control in the brain \u00b6 Box 4: Linear control and network controllability \u00b6 04.03. The future of brain network control \u00b6 05. Conclusions and future directions in the neurophysics of brain networks \u00b6 References \u00b6 Scott, A. Neurophysics (Wiley, 1977). Koch, C. & Poggio, T. A theoretical analysis of electrical properties of spines. Proc. R. Soc. Lond. B Biol. Sci. 218, 455\u2013477 (1983). Fries, P. Rhythms for cognition: communication through coherence. Neuron 88, 220\u2013235 (2015). \u00b6 img{width: 50%; float: right;}","title":"190517 Lynn, Christopher W., 2019"},{"location":"190517_LynnChristopherW_2019/#toc","text":"00. Abstract 01. Introduction 02. The physics of brain network structure Box 1 02.01. Measureing brain netrork structure 02.02. Modeling brain network structure 02.02.01. Random structure 02.02.02. Community structure 02.02.03. Small-world structure 02.02.04. Hub structure 02.02.05. Spatial structure 02.02.06. Competition between structural properties 02.03. The future of brain network structure Box 2 03. The physics of brain network function 03.01. Measuring brain network function 03.02. Modeling brain network function 03.02.01. Artificial models 03.02.02. Biophysical models 03.03. The future of brain network function Box 3 04. Perteration experiments and the physics of brain network control 04.01. Trageted perturbations and clinical interventions 04.02. Network control in the brain Box 4 04.03. The future of brain network control 05. Conclusions and future directions in the neurophysics of brain networks","title":"ToC"},{"location":"190517_LynnChristopherW_2019/#summary","text":"","title":"Summary"},{"location":"190517_LynnChristopherW_2019/#further","text":"","title":"Further"},{"location":"190517_LynnChristopherW_2019/#terminology","text":"","title":"Terminology"},{"location":"190517_LynnChristopherW_2019/#original","text":"","title":"Original"},{"location":"190517_LynnChristopherW_2019/#00_abstract","text":"The brain is characterized by heterogeneous patterns of structural connections supporting unparalleled feat s of cognition and a wide range of behaviours. New non-invasive imaging techniques now allow comprehensive mapping of these patterns. However, a fundamental challenge remains to understand how the brain\u2019s structural wiring supports cognitive processes, with major implications for personalized mental health treatments. Here, we review recent efforts to meet this challenge, drawing on physics intuitions, models and theories, spanning the domains of statistical mechanics, information theory, dynamical systems and control. We first describe the organizing principles of brain network architecture instantiate d in structural wiring under constraints of spatial embedding and energy minimization. We then survey models of brain network function that stipulate how neural activity propagates along structural connections. Finally, we discuss perturbative experiments and models for brain network control; these use the physics of signal transmission along structural connections to infer intrinsic control processes that support goal-directed behaviour and to inform stimulation-based therapies for neurological and psychiatric disease. Throughout, we highlight open questions that invite the creative efforts of pioneering physicists.","title":"00. Abstract"},{"location":"190517_LynnChristopherW_2019/#0001_key_points","text":"From the first measurement of the nerve impulse by Hermann von Helmholtz in 1849 to the cutting-edge superconducting devices used in magnetoencephalography, physics and neuroscience have always been inextricably linked. Today, network neuroscience \u2014 the study of the brain as a complex web of interacting components \u2014 draws intuitions and techniques from nearly every branch of physics. The architecture of structural connections between neurons or brain regions is constrained by requirements of energy minimization and efficient information transfer. The materialization of long-range correlations and synchronization from the collective firing of individual neurons conjures notions of emergence and criticality from statistical mechanics. Together, these investigations of brain network structure and function guide targeted treatments for cognitive disorders using theories of network control. Now more than ever, understanding the complexities of the mind lies at the feet of curious and pioneering physicists.","title":"00.01. Key points"},{"location":"190517_LynnChristopherW_2019/#01_introduction","text":"It is our good fortune as physicists to seek to understand the nature of the observable world around us. In this inquiry, we need not reach to contemporary science to appreciate the fact that our perception of the world around us is inextricably linked to the world within us: the mind. Indeed, even Aristotle c. 350 B.C. noted that it is by mapping the structure of the world that the human comes to understand their own mind 1 . \u201cMind thinks itself because it shares the nature of the object of thought; for it becomes an object of thought in coming into contact with and thinking its objects, so that mind and object of thought are the same\u201d 2 . Over the ensuing 2000-plus years, it has not completely escaped notice that the mappers of the world have unique contributions to offer the mapping of the mind (from Thales of Miletus, c. 624\u2013546 B.C., to Leonardo Da Vinci, 1452\u20131519). More recently, it is notable that nearly all famous physicists of the early 20th century \u2013 Albert Einstein, Niels Bohr, Erwin Schroedinger, Werner Heisenberg, Max Born \u2013 considered the philosophical implications of their observations and theories 3 . In the post-war era, philosophical musings turned to particularly conspicuous empirical contributions at the intersection of neuroscience and artificial intelligence, spanning polymath John von Neumann\u2019s work enhancing our understanding of computational architectures 4 and physicist John Hopfield\u2019s invention of the associative neural network, which revolutionized our understanding of collective computation 5 . In the contemporary study of the mind and its fundamental organ \u2013 the brain \u2013 nearly all of the domains of physics, perhaps with the exception of relativity, are not only relevant but truly essential, motivating the early coinage of the term neurophysics some four decades ago 6 . The fundamentals of electricity and magnetism prove critical for building theoretical models of neurons and the transmission of action potentials 7 . These theories are being increasingly informed by mechanics to understand how force-generating and load-bearing proteins bend, curl, kink, buckle, constrict, and stretch to mediate neuronal signaling and plasticity 8 . Principles from thermodynamics come into play when predicting how the brain samples the environment (action) or shifts the distribution of information that it encodes (perception) 9 . Collectively, theories of brain function are either buttress ed or dismantle d by imaging, with common tools including magnetic resonance imaging 10 and magnetoencephalography 11 , the latter being built on superconducting quantum interference devices and next-generation quantum sensors that can be embedded into a system that can be worn like a helmet, revolutionizing our ability to measure brain function while allowing free and natural movement 12 . Moreover, recent developments in nanoscale analysis tools and in the design and synthesis of nanomaterials have generated optical, electrical, and chemical methods to explore brain function by enabling simultaneous measurement and manipulation of the activity of thousands or even millions of neurons 13 . Beyond its relevance for continued imaging advance- ments 14 , optics has come to the fore of neuroscience over the last decade with the development of optogenetics, an approach that uses light to alter neural processing at the level of single spikes and synaptic events, offering reliable, millisecond-timescale control of excitatory and inhibitory synaptic transmission 15 . Such astounding advances, enabled by the intersection of physics and neuroscience, have motivated the construction of a National Brain Observatory at the Argonne National Laboratory (Director: Peter Littlewood, previously of Cavendish Laboratories) funded by the National Sci- ence Foundation, as well as frequent media coverage including titles in the APS News such as \u201cPhysicists, the Brain is Calling You.\u201d16 And as physicists answer the call, our understanding of the brain deepens and our ability to mark and measure its component parts expands. Yet alongside this growing systematization and archivation, we have begun to face an increasing realization that it is the interactions between hundreds or thousands of neurons that generate the mind\u2019s functional states 13. Indeed, from interactions among neural components emerge computation 17, commu- nication 18, and information propagation 19. We can confidently state of neuroscience what Henri Poincare, the French mathematician, theoretical physicist, and philosopher of science, states of sci- ence generally: \u201cThe aim of science is not things themselves, as the dogmatists in their simplicity imagine, but the relations among things; outside these relations there is no reality knowable.\u201d20 The overarching goal of mapping these interactions in neural systems has motivated multibillion-dollar investments across the United States (the Brain Initiative generally, and the Human Connectome Project specifically 21), the European Union (the Blue Brain Project 22), China (the China Brain Project 23), and Japan (Japan\u2019s Brain/MINDS project 24). While it is clear that interactions are paramount, exactly how the functions of the mind arise from these interactions remains one of the fundamental open questions of brain science 25. To the physicist, such a question appears to exist naturally within the purview of statistical mechanics 26, with one major caveat: the interaction patterns observed in the brain are far from regular, such as those observed in crystalline structures, and are also far from random, such as those observed in fully disordered systems 27. Indeed, the observed heterogeneity of interaction patterns in neural systems \u2013 across a range of spatial and temporal scales \u2013 generally limits the utility of basic contin- uum models or mean-field theories, which would otherwise comprise our natural first approaches. Fortunately, similar observations of interaction heterogeneity have been made in other technologi- cal, social, and biological systems, leading to concerted efforts to develop a statistical mechanics of complex networks 28. The resultant area of inquiry includes criteria for building a network model of a complex system 29, statistics to quantify the architecture of that network 30, models to stipulate the dynamics that can occur both in and on a network 31\u201333, and theories of network function and control 34,35. Here, we provide a brief review for the curious physicist, spanning the network-based ap- proaches, statistics, models, and theories that have recently been used to understand the brain. Importantly, the interpretations that can be rationally drawn from all such efforts depend upon the nature of the network representation 29, including its descriptive, explanatory, and predictive valid- ity \u2013 topics that are treated with some philosophical rigor elsewhere 36. Following a simple primer on the nature of network models, we discuss the physics of brain network structure, beginning with an exposition regarding measurement before turning to an exposition regarding modeling. In a parallel line of discourse, we then discuss the physics of brain network function, followed by a description of perturbation experiments and brain network control. In each section we separate our remarks into the known and the unknown, the past and the future, the fact and the speculation. Our goal is to provide an accessible introduction to the field, and to inspire the younger generation of physicists to courageously tackle some of the most pressing open questions surrounding the inner workings of the mind.","title":"01. Introduction"},{"location":"190517_LynnChristopherW_2019/#02_the_physics_of_brain_network_structure","text":"","title":"02. The physics of brain network structure"},{"location":"190517_LynnChristopherW_2019/#box_1_a_simple_primer_on_networks","text":"","title":"Box 1: A simple primer on networks"},{"location":"190517_LynnChristopherW_2019/#0201_measureing_brain_netrork_structure","text":"","title":"02.01. Measureing brain netrork structure"},{"location":"190517_LynnChristopherW_2019/#0202_modeling_brain_network_structure","text":"","title":"02.02. Modeling brain network structure"},{"location":"190517_LynnChristopherW_2019/#020201_random_structure","text":"","title":"02.02.01. Random structure"},{"location":"190517_LynnChristopherW_2019/#020202_community_structure","text":"","title":"02.02.02. Community structure"},{"location":"190517_LynnChristopherW_2019/#020203_small-world_structure","text":"","title":"02.02.03. Small-world structure"},{"location":"190517_LynnChristopherW_2019/#020204_hub_structure","text":"","title":"02.02.04. Hub structure"},{"location":"190517_LynnChristopherW_2019/#020205_spatial_structure","text":"","title":"02.02.05. Spatial structure"},{"location":"190517_LynnChristopherW_2019/#020206_competition_between_structural_properties","text":"","title":"02.02.06. Competition between structural properties"},{"location":"190517_LynnChristopherW_2019/#0203_the_future_of_brain_network_structure","text":"","title":"02.03. The future of brain network structure"},{"location":"190517_LynnChristopherW_2019/#box_2_bridging_spatiotemporal_scales","text":"","title":"Box 2: Bridging spatiotemporal scales"},{"location":"190517_LynnChristopherW_2019/#03_the_physics_of_brain_network_function","text":"","title":"03. The physics of brain network function"},{"location":"190517_LynnChristopherW_2019/#0301_measuring_brain_network_function","text":"","title":"03.01. Measuring brain network function"},{"location":"190517_LynnChristopherW_2019/#0302_modeling_brain_network_function","text":"","title":"03.02. Modeling brain network function"},{"location":"190517_LynnChristopherW_2019/#030201_artificial_models","text":"","title":"03.02.01. Artificial models"},{"location":"190517_LynnChristopherW_2019/#030202_biophysical_models","text":"","title":"03.02.02. Biophysical models"},{"location":"190517_LynnChristopherW_2019/#0303_the_future_of_brain_network_function","text":"","title":"03.03. The future of brain network function"},{"location":"190517_LynnChristopherW_2019/#box_3_information_theory_and_network_neuroscience","text":"","title":"Box 3: Information theory and network neuroscience"},{"location":"190517_LynnChristopherW_2019/#04_perteration_experiments_and_the_physics_of_brain_network_control","text":"","title":"04. Perteration experiments and the physics of brain network control"},{"location":"190517_LynnChristopherW_2019/#0401_trageted_perturbations_and_clinical_interventions","text":"","title":"04.01. Trageted perturbations and clinical interventions"},{"location":"190517_LynnChristopherW_2019/#0402_network_control_in_the_brain","text":"","title":"04.02. Network control in the brain"},{"location":"190517_LynnChristopherW_2019/#box_4_linear_control_and_network_controllability","text":"","title":"Box 4: Linear control and network controllability"},{"location":"190517_LynnChristopherW_2019/#0403_the_future_of_brain_network_control","text":"","title":"04.03. The future of brain network control"},{"location":"190517_LynnChristopherW_2019/#05_conclusions_and_future_directions_in_the_neurophysics_of_brain_networks","text":"","title":"05. Conclusions and future directions in the neurophysics of brain networks"},{"location":"190517_LynnChristopherW_2019/#references","text":"Scott, A. Neurophysics (Wiley, 1977). Koch, C. & Poggio, T. A theoretical analysis of electrical properties of spines. Proc. R. Soc. Lond. B Biol. Sci. 218, 455\u2013477 (1983). Fries, P. Rhythms for cognition: communication through coherence. Neuron 88, 220\u2013235 (2015).","title":"References"},{"location":"190524_KopecAshleyM_2019/","text":"19-05-24 Kopec, Ashley M., 2019 \u00b6 Kopec, A. M., Smith, C. J., & Bilbo, S. D. (2019). Neuro-Immune Mechanisms Regulating Social Behavior: Dopamine as Mediator?. Trends in neurosciences. @article{kopec2019neuro, title={Neuro-Immune Mechanisms Regulating Social Behavior: Dopamine as Mediator?}, author={Kopec, Ashley M and Smith, Caroline J and Bilbo, Staci D}, journal={Trends in neurosciences}, year={2019}, publisher={Elsevier} } Original | [Mendeley] ToC \u00b6 00. Abstruct 01. Social Behavior in Health and Disease 02. The 'Behavioral' Immune System 02.01. Immune Signaling Changes Social Behavior 02.02. Social Context Changes Immune Function 03. Dopaminergic Signaling Modifies Both Social Behavior and Immune Function 03.01. Dopaminergic Regulation of Social Behaviors 03.02. Central Dopaminergic Regulation of Immune Function 04. Putting the Pieces Together: Adolescent Brain, Behavioral, and Nuero-immune Development 05. Concluding Remarks 06. References 00. Abstract \u00b6 01. Social Behavior in Health and Disease \u00b6 02. The 'Behavioral' Immune System \u00b6 02.01. Immune Signaling Changes Social Behavior \u00b6 02.02. Social Context Changes Immune Function \u00b6 03. Dopaminergic Signaling Modifies Both Social Behavior and Immune Function \u00b6 03.01. Dopaminergic Regulation of Social Behaviors \u00b6 03.02. Central Dopaminergic Regulation of Immune Function \u00b6 04. Putting the Pieces Together: Adolescent Brain, Behavioral, and Nuero-immune Development \u00b6 05. Concluding Remarks \u00b6 \u00b6 img{width: 50%; float: right;}","title":"190524 Kopec, Ashley M. 2019"},{"location":"190524_KopecAshleyM_2019/#toc","text":"00. Abstruct 01. Social Behavior in Health and Disease 02. The 'Behavioral' Immune System 02.01. Immune Signaling Changes Social Behavior 02.02. Social Context Changes Immune Function 03. Dopaminergic Signaling Modifies Both Social Behavior and Immune Function 03.01. Dopaminergic Regulation of Social Behaviors 03.02. Central Dopaminergic Regulation of Immune Function 04. Putting the Pieces Together: Adolescent Brain, Behavioral, and Nuero-immune Development 05. Concluding Remarks 06. References","title":"ToC"},{"location":"190524_KopecAshleyM_2019/#00_abstract","text":"","title":"00. Abstract"},{"location":"190524_KopecAshleyM_2019/#01_social_behavior_in_health_and_disease","text":"","title":"01. Social Behavior in Health and Disease"},{"location":"190524_KopecAshleyM_2019/#02_the_behavioral_immune_system","text":"","title":"02. The 'Behavioral' Immune System"},{"location":"190524_KopecAshleyM_2019/#0201_immune_signaling_changes_social_behavior","text":"","title":"02.01. Immune Signaling Changes Social Behavior"},{"location":"190524_KopecAshleyM_2019/#0202_social_context_changes_immune_function","text":"","title":"02.02. Social Context Changes Immune Function"},{"location":"190524_KopecAshleyM_2019/#03_dopaminergic_signaling_modifies_both_social_behavior_and_immune_function","text":"","title":"03. Dopaminergic Signaling Modifies Both Social Behavior and Immune Function"},{"location":"190524_KopecAshleyM_2019/#0301_dopaminergic_regulation_of_social_behaviors","text":"","title":"03.01. Dopaminergic Regulation of Social Behaviors"},{"location":"190524_KopecAshleyM_2019/#0302_central_dopaminergic_regulation_of_immune_function","text":"","title":"03.02. Central Dopaminergic Regulation of Immune Function"},{"location":"190524_KopecAshleyM_2019/#04_putting_the_pieces_together_adolescent_brain_behavioral_and_nuero-immune_development","text":"","title":"04. Putting the Pieces Together: Adolescent Brain, Behavioral, and Nuero-immune Development"},{"location":"190524_KopecAshleyM_2019/#05_concluding_remarks","text":"","title":"05. Concluding Remarks"},{"location":"190604_PapeKatrin_2019/","text":"19-06-04 Pape, Katrin, 2019 \u00b6 Pape, K., Tamouza, R., Leboyer, M. and Zipp, F., 2019. Immunoneuropsychiatry\u2014novel perspectives on brain disorders. Nature Reviews Neurology, p.1. Original | Mendeley Contents \u00b6 00. Abstract 01. Box 01. Neuropsychiatric disorders with inflammatory disturbance 02. Neuroimmune interplay in brain health 03. Cognitive performance in inflammation 03.00. Fig. 1 | The interplay between the immune system and the CNS. 03.01. Low-grade inflammaiton 03.02. Infection-related inflammation 04. Autoimmunity 04.01. Lessons from multiple sclerosis Fig. 2 | Overlap of neuropsychiatric disorders. 04.02. Lessons from autoimmune psychosis 05. Immunopsychiatry - an emerging field 06. Immunomodulatory treatment Box 02. Immunomodulatory drugs available for the treatment of CNS autoimmune disorders 07. Resilience and reserve 08. New clinical approach Fig. 3 | A proposed clinical pathway for patients with new-onset neuropsychiatric symptoms. 09. Conclusion Box 03. Unresoloved questions to direct future reseach 10. References 00. Abstract \u00b6 classical neuroinflammatory disease MS autoimmune encephalitis psychiatric diseases such as schizophrenia autism spectrum disorder bipolar disorder depression pathways: microglial activation pro-inflammatory cytokines molecular mimicry anti-neuronal autoantibodies self-reactive T cells disturbance of the blood\u2013brain barrier Immune processes have a vital role in CNS homeostasis, resilience and brain reserve. Our cognitive and social abilities rely on a highly sensitive and fine-tuned equilibrium of immune responses that involve both innate and adaptive immunity. Autoimmunity , chronic inflammation, infection and psychosocial stress can tip the scales towards disruption of higher-order networks. However, not only classical neuroinflammatory diseases, such as multiple sclerosis and autoimmune encephalitis, are caused by immune dysregulation that affects CNS function. Recent insight indicates that similar processes are involved in psychiatric diseases such as schizophrenia, autism spectrum disorder, bipolar disorder and depression . Pathways that are common to these disorders include microglial activation, pro-inflammatory cytokines, molecular mimicry , anti-neuronal autoantibodies, self-reactive T cells and disturbance of the blood\u2013brain barrier . These discoveries challenge our traditional classification of neurological and psychiatric diseases. New clinical paths are required to identify subgroups of neuropsychiatric disorders that are phenotypically distinct but pathogenically related and to pave the way for mechanism-based immune treatments. Combined expertise from neurologists and psychiatrists will foster translation of these paths into clinical practice. The aim of this Review is to highlight outstanding findings that have transformed our understanding of neuropsychiatric diseases and to suggest new diagnostic and therapeutic criteria for the emerging field of immunoneuropsychiatry . 01. \u00b6 Crosstalk between the immune and nervous systems is receiving increasing attention in a wide spectrum of neurological and psychiatric diseases. As pathways emerge that are common to disorders from both fields, the traditional boundaries between neurological and psychiatric disorders are becoming blurred . Novel discoveries about the roles of the immune system in CNS function and in disease together with tremendous developments in immune therapies make this topic of great interest. This rapidly developing research is providing new perspectives not only on disease and therapeutic targets but also on brain reserve and resilience in neuropsychiatric disorders. In this Review, we first give an overview of neuro-immune interplay and inflammatory influences in the healthy brain and in disease. Accumulating data suggest immune and autoimmune contributions to a wide variety of neurological and psychiatric disorders ( BOX 1 ). These findings are reflected in a growing number of therapeutic studies of mechanism-based immune treatments in subgroups of patients with neurological and psychiatric disorders. In addition, we discuss recent insights into the role of psychosocial stress and infectious events in the CNS that provide a mechanistic cornerstone for our understanding of neuroinflammatory processes and brain diseases. On the basis of the find-ings and studies discussed, we suggest a new clinical approach to neuropsychiatric disorders from an immunological perspective. Box 01. Neuropsychiatric disorders with inflammatory disturbance \u00b6 Involvement of immune dysfunction in pathogenesis has been studied in a broad range of neuropsychiatric diseases. This Review focuses on the following disorders: Multiple sclerosis Neuromyelitis optica spectrum disorder Autoimmune encephalitis (paraneoplastic, idiopathic or triggered by infection) Schizophrenia Autism spectrum disorders Depression Bipolar disorder Dementia, especially Alzheimer disease In addition, immune mechanisms are likely to contribute to many other disorders. These disorders include the following: Myelin oligodendrocyte glycoprotein antibody spectrum disorder Chronic inflammatory optic neuritis Rasmussen encephalitis Susac syndrome Stroke Parkinson disease Amyotrophic lateral sclerosis Huntington disease Obsessive\u2013compulsive disorder Anxiety disorders Eating disorders 02. Neuroimmune interplay in brain health \u00b6 interplay between neurons, glial cells, immune system conntributes to functional propertyies The CNS has traditionally been regarded as a site of immune privilege. However, despite being protected by specialized physical barriers, the brain is neither inert nor immunologically separated from the peripheral immune system. Instead, an interplay between neurons, glial cells and the immune system contributes to functional properties, such as cognition, social behaviour and learning performance in the healthy brain. micloglia chemokine / chemokine receptor extracellular vesicles / signaling modelecules Increasing evidence indicates that specific commu-nication occurs between neurons and microglia, the brain parenchyma-resident macrophages that account for ~10% of CNS cells. Microglia permanently survey their local environment with fine, motile processes [1] and, during brain development, they have an impor-tant role in synaptic pruning mediated by the comple-ment proteins C1q and C3, as they phagocytose the complement-tagged synapses [2],[3] . In addition to micro-glia, soluble factors such as chemokines and chemokine receptors also contribute to physiological developmental processes in the CNS; for example, CXC-chemokine receptor 4 (CXCR4) and CXCR7 are involved in the migration of cortical interneurons 4 . In the healthy adult brain, microglia have a role in the homeostasis of synaptic circuits in the CNS [5] . Their expression of receptors for purines (such as ATP) and common neurotransmitters (such as glutamate) enables them to sense local neuronal activity [6] . In response, microglia can directly contact neurons via outgrowth of processes5 or can indirectly modulate neuronal firing rate via release of extracellular vesicles [7] or signalling molecules, such as tumour necro-sis factor (TNF) [8] . In this way, microglia contribute to activity-induced synaptic plasticity, for example, during motor learning and memory. During systemic inflam-mation, microglia can become activated and produce pro-inflammatory mediators and induce phagocytosis 9 . Microglia comprise the innate CNS immune compartment; the existence of adaptive immune cells in the brain has long been considered a sign of disease. However, in healthy individuals, antigen-presenting cells and T cells patrol the brain\u2019s borders, residing in the meninges, the choroid plexus and the cerebro spinal fluid (CSF) 10 . These patrol cells provide additional protection to that provided by the blood\u2013brain barrier (BBB) \u2014 a physical boundary that consists of the basal lamina of endothelial cells, tight junctions between them and astrocyte end-feet processes \u2014 and the blood\u2013CSF barrier at the choroid plexus [11] , which is composed of epithelial cells [12] . Notably, evidence suggests that the choroid plexus has a role not only in transmigration but also in stimulation of T cells in response to peripheral inflammatory signals 13 . maintainance of neuronal fn Although T cells do not generally penetrate the parenchyma in non-inflammatory conditions, they can release soluble cytokines that affect CNS function [14],[15] . Studies in mice have demonstrated that adaptive immunity is necessary for cognitive performance: mice with severe combined immune deficiency exhibited impaired spatial learning and memory, but symptoms were reversed by injection of exogenous T cells 16 . Similar observations have been made for social behaviour14. From a neuro-biological perspective, levels of neurogenesis in adult transgenic mice that overexpress a CNS-specific T cell receptor are higher than those in mice that overexpress a T cell receptor for a non-CNS-specific antigen [17] . The advantageous effects of self-reactive T cells on the maintenance of neuronal function have led to the concept of protective autoimmunity , in which adaptive immune cell function maintains tissue function. Further beneficial roles of the immune response in the CNS \u2014 again going against the anticipated pathogenic activity of inflamma-tion \u2014 include protective roles for T helper 2 (TH2) cells in CNS injury and for IL-4 signalling in neurons in mul-tiple sclerosis (MS) models [18], 19 . The complex and diverse roles of adaptive immune cells are further underlined by the finding that a lack of B cells does not impair learning behaviour in mice [20] . drainage system glymphatic system (glial-dependent perivascular network) meningeal lymphatic system The interplay between the CNS and the periphery is mediated by two drainage systems. The first is the glymphatic system , a glial-dependent perivascular network that ensures provision of nutrients for neurons and glia and clearance of extracellular metabolites, such as lactate [21] . Activation of the glymphatic system is higher during sleep than during wakefulness, underlining the impor-tance of sleep for removal of potentially neurotoxic waste products 22 . The second is the meningeal lymphatic system that lines the dural sinuses and allows drainage from the CNS to deep cervical lymph nodes, the discovery of which established the missing link to the peripheral immune system 23 ,[24] . Via this system, molecules and immune cells from the CNS can be transported to lymphoid organs and evoke an immune response that involves subsequent migration of immune cells to the brain [25] . 03. Cognitive performance in inflammation \u00b6 03.00. \u00b6 As described above, innate and adaptive immunity are essential for CNS homeostasis. Consequently, distur-bances in the equilibrium of immune cells, neurons and glial cells needed for healthy CNS function are likely to modify cognitive performance ( FIG. 1a ). In this section, we consider the evidence that low-grade inflammation and inflammation in response to infection can alter cognitive performance. Recognition of pathogen-associated or damage- associated molecular patterns and subsequent phago-cytosis or cytokine production by microglia or invad-ing macrophages, representing the innate immune response in the CNS, has to be distinguished from the antigen-specific responsiveness of adaptive immune cells. Fig. 1 | The interplay between the immune system and the CNS. \u00b6 a | A certain level of inflammation and autoimmunity is necessary for optimal function of the CNS, but an overwhelming immune reaction leads to neuronal loss and impaired cognition. b | Immune cell infiltration into the CNS in disease. Glial cells, including microglia and astrocytes, are resident in the CNS. Lymphocytes and non-microglia myeloid cells are both thought to be present in the lymphatic vessels in health but might transmigrate through the blood\u2013brain barrier in inflammatory diseases. 03.01. Low-grade inflammaiton \u00b6 sickness behavior euflammation A compelling example of the pathogenic influence of the immune system on brain function is the change in mood, social behaviour and cognitive abilities \u2014 known as sickness behaviour \u2014 upon infection and systemic inflammation. The release of pro-inflammatory cytokines, such as IL-1\u03b2, IL-6 and TNF, outside the CNS affects the brain via neural (mainly vagal) pathways, interaction with cytokine receptors on cerebral endothelial cells and/or microglial activation 26 . The behavioural effects, such as social with-drawal and fatigue, are assumed to be adaptive responses that increase survival of the host 27 . Similarly, treatment with the cytokine IFN\u03b2, for MS or chronic viral hepa-titis, for example, has been associated with depressive symptoms as an adverse effect [28] . Sickness behaviour can be diminished after repeated subthreshold exposure to pathogens, a mechanism that is referred to as euflammation and that evidence suggests is the result of tolerogenic processes [29] . inflammaging In contrast to this transient low-grade inflammation, chronic low-grade inflammation can occur and cause neurotoxicity and neurodegeneration [30] . For example, cytokines can have neurotoxic effects by increasing pro-duction of reactive oxygen species, reducing monoamine transmission and potentiating glutamatergic transmis-sion. Chronic, sterile, low-grade inflammation occurs during human ageing and can contribute to age-related diseases; this process is referred to as inflammaging 31 . Chronic inflammation is implicated in schizophrenia and other psychiatric disorders32, and systemic inflammation, as well as acute infections, have been associated with an increased rate of cognitive decline and exacerbation of symptoms in patients with Alzheimer disease (AD) [33] . Chronic low-grade inflammation is also present in people who are obese [34] , and this inflammation influences cognitive performance by damaging neuronal circuits and the BBB and by activating pro-inflammatory immune cells [35] . These obesity-related mechanisms were initially observed in the hypothalamus, but some evidence sug-gests that they can also occur in the hippocampus, cortex, brainstem and amygdala [36] . beneficial euflammation sickness behavior adverse chronic inflammation \u21d2 neuronal damage cognitive decline dementia \u2234 right balance of inflammation Overall, in the interplay between the brain and the immune system, inflammation is a double-edged sword. Beneficial effects such as euflammation and evolution-arily advantageous sickness behaviour are on one side, but chronic inflammation that leads to neuronal dam-age, cognitive decline and possibly dementia is on the other side. The right balance of inflammation is needed for optimal CNS function. 03.02. Infection-related inflammation \u00b6 Sydenham chorea paediatric autoimmune neuropsychiatric disorders associated with S. pyogenes ( PANDAS ) Infection can initiate chronic inflammation that alters cognitive function. Data from a Danish longitudinal register showed that prior hospitalization for an autoimmune disease or infection increased the risk of a major mood disorder by 45% and 62%, respectively [37] . In rodents, even infection and systemic inflammation in the fetus during the prenatal or perinatal period can cause long-term cognitive damage, including learning, memory and attention abnormalities, a model that explains why early infection increases the risk of psychosis in young adulthood [38],[39] . Similarly, a viral infection in the mother during the first trimester of pregnancy and bacterial infection during the second trimester of pregnancy were associated with the development of autism spectrum disorder (ASD), a pervasive neurodevelopmental disorder defined by impairments in social skills and stereotypical behaviour, among the children in a large Danish cohort [40] . In children, throat infection with Streptococcus pyogenes has repeatedly been associated with subsequent neuropsychiatric disorders [41] . In particular, these disorders include Sydenham chorea , a movement disorder related to rheumatic fever that occurs in temporal relationship to group A streptococcal infection [42] , and conditions encompassed by the umbrella term of paediatric autoimmune neuropsychiatric disorders associated with S. pyogenes ( PANDAS ) and concomitant with obsessive\u2013compulsive disorder, tic disorder or choreiform motoric hyperactivity [43] . The suspected pathophysiology of PANDAS is cross reactivity of anti-streptococcus A antibodies with brain tissue due to molecular mimicry [44] that initiates an adaptive immune cell response. Nevertheless, in a prospective study of children with post-streptococcal neuropsy-chiatric symptoms, no correlation was seen between clinical symptoms and a change in autoimmune markers, such as anti-neuronal antibodies or inflam matory cytokines [45] . Therefore, despite confirmation on an epide miological level, the aetiological association between streptococcal infection and psychiatric symptoms is still controversial and the subject of an ongoing, large observational study [46] . Severe infections, such as herpes simplex virus (HSV) encephalitis at any time in the lifespan, have been associated with long-term impairment of memory and brain atrophy [47] . Herpes family viruses persist in the host and require constant immune surveillance in order to prevent reactivation. For example, latent cytomegalovirus (CMV) infection leads to accumulation of functionally exhausted effector T cells while the naive T cell pool is diminished [48] . In the prospective Northern Manhattan Study, a high infectious burden (assessed with serological markers for Chlamydia pneumoniae , Helicobacter pylori , CMV, HSV-1 and HSV-2) was associated with cognitive decline independently of cardiovascular risk among a cohort of 1,625 patients [49] . Evidence suggests that among patients with schizophrenia or bipolar disorder, infections with Toxoplasma gondii, HSV and CMV affect the cognitive dysfunction already present in these disorders, in particular working memory [50] . Another example of the intricate relationship between cognition and chronic infection is HIV-associated neurocognitive disorder, which emerges in patients with HIV infection despite highly active antiretroviral therapy [51] . Our understanding of the mechanistic links between infectious burden and neuropsychiatric diseases is still in its infancy. In addition to molecular mimicry that triggers autoimmune reactions, a possible mecha-nism is stress-induced potentiation of microglial inflammasome activation, which causes an increase in pro-inflammatory mediators [52],[53] . In this way, both innate (microglial activation with subsequent phagocytosis or cytokine production) and adaptive immune responses (antigen-specific responsiveness and production of anti-bodies) contribute to pathology. In addition, infectious agents might act via epigenetic pathways to modulate the innate immune cell repertoire, thereby influencing the risk profile for neurodegenerative disorders [54] . In addition to the cumulative effect of infectious burden, pathogens contribute directly to neurodegen-eration, and therefore cognitive decline, in elderly people. One example that has been investigated extensively but is still incompletely understood is the association between HSV infection and AD. On a population level, data from a Swedish cohort suggest that reactivation of HSV-1, indicated by high serum levels of anti-HSV immunoglobulin M (IgM) antibodies, increases the risk of developing AD [55] . In vitro experiments have shown that HSV-1 induces increases in intracellular amyloid-\u03b2 (A\u03b2) levels and tau phosphorylation, both of which are markers of AD [56] . In addition, HSV-1 DNA has been found in amyloid plaques in patients with AD, further underlining a link between the virus and pathological aggregations [57] . Virus-mediated cytotoxicity does not explain all clinical developments in AD, but viruses and amyloid plaques are thought to alter the neuroimmune crosstalk in various ways. For example, HSV-1 proteins debilitate neuronal autophagy and, consequently, antigen presentation [58] . In mice, recurrent asymptomatic activation of HSV-1 leads to upregulation of markers of neuroinflammation (for example, Toll-like recep-tor-4, IFN\u03b1 and IFN\u03b2) and early neurodegeneration [59] . In addition, extracellular deposits of A\u03b2 activate resting microglia and trigger them to attack neurons via various pathways, such as NADPH oxidase activation, inducible nitric oxide synthase expression and phagocytosis [60] . In summary, owing to the sensitive equilibrium of the immune system and brain function, infectious agents might tip the scales towards pathology. Furthermore, infections can trigger autoimmune phenomena that might contribute to a broad spectrum of neuropsychiatric diseases, discussed in more detail below. 04. Autoimmunity \u00b6 04.00. \u00b6 Nonspecific inflammatory processes underlie the low-grade inflammation described above, but neuro-inflammation can also be driven by autoimmunity. Autoimmune responses can be antibody driven or cell driven and, depending on the autoantigen involved, can involve antigen presentation by major histocompatibility complex (MHC) class II. Autoimmune responses can follow infection, and molecular mimicry can underlie these responses [41], 61 . Innate and adaptive immune mecha nisms have roles in autoimmunity. What we know about autoimmune responses and neuro psychiatric disorders is largely based on specific disorders, discussed in detail below. 04.01. Lessons from multiple sclerosis \u00b6 Neurological autoimmune diseases are mainly considered to be mediated by autoaggressive lymphocytes or anti-neuronal or anti-glial autoantibodies. These conditions include neuro myelitis optica and potentially disabling, or sometimes life-threatening, autoimmune encephalomyelitis. MS is the classical and most common chronic autoimmune encephalomyelitis; this disease leads to demyelination and progressive neurodegeneration. In early MS, most patients experience transient physical deficits, such as optic neuritis, hemiparesis or sensory disturbances, in a relapsing\u2013remitting presentation. Although these physical deficits are the most obvious consequences, an increasing body of evidence illustrates that cognitive deficits are abundant and extensive in MS [62] . Patients with MS can experience neuropsychological symptoms, such as mild cognitive alterations or depression, related not only to relapses but also to phases of remission [63] . The most common cognitive deficits, which occur in 40\u201360% of patients with MS, are reduced processing speed and impaired memory and/or executive function, and these symptoms can improve to some extent with immunomodulatory treatment [64] . Behavioural signs, such as anxiety, can also occur and are currently seen as comorbidities but might be a consequence of the pathology [65] . Notably, cognitive impairment at the time of MS diagnosis predicts disability progression, transition to secondary progressive MS and cortical thinning 66 . In line with these findings, the risk of cognitive impairment increases with progression of disease and is high-est in secondary progressive MS [67] . Nevertheless, 30\u201340% of patients with clinically isolated syndrome, which is considered to be a precursor of MS, have cognitive deficits [68],[69] , and in a study of so-called benign MS (defined by an Expanded Disability Status Scale (EDSS) score \u22643.0 despite a disease duration \u226515 years), up to 45% of patients had cognitive impairment [70] . Inflammation in MS, the local effects of which include microglial activation and myelin and neuronal damage, is thought to be primarily mediated by auto reactive T cells. Pro-inflammatory T H 17 cells are thought to migrate into the CNS and subsequently increase BBB permeability, enabling invasion of other immune cells [71] ( FIG. 1b ). Other important players include regulatory T cells, which are functionally impaired in people with MS [72] , and B cells, the role of which is highlighted by the effects of B cell-depleting therapies [73] . Relapses in patients with MS are influenced by stressful life events and by infections: a systematic meta-analysis has shown that upper respiratory tract infections have the most pronounced effect on relapse rates 74 . These observations underline the role of inflammation in relapse activity, as both systemic infection and stress induce a pro-inflammatory status, as discussed in more detail below. A specific autoantigen has not yet been identified in MS, but autoreactive T cells are widely assumed to target proteins of the myelin sheath, causing demyelination and, thereby, white matter damage. Demyelination deprives neurons of protective factors and has been described to decrease axonal transport and synaptic density in demyelinated hippocampi from post-mortem MS brains [75] . However, direct recognition of neurons by patho genic T cells has also been proposed [76],[77] , and neuronal loss could occur as a result of Wallerian degene ration and mitochondrial dysfunction [78] . Grey matter damage, measured as the cortical lesion load or cortical thinning, is a key predictor of progressive disease and cognitive decline [79] . Even in clinically stable patients, the presence of gadolinium-enhancing lesions on brain MRI is associated with impaired performance in the Paced Auditory Serial Addition Test (PASAT) , a screening tool for cognitive dysfunction [80] . These findings underline the fact that active neuroinflammation detectable with gadoli nium enhancement affects cognitive function and might cause detrimental effects on neuronal networks and connectivity. Although rare, psychotic symptoms can also occur in MS. A genetic overlap between MS and schizophrenia has been identified, and the genes that are common to both conditions are immune related 81 . Despite being separate disease entities, MS and classical psychiatric disorders are accompanied by higher-order network disturbances 82 , 83 ,[84],[85] ( FIG. 2a ). The study of MS has provided fundamental insights into the crosstalk between the immune system and the nervous system, and these insights will be highly valuable for the study of other diseases in which underlying immune pathology has recently been identified, discussed below. Fig. 2 | Overlap of neuropsychiatric disorders. \u00b6 a | Higher-order network disturbances are caused by common pathophysiological mechanisms of immune dysregulation (top). How clinically distinct phenotypes emerge from these pathways is an unresolved question (bottom). b | As pathways emerge that are common to neurological and psychiatric autoimmune brain diseases, the traditional boundaries between these disorders become blurred. Identification of subgroups of patients who are likely to benefit from immunotherapeutic approaches is crucial. CRP, C-reactive protein. 04.02. Lessons from autoimmune psychosis \u00b6 HSV encephalitis autoAb against: GABA A R AMPA (subunit GluA2) A compelling example of the overlap between psychiatric and neuro-logical pathologies is the occurrence of psychosis as a result of autoantibodies against neuropil, which can develop spontaneously in neoplastic diseases or after viral infections, particularly HSV encephalitis [86] . Some of these autoantibodies target intracellular antigens, such as onconeural proteins or 65 kDa glutamic acid decarboxylase ( GAD65 ). The presence of these antibodies is diagnostically useful but considered to be an epiphenomenon , as cytotoxic T cells are responsible for neuronal damage that is associated with neuropsy-chiatric symptoms 87 . However, a broad spectrum of extracellular antigens involved in synaptic transmission and plasticity are located on the cell surface; therefore, autoantibodies against these antigens that are present in patients with psychosis seem highly likely to have a direct pathogenic role. One example of such an anti-body is the autoantibody against the GABA A receptor , a postsynaptic chloride channel that mediates fast inhibitory neurotransmission in the mammalian brain; this autoantibody causes encephalitis with therapy-resistant epileptic seizures [88] . Moreover, antibodies against the \u03b1-amino-3-hydroxy-5-methyl-4-isoxazole propionic acid ( AMPA ) receptor can have a pathological role: administration of human pathogenic antibodies against the AMPA receptor subunit GluA2 impairs long-term synaptic plasticity in vitro and affects learning and memory in mice in vivo [89] . NMDA More controversial is the importance of antibodies against the N-methyl- D -aspartate ( NMDA ) glutamate receptor, one of the most commonly observed antigens in autoimmune encephalitis. Patients with this condition often present with anxiety, sleep disorders, mania, paranoia, memory impairment and disintegration of language, followed by a phase in which agitation and catatonia alternate, accompanied by abnormal movements and autonomic instability [90] . In humans, antibody titres in the CSF correlate with the clinical course of disease [90] . Experiments in rats indicate a pathophysiological role for anti-NMDA receptor antibodies. For example, in rat hippocampal neurons that have been incubated with anti-NMDA receptor antibodies, NMDA receptor internalization and reduced NMDA receptor-mediated synaptic currents were observed [91] . In addition, treatment of rats with NMDA receptor blockers evoked stereotyped behaviour and cataleptic freezing that were comparable to symptoms of autoimmune encephalitis in humans [92] . NMDA receptor encephalitis can occur after HSV encephalitis in up to 27% of patients, indicating a role for molecular mimicry 61 . Furthermore, antibodies that cross-react with the NMDA receptor have been found in patients with neuropsychiatric systemic lupus erythematosus, and these antibodies cause neuronal damage via activated microglia and complement component C1q when administered to mice [93],[94] . Despite this evidence, the pathological role of neurotransmitter receptor autoantibodies came into question when anti-NMDA receptor autoantibodies were detected in serum from ~10% of healthy controls as well as in patients with pure psychotic symptoms [95] . Similar results have been seen for a variety of brain antigens, challenging the pathological relevance of CNS-specific autoantibodies [96] ; BBB integrity has been proposed as a pivotal factor in the clinical outcome [95] . In another study, anti-NMDA receptor autoantibodies from psychotic patients altered synaptic transmission and long-term potentiation in cultured neurons and in mouse brain whereas those from healthy controls did not 97 . Independent of immune pathogenesis, glutamate receptor hypofunction has been proposed as a key factor in the development of schizophrenia [98] . In combination, these results provide evidence that specific and different mechanisms underlie encephalitis and autoimmune psychosis, in which autoantibodies against NMDA receptors have different effects on the organization of the glutamate synapse through receptor internalization or abnormal NMDA receptor dynamics. Given that patients with schizophrenia who are positive for anti-NMDA receptor antibodies do not exhibit classical signs of encephalitis 97 , we conclude that different immune alterations mediated by similar antibodies result in different neuropsychiatric entities. 05. Immunopsychiatry - an emerging field \u00b6 On the basis of the data discussed above, the concept of autoimmune psychosis has become a compelling example of the interface between neurological (autoimmune encephalitis) and psychiatric (psychosis) disorders. In this section, we discuss accumulating evidence that suggests that immune dysregulation is involved in a broad spectrum of psychiatric diseases. Owing to a lack of specific markers, studies in the emerging field of immunopsychiatry currently focus on systemic measures of inflammation. A growing number of studies have identified nonspecific inflammatory disturbances in subgroups of patients , but knowledge of specific pathogenic pathways remains scarce. One overarching concept is that immunopsychiatric diseases involve a generally overactivated immune system. In general, overactivation of the immune system is thought to increase brain vulnerability, increasing the risk of psychiatric symptoms upon a so-called second hit later in life 99 . On an epidemiological level, this hypothesis is supported by a link between psychiatric diseases, such as schizophrenia, depression and anxiety, and systemic autoimmune disorders [100],[101] . Case reports similarly support this hypothesis; one striking example is the development of severe psychosis in an individual who received a stem cell transplantation from their brother with schizophrenia, indicating that adoptive immune transfer had a role [102] . Furthermore, extensive studies of cytokine profiles in disorders such as schizophrenia, depression, suicidal ideation and post-traumatic stress disorder reveal wide-ranging dysregulation of mainly pro-inflammatory markers, in particular IL-6, IL-2 receptor, IL-1\u03b2, IL-17 A and C-reactive protein (CRP) [103]\u2013[107] . Several studies identified a positive correlation of serum levels of inflammatory markers with disease severity and a negative correlation with cognitive performance [103],[108] . Changes in the innate CNS immune compartment have also been associated with psychiatric disorders. Either excessive upregulation or downregulation of microglia function evokes detrimental effects. For example, genetic defects in microglial signalling pathways cause abnormal development of brain circuits and neuro-psychiatric symptoms, as in hereditary diffuse leukoencephalopathy with spheroids (HDLS) [109] . Disruption of microglial function in later life can be initiated by psycho social and environmental factors. For example, in a mouse model, chronic stress with depressive behaviour has been associated with microglial loss in the hippocampus [110] . By contrast, depression and schizophrenia have been associated with increased microglial activity measured with PET, and this increased activity could be used to identify patients at high risk of disease exacerbations [111],[112] . Although microglial activation in humans has been studied only indirectly, early life challenges are assumed to \u2018prime\u2019 microglia and increase their response to subsequent inflammatory stimuli [113] . A crucial discovery that many immune factors are influential prenatally led to use of the so-called maternal immune activation model in order to study ASD and schizophrenia in rodents. In this model, IL-6 was identified as a key mediator of inflammatory effects on fetal brain development [114] . In mice, autism-like behaviour in offspring requires the presence of mater-nal ROR\u03b3t-positive TH17 cells and IL-17a downstream of IL-6 (REF.[115]). In subsequent studies, a mechanism of ASD pathogenesis has been proposed in which T H 17-dependent loss of inhibitory interneuron networks leads to increased cortical activation in the primary somatosensory cortex [116] . A study published in 2017 provides evidence that maternal gut commensal bacteria have a role in IL-17a production in mothers [117] . These findings support the idea of a gut\u2013immune\u2013brain axis that has been implicated in classical autoimmunity (such as modulation of the balance between pro-inflammatory and regulatory T cells by gut bacteria in MS [118] ) and are in line with previous reports of an altered intestinal barrier in people with ASD and their close relatives [119] . In rodents, the role of maternal immune activation in ASD was confirmed at the transcriptome level, in which immune activation in the mother caused dysregulation in the fetal brain expression of genes involved in developmental processes [120] . Although animal studies provide the proof of concept and agree with findings in diseases of neurological immune dysfunction, such as MS, the mechanisms described are plausible in humans but remain speculative. Cytokine signalling is not the only aspect of maternal immunity that affects the developing fetal brain \u2014 B cell-mediated immune responses and production of antibodies can also have effects. For example, in an epidemio logical study, antibodies against fetal brain tissue were detected in the serum of mothers of children with ASD but not in mothers of healthy children [121] . Another study produced the same observation in up to 23% of mothers of children with ASD [122] . In animal studies, intravenous administration of 73 kDa and 37 kDa IgG from mothers of children with ASD to pregnant rhesus macaques led to abnormal social behaviour in the macaque offspring [123] . Targets for these antibodies include proteins with functions in neurodevelopment, such as contactin-associated protein-like 2 (CASPR2) [122],[124] . Although the exact mecha nism by which these maternal antibodies affect the fetus is not yet understood, the findings in humans and the adoptive transfer study suggest that placental transfer of maternal antibodies contributes to disease development, at least in a subgroup of patients. Aberrant genetic regulation underlies most of the immune dysfunction discussed above, and a polygenic contribution to the risk of several severe psychiatric disorders has been confirmed in a meta-analysis of genome-wide association studies in which the MHC was the most relevant shared risk locus [125] . Genes in the MHC are involved in physiological processes, including CNS development and homeostasis, but are common among neuropsychiatric disease-associated loci [126],[127] . For example, the MHC-resident complement C4 cluster has been implicated in the risk of schizophrenia [128],[129] . Therefore, fine dissection of the genetic contribution of the MHC is a promising approach to increasing our understanding of brain disorders. Although we are only beginning to develop a deeper, mechanistic understanding of immunological pathways involved in the pathogenesis of psychiatric disorders, the emerging field of immunopsychiatry provides new perspectives on the disorders discussed. As even early evolutionary processes such as the integration of sequences of human endogenous retroviruses into the genome presumably through repeated infections were found to be associated with neurological and psychiatric disorders [130],[131] , it becomes clear that dysfunctional processes occur on an ancestral, maternal and individual level. 06. Immunomodulatory treatment \u00b6 Classical neurological autoimmune diseases have long been treated with immunomodulatory drugs. Routinely used treatments for acute exacerbations include steroids, plasmapheresis, intravenous immunoglobulin, cyclo-phosphamide and B cell-depleting monoclonal anti-bodies; a broadening range of disease-modifying drugs is emerging from the development of therapies for MS (BOX 2). Immunosuppression is also an established treat-ment strategy for autoimmune encephalitis, but clinical studies of such treatments for psychiatric diseases are still in progress. In a pilot study published in 2018, treatment with high-dose intravenous immunoglobulin had positive effects on scores in several cognitive and behavioural tests for children with ASD and evidence of a dysregu-lated immune system132. In another study, intravenous immunoglobulin had beneficial effects for subgroups of children with ASD and inflammation133. A series of case reports of immune modulatory treatment in PANDAS disorders demonstrates how an understanding that auto-inflammatory processes are involved in the patho-genesis can lead to successful treatment134. However, syste matic treatment approaches are still lacking and are currently merely performed with antibiotics. Currently, monoclonal antibody treatments are being trialled in schizophrenia. In one study, use of the mono clonal antibody natalizumab, which targets the cell adhesion molecule \u03b14 integrin and is currently used for treatment of MS, is being tested135. The recombinant humanized anti-human IL-6 receptor monoclonal anti-body tocilizumab is being tested in two136,137 ongoing studies, one of which136 focuses on patients with signs of peripheral inflammation. In one case report, remis-sion of treatment-resistant schizophrenia was seen in a patient who underwent bone marrow transplantation for acute myeloid leukaemia, providing evidence that this treatment could be an effective, or even curative, option for severe schizophrenia138. Similarly, infusion of autologous umbilical cord blood into children with ASD led to clinical improvement of core autistic symptoms139. Evidence that immune dysfunction is involved in depression is supported by accumulating data that sug-gest that established antidepressant drugs such as selec-tive serotonin reuptake inhibitors (SSRIs) exert their effects, at least partly, via anti-inflammatory effects. For example, a meta-analysis showed that SSRIs reduce levels of IL-1\u03b2 and IL-6 in patients with depression140. In line with these observations, synthetic metabolites of tryptophan, a precursor of serotonin, suppressed pro-inflammatory T cells and autoimmune neuro-inflammation in experimental autoimmune encephalitis, a mouse model of MS141. Additional support for the con-cept that immune pathology underlies depression comes from a meta-analysis of randomized clinical trials that showed that, despite heterogeneous study results, non-steroidal anti-inflammatory drugs had an overall positive effect on depression142. In rats, the microglia inhibitor minocycline improved symptoms of depression in the forced swimming test143, an observation that is consistent with reports that microglia are overactivated in depres-sion112. A large randomized trial of minocycline has not been done in humans, but a meta-analysis of existing clinical data indicates beneficial effects of mino cycline in patients with depression, thereby providing a proof of concept144. Minocycline also had positive effects as an adjunctive treatment to risperidone in children with ASD145, although not as a single medication in a small pilot study146. In addition to its inhibitory effects on microglia, minocycline has antibiotic properties that might modify the gut flora, which could contri bute to beneficial treatment outcomes. Furthermore, in depres-sion, anti-inflammatory treatment with the TNF antago-nist infliximab had beneficial effects in patients with treatment-resistant depression and high baseline levels of inflammatory biomarkers147. In summary, immunomodulatory therapies are effec-tive options for a range of neurological and psychiatric diseases. To develop individualized therapeutic strate-gies, subgroups of patients with psychiatric disorders and signs of immune dysregulation must be identified ( FIG. 2b ). Clinical studies are more likely to reveal con-sistent positive effects if they include patients with an inflammatory phenotype, indicated by MRI or high levels of inflammation markers in the CSF or serum. Furthermore, we assume that there is a narrow thera-peutic time window for maximal therapeutic efficacy and avoidance of irreversible CNS injury. For example, timely immunomodulatory treatment in autoimmune encephalitis probably prevents neuronal damage and permanent neurological deficits148, and this concept might hold true for other neuropsychiatric disorders. Box 02. Immunomodulatory drugs available for the treatment of CNS autoimmune disorders \u00b6 Immunomodulatory drugs and treatments applied in a broad range of autoimmune diseases Steroids Plasmapheresis Intravenous immunoglobulin Cyclophosphamide Methotrexate Mycophenolate mofetil Azathioprin Mitoxantrone Rituximab Ocrelizumab Infliximab Disease-modifying drugs from the range of therapeutics for multiple sclerosis IFN\u03b2 Glatiramer acetate Teriflunomide Dimethyl fumarate Fingolimod Natalizumab Alemtuzumab Cladribine Rituximab Ocrelizumab Other drugs with partial immunomodulatory effects Nonsteroidal anti-inflammatory drugs Selective serotonin reuptake inhibitors Minocycline Tryptophan metabolites 07. Resilience and reserve \u00b6 A well-recognized risk factor for exacerbation in MS and many psychiatric disorders is psychosocial stress. Some evidence suggests that the association between psychiatric disorders and stress is mediated, at least in part, by neuroinflammatory processes, including microglial activation. For example, the \u2018two-hit hypo-thesis\u2019 proposes that stress in early life increases base-line microglial activity, thereby increasing the risk of psychiatric disorders upon subsequent challenges later in life149,150. Accordingly, a meta-analysis of clinical stud-ies demonstrated a pro-inflammatory status (high levels of CRP, IL-6 and TNF) in adults with a history of child-hood trauma151. Similarly, in a prospective cohort study, patients with a history of childhood maltreatment and depression had higher levels of CRP than participants with depression only152. The question remains, however, of why some indi-viduals are more resilient to stressful events than others. In a mouse model of chronic social defeat stress, BBB integrity was altered in the nucleus accumbens owing to downregulation of the tight junction protein claudin 5, thereby allowing influx of IL-6 into the brain paren-chyma153. Importantly, however, loss of claudin 5 was observed only in stress-susceptible mice \u2014 susceptibil-ity to stress was observed as social avoidance behaviour after several days of exposure to social defeat by a larger, physically aggressive mouse; by contrast, this behaviour was not seen in mice classified as resilient. Epigenetic modulation of IL-6 seems to further promote resilience in mice, as an inhibitor of IL-6 DNA methylation that decreased IL-6 production reduced depressive symptoms in a mouse model154. Furthermore, adaptive immune cells are needed for coping with stress. For example, exposure of mice to acute psychological stress in the form of predator odour increases T cell trafficking to the brain155, and T cell-deficient mice displayed more signs of maladaptation after stress156. In addition, analogous to findings for neurogenesis discussed above, self-reactive, brain-specific T cells are needed for resilience in mice, leading to the idea that vaccination with CNS-related peptides could stimulate coping behaviour157. From studies of chronic autoimmune inflammation of the CNS in MS, we know that the brain has reserve capacities30. Our recent studies indicate that the cortical network responds to an inflammatory attack with a TNF-dependent increase in cortical synaptic strength65. This upregulation of cortical neuronal activity presum-ably reflects repair mechanisms initially but eventually causes maladaptive development of anxiety behaviour despite otherwise complete remission in disability. Analysis of higher-order networks in patients who progress rapidly and those who do not could increase our understanding of brain reserve and identify future therapeutic approaches. 08. New clinical approach \u00b6 Fig. 3 | A proposed clinical pathway for patients with new-onset neuropsychiatric symptoms. \u00b6 Blue boxes indicate investigations required and orange boxes indicate diagnoses. CMV, cytomegalovirus; CRP, C-reactive protein; CSF, cerebrospinal fluid; ESR , erythrocyte sedimentation rate; HSV, herpes simplex virus; MOG, myelin oligodendrocyte glycoprotein; NMOSD, neuromyelitis optica spectrum disorder ; OCB, oligoclonal bands; VZV, varicella zoster virus. \u00b6 The idea that the CNS is a site of immune privilege has shifted with a growing understanding that immune cells have multifaceted roles in CNS homeostasis, even in the healthy brain. Mechanistic aspects of the neuroim-mune interplay have been revealed, including roles for peripherally induced adaptive immunity and for cross-talk between neurons and CNS-resident microglia. The involvement of astrocytes has been studied to a lesser extent but is becoming a focus of research. Associations of the immune system with brain dis-eases have been reported for decades, initially in case reports but later in epidemiological studies. Studies of the immune system in psychiatric disorders have thus far focused on classical markers of inflammation, such as CRP and erythrocyte sedimentation rate (ESR), TNF or IL-6 in serum, as more sensitive or specific markers are often lacking. A level of inflammation that strikes a highly sensitive equilibrium seems to be required for health. Consequently, any disturbance can cause detrimental effects not only on focal neurological symptoms but also, more importantly, on higher-order network function. As common immunological aetiologies of neurologi-cal and psychiatric diseases are discovered, the traditional classification of these diseases is increasingly challenged. In order to develop promising immune-based thera-peutic approaches for psychiatric diseases, we propose new diagnostic pathways ( FIG. 3 ). In this proposed algo-rithm, individuals who present with new neuropsy-chiatric symptoms, such as cognitive and behavioural disturbances, epileptic seizures and psychosis, must be screened for blood, CSF and imaging markers of inflammation. Psychiatric symptoms in patients with classical neuroinflammatory disorders must be con-sidered when making treatment decisions. If there are any signs of active inflammation, immunomodulatory treatment should be discussed. 09. Conclusion \u00b6 For decades, or even centuries, of medical history, neuro-psychiatric diseases have been diagnosed and classified on the basis of phenomenological criteria. The findings discussed in this Review provide novel perspectives and indicate the need for a future classification of neuropsy-chiatric disorders on the basis of pathogenic mechanisms, at least for subgroups of patients. In this context, diag-nostic algorithms should classify patients into inflamma-tory and non-inflammatory subgroups to help reach an individualized treatment decision. For example, future understanding of neuropsychiatric diseases might no longer divide them into descriptive entities, such as MS, neuromyelitis optica spectrum disorder, auto immune encephalitis, schizophrenia, ASD and bipolar dis order, but into pathogenic entities, such as autoimmune antibody-mediated or T cell-mediated brain disorders. Although the knowledge of immune involvement in neuropsychiatric disorders has emerged over several years, necessary changes to the clinical management of patients with these disorders have scarcely been imple-mented. Furthermore, many questions remain unsolved ( BOX 3 ), such as why apparently similar mechanisms lead to clinically distinct disorders and whether immunolog-ical disturbances are always a cause of certain disorders or sometimes a consequence. The next steps will require combined experience from neurologists and psychiatrists in a new field of immunoneuropsychiatry. Box 03. Unresoloved questions to direct future reseach \u00b6 Which pathways mediate the interplay between levels of inflammation and cognitive performance? Why do apparently similar immunological mechanisms lead to clinically distinct disorders? What are the precise mechanisms of interaction between immune cells and higher-order networks in neuropsychiatric diseases? How can this crosstalk be addressed therapeutically? Are immunological disturbances the cause or consequence of neuropsychiatric diseases? How can we better identify subgroups of patients who are likely to benefit from immunotherapies? 10. References \u00b6 [Nimmerjahn, A., Kirchhoff, F. & Helmchen, F. Resting microglial cells are highly dynamic surveillants of brain parenchyma in vivo. Science 308, 1314\u20131318 (2005).][1] [Stevens, B. et al. The classical complement cascade mediates CNS synapse elimination. Cell 131, 1164\u20131178 (2007).][2] [Paolicelli, R. C. et al. Synaptic pruning by microglia is necessary for normal brain development. Science 333, 1456\u20131458 (2011).][3] Sanchez-Alcaniz, J. A. et al. Cxcr7 controls neuronal migration by regulating chemokine responsiveness. Neuron 69, 77\u201390 (2011). [Wake, H., Moorhouse, A. J., Jinno, S., Kohsaka, S. & Nabekura, J. Resting microglia directly monitor the functional state of synapses in vivo and determine the fate of ischemic terminals. J. Neurosci. 29, 3974\u20133980 (2009).][5] [Pocock, J. M. & Kettenmann, H. Neurotransmitter receptors on microglia. Trends Neurosci. 30, 527\u2013535 (2007).][6] [Antonucci, F. et al. Microvesicles released from microglia stimulate synaptic activity via enhanced sphingolipid metabolism. EMBO J. 31, 1231\u20131240 (2012).][7] [Beattie, E. C. et al. Control of synaptic strength by glial TNF\u03b1. Science 295, 2282\u20132285 (2002).][8] Gertig, U. & Hanisch, U. K. Microglial diversity by responses and responders. Front. Cell Neurosci. 8, 101 (2014). Ellwardt, E., Walsh, J. T., Kipnis, J. & Zipp, F. Understanding the role of T cells in CNS homeostasis. Trends Immunol. 37, 154\u2013165 (2016). [Engelhardt, B. & Ransohoff, R. M. Capture, crawl, cross: the T cell code to breach the blood-brain barriers. Trends Immunol. 33, 579\u2013589 (2012).][11] [Schwartz, M. & Deczkowska, A. Neurological disease as a failure of brain-immune crosstalk: the multiple faces of neuroinflammation. Trends Immunol. 37, 668\u2013679 (2016).][12] Strominger, I. et al. The choroid plexus functions as a niche for T-cell stimulation within the central nervous system. Front. Immunol. 9, 1066 (2018). [Filiano, A. J. et al. Unexpected role of interferon- gamma in regulating neuronal connectivity and social behaviour. Nature 535, 425\u2013429 (2016).][14] [Derecki, N. C. et al. Regulation of learning and memory by meningeal immunity: a key role for IL-4. J. Exp. Med. 207, 1067\u20131080 (2010).][15] Kipnis, J., Cohen, H., Cardon, M., Ziv, Y. & Schwartz, M. T cell deficiency leads to cognitive dysfunction: implications for therapeutic vaccination for schizophrenia and other psychiatric conditions. Proc. Natl Acad. Sci. USA 101, 8180\u20138185 (2004). This study shows that a lack of mature T cells causes cognitive and behavioural impairment in mice, underlining the role of adaptive immunity for normal CNS function. [Ziv, Y. et al. Immune cells contribute to the maintenance of neurogenesis and spatial learning abilities in adulthood. Nat. Neurosci. 9, 268\u2013275 (2006).][17] [Walsh, J. T. et al. MHCII-independent CD4+ T cells protect injured CNS neurons via IL-4. J. Clin. Invest. 125, 699\u2013714 (2015).][18] Vogelaar, C. F. et al. Fast direct neuronal signaling via the IL-4 receptor as therapeutic target in neuroinflammation. Sci. Transl Med. 10, eaao2304 (2018). This study reveals a beneficial effect of intrathecal IL-4 treatment on progression in experimental autoimmune encephalomyelitis, thereby highlighting a new pathway of neuroimmune interplay as a potential therapeutic target. [Radjavi, A., Smirnov, I. & Kipnis, J. Brain antigen-reactive CD4+ T cells are sufficient to support learning behavior in mice with limited T cell repertoire. Brain Behav. Immun. 35, 58\u201363 (2014).][20] [Iliff, J. J. et al. A paravascular pathway facilitates CSF flow through the brain parenchyma and the clearance of interstitial solutes, including amyloid beta. Sci. Transl Med. 4, 147ra111 (2012).][21] Xie, L. et al. Sleep drives metabolite clearance from the adult brain. Science 342, 373\u2013377 (2013). Louveau, A. et al. Structural and functional features of central nervous system lymphatic vessels. Nature 523, 337\u2013341 (2015). This article presents the first description of the re-discovery of the meningeal lymphatic system. [Absinta, M. et al. Human and nonhuman primate meninges harbor lymphatic vessels that can be visualized noninvasively by MRI. eLife 6, e29738 (2017).][24] [Traka, M., Podojil, J. R., McCarthy, D. P., Miller, S. D. & Popko, B. Oligodendrocyte death results in immune-mediated CNS demyelination. Nat. Neurosci. 19, 65\u201374 (2016).][25] D\u2019Mello, C. & Swain, M. G. Immune-to-brain communication pathways in inflammation-associated sickness and depression. Curr. Top. Behav. Neurosci. 31, 73\u201394 (2017). Kelley, K. W. et al. Cytokine-induced sickness behavior. Brain Behav. Immun. 17 (Suppl. 1), 112\u2013118 (2003). [Neilley, L. K., Goodin, D. S., Goodkin, D. E. & Hauser, S. L. Side effect profile of interferon beta-1b in MS: results of an open label trial. Neurology 46, 552\u2013554 (1996).][28] [Tarr, A. J., Liu, X., Reed, N. S. & Quan, N. Kinetic characteristics of euflammation: the induction of controlled inflammation without overt sickness behavior. Brain Behav. Immun. 42, 96\u2013108 (2014).]][29] Larochelle, C., Uphaus, T., Prat, A. & Zipp, F. Secondary progression in multiple sclerosis: neuronal exhaustion or distinct pathology? Trends Neurosci. 39, 325\u2013339 (2016). Franceschi, C. et al. Inflamm-aging. An evolutionary perspective on immunosenescence. Ann. NY Acad. Sci. 908, 244\u2013254 (2000). Marrie, R. A. et al. Increased incidence of psychiatric disorders in immune-mediated inflammatory disease. J. Psychosom. Res. 101, 17\u201323 (2017). Perry, V. H., Nicoll, J. A. & Holmes, C. Microglia in neurodegenerative disease. Nat. Rev. Neurol. 6, 193\u2013201 (2010). Lumeng, C. N. & Saltiel, A. R. Inflammatory links between obesity and metabolic disease. J. Clin. Invest. 121, 2111\u20132117 (2011). Stranahan, A. M., Hao, S., Dey, A., Yu, X. & Baban, B. Blood-brain barrier breakdown promotes macrophage infiltration and cognitive impairment in leptin receptor-deficient mice. J. Cereb. Blood Flow Metab. 36, 2108\u20132121 (2016). Guillemot-Legris, O. & Muccioli, G. G. Obesity-induced neuroinflammation: beyond the hypothalamus. Trends Neurosci. 40, 237\u2013253 (2017). Benros, M. E. et al. Autoimmune diseases and severe infections as risk factors for schizophrenia: a 30-year population-based register study. Am. J. Psychiatry 168, 1303\u20131310 (2011). This large Danish longitudinal register shows that both autoimmune disease and prior hospitalization for infection increase the risk of schizophrenia on an epidemiological level. Khandaker, G. M. et al. Inflammation and immunity in schizophrenia: implications for pathophysiology and treatment. Lancet Psychiatry 2, 258\u2013270 (2015). Canetta, S. et al. Elevated maternal C-reactive protein and increased risk of schizophrenia in a national birth cohort. Am. J. Psychiatry 171, 960\u2013968 (2014). Atladottir, H. O. et al. Maternal infection requiring hospitalization during pregnancy and autism spectrum disorders. J. Autism Dev. Disord. 40, 1423\u20131430 (2010). Orlovska, S. et al. Association of streptococcal throat infection with mental disorders: testing key aspects of the PANDAS hypothesis in a nationwide study. JAMA Psychiatry 74, 740\u2013746 (2017). Zomorrodi, A. & Wald, E. R. Sydenham\u2019s chorea in western Pennsylvania. Pediatrics 11 7, e675\u2013e679 (2006). Swedo, S. E. et al. Pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections: clinical description of the first 50 cases. Am. J. Psychiatry 155, 264\u2013271 (1998). Bronze, M. S. & Dale, J. B. Epitopes of streptococcal M proteins that evoke antibodies that cross-react with human brain. J. Immunol. 151, 2820\u20132828 (1993). Singer, H. S., Gause, C., Morris, C. & Lopez, P. Serial immune markers do not correlate with clinical exacerbations in pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections. Pediatrics 121, 1198\u20131205 (2008). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT02889016 (2016). Kapur, N. et al. Herpes simplex encephalitis: long term magnetic resonance imaging and neuropsychological profile. J. Neurol. Neurosurg. Psychiatry 57, 1334\u20131342 (1994). Almanzar, G. et al. Long-term cytomegalovirus infection leads to significant changes in the composition of the CD8+ T cell repertoire, which may be the basis for an imbalance in the cytokine production profile in elderly persons. J. Virol. 79, 3675\u20133683 (2005). Katan, M. et al. Infectious burden and cognitive function: the Northern Manhattan Study. Neurology 80, 1209\u20131215 (2013). This cohort study shows that infectious burden measured as serological exposure to common pathogens was associated with cognitive impairment independent of cardiovascular risk profile. Hamdani, N. et al. Effects of cumulative herpesviridae and Toxoplasma gondii infections on cognitive function in healthy, bipolar, and schizophrenia subjects. J. Clin. Psychiatry 78, e18\u2013e27 (2017). Kamminga, J., Cysique, L. A., Lu, G., Batchelor, J. & Brew, B. J. Validity of cognitive screens for HIV-associated neurocognitive disorder: a systematic review and an informed screen selection guide. Curr. HIV/AIDS Rep. 10, 342\u2013355 (2013). Zhang, C. J. et al. TLR-stimulated IRAKM activates caspase-8 inflammasome in microglia and promotes neuroinflammation. J. Clin. Invest. 128, 5399\u20135412 (2018). Heneka, M. T., McManus, R. M. & Latz, E. Inflammasome signalling in brain function and neurodegenerative disease. Nat. Rev. Neurosci. 19, 610\u2013621 (2018). Wendeln, A. C. et al. Innate immune memory in the brain shapes neurological disease hallmarks. Nature 556, 332\u2013338 (2018). This study shows that peripheral inflammatory stimuli induce differential epigenetic modulation of brain-resident microglia, influencing symptoms in a mouse model of AD; these findings provide a mechanistic link between inflammation, innate immunity and neuropsychiatric disease. Lovheim, H., Gilthorpe, J., Adolfsson, R., Nilsson, L. G. & Elgh, F. Reactivated herpes simplex infection increases the risk of Alzheimer\u2019s disease. Alzheimers Dement. 11 , 593\u2013599 (2015). Fulop, T., Itzhaki, R. F., Balin, B. J., Miklossy, J. & Barron, A. E. Role of microbes in the development of Alzheimer\u2019s disease: state of the art - an International Symposium presented at the 2017 IAGG Congress in San Francisco. Front. Genet. 9, 362 (2018). Wozniak, M. A., Mee, A. P. & Itzhaki, R. F. Herpes simplex virus type 1 DNA is located within Alzheimer\u2019s disease amyloid plaques. J. Pathol. 217, 131\u2013138 (2009). O\u2019Connell, D. & Liang, C. Autophagy interaction with herpes simplex virus type-1 infection. Autophagy 12, 451\u2013459 (2016). Martin, C. et al. Inflammatory and neurodegeneration markers during asymptomatic HSV-1 reactivation. J. Alzheimers Dis. 39, 849\u2013859 (2014). Brown, G. C. & Neher, J. J. Inflammatory neurodegeneration and mechanisms of microglial killing of neurons. Mol. Neurobiol. 41, 242\u2013247 (2010). Armangue, T. et al. Frequency, symptoms, risk factors, and outcomes of autoimmune encephalitis after herpes simplex encephalitis: a prospective observational study and retrospective analysis. Lancet Neurol. 17, 760\u2013772 (2018). This combined prospective observational study and retrospective analysis characterizes the frequency and the natural course of autoimmune encephalitis after herpes simplex encephalitis. Carta, M. G. et al. The risk of bipolar disorders in multiple sclerosis. J. Affect. Disord. 155, 255\u2013260 (2014). Feinstein, A., Magalhaes, S., Richard, J. F., Audet, B. & Moore, C. The link between multiple sclerosis and depression. Nat. Rev. Neurol. 10, 507\u2013517 (2014). Patti, F. Treatment of cognitive impairment in patients with multiple sclerosis. Expert Opin. Investig. Drugs 21, 1679\u20131699 (2012). [Ellwardt, E. et al. Maladaptive cortical hyperactivity upon recovery from experimental autoimmune encephalomyelitis. Nat. Neurosci. 21, 1392\u20131403 (2018). This study identifies the emergence of cortical network hyperexcitability and elevated anxiety in remission after neuroinflammatory attack to the brain; this network instability with anxiety behaviour, also known in patients with MS, represents the early stages of neurodegeneration.][65] Pitteri, M., Romualdi, C., Magliozzi, R., Monaco, S. & Calabrese, M. Cognitive impairment predicts disability progression and cortical thinning in MS: an 8-year study. Mult. Scler. 23, 848\u2013854 (2017). Potagas, C. et al. Cognitive impairment in different MS subtypes and clinically isolated syndromes. J. Neurol. Sci. 267, 100\u2013106 (2008). Ruano, L. et al. Age and disability drive cognitive impairment in multiple sclerosis across disease subtypes. Mult. Scler. 23, 1258\u20131267 (2017). Di Filippo, M., Portaccio, E., Mancini, A. & Calabresi, P. Multiple sclerosis and cognition: synaptic failure and network dysfunction. Nat. Rev. Neurosci. 19, 599\u2013609 (2018). Amato, M. P. et al. Benign multiple sclerosis: cognitive, psychological and social aspects in a clinical cohort. J. Neurol. 253, 1054\u20131059 (2006). Nylander, A. & Hafler, D. A. Multiple sclerosis. J. Clin. Invest. 122, 1180\u20131188 (2012). Viglietta, V., Baecher-Allan, C., Weiner, H. L. & Hafler, D. A. Loss of functional suppression by CD4+CD25+ regulatory T cells in patients with multiple sclerosis. J. Exp. Med. 199, 971\u2013979 (2004). Hauser, S. L. et al. Ocrelizumab versus interferon beta-1a in relapsing multiple sclerosis. N. Engl. J. Med. 376, 221\u2013234 (2017). McKay, K. A., Jahanfar, S., Duggan, T., Tkachuk, S. & Tremlett, H. Factors associated with onset, relapses or progression in multiple sclerosis: a systematic review. Neurotoxicology 61, 189\u2013212 (2017). Dutta, R. et al. Demyelination causes synaptic alterations in hippocampi from multiple sclerosis patients. Ann. Neurol. 69, 445\u2013454 (2011). Liblau, R. S., Gonzalez-Dunia, D., Wiendl, H. & Zipp, F. Neurons as targets for T cells in the nervous system. Trends Neurosci. 36, 315\u2013324 (2013). Kebir, H. et al. Human TH17 lymphocytes promote blood-brain barrier disruption and central nervous system inflammation. Nat. Med. 13, 1173\u20131175 (2007). Campbell, G. R. et al. Mitochondrial DNA deletions and neurodegeneration in multiple sclerosis. Ann. Neurol. 69, 481\u2013492 (2011). Calabrese, M. et al. Cortical lesion load associates with progression of disability in multiple sclerosis. Brain 135, 2952\u20132961 (2012). Bellmann-Strobl, J. et al. Poor PASAT performance correlates with MRI contrast enhancement in multiple sclerosis. Neurology 73, 1624\u20131627 (2009). Andreassen, O. A. et al. Genetic pleiotropy between multiple sclerosis and schizophrenia but not bipolar disorder: differential involvement of immune-related gene loci. Mol. Psychiatry 20, 207\u2013214 (2015). Charalambous, T. et al. Structural network disruption markers explain disability in multiple sclerosis. J. Neurol. Neurosurg. Psychiatry 90, 219\u2013226 (2019). Fleischer, V. et al. Graph theoretical framework of brain networks in multiple sclerosis: a review of concepts. Neuroscience https://doi.org/10.1016/ j.neuroscience.2017.10.033 (2017). Dong, D., Wang, Y., Chang, X., Luo, C. & Yao, D. Dysfunction of large-scale brain networks in schizophrenia: a meta-analysis of resting-state functional connectivity. Schizophr. Bull. 44, 168\u2013181 (2018). Kaiser, R. H., Andrews-Hanna, J. R., Wager, T. D. & Pizzagalli, D. A. Large-scale network dysfunction in major depressive disorder: a meta-analysis of resting-state functional connectivity. JAMA Psychiatry 72, 603\u2013611 (2015). Armangue, T. et al. Autoimmune post-herpes simplex encephalitis of adults and teenagers. Neurology 85, 1736\u20131743 (2015). Bien, C. G. et al. Immunopathology of autoantibody-associated encephalitides: clues for pathogenesis. Brain 135, 1622\u20131638 (2012). Petit-Pedrol, M. et al. Encephalitis with refractory seizures, status epilepticus, and antibodies to the GABAA receptor: a case series, characterisation of the antigen, and analysis of the effects of antibodies. Lancet Neurol. 13, 276\u2013286 (2014). Haselmann, H. et al. Human autoantibodies against the AMPA receptor subunit GluA2 induce receptor reorganization and memory dysfunction. Neuron 100, 91\u2013105 (2018). Dalmau, J., Lancaster, E., Martinez-Hernandez, E., Rosenfeld, M. R. & Balice-Gordon, R. Clinical experience and laboratory investigations in patients with anti-NMDAR encephalitis. Lancet Neurol. 10, 63\u201374 (2011). Hughes, E. G. et al. Cellular and synaptic mechanisms of anti-NMDA receptor encephalitis. J. Neurosci. 30, 5866\u20135875 (2010). Haggerty, G. C., Forney, R. B. & Johnson, J. M. The effect of a single administration of phencyclidine on behavior in the rat over a 21-day period. Toxicol. Appl. Pharmacol. 75, 444\u2013453 (1984). DeGiorgio, L. A. et al. A subset of lupus anti-DNA antibodies cross-reacts with the NR2 glutamate receptor in systemic lupus erythematosus. Nat. Med. 7, 1189\u20131193 (2001). Nestor, J. et al. Lupus antibodies induce behavioral changes mediated by microglia and blocked by ACE inhibitors. J. Exp. Med. 215, 2554\u20132566 (2018). Hammer, C. et al. Neuropsychiatric disease relevance of circulating anti-NMDA receptor autoantibodies depends on blood-brain barrier integrity. Mol. Psychiatry 19, 1143\u20131149 (2014). Dahm, L. et al. Seroprevalence of autoantibodies against brain antigens in health and disease. Ann. Neurol. 76, 82\u201394 (2014). Jezequel, J. et al. Dynamic disorganization of synaptic NMDA receptors triggered by autoantibodies from psychotic patients. Nat. Commun. 8, 1791 (2017). This study reveals differential effects of autoantibodies against the glutamate NMDA receptor on synaptic transmission and long-term potentiation in patients with psychosis versus healthy controls, providing a mechanistic framework for different clinical outcomes despite similar autoantibodies Kantrowitz, J. & Javitt, D. C. Glutamatergic transmission in schizophrenia: from basic research to clinical practice. Curr. Opin. Psychiatry 25, 96\u2013102 (2012). Bergink, V., Gibney, S. M. & Drexhage, H. A. Autoimmunity, inflammation, and psychosis: a search for peripheral markers. Biol. Psychiatry 75, 324\u2013331 (2014). Cullen, A. E. et al. Associations between non- neurological autoimmune disorders and psychosis: a meta-analysis. Biol. Psychiatry 85, 35\u201348 (2018). Siegmann, E. M. et al. Association of depression and anxiety disorders with autoimmune thyroiditis: a systematic review and meta-analysis. JAMA Psychiatry 75, 577\u2013584 (2018). Sommer, I. E. et al. Severe chronic psychosis after allogeneic SCT from a schizophrenic sibling. Bone Marrow Transplant. 50, 153\u2013154 (2015). Dahan, S. et al. The relationship between serum cytokine levels and degree of psychosis in patients with schizophrenia. Psychiatry Res. 268, 467\u2013472 (2018). Haapakoski, R., Mathieu, J., Ebmeier, K. P., Alenius, H. & Kivimaki, M. Cumulative meta-analysis of interleukins 6 and 1\u03b2, tumour necrosis factor \u03b1 and C-reactive protein in patients with major depressive disorder. Brain Behav. Immun. 49, 206\u2013215 (2015). Black, C. & Miller, B. J. Meta-analysis of cytokines and chemokines in suicidality: distinguishing suicidal versus nonsuicidal patients. Biol. Psychiatry 78, 28\u201337 (2015). Al-Ayadhi, L. Y. & Mostafa, G. A. Elevated serum levels of interleukin-17A in children with autism. J. Neuroinflamm. 9, 158 (2012). Passos, I. C. et al. Inflammatory markers in post-traumatic stress disorder: a systematic review, meta-analysis, and meta-regression. Lancet Psychiatry 2, 1002\u20131012 (2015). Bulzacka, E. et al. Chronic peripheral inflammation is associated with cognitive impairment in schizophrenia: results from the multicentric FACE-SZ dataset. Schizophr. Bull. 42, 1290\u20131302 (2016). Rademakers, R. et al. Mutations in the colony stimulating factor 1 receptor (CSF1R) gene cause hereditary diffuse leukoencephalopathy with spheroids. Nat. Genet. 44, 200\u2013205 (2011). Tong, L. et al. Microglia loss contributes to the development of major depression induced by different types of chronic stresses. Neurochem. Res. 42, 2698\u20132711 (2017). Bloomfield, P. S. et al. Microglial activity in people at ultra high risk of psychosis and in schizophrenia: an [11C]PBR28 PET brain imaging study. Am. J. Psychiatry 173, 44\u201352 (2016). This PET imaging study shows that patients with schizophrenia and individuals with a high risk of psychosis exhibit increased microglial activity, indicating a connection between neuroinflammation and the risk of psychosis. Wachholz, S. et al. Microglia activation is associated with IFN-alpha induced depressive-like behavior. Brain Behav. Immun. 55, 105\u2013113 (2016). Norden, D. M., Muccigrosso, M. M. & Godbout, J. P. Microglial priming and enhanced reactivity to secondary insult in aging, and traumatic CNS injury, and neurodegenerative disease. Neuropharmacology 96, 29\u201341 (2015). Smith, S. E., Li, J., Garbett, K., Mirnics, K. & Patterson, P. H. Maternal immune activation alters fetal brain development through interleukin-6. J. Neurosci. 27, 10695\u201310702 (2007). Choi, G. B. et al. The maternal interleukin-17a pathway in mice promotes autism-like phenotypes in offspring. Science 351, 933\u2013939 (2016). The results of this study indicate that the development of ASD-like phenotypes in offspring in the murine model of maternal immune activation is mediated by TH17 cells. Shin Yim, Y. et al. Reversing behavioural abnormalities in mice exposed to maternal inflammation. Nature 549, 482\u2013487 (2017). Kim, S. et al. Maternal gut bacteria promote neurodevelopmental abnormalities in mouse offspring. Nature 549, 528\u2013532 (2017). This study shows that the production of IL-17 in the maternal immune activation model in mice depends on the composition of maternal intestinal bacteria, underlining the role of the gut\u2013immune\u2013brain axis. Cekanaviciute, E. et al. Gut bacteria from multiple sclerosis patients modulate human T cells and exacerbate symptoms in mouse models. Proc. Natl Acad. Sci. USA 114 , 10713\u201310718 (2017). de Magistris, L. et al. Alterations of the intestinal barrier in patients with autism spectrum disorders and in their first-degree relatives. J. Pediatr. Gastroenterol. Nutr. 51, 418\u2013424 (2010). Lombardo, M. V. et al. Maternal immune activation dysregulation of the fetal brain transcriptome and relevance to the pathophysiology of autism spectrum disorder. Mol. Psychiatry 23, 1001\u20131013 (2018). Zimmerman, A. W. et al. Maternal antibrain antibodies in autism. Brain Behav. Immun. 21, 351\u2013357 (2007). Braunschweig, D. et al. Autism-specific maternal autoantibodies recognize critical proteins in developing brain. Transl Psychiatry 3, e277 (2013). Bauman, M. D. et al. Maternal antibodies from mothers of children with autism alter brain growth and social behavior development in the rhesus monkey. Transl Psychiatry 3, e278 (2013). Brimberg, L. et al. Caspr2-reactive antibody cloned from a mother of an ASD child mediates an ASD-like phenotype in mice. Mol. Psychiatry 21, 1663\u20131671 (2016). This study describes the isolation and characterization of monoclonal brain-reactive antibodies from a mother of a child with ASD. Cross-Disorder Group of the Psychiatric Genomics Consortium. Identification of risk loci with shared effects on five major psychiatric disorders: a genome-wide analysis. Lancet 381, 1371\u20131379 (2013). Trowsdale, J. & Knight, J. C. Major histocompatibility complex genomics and human disease. Annu. Rev. Genomics Hum. Genet. 14, 301\u2013323 (2013). Dendrou, C. A., Petersen, J., Rossjohn, J. & Fugger, L. HLA variation and disease. Nat. Rev. Immunol. 18, 325\u2013339 (2018). Sekar, A. et al. Schizophrenia risk from complex variation of complement component 4. Nature 530, 177\u2013183 (2016). Prasad, K. M. et al. Neuropil contraction in relation to Complement C4 gene copy numbers in independent cohorts of adolescent-onset and young adult-onset schizophrenia patients-a pilot study. Transl Psychiatry 8, 134 (2018). Kury, P. et al. Human endogenous retroviruses in neurological diseases. Trends Mol. Med. 24, 379\u2013394 (2018). Perron, H. et al. Molecular characteristics of human endogenous retrovirus type-W in schizophrenia and bipolar disorder. Transl Psychiatry 2, e201 (2012). Melamed, I. R., Heffron, M., Testori, A. & Lipe, K. A pilot study of high-dose intravenous immunoglobulin 5% for autism: impact on autism spectrum and markers of neuroinflammation. Autism Res. 11, 421\u2013433 (2018). Connery, K. et al. Intravenous immunoglobulin for the treatment of autoimmune encephalopathy in children with autism. Transl Psychiatry 8, 148 (2018). This article and that by Melamed et al. (2018) describe pilot studies that provided the first evidence for a beneficial effect of intravenous immunoglobulin for the treatment of children with ASD and signs of inflammation. Swedo, S. E., Frankovich, J. & Murphy, T. K. Overview of treatment of pediatric acute-onset neuropsychiatric syndrome. J. Child Adolesc. Psychopharmacol. 27, 562\u2013565 (2017). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT03093064 (2017). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT02874573 (2018). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT02034474 (2018). Miyaoka, T. et al. Remission of psychosis in treatment- resistant schizophrenia following bone marrow transplantation: a case report. Front. Psychiatry 8, 174 (2017). Dawson, G. et al. Autologous cord blood infusions are safe and feasible in young children with autism spectrum disorder: results of a single-center phase I open-label trial. Stem Cells Transl Med. 6, 1332\u20131339 (2017). Hannestad, J., DellaGioia, N. & Bloch, M. The effect of antidepressant medication treatment on serum levels of inflammatory cytokines: a meta-analysis. Neuropsychopharmacology 36, 2452\u20132459 (2011). Platten, M. et al. Treatment of autoimmune neuroinflammation with a synthetic tryptophan metabolite. Science 310, 850\u2013855 (2005). Kohler, O. et al. Effect of anti-inflammatory treatment on depression, depressive symptoms, and adverse effects: a systematic review and meta-analysis of randomized clinical trials. JAMA Psychiatry 71, 1381\u20131391 (2014). Molina-Hernandez, M., Tellez-Alcantara, N. P., Perez-Garcia, J., Olivera-Lopez, J. I. & Jaramillo-Jaimes, M. T. Antidepressant-like actions of minocycline combined with several glutamate antagonists. Prog. Neuropsychopharmacol. Biol. Psychiatry 32, 380\u2013386 (2008). Rosenblat, J. D. & McIntyre, R. S. Efficacy and tolerability of minocycline for depression: a systematic review and meta-analysis of clinical trials. J. Affect. Disord. 227, 219\u2013225 (2018). This systematic review and meta-analysis provides a proof of concept for the antidepressant effects of minocycline. Ghaleiha, A. et al. Minocycline as adjunctive treatment to risperidone in children with autistic disorder: a randomized, double-blind placebo-controlled trial. J. Child Adolesc. Psychopharmacol. 26, 784\u2013791 (2016). Pardo, C. A. et al. A pilot open-label trial of minocycline in patients with autism and regressive features. J. Neurodev. Disord. 5, 9 (2013). Raison, C. L. et al. A randomized controlled trial of the tumor necrosis factor antagonist infliximab for treatment-resistant depression: the role of baseline inflammatory biomarkers. JAMA Psychiatry 70, 31\u201341 (2013). Kalman, B. Autoimmune encephalitides: a broadening field of treatable conditions. Neurologist 22, 1\u201313 (2017). Howes, O. D. & McCutcheon, R. Inflammation and the neural diathesis-stress hypothesis of schizophrenia: a reconceptualization. Transl Psychiatry 7, e1024 (2017). Fonken, L. K., Frank, M. G., Gaudet, A. D. & Maier, S. F. Stress and aging act through common mechanisms to elicit neuroinflammatory priming. Brain Behav. Immun. 73, 133\u2013148 (2018). Baumeister, D., Akhtar, R., Ciufolini, S., Pariante, C. M. & Mondelli, V. Childhood trauma and adulthood inflammation: a meta-analysis of peripheral C-reactive protein, interleukin-6 and tumour necrosis factor-\u03b1. Mol. Psychiatry 21, 642\u2013649 (2016). Danese, A. et al. Elevated inflammation levels in depressed adults with a history of childhood maltreatment. Arch. Gen. Psychiatry 65, 409\u2013415 (2008). Menard, C. et al. Social stress induces neurovascular pathology promoting depression. Nat. Neurosci. 20, 1752\u20131760 (2017). This study reveals mechanistic links between psychosocial stress and depression via disruption of BBB integrity by downregulation of claudin 5 in mice. Wang, J. et al. Epigenetic modulation of inflammation and synaptic plasticity promotes resilience against stress in mice. Nat. Commun. 9, 477 (2018). Lewitus, G. M., Cohen, H. & Schwartz, M. Reducing post-traumatic anxiety by immunization. Brain Behav. Immun. 22, 1108\u20131114 (2008). Cohen, H. et al. Maladaptation to mental stress mitigated by the adaptive immune system via depletion of naturally occurring regulatory CD4+CD25+ cells. J. Neurobiol. 66, 552\u2013563 (2006). Lewitus, G. M. et al. Vaccination as a novel approach for treating depressive behavior. Biol. Psychiatry 65, 283\u2013288 (2009). \u00b6 img{width: 66%; float: right;} img:hover{width: 100%;} strong, a, a:hover, a:visited{color: #007448;} a:visited{font-weight: bold;}","title":"190604 Pape, Katrin, 2019"},{"location":"190604_PapeKatrin_2019/#contents","text":"00. Abstract 01. Box 01. Neuropsychiatric disorders with inflammatory disturbance 02. Neuroimmune interplay in brain health 03. Cognitive performance in inflammation 03.00. Fig. 1 | The interplay between the immune system and the CNS. 03.01. Low-grade inflammaiton 03.02. Infection-related inflammation 04. Autoimmunity 04.01. Lessons from multiple sclerosis Fig. 2 | Overlap of neuropsychiatric disorders. 04.02. Lessons from autoimmune psychosis 05. Immunopsychiatry - an emerging field 06. Immunomodulatory treatment Box 02. Immunomodulatory drugs available for the treatment of CNS autoimmune disorders 07. Resilience and reserve 08. New clinical approach Fig. 3 | A proposed clinical pathway for patients with new-onset neuropsychiatric symptoms. 09. Conclusion Box 03. Unresoloved questions to direct future reseach 10. References","title":"Contents"},{"location":"190604_PapeKatrin_2019/#00_abstract","text":"classical neuroinflammatory disease MS autoimmune encephalitis psychiatric diseases such as schizophrenia autism spectrum disorder bipolar disorder depression pathways: microglial activation pro-inflammatory cytokines molecular mimicry anti-neuronal autoantibodies self-reactive T cells disturbance of the blood\u2013brain barrier Immune processes have a vital role in CNS homeostasis, resilience and brain reserve. Our cognitive and social abilities rely on a highly sensitive and fine-tuned equilibrium of immune responses that involve both innate and adaptive immunity. Autoimmunity , chronic inflammation, infection and psychosocial stress can tip the scales towards disruption of higher-order networks. However, not only classical neuroinflammatory diseases, such as multiple sclerosis and autoimmune encephalitis, are caused by immune dysregulation that affects CNS function. Recent insight indicates that similar processes are involved in psychiatric diseases such as schizophrenia, autism spectrum disorder, bipolar disorder and depression . Pathways that are common to these disorders include microglial activation, pro-inflammatory cytokines, molecular mimicry , anti-neuronal autoantibodies, self-reactive T cells and disturbance of the blood\u2013brain barrier . These discoveries challenge our traditional classification of neurological and psychiatric diseases. New clinical paths are required to identify subgroups of neuropsychiatric disorders that are phenotypically distinct but pathogenically related and to pave the way for mechanism-based immune treatments. Combined expertise from neurologists and psychiatrists will foster translation of these paths into clinical practice. The aim of this Review is to highlight outstanding findings that have transformed our understanding of neuropsychiatric diseases and to suggest new diagnostic and therapeutic criteria for the emerging field of immunoneuropsychiatry .","title":"00. Abstract"},{"location":"190604_PapeKatrin_2019/#01","text":"Crosstalk between the immune and nervous systems is receiving increasing attention in a wide spectrum of neurological and psychiatric diseases. As pathways emerge that are common to disorders from both fields, the traditional boundaries between neurological and psychiatric disorders are becoming blurred . Novel discoveries about the roles of the immune system in CNS function and in disease together with tremendous developments in immune therapies make this topic of great interest. This rapidly developing research is providing new perspectives not only on disease and therapeutic targets but also on brain reserve and resilience in neuropsychiatric disorders. In this Review, we first give an overview of neuro-immune interplay and inflammatory influences in the healthy brain and in disease. Accumulating data suggest immune and autoimmune contributions to a wide variety of neurological and psychiatric disorders ( BOX 1 ). These findings are reflected in a growing number of therapeutic studies of mechanism-based immune treatments in subgroups of patients with neurological and psychiatric disorders. In addition, we discuss recent insights into the role of psychosocial stress and infectious events in the CNS that provide a mechanistic cornerstone for our understanding of neuroinflammatory processes and brain diseases. On the basis of the find-ings and studies discussed, we suggest a new clinical approach to neuropsychiatric disorders from an immunological perspective.","title":"01."},{"location":"190604_PapeKatrin_2019/#box_01_neuropsychiatric_disorders_with_inflammatory_disturbance","text":"Involvement of immune dysfunction in pathogenesis has been studied in a broad range of neuropsychiatric diseases. This Review focuses on the following disorders: Multiple sclerosis Neuromyelitis optica spectrum disorder Autoimmune encephalitis (paraneoplastic, idiopathic or triggered by infection) Schizophrenia Autism spectrum disorders Depression Bipolar disorder Dementia, especially Alzheimer disease In addition, immune mechanisms are likely to contribute to many other disorders. These disorders include the following: Myelin oligodendrocyte glycoprotein antibody spectrum disorder Chronic inflammatory optic neuritis Rasmussen encephalitis Susac syndrome Stroke Parkinson disease Amyotrophic lateral sclerosis Huntington disease Obsessive\u2013compulsive disorder Anxiety disorders Eating disorders","title":"Box 01. Neuropsychiatric disorders with inflammatory disturbance"},{"location":"190604_PapeKatrin_2019/#02_neuroimmune_interplay_in_brain_health","text":"interplay between neurons, glial cells, immune system conntributes to functional propertyies The CNS has traditionally been regarded as a site of immune privilege. However, despite being protected by specialized physical barriers, the brain is neither inert nor immunologically separated from the peripheral immune system. Instead, an interplay between neurons, glial cells and the immune system contributes to functional properties, such as cognition, social behaviour and learning performance in the healthy brain. micloglia chemokine / chemokine receptor extracellular vesicles / signaling modelecules Increasing evidence indicates that specific commu-nication occurs between neurons and microglia, the brain parenchyma-resident macrophages that account for ~10% of CNS cells. Microglia permanently survey their local environment with fine, motile processes [1] and, during brain development, they have an impor-tant role in synaptic pruning mediated by the comple-ment proteins C1q and C3, as they phagocytose the complement-tagged synapses [2],[3] . In addition to micro-glia, soluble factors such as chemokines and chemokine receptors also contribute to physiological developmental processes in the CNS; for example, CXC-chemokine receptor 4 (CXCR4) and CXCR7 are involved in the migration of cortical interneurons 4 . In the healthy adult brain, microglia have a role in the homeostasis of synaptic circuits in the CNS [5] . Their expression of receptors for purines (such as ATP) and common neurotransmitters (such as glutamate) enables them to sense local neuronal activity [6] . In response, microglia can directly contact neurons via outgrowth of processes5 or can indirectly modulate neuronal firing rate via release of extracellular vesicles [7] or signalling molecules, such as tumour necro-sis factor (TNF) [8] . In this way, microglia contribute to activity-induced synaptic plasticity, for example, during motor learning and memory. During systemic inflam-mation, microglia can become activated and produce pro-inflammatory mediators and induce phagocytosis 9 . Microglia comprise the innate CNS immune compartment; the existence of adaptive immune cells in the brain has long been considered a sign of disease. However, in healthy individuals, antigen-presenting cells and T cells patrol the brain\u2019s borders, residing in the meninges, the choroid plexus and the cerebro spinal fluid (CSF) 10 . These patrol cells provide additional protection to that provided by the blood\u2013brain barrier (BBB) \u2014 a physical boundary that consists of the basal lamina of endothelial cells, tight junctions between them and astrocyte end-feet processes \u2014 and the blood\u2013CSF barrier at the choroid plexus [11] , which is composed of epithelial cells [12] . Notably, evidence suggests that the choroid plexus has a role not only in transmigration but also in stimulation of T cells in response to peripheral inflammatory signals 13 . maintainance of neuronal fn Although T cells do not generally penetrate the parenchyma in non-inflammatory conditions, they can release soluble cytokines that affect CNS function [14],[15] . Studies in mice have demonstrated that adaptive immunity is necessary for cognitive performance: mice with severe combined immune deficiency exhibited impaired spatial learning and memory, but symptoms were reversed by injection of exogenous T cells 16 . Similar observations have been made for social behaviour14. From a neuro-biological perspective, levels of neurogenesis in adult transgenic mice that overexpress a CNS-specific T cell receptor are higher than those in mice that overexpress a T cell receptor for a non-CNS-specific antigen [17] . The advantageous effects of self-reactive T cells on the maintenance of neuronal function have led to the concept of protective autoimmunity , in which adaptive immune cell function maintains tissue function. Further beneficial roles of the immune response in the CNS \u2014 again going against the anticipated pathogenic activity of inflamma-tion \u2014 include protective roles for T helper 2 (TH2) cells in CNS injury and for IL-4 signalling in neurons in mul-tiple sclerosis (MS) models [18], 19 . The complex and diverse roles of adaptive immune cells are further underlined by the finding that a lack of B cells does not impair learning behaviour in mice [20] . drainage system glymphatic system (glial-dependent perivascular network) meningeal lymphatic system The interplay between the CNS and the periphery is mediated by two drainage systems. The first is the glymphatic system , a glial-dependent perivascular network that ensures provision of nutrients for neurons and glia and clearance of extracellular metabolites, such as lactate [21] . Activation of the glymphatic system is higher during sleep than during wakefulness, underlining the impor-tance of sleep for removal of potentially neurotoxic waste products 22 . The second is the meningeal lymphatic system that lines the dural sinuses and allows drainage from the CNS to deep cervical lymph nodes, the discovery of which established the missing link to the peripheral immune system 23 ,[24] . Via this system, molecules and immune cells from the CNS can be transported to lymphoid organs and evoke an immune response that involves subsequent migration of immune cells to the brain [25] .","title":"02. Neuroimmune interplay in brain health"},{"location":"190604_PapeKatrin_2019/#03_cognitive_performance_in_inflammation","text":"","title":"03. Cognitive performance in inflammation"},{"location":"190604_PapeKatrin_2019/#0300","text":"As described above, innate and adaptive immunity are essential for CNS homeostasis. Consequently, distur-bances in the equilibrium of immune cells, neurons and glial cells needed for healthy CNS function are likely to modify cognitive performance ( FIG. 1a ). In this section, we consider the evidence that low-grade inflammation and inflammation in response to infection can alter cognitive performance. Recognition of pathogen-associated or damage- associated molecular patterns and subsequent phago-cytosis or cytokine production by microglia or invad-ing macrophages, representing the innate immune response in the CNS, has to be distinguished from the antigen-specific responsiveness of adaptive immune cells.","title":"03.00."},{"location":"190604_PapeKatrin_2019/#fig_1_the_interplay_between_the_immune_system_and_the_cns","text":"a | A certain level of inflammation and autoimmunity is necessary for optimal function of the CNS, but an overwhelming immune reaction leads to neuronal loss and impaired cognition. b | Immune cell infiltration into the CNS in disease. Glial cells, including microglia and astrocytes, are resident in the CNS. Lymphocytes and non-microglia myeloid cells are both thought to be present in the lymphatic vessels in health but might transmigrate through the blood\u2013brain barrier in inflammatory diseases.","title":"Fig. 1 | The interplay between the immune system and the CNS."},{"location":"190604_PapeKatrin_2019/#0301_low-grade_inflammaiton","text":"sickness behavior euflammation A compelling example of the pathogenic influence of the immune system on brain function is the change in mood, social behaviour and cognitive abilities \u2014 known as sickness behaviour \u2014 upon infection and systemic inflammation. The release of pro-inflammatory cytokines, such as IL-1\u03b2, IL-6 and TNF, outside the CNS affects the brain via neural (mainly vagal) pathways, interaction with cytokine receptors on cerebral endothelial cells and/or microglial activation 26 . The behavioural effects, such as social with-drawal and fatigue, are assumed to be adaptive responses that increase survival of the host 27 . Similarly, treatment with the cytokine IFN\u03b2, for MS or chronic viral hepa-titis, for example, has been associated with depressive symptoms as an adverse effect [28] . Sickness behaviour can be diminished after repeated subthreshold exposure to pathogens, a mechanism that is referred to as euflammation and that evidence suggests is the result of tolerogenic processes [29] . inflammaging In contrast to this transient low-grade inflammation, chronic low-grade inflammation can occur and cause neurotoxicity and neurodegeneration [30] . For example, cytokines can have neurotoxic effects by increasing pro-duction of reactive oxygen species, reducing monoamine transmission and potentiating glutamatergic transmis-sion. Chronic, sterile, low-grade inflammation occurs during human ageing and can contribute to age-related diseases; this process is referred to as inflammaging 31 . Chronic inflammation is implicated in schizophrenia and other psychiatric disorders32, and systemic inflammation, as well as acute infections, have been associated with an increased rate of cognitive decline and exacerbation of symptoms in patients with Alzheimer disease (AD) [33] . Chronic low-grade inflammation is also present in people who are obese [34] , and this inflammation influences cognitive performance by damaging neuronal circuits and the BBB and by activating pro-inflammatory immune cells [35] . These obesity-related mechanisms were initially observed in the hypothalamus, but some evidence sug-gests that they can also occur in the hippocampus, cortex, brainstem and amygdala [36] . beneficial euflammation sickness behavior adverse chronic inflammation \u21d2 neuronal damage cognitive decline dementia \u2234 right balance of inflammation Overall, in the interplay between the brain and the immune system, inflammation is a double-edged sword. Beneficial effects such as euflammation and evolution-arily advantageous sickness behaviour are on one side, but chronic inflammation that leads to neuronal dam-age, cognitive decline and possibly dementia is on the other side. The right balance of inflammation is needed for optimal CNS function.","title":"03.01. Low-grade inflammaiton"},{"location":"190604_PapeKatrin_2019/#0302_infection-related_inflammation","text":"Sydenham chorea paediatric autoimmune neuropsychiatric disorders associated with S. pyogenes ( PANDAS ) Infection can initiate chronic inflammation that alters cognitive function. Data from a Danish longitudinal register showed that prior hospitalization for an autoimmune disease or infection increased the risk of a major mood disorder by 45% and 62%, respectively [37] . In rodents, even infection and systemic inflammation in the fetus during the prenatal or perinatal period can cause long-term cognitive damage, including learning, memory and attention abnormalities, a model that explains why early infection increases the risk of psychosis in young adulthood [38],[39] . Similarly, a viral infection in the mother during the first trimester of pregnancy and bacterial infection during the second trimester of pregnancy were associated with the development of autism spectrum disorder (ASD), a pervasive neurodevelopmental disorder defined by impairments in social skills and stereotypical behaviour, among the children in a large Danish cohort [40] . In children, throat infection with Streptococcus pyogenes has repeatedly been associated with subsequent neuropsychiatric disorders [41] . In particular, these disorders include Sydenham chorea , a movement disorder related to rheumatic fever that occurs in temporal relationship to group A streptococcal infection [42] , and conditions encompassed by the umbrella term of paediatric autoimmune neuropsychiatric disorders associated with S. pyogenes ( PANDAS ) and concomitant with obsessive\u2013compulsive disorder, tic disorder or choreiform motoric hyperactivity [43] . The suspected pathophysiology of PANDAS is cross reactivity of anti-streptococcus A antibodies with brain tissue due to molecular mimicry [44] that initiates an adaptive immune cell response. Nevertheless, in a prospective study of children with post-streptococcal neuropsy-chiatric symptoms, no correlation was seen between clinical symptoms and a change in autoimmune markers, such as anti-neuronal antibodies or inflam matory cytokines [45] . Therefore, despite confirmation on an epide miological level, the aetiological association between streptococcal infection and psychiatric symptoms is still controversial and the subject of an ongoing, large observational study [46] . Severe infections, such as herpes simplex virus (HSV) encephalitis at any time in the lifespan, have been associated with long-term impairment of memory and brain atrophy [47] . Herpes family viruses persist in the host and require constant immune surveillance in order to prevent reactivation. For example, latent cytomegalovirus (CMV) infection leads to accumulation of functionally exhausted effector T cells while the naive T cell pool is diminished [48] . In the prospective Northern Manhattan Study, a high infectious burden (assessed with serological markers for Chlamydia pneumoniae , Helicobacter pylori , CMV, HSV-1 and HSV-2) was associated with cognitive decline independently of cardiovascular risk among a cohort of 1,625 patients [49] . Evidence suggests that among patients with schizophrenia or bipolar disorder, infections with Toxoplasma gondii, HSV and CMV affect the cognitive dysfunction already present in these disorders, in particular working memory [50] . Another example of the intricate relationship between cognition and chronic infection is HIV-associated neurocognitive disorder, which emerges in patients with HIV infection despite highly active antiretroviral therapy [51] . Our understanding of the mechanistic links between infectious burden and neuropsychiatric diseases is still in its infancy. In addition to molecular mimicry that triggers autoimmune reactions, a possible mecha-nism is stress-induced potentiation of microglial inflammasome activation, which causes an increase in pro-inflammatory mediators [52],[53] . In this way, both innate (microglial activation with subsequent phagocytosis or cytokine production) and adaptive immune responses (antigen-specific responsiveness and production of anti-bodies) contribute to pathology. In addition, infectious agents might act via epigenetic pathways to modulate the innate immune cell repertoire, thereby influencing the risk profile for neurodegenerative disorders [54] . In addition to the cumulative effect of infectious burden, pathogens contribute directly to neurodegen-eration, and therefore cognitive decline, in elderly people. One example that has been investigated extensively but is still incompletely understood is the association between HSV infection and AD. On a population level, data from a Swedish cohort suggest that reactivation of HSV-1, indicated by high serum levels of anti-HSV immunoglobulin M (IgM) antibodies, increases the risk of developing AD [55] . In vitro experiments have shown that HSV-1 induces increases in intracellular amyloid-\u03b2 (A\u03b2) levels and tau phosphorylation, both of which are markers of AD [56] . In addition, HSV-1 DNA has been found in amyloid plaques in patients with AD, further underlining a link between the virus and pathological aggregations [57] . Virus-mediated cytotoxicity does not explain all clinical developments in AD, but viruses and amyloid plaques are thought to alter the neuroimmune crosstalk in various ways. For example, HSV-1 proteins debilitate neuronal autophagy and, consequently, antigen presentation [58] . In mice, recurrent asymptomatic activation of HSV-1 leads to upregulation of markers of neuroinflammation (for example, Toll-like recep-tor-4, IFN\u03b1 and IFN\u03b2) and early neurodegeneration [59] . In addition, extracellular deposits of A\u03b2 activate resting microglia and trigger them to attack neurons via various pathways, such as NADPH oxidase activation, inducible nitric oxide synthase expression and phagocytosis [60] . In summary, owing to the sensitive equilibrium of the immune system and brain function, infectious agents might tip the scales towards pathology. Furthermore, infections can trigger autoimmune phenomena that might contribute to a broad spectrum of neuropsychiatric diseases, discussed in more detail below.","title":"03.02. Infection-related inflammation"},{"location":"190604_PapeKatrin_2019/#04_autoimmunity","text":"","title":"04. Autoimmunity"},{"location":"190604_PapeKatrin_2019/#0400","text":"Nonspecific inflammatory processes underlie the low-grade inflammation described above, but neuro-inflammation can also be driven by autoimmunity. Autoimmune responses can be antibody driven or cell driven and, depending on the autoantigen involved, can involve antigen presentation by major histocompatibility complex (MHC) class II. Autoimmune responses can follow infection, and molecular mimicry can underlie these responses [41], 61 . Innate and adaptive immune mecha nisms have roles in autoimmunity. What we know about autoimmune responses and neuro psychiatric disorders is largely based on specific disorders, discussed in detail below.","title":"04.00."},{"location":"190604_PapeKatrin_2019/#0401_lessons_from_multiple_sclerosis","text":"Neurological autoimmune diseases are mainly considered to be mediated by autoaggressive lymphocytes or anti-neuronal or anti-glial autoantibodies. These conditions include neuro myelitis optica and potentially disabling, or sometimes life-threatening, autoimmune encephalomyelitis. MS is the classical and most common chronic autoimmune encephalomyelitis; this disease leads to demyelination and progressive neurodegeneration. In early MS, most patients experience transient physical deficits, such as optic neuritis, hemiparesis or sensory disturbances, in a relapsing\u2013remitting presentation. Although these physical deficits are the most obvious consequences, an increasing body of evidence illustrates that cognitive deficits are abundant and extensive in MS [62] . Patients with MS can experience neuropsychological symptoms, such as mild cognitive alterations or depression, related not only to relapses but also to phases of remission [63] . The most common cognitive deficits, which occur in 40\u201360% of patients with MS, are reduced processing speed and impaired memory and/or executive function, and these symptoms can improve to some extent with immunomodulatory treatment [64] . Behavioural signs, such as anxiety, can also occur and are currently seen as comorbidities but might be a consequence of the pathology [65] . Notably, cognitive impairment at the time of MS diagnosis predicts disability progression, transition to secondary progressive MS and cortical thinning 66 . In line with these findings, the risk of cognitive impairment increases with progression of disease and is high-est in secondary progressive MS [67] . Nevertheless, 30\u201340% of patients with clinically isolated syndrome, which is considered to be a precursor of MS, have cognitive deficits [68],[69] , and in a study of so-called benign MS (defined by an Expanded Disability Status Scale (EDSS) score \u22643.0 despite a disease duration \u226515 years), up to 45% of patients had cognitive impairment [70] . Inflammation in MS, the local effects of which include microglial activation and myelin and neuronal damage, is thought to be primarily mediated by auto reactive T cells. Pro-inflammatory T H 17 cells are thought to migrate into the CNS and subsequently increase BBB permeability, enabling invasion of other immune cells [71] ( FIG. 1b ). Other important players include regulatory T cells, which are functionally impaired in people with MS [72] , and B cells, the role of which is highlighted by the effects of B cell-depleting therapies [73] . Relapses in patients with MS are influenced by stressful life events and by infections: a systematic meta-analysis has shown that upper respiratory tract infections have the most pronounced effect on relapse rates 74 . These observations underline the role of inflammation in relapse activity, as both systemic infection and stress induce a pro-inflammatory status, as discussed in more detail below. A specific autoantigen has not yet been identified in MS, but autoreactive T cells are widely assumed to target proteins of the myelin sheath, causing demyelination and, thereby, white matter damage. Demyelination deprives neurons of protective factors and has been described to decrease axonal transport and synaptic density in demyelinated hippocampi from post-mortem MS brains [75] . However, direct recognition of neurons by patho genic T cells has also been proposed [76],[77] , and neuronal loss could occur as a result of Wallerian degene ration and mitochondrial dysfunction [78] . Grey matter damage, measured as the cortical lesion load or cortical thinning, is a key predictor of progressive disease and cognitive decline [79] . Even in clinically stable patients, the presence of gadolinium-enhancing lesions on brain MRI is associated with impaired performance in the Paced Auditory Serial Addition Test (PASAT) , a screening tool for cognitive dysfunction [80] . These findings underline the fact that active neuroinflammation detectable with gadoli nium enhancement affects cognitive function and might cause detrimental effects on neuronal networks and connectivity. Although rare, psychotic symptoms can also occur in MS. A genetic overlap between MS and schizophrenia has been identified, and the genes that are common to both conditions are immune related 81 . Despite being separate disease entities, MS and classical psychiatric disorders are accompanied by higher-order network disturbances 82 , 83 ,[84],[85] ( FIG. 2a ). The study of MS has provided fundamental insights into the crosstalk between the immune system and the nervous system, and these insights will be highly valuable for the study of other diseases in which underlying immune pathology has recently been identified, discussed below.","title":"04.01. Lessons from multiple sclerosis"},{"location":"190604_PapeKatrin_2019/#fig_2_overlap_of_neuropsychiatric_disorders","text":"a | Higher-order network disturbances are caused by common pathophysiological mechanisms of immune dysregulation (top). How clinically distinct phenotypes emerge from these pathways is an unresolved question (bottom). b | As pathways emerge that are common to neurological and psychiatric autoimmune brain diseases, the traditional boundaries between these disorders become blurred. Identification of subgroups of patients who are likely to benefit from immunotherapeutic approaches is crucial. CRP, C-reactive protein.","title":"Fig. 2 | Overlap of neuropsychiatric disorders."},{"location":"190604_PapeKatrin_2019/#0402_lessons_from_autoimmune_psychosis","text":"HSV encephalitis autoAb against: GABA A R AMPA (subunit GluA2) A compelling example of the overlap between psychiatric and neuro-logical pathologies is the occurrence of psychosis as a result of autoantibodies against neuropil, which can develop spontaneously in neoplastic diseases or after viral infections, particularly HSV encephalitis [86] . Some of these autoantibodies target intracellular antigens, such as onconeural proteins or 65 kDa glutamic acid decarboxylase ( GAD65 ). The presence of these antibodies is diagnostically useful but considered to be an epiphenomenon , as cytotoxic T cells are responsible for neuronal damage that is associated with neuropsy-chiatric symptoms 87 . However, a broad spectrum of extracellular antigens involved in synaptic transmission and plasticity are located on the cell surface; therefore, autoantibodies against these antigens that are present in patients with psychosis seem highly likely to have a direct pathogenic role. One example of such an anti-body is the autoantibody against the GABA A receptor , a postsynaptic chloride channel that mediates fast inhibitory neurotransmission in the mammalian brain; this autoantibody causes encephalitis with therapy-resistant epileptic seizures [88] . Moreover, antibodies against the \u03b1-amino-3-hydroxy-5-methyl-4-isoxazole propionic acid ( AMPA ) receptor can have a pathological role: administration of human pathogenic antibodies against the AMPA receptor subunit GluA2 impairs long-term synaptic plasticity in vitro and affects learning and memory in mice in vivo [89] . NMDA More controversial is the importance of antibodies against the N-methyl- D -aspartate ( NMDA ) glutamate receptor, one of the most commonly observed antigens in autoimmune encephalitis. Patients with this condition often present with anxiety, sleep disorders, mania, paranoia, memory impairment and disintegration of language, followed by a phase in which agitation and catatonia alternate, accompanied by abnormal movements and autonomic instability [90] . In humans, antibody titres in the CSF correlate with the clinical course of disease [90] . Experiments in rats indicate a pathophysiological role for anti-NMDA receptor antibodies. For example, in rat hippocampal neurons that have been incubated with anti-NMDA receptor antibodies, NMDA receptor internalization and reduced NMDA receptor-mediated synaptic currents were observed [91] . In addition, treatment of rats with NMDA receptor blockers evoked stereotyped behaviour and cataleptic freezing that were comparable to symptoms of autoimmune encephalitis in humans [92] . NMDA receptor encephalitis can occur after HSV encephalitis in up to 27% of patients, indicating a role for molecular mimicry 61 . Furthermore, antibodies that cross-react with the NMDA receptor have been found in patients with neuropsychiatric systemic lupus erythematosus, and these antibodies cause neuronal damage via activated microglia and complement component C1q when administered to mice [93],[94] . Despite this evidence, the pathological role of neurotransmitter receptor autoantibodies came into question when anti-NMDA receptor autoantibodies were detected in serum from ~10% of healthy controls as well as in patients with pure psychotic symptoms [95] . Similar results have been seen for a variety of brain antigens, challenging the pathological relevance of CNS-specific autoantibodies [96] ; BBB integrity has been proposed as a pivotal factor in the clinical outcome [95] . In another study, anti-NMDA receptor autoantibodies from psychotic patients altered synaptic transmission and long-term potentiation in cultured neurons and in mouse brain whereas those from healthy controls did not 97 . Independent of immune pathogenesis, glutamate receptor hypofunction has been proposed as a key factor in the development of schizophrenia [98] . In combination, these results provide evidence that specific and different mechanisms underlie encephalitis and autoimmune psychosis, in which autoantibodies against NMDA receptors have different effects on the organization of the glutamate synapse through receptor internalization or abnormal NMDA receptor dynamics. Given that patients with schizophrenia who are positive for anti-NMDA receptor antibodies do not exhibit classical signs of encephalitis 97 , we conclude that different immune alterations mediated by similar antibodies result in different neuropsychiatric entities.","title":"04.02. Lessons from autoimmune psychosis"},{"location":"190604_PapeKatrin_2019/#05_immunopsychiatry_-_an_emerging_field","text":"On the basis of the data discussed above, the concept of autoimmune psychosis has become a compelling example of the interface between neurological (autoimmune encephalitis) and psychiatric (psychosis) disorders. In this section, we discuss accumulating evidence that suggests that immune dysregulation is involved in a broad spectrum of psychiatric diseases. Owing to a lack of specific markers, studies in the emerging field of immunopsychiatry currently focus on systemic measures of inflammation. A growing number of studies have identified nonspecific inflammatory disturbances in subgroups of patients , but knowledge of specific pathogenic pathways remains scarce. One overarching concept is that immunopsychiatric diseases involve a generally overactivated immune system. In general, overactivation of the immune system is thought to increase brain vulnerability, increasing the risk of psychiatric symptoms upon a so-called second hit later in life 99 . On an epidemiological level, this hypothesis is supported by a link between psychiatric diseases, such as schizophrenia, depression and anxiety, and systemic autoimmune disorders [100],[101] . Case reports similarly support this hypothesis; one striking example is the development of severe psychosis in an individual who received a stem cell transplantation from their brother with schizophrenia, indicating that adoptive immune transfer had a role [102] . Furthermore, extensive studies of cytokine profiles in disorders such as schizophrenia, depression, suicidal ideation and post-traumatic stress disorder reveal wide-ranging dysregulation of mainly pro-inflammatory markers, in particular IL-6, IL-2 receptor, IL-1\u03b2, IL-17 A and C-reactive protein (CRP) [103]\u2013[107] . Several studies identified a positive correlation of serum levels of inflammatory markers with disease severity and a negative correlation with cognitive performance [103],[108] . Changes in the innate CNS immune compartment have also been associated with psychiatric disorders. Either excessive upregulation or downregulation of microglia function evokes detrimental effects. For example, genetic defects in microglial signalling pathways cause abnormal development of brain circuits and neuro-psychiatric symptoms, as in hereditary diffuse leukoencephalopathy with spheroids (HDLS) [109] . Disruption of microglial function in later life can be initiated by psycho social and environmental factors. For example, in a mouse model, chronic stress with depressive behaviour has been associated with microglial loss in the hippocampus [110] . By contrast, depression and schizophrenia have been associated with increased microglial activity measured with PET, and this increased activity could be used to identify patients at high risk of disease exacerbations [111],[112] . Although microglial activation in humans has been studied only indirectly, early life challenges are assumed to \u2018prime\u2019 microglia and increase their response to subsequent inflammatory stimuli [113] . A crucial discovery that many immune factors are influential prenatally led to use of the so-called maternal immune activation model in order to study ASD and schizophrenia in rodents. In this model, IL-6 was identified as a key mediator of inflammatory effects on fetal brain development [114] . In mice, autism-like behaviour in offspring requires the presence of mater-nal ROR\u03b3t-positive TH17 cells and IL-17a downstream of IL-6 (REF.[115]). In subsequent studies, a mechanism of ASD pathogenesis has been proposed in which T H 17-dependent loss of inhibitory interneuron networks leads to increased cortical activation in the primary somatosensory cortex [116] . A study published in 2017 provides evidence that maternal gut commensal bacteria have a role in IL-17a production in mothers [117] . These findings support the idea of a gut\u2013immune\u2013brain axis that has been implicated in classical autoimmunity (such as modulation of the balance between pro-inflammatory and regulatory T cells by gut bacteria in MS [118] ) and are in line with previous reports of an altered intestinal barrier in people with ASD and their close relatives [119] . In rodents, the role of maternal immune activation in ASD was confirmed at the transcriptome level, in which immune activation in the mother caused dysregulation in the fetal brain expression of genes involved in developmental processes [120] . Although animal studies provide the proof of concept and agree with findings in diseases of neurological immune dysfunction, such as MS, the mechanisms described are plausible in humans but remain speculative. Cytokine signalling is not the only aspect of maternal immunity that affects the developing fetal brain \u2014 B cell-mediated immune responses and production of antibodies can also have effects. For example, in an epidemio logical study, antibodies against fetal brain tissue were detected in the serum of mothers of children with ASD but not in mothers of healthy children [121] . Another study produced the same observation in up to 23% of mothers of children with ASD [122] . In animal studies, intravenous administration of 73 kDa and 37 kDa IgG from mothers of children with ASD to pregnant rhesus macaques led to abnormal social behaviour in the macaque offspring [123] . Targets for these antibodies include proteins with functions in neurodevelopment, such as contactin-associated protein-like 2 (CASPR2) [122],[124] . Although the exact mecha nism by which these maternal antibodies affect the fetus is not yet understood, the findings in humans and the adoptive transfer study suggest that placental transfer of maternal antibodies contributes to disease development, at least in a subgroup of patients. Aberrant genetic regulation underlies most of the immune dysfunction discussed above, and a polygenic contribution to the risk of several severe psychiatric disorders has been confirmed in a meta-analysis of genome-wide association studies in which the MHC was the most relevant shared risk locus [125] . Genes in the MHC are involved in physiological processes, including CNS development and homeostasis, but are common among neuropsychiatric disease-associated loci [126],[127] . For example, the MHC-resident complement C4 cluster has been implicated in the risk of schizophrenia [128],[129] . Therefore, fine dissection of the genetic contribution of the MHC is a promising approach to increasing our understanding of brain disorders. Although we are only beginning to develop a deeper, mechanistic understanding of immunological pathways involved in the pathogenesis of psychiatric disorders, the emerging field of immunopsychiatry provides new perspectives on the disorders discussed. As even early evolutionary processes such as the integration of sequences of human endogenous retroviruses into the genome presumably through repeated infections were found to be associated with neurological and psychiatric disorders [130],[131] , it becomes clear that dysfunctional processes occur on an ancestral, maternal and individual level.","title":"05. Immunopsychiatry - an emerging field"},{"location":"190604_PapeKatrin_2019/#06_immunomodulatory_treatment","text":"Classical neurological autoimmune diseases have long been treated with immunomodulatory drugs. Routinely used treatments for acute exacerbations include steroids, plasmapheresis, intravenous immunoglobulin, cyclo-phosphamide and B cell-depleting monoclonal anti-bodies; a broadening range of disease-modifying drugs is emerging from the development of therapies for MS (BOX 2). Immunosuppression is also an established treat-ment strategy for autoimmune encephalitis, but clinical studies of such treatments for psychiatric diseases are still in progress. In a pilot study published in 2018, treatment with high-dose intravenous immunoglobulin had positive effects on scores in several cognitive and behavioural tests for children with ASD and evidence of a dysregu-lated immune system132. In another study, intravenous immunoglobulin had beneficial effects for subgroups of children with ASD and inflammation133. A series of case reports of immune modulatory treatment in PANDAS disorders demonstrates how an understanding that auto-inflammatory processes are involved in the patho-genesis can lead to successful treatment134. However, syste matic treatment approaches are still lacking and are currently merely performed with antibiotics. Currently, monoclonal antibody treatments are being trialled in schizophrenia. In one study, use of the mono clonal antibody natalizumab, which targets the cell adhesion molecule \u03b14 integrin and is currently used for treatment of MS, is being tested135. The recombinant humanized anti-human IL-6 receptor monoclonal anti-body tocilizumab is being tested in two136,137 ongoing studies, one of which136 focuses on patients with signs of peripheral inflammation. In one case report, remis-sion of treatment-resistant schizophrenia was seen in a patient who underwent bone marrow transplantation for acute myeloid leukaemia, providing evidence that this treatment could be an effective, or even curative, option for severe schizophrenia138. Similarly, infusion of autologous umbilical cord blood into children with ASD led to clinical improvement of core autistic symptoms139. Evidence that immune dysfunction is involved in depression is supported by accumulating data that sug-gest that established antidepressant drugs such as selec-tive serotonin reuptake inhibitors (SSRIs) exert their effects, at least partly, via anti-inflammatory effects. For example, a meta-analysis showed that SSRIs reduce levels of IL-1\u03b2 and IL-6 in patients with depression140. In line with these observations, synthetic metabolites of tryptophan, a precursor of serotonin, suppressed pro-inflammatory T cells and autoimmune neuro-inflammation in experimental autoimmune encephalitis, a mouse model of MS141. Additional support for the con-cept that immune pathology underlies depression comes from a meta-analysis of randomized clinical trials that showed that, despite heterogeneous study results, non-steroidal anti-inflammatory drugs had an overall positive effect on depression142. In rats, the microglia inhibitor minocycline improved symptoms of depression in the forced swimming test143, an observation that is consistent with reports that microglia are overactivated in depres-sion112. A large randomized trial of minocycline has not been done in humans, but a meta-analysis of existing clinical data indicates beneficial effects of mino cycline in patients with depression, thereby providing a proof of concept144. Minocycline also had positive effects as an adjunctive treatment to risperidone in children with ASD145, although not as a single medication in a small pilot study146. In addition to its inhibitory effects on microglia, minocycline has antibiotic properties that might modify the gut flora, which could contri bute to beneficial treatment outcomes. Furthermore, in depres-sion, anti-inflammatory treatment with the TNF antago-nist infliximab had beneficial effects in patients with treatment-resistant depression and high baseline levels of inflammatory biomarkers147. In summary, immunomodulatory therapies are effec-tive options for a range of neurological and psychiatric diseases. To develop individualized therapeutic strate-gies, subgroups of patients with psychiatric disorders and signs of immune dysregulation must be identified ( FIG. 2b ). Clinical studies are more likely to reveal con-sistent positive effects if they include patients with an inflammatory phenotype, indicated by MRI or high levels of inflammation markers in the CSF or serum. Furthermore, we assume that there is a narrow thera-peutic time window for maximal therapeutic efficacy and avoidance of irreversible CNS injury. For example, timely immunomodulatory treatment in autoimmune encephalitis probably prevents neuronal damage and permanent neurological deficits148, and this concept might hold true for other neuropsychiatric disorders.","title":"06. Immunomodulatory treatment"},{"location":"190604_PapeKatrin_2019/#box_02_immunomodulatory_drugs_available_for_the_treatment_of_cns_autoimmune_disorders","text":"Immunomodulatory drugs and treatments applied in a broad range of autoimmune diseases Steroids Plasmapheresis Intravenous immunoglobulin Cyclophosphamide Methotrexate Mycophenolate mofetil Azathioprin Mitoxantrone Rituximab Ocrelizumab Infliximab Disease-modifying drugs from the range of therapeutics for multiple sclerosis IFN\u03b2 Glatiramer acetate Teriflunomide Dimethyl fumarate Fingolimod Natalizumab Alemtuzumab Cladribine Rituximab Ocrelizumab Other drugs with partial immunomodulatory effects Nonsteroidal anti-inflammatory drugs Selective serotonin reuptake inhibitors Minocycline Tryptophan metabolites","title":"Box 02. Immunomodulatory drugs available for the treatment of CNS autoimmune disorders"},{"location":"190604_PapeKatrin_2019/#07_resilience_and_reserve","text":"A well-recognized risk factor for exacerbation in MS and many psychiatric disorders is psychosocial stress. Some evidence suggests that the association between psychiatric disorders and stress is mediated, at least in part, by neuroinflammatory processes, including microglial activation. For example, the \u2018two-hit hypo-thesis\u2019 proposes that stress in early life increases base-line microglial activity, thereby increasing the risk of psychiatric disorders upon subsequent challenges later in life149,150. Accordingly, a meta-analysis of clinical stud-ies demonstrated a pro-inflammatory status (high levels of CRP, IL-6 and TNF) in adults with a history of child-hood trauma151. Similarly, in a prospective cohort study, patients with a history of childhood maltreatment and depression had higher levels of CRP than participants with depression only152. The question remains, however, of why some indi-viduals are more resilient to stressful events than others. In a mouse model of chronic social defeat stress, BBB integrity was altered in the nucleus accumbens owing to downregulation of the tight junction protein claudin 5, thereby allowing influx of IL-6 into the brain paren-chyma153. Importantly, however, loss of claudin 5 was observed only in stress-susceptible mice \u2014 susceptibil-ity to stress was observed as social avoidance behaviour after several days of exposure to social defeat by a larger, physically aggressive mouse; by contrast, this behaviour was not seen in mice classified as resilient. Epigenetic modulation of IL-6 seems to further promote resilience in mice, as an inhibitor of IL-6 DNA methylation that decreased IL-6 production reduced depressive symptoms in a mouse model154. Furthermore, adaptive immune cells are needed for coping with stress. For example, exposure of mice to acute psychological stress in the form of predator odour increases T cell trafficking to the brain155, and T cell-deficient mice displayed more signs of maladaptation after stress156. In addition, analogous to findings for neurogenesis discussed above, self-reactive, brain-specific T cells are needed for resilience in mice, leading to the idea that vaccination with CNS-related peptides could stimulate coping behaviour157. From studies of chronic autoimmune inflammation of the CNS in MS, we know that the brain has reserve capacities30. Our recent studies indicate that the cortical network responds to an inflammatory attack with a TNF-dependent increase in cortical synaptic strength65. This upregulation of cortical neuronal activity presum-ably reflects repair mechanisms initially but eventually causes maladaptive development of anxiety behaviour despite otherwise complete remission in disability. Analysis of higher-order networks in patients who progress rapidly and those who do not could increase our understanding of brain reserve and identify future therapeutic approaches.","title":"07. Resilience and reserve"},{"location":"190604_PapeKatrin_2019/#08_new_clinical_approach","text":"","title":"08. New clinical approach"},{"location":"190604_PapeKatrin_2019/#fig_3_a_proposed_clinical_pathway_for_patients_with_new-onset_neuropsychiatric_symptoms","text":"Blue boxes indicate investigations required and orange boxes indicate diagnoses. CMV, cytomegalovirus; CRP, C-reactive protein; CSF, cerebrospinal fluid; ESR , erythrocyte sedimentation rate; HSV, herpes simplex virus; MOG, myelin oligodendrocyte glycoprotein; NMOSD, neuromyelitis optica spectrum disorder ; OCB, oligoclonal bands; VZV, varicella zoster virus.","title":"Fig. 3 | A proposed clinical pathway for patients with new-onset neuropsychiatric symptoms."},{"location":"190604_PapeKatrin_2019/#09_conclusion","text":"For decades, or even centuries, of medical history, neuro-psychiatric diseases have been diagnosed and classified on the basis of phenomenological criteria. The findings discussed in this Review provide novel perspectives and indicate the need for a future classification of neuropsy-chiatric disorders on the basis of pathogenic mechanisms, at least for subgroups of patients. In this context, diag-nostic algorithms should classify patients into inflamma-tory and non-inflammatory subgroups to help reach an individualized treatment decision. For example, future understanding of neuropsychiatric diseases might no longer divide them into descriptive entities, such as MS, neuromyelitis optica spectrum disorder, auto immune encephalitis, schizophrenia, ASD and bipolar dis order, but into pathogenic entities, such as autoimmune antibody-mediated or T cell-mediated brain disorders. Although the knowledge of immune involvement in neuropsychiatric disorders has emerged over several years, necessary changes to the clinical management of patients with these disorders have scarcely been imple-mented. Furthermore, many questions remain unsolved ( BOX 3 ), such as why apparently similar mechanisms lead to clinically distinct disorders and whether immunolog-ical disturbances are always a cause of certain disorders or sometimes a consequence. The next steps will require combined experience from neurologists and psychiatrists in a new field of immunoneuropsychiatry.","title":"09. Conclusion"},{"location":"190604_PapeKatrin_2019/#box_03_unresoloved_questions_to_direct_future_reseach","text":"Which pathways mediate the interplay between levels of inflammation and cognitive performance? Why do apparently similar immunological mechanisms lead to clinically distinct disorders? What are the precise mechanisms of interaction between immune cells and higher-order networks in neuropsychiatric diseases? How can this crosstalk be addressed therapeutically? Are immunological disturbances the cause or consequence of neuropsychiatric diseases? How can we better identify subgroups of patients who are likely to benefit from immunotherapies?","title":"Box 03. Unresoloved questions to direct future reseach"},{"location":"190604_PapeKatrin_2019/#10_references","text":"[Nimmerjahn, A., Kirchhoff, F. & Helmchen, F. Resting microglial cells are highly dynamic surveillants of brain parenchyma in vivo. Science 308, 1314\u20131318 (2005).][1] [Stevens, B. et al. The classical complement cascade mediates CNS synapse elimination. Cell 131, 1164\u20131178 (2007).][2] [Paolicelli, R. C. et al. Synaptic pruning by microglia is necessary for normal brain development. Science 333, 1456\u20131458 (2011).][3] Sanchez-Alcaniz, J. A. et al. Cxcr7 controls neuronal migration by regulating chemokine responsiveness. Neuron 69, 77\u201390 (2011). [Wake, H., Moorhouse, A. J., Jinno, S., Kohsaka, S. & Nabekura, J. Resting microglia directly monitor the functional state of synapses in vivo and determine the fate of ischemic terminals. J. Neurosci. 29, 3974\u20133980 (2009).][5] [Pocock, J. M. & Kettenmann, H. Neurotransmitter receptors on microglia. Trends Neurosci. 30, 527\u2013535 (2007).][6] [Antonucci, F. et al. Microvesicles released from microglia stimulate synaptic activity via enhanced sphingolipid metabolism. EMBO J. 31, 1231\u20131240 (2012).][7] [Beattie, E. C. et al. Control of synaptic strength by glial TNF\u03b1. Science 295, 2282\u20132285 (2002).][8] Gertig, U. & Hanisch, U. K. Microglial diversity by responses and responders. Front. Cell Neurosci. 8, 101 (2014). Ellwardt, E., Walsh, J. T., Kipnis, J. & Zipp, F. Understanding the role of T cells in CNS homeostasis. Trends Immunol. 37, 154\u2013165 (2016). [Engelhardt, B. & Ransohoff, R. M. Capture, crawl, cross: the T cell code to breach the blood-brain barriers. Trends Immunol. 33, 579\u2013589 (2012).][11] [Schwartz, M. & Deczkowska, A. Neurological disease as a failure of brain-immune crosstalk: the multiple faces of neuroinflammation. Trends Immunol. 37, 668\u2013679 (2016).][12] Strominger, I. et al. The choroid plexus functions as a niche for T-cell stimulation within the central nervous system. Front. Immunol. 9, 1066 (2018). [Filiano, A. J. et al. Unexpected role of interferon- gamma in regulating neuronal connectivity and social behaviour. Nature 535, 425\u2013429 (2016).][14] [Derecki, N. C. et al. Regulation of learning and memory by meningeal immunity: a key role for IL-4. J. Exp. Med. 207, 1067\u20131080 (2010).][15] Kipnis, J., Cohen, H., Cardon, M., Ziv, Y. & Schwartz, M. T cell deficiency leads to cognitive dysfunction: implications for therapeutic vaccination for schizophrenia and other psychiatric conditions. Proc. Natl Acad. Sci. USA 101, 8180\u20138185 (2004). This study shows that a lack of mature T cells causes cognitive and behavioural impairment in mice, underlining the role of adaptive immunity for normal CNS function. [Ziv, Y. et al. Immune cells contribute to the maintenance of neurogenesis and spatial learning abilities in adulthood. Nat. Neurosci. 9, 268\u2013275 (2006).][17] [Walsh, J. T. et al. MHCII-independent CD4+ T cells protect injured CNS neurons via IL-4. J. Clin. Invest. 125, 699\u2013714 (2015).][18] Vogelaar, C. F. et al. Fast direct neuronal signaling via the IL-4 receptor as therapeutic target in neuroinflammation. Sci. Transl Med. 10, eaao2304 (2018). This study reveals a beneficial effect of intrathecal IL-4 treatment on progression in experimental autoimmune encephalomyelitis, thereby highlighting a new pathway of neuroimmune interplay as a potential therapeutic target. [Radjavi, A., Smirnov, I. & Kipnis, J. Brain antigen-reactive CD4+ T cells are sufficient to support learning behavior in mice with limited T cell repertoire. Brain Behav. Immun. 35, 58\u201363 (2014).][20] [Iliff, J. J. et al. A paravascular pathway facilitates CSF flow through the brain parenchyma and the clearance of interstitial solutes, including amyloid beta. Sci. Transl Med. 4, 147ra111 (2012).][21] Xie, L. et al. Sleep drives metabolite clearance from the adult brain. Science 342, 373\u2013377 (2013). Louveau, A. et al. Structural and functional features of central nervous system lymphatic vessels. Nature 523, 337\u2013341 (2015). This article presents the first description of the re-discovery of the meningeal lymphatic system. [Absinta, M. et al. Human and nonhuman primate meninges harbor lymphatic vessels that can be visualized noninvasively by MRI. eLife 6, e29738 (2017).][24] [Traka, M., Podojil, J. R., McCarthy, D. P., Miller, S. D. & Popko, B. Oligodendrocyte death results in immune-mediated CNS demyelination. Nat. Neurosci. 19, 65\u201374 (2016).][25] D\u2019Mello, C. & Swain, M. G. Immune-to-brain communication pathways in inflammation-associated sickness and depression. Curr. Top. Behav. Neurosci. 31, 73\u201394 (2017). Kelley, K. W. et al. Cytokine-induced sickness behavior. Brain Behav. Immun. 17 (Suppl. 1), 112\u2013118 (2003). [Neilley, L. K., Goodin, D. S., Goodkin, D. E. & Hauser, S. L. Side effect profile of interferon beta-1b in MS: results of an open label trial. Neurology 46, 552\u2013554 (1996).][28] [Tarr, A. J., Liu, X., Reed, N. S. & Quan, N. Kinetic characteristics of euflammation: the induction of controlled inflammation without overt sickness behavior. Brain Behav. Immun. 42, 96\u2013108 (2014).]][29] Larochelle, C., Uphaus, T., Prat, A. & Zipp, F. Secondary progression in multiple sclerosis: neuronal exhaustion or distinct pathology? Trends Neurosci. 39, 325\u2013339 (2016). Franceschi, C. et al. Inflamm-aging. An evolutionary perspective on immunosenescence. Ann. NY Acad. Sci. 908, 244\u2013254 (2000). Marrie, R. A. et al. Increased incidence of psychiatric disorders in immune-mediated inflammatory disease. J. Psychosom. Res. 101, 17\u201323 (2017). Perry, V. H., Nicoll, J. A. & Holmes, C. Microglia in neurodegenerative disease. Nat. Rev. Neurol. 6, 193\u2013201 (2010). Lumeng, C. N. & Saltiel, A. R. Inflammatory links between obesity and metabolic disease. J. Clin. Invest. 121, 2111\u20132117 (2011). Stranahan, A. M., Hao, S., Dey, A., Yu, X. & Baban, B. Blood-brain barrier breakdown promotes macrophage infiltration and cognitive impairment in leptin receptor-deficient mice. J. Cereb. Blood Flow Metab. 36, 2108\u20132121 (2016). Guillemot-Legris, O. & Muccioli, G. G. Obesity-induced neuroinflammation: beyond the hypothalamus. Trends Neurosci. 40, 237\u2013253 (2017). Benros, M. E. et al. Autoimmune diseases and severe infections as risk factors for schizophrenia: a 30-year population-based register study. Am. J. Psychiatry 168, 1303\u20131310 (2011). This large Danish longitudinal register shows that both autoimmune disease and prior hospitalization for infection increase the risk of schizophrenia on an epidemiological level. Khandaker, G. M. et al. Inflammation and immunity in schizophrenia: implications for pathophysiology and treatment. Lancet Psychiatry 2, 258\u2013270 (2015). Canetta, S. et al. Elevated maternal C-reactive protein and increased risk of schizophrenia in a national birth cohort. Am. J. Psychiatry 171, 960\u2013968 (2014). Atladottir, H. O. et al. Maternal infection requiring hospitalization during pregnancy and autism spectrum disorders. J. Autism Dev. Disord. 40, 1423\u20131430 (2010). Orlovska, S. et al. Association of streptococcal throat infection with mental disorders: testing key aspects of the PANDAS hypothesis in a nationwide study. JAMA Psychiatry 74, 740\u2013746 (2017). Zomorrodi, A. & Wald, E. R. Sydenham\u2019s chorea in western Pennsylvania. Pediatrics 11 7, e675\u2013e679 (2006). Swedo, S. E. et al. Pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections: clinical description of the first 50 cases. Am. J. Psychiatry 155, 264\u2013271 (1998). Bronze, M. S. & Dale, J. B. Epitopes of streptococcal M proteins that evoke antibodies that cross-react with human brain. J. Immunol. 151, 2820\u20132828 (1993). Singer, H. S., Gause, C., Morris, C. & Lopez, P. Serial immune markers do not correlate with clinical exacerbations in pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections. Pediatrics 121, 1198\u20131205 (2008). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT02889016 (2016). Kapur, N. et al. Herpes simplex encephalitis: long term magnetic resonance imaging and neuropsychological profile. J. Neurol. Neurosurg. Psychiatry 57, 1334\u20131342 (1994). Almanzar, G. et al. Long-term cytomegalovirus infection leads to significant changes in the composition of the CD8+ T cell repertoire, which may be the basis for an imbalance in the cytokine production profile in elderly persons. J. Virol. 79, 3675\u20133683 (2005). Katan, M. et al. Infectious burden and cognitive function: the Northern Manhattan Study. Neurology 80, 1209\u20131215 (2013). This cohort study shows that infectious burden measured as serological exposure to common pathogens was associated with cognitive impairment independent of cardiovascular risk profile. Hamdani, N. et al. Effects of cumulative herpesviridae and Toxoplasma gondii infections on cognitive function in healthy, bipolar, and schizophrenia subjects. J. Clin. Psychiatry 78, e18\u2013e27 (2017). Kamminga, J., Cysique, L. A., Lu, G., Batchelor, J. & Brew, B. J. Validity of cognitive screens for HIV-associated neurocognitive disorder: a systematic review and an informed screen selection guide. Curr. HIV/AIDS Rep. 10, 342\u2013355 (2013). Zhang, C. J. et al. TLR-stimulated IRAKM activates caspase-8 inflammasome in microglia and promotes neuroinflammation. J. Clin. Invest. 128, 5399\u20135412 (2018). Heneka, M. T., McManus, R. M. & Latz, E. Inflammasome signalling in brain function and neurodegenerative disease. Nat. Rev. Neurosci. 19, 610\u2013621 (2018). Wendeln, A. C. et al. Innate immune memory in the brain shapes neurological disease hallmarks. Nature 556, 332\u2013338 (2018). This study shows that peripheral inflammatory stimuli induce differential epigenetic modulation of brain-resident microglia, influencing symptoms in a mouse model of AD; these findings provide a mechanistic link between inflammation, innate immunity and neuropsychiatric disease. Lovheim, H., Gilthorpe, J., Adolfsson, R., Nilsson, L. G. & Elgh, F. Reactivated herpes simplex infection increases the risk of Alzheimer\u2019s disease. Alzheimers Dement. 11 , 593\u2013599 (2015). Fulop, T., Itzhaki, R. F., Balin, B. J., Miklossy, J. & Barron, A. E. Role of microbes in the development of Alzheimer\u2019s disease: state of the art - an International Symposium presented at the 2017 IAGG Congress in San Francisco. Front. Genet. 9, 362 (2018). Wozniak, M. A., Mee, A. P. & Itzhaki, R. F. Herpes simplex virus type 1 DNA is located within Alzheimer\u2019s disease amyloid plaques. J. Pathol. 217, 131\u2013138 (2009). O\u2019Connell, D. & Liang, C. Autophagy interaction with herpes simplex virus type-1 infection. Autophagy 12, 451\u2013459 (2016). Martin, C. et al. Inflammatory and neurodegeneration markers during asymptomatic HSV-1 reactivation. J. Alzheimers Dis. 39, 849\u2013859 (2014). Brown, G. C. & Neher, J. J. Inflammatory neurodegeneration and mechanisms of microglial killing of neurons. Mol. Neurobiol. 41, 242\u2013247 (2010). Armangue, T. et al. Frequency, symptoms, risk factors, and outcomes of autoimmune encephalitis after herpes simplex encephalitis: a prospective observational study and retrospective analysis. Lancet Neurol. 17, 760\u2013772 (2018). This combined prospective observational study and retrospective analysis characterizes the frequency and the natural course of autoimmune encephalitis after herpes simplex encephalitis. Carta, M. G. et al. The risk of bipolar disorders in multiple sclerosis. J. Affect. Disord. 155, 255\u2013260 (2014). Feinstein, A., Magalhaes, S., Richard, J. F., Audet, B. & Moore, C. The link between multiple sclerosis and depression. Nat. Rev. Neurol. 10, 507\u2013517 (2014). Patti, F. Treatment of cognitive impairment in patients with multiple sclerosis. Expert Opin. Investig. Drugs 21, 1679\u20131699 (2012). [Ellwardt, E. et al. Maladaptive cortical hyperactivity upon recovery from experimental autoimmune encephalomyelitis. Nat. Neurosci. 21, 1392\u20131403 (2018). This study identifies the emergence of cortical network hyperexcitability and elevated anxiety in remission after neuroinflammatory attack to the brain; this network instability with anxiety behaviour, also known in patients with MS, represents the early stages of neurodegeneration.][65] Pitteri, M., Romualdi, C., Magliozzi, R., Monaco, S. & Calabrese, M. Cognitive impairment predicts disability progression and cortical thinning in MS: an 8-year study. Mult. Scler. 23, 848\u2013854 (2017). Potagas, C. et al. Cognitive impairment in different MS subtypes and clinically isolated syndromes. J. Neurol. Sci. 267, 100\u2013106 (2008). Ruano, L. et al. Age and disability drive cognitive impairment in multiple sclerosis across disease subtypes. Mult. Scler. 23, 1258\u20131267 (2017). Di Filippo, M., Portaccio, E., Mancini, A. & Calabresi, P. Multiple sclerosis and cognition: synaptic failure and network dysfunction. Nat. Rev. Neurosci. 19, 599\u2013609 (2018). Amato, M. P. et al. Benign multiple sclerosis: cognitive, psychological and social aspects in a clinical cohort. J. Neurol. 253, 1054\u20131059 (2006). Nylander, A. & Hafler, D. A. Multiple sclerosis. J. Clin. Invest. 122, 1180\u20131188 (2012). Viglietta, V., Baecher-Allan, C., Weiner, H. L. & Hafler, D. A. Loss of functional suppression by CD4+CD25+ regulatory T cells in patients with multiple sclerosis. J. Exp. Med. 199, 971\u2013979 (2004). Hauser, S. L. et al. Ocrelizumab versus interferon beta-1a in relapsing multiple sclerosis. N. Engl. J. Med. 376, 221\u2013234 (2017). McKay, K. A., Jahanfar, S., Duggan, T., Tkachuk, S. & Tremlett, H. Factors associated with onset, relapses or progression in multiple sclerosis: a systematic review. Neurotoxicology 61, 189\u2013212 (2017). Dutta, R. et al. Demyelination causes synaptic alterations in hippocampi from multiple sclerosis patients. Ann. Neurol. 69, 445\u2013454 (2011). Liblau, R. S., Gonzalez-Dunia, D., Wiendl, H. & Zipp, F. Neurons as targets for T cells in the nervous system. Trends Neurosci. 36, 315\u2013324 (2013). Kebir, H. et al. Human TH17 lymphocytes promote blood-brain barrier disruption and central nervous system inflammation. Nat. Med. 13, 1173\u20131175 (2007). Campbell, G. R. et al. Mitochondrial DNA deletions and neurodegeneration in multiple sclerosis. Ann. Neurol. 69, 481\u2013492 (2011). Calabrese, M. et al. Cortical lesion load associates with progression of disability in multiple sclerosis. Brain 135, 2952\u20132961 (2012). Bellmann-Strobl, J. et al. Poor PASAT performance correlates with MRI contrast enhancement in multiple sclerosis. Neurology 73, 1624\u20131627 (2009). Andreassen, O. A. et al. Genetic pleiotropy between multiple sclerosis and schizophrenia but not bipolar disorder: differential involvement of immune-related gene loci. Mol. Psychiatry 20, 207\u2013214 (2015). Charalambous, T. et al. Structural network disruption markers explain disability in multiple sclerosis. J. Neurol. Neurosurg. Psychiatry 90, 219\u2013226 (2019). Fleischer, V. et al. Graph theoretical framework of brain networks in multiple sclerosis: a review of concepts. Neuroscience https://doi.org/10.1016/ j.neuroscience.2017.10.033 (2017). Dong, D., Wang, Y., Chang, X., Luo, C. & Yao, D. Dysfunction of large-scale brain networks in schizophrenia: a meta-analysis of resting-state functional connectivity. Schizophr. Bull. 44, 168\u2013181 (2018). Kaiser, R. H., Andrews-Hanna, J. R., Wager, T. D. & Pizzagalli, D. A. Large-scale network dysfunction in major depressive disorder: a meta-analysis of resting-state functional connectivity. JAMA Psychiatry 72, 603\u2013611 (2015). Armangue, T. et al. Autoimmune post-herpes simplex encephalitis of adults and teenagers. Neurology 85, 1736\u20131743 (2015). Bien, C. G. et al. Immunopathology of autoantibody-associated encephalitides: clues for pathogenesis. Brain 135, 1622\u20131638 (2012). Petit-Pedrol, M. et al. Encephalitis with refractory seizures, status epilepticus, and antibodies to the GABAA receptor: a case series, characterisation of the antigen, and analysis of the effects of antibodies. Lancet Neurol. 13, 276\u2013286 (2014). Haselmann, H. et al. Human autoantibodies against the AMPA receptor subunit GluA2 induce receptor reorganization and memory dysfunction. Neuron 100, 91\u2013105 (2018). Dalmau, J., Lancaster, E., Martinez-Hernandez, E., Rosenfeld, M. R. & Balice-Gordon, R. Clinical experience and laboratory investigations in patients with anti-NMDAR encephalitis. Lancet Neurol. 10, 63\u201374 (2011). Hughes, E. G. et al. Cellular and synaptic mechanisms of anti-NMDA receptor encephalitis. J. Neurosci. 30, 5866\u20135875 (2010). Haggerty, G. C., Forney, R. B. & Johnson, J. M. The effect of a single administration of phencyclidine on behavior in the rat over a 21-day period. Toxicol. Appl. Pharmacol. 75, 444\u2013453 (1984). DeGiorgio, L. A. et al. A subset of lupus anti-DNA antibodies cross-reacts with the NR2 glutamate receptor in systemic lupus erythematosus. Nat. Med. 7, 1189\u20131193 (2001). Nestor, J. et al. Lupus antibodies induce behavioral changes mediated by microglia and blocked by ACE inhibitors. J. Exp. Med. 215, 2554\u20132566 (2018). Hammer, C. et al. Neuropsychiatric disease relevance of circulating anti-NMDA receptor autoantibodies depends on blood-brain barrier integrity. Mol. Psychiatry 19, 1143\u20131149 (2014). Dahm, L. et al. Seroprevalence of autoantibodies against brain antigens in health and disease. Ann. Neurol. 76, 82\u201394 (2014). Jezequel, J. et al. Dynamic disorganization of synaptic NMDA receptors triggered by autoantibodies from psychotic patients. Nat. Commun. 8, 1791 (2017). This study reveals differential effects of autoantibodies against the glutamate NMDA receptor on synaptic transmission and long-term potentiation in patients with psychosis versus healthy controls, providing a mechanistic framework for different clinical outcomes despite similar autoantibodies Kantrowitz, J. & Javitt, D. C. Glutamatergic transmission in schizophrenia: from basic research to clinical practice. Curr. Opin. Psychiatry 25, 96\u2013102 (2012). Bergink, V., Gibney, S. M. & Drexhage, H. A. Autoimmunity, inflammation, and psychosis: a search for peripheral markers. Biol. Psychiatry 75, 324\u2013331 (2014). Cullen, A. E. et al. Associations between non- neurological autoimmune disorders and psychosis: a meta-analysis. Biol. Psychiatry 85, 35\u201348 (2018). Siegmann, E. M. et al. Association of depression and anxiety disorders with autoimmune thyroiditis: a systematic review and meta-analysis. JAMA Psychiatry 75, 577\u2013584 (2018). Sommer, I. E. et al. Severe chronic psychosis after allogeneic SCT from a schizophrenic sibling. Bone Marrow Transplant. 50, 153\u2013154 (2015). Dahan, S. et al. The relationship between serum cytokine levels and degree of psychosis in patients with schizophrenia. Psychiatry Res. 268, 467\u2013472 (2018). Haapakoski, R., Mathieu, J., Ebmeier, K. P., Alenius, H. & Kivimaki, M. Cumulative meta-analysis of interleukins 6 and 1\u03b2, tumour necrosis factor \u03b1 and C-reactive protein in patients with major depressive disorder. Brain Behav. Immun. 49, 206\u2013215 (2015). Black, C. & Miller, B. J. Meta-analysis of cytokines and chemokines in suicidality: distinguishing suicidal versus nonsuicidal patients. Biol. Psychiatry 78, 28\u201337 (2015). Al-Ayadhi, L. Y. & Mostafa, G. A. Elevated serum levels of interleukin-17A in children with autism. J. Neuroinflamm. 9, 158 (2012). Passos, I. C. et al. Inflammatory markers in post-traumatic stress disorder: a systematic review, meta-analysis, and meta-regression. Lancet Psychiatry 2, 1002\u20131012 (2015). Bulzacka, E. et al. Chronic peripheral inflammation is associated with cognitive impairment in schizophrenia: results from the multicentric FACE-SZ dataset. Schizophr. Bull. 42, 1290\u20131302 (2016). Rademakers, R. et al. Mutations in the colony stimulating factor 1 receptor (CSF1R) gene cause hereditary diffuse leukoencephalopathy with spheroids. Nat. Genet. 44, 200\u2013205 (2011). Tong, L. et al. Microglia loss contributes to the development of major depression induced by different types of chronic stresses. Neurochem. Res. 42, 2698\u20132711 (2017). Bloomfield, P. S. et al. Microglial activity in people at ultra high risk of psychosis and in schizophrenia: an [11C]PBR28 PET brain imaging study. Am. J. Psychiatry 173, 44\u201352 (2016). This PET imaging study shows that patients with schizophrenia and individuals with a high risk of psychosis exhibit increased microglial activity, indicating a connection between neuroinflammation and the risk of psychosis. Wachholz, S. et al. Microglia activation is associated with IFN-alpha induced depressive-like behavior. Brain Behav. Immun. 55, 105\u2013113 (2016). Norden, D. M., Muccigrosso, M. M. & Godbout, J. P. Microglial priming and enhanced reactivity to secondary insult in aging, and traumatic CNS injury, and neurodegenerative disease. Neuropharmacology 96, 29\u201341 (2015). Smith, S. E., Li, J., Garbett, K., Mirnics, K. & Patterson, P. H. Maternal immune activation alters fetal brain development through interleukin-6. J. Neurosci. 27, 10695\u201310702 (2007). Choi, G. B. et al. The maternal interleukin-17a pathway in mice promotes autism-like phenotypes in offspring. Science 351, 933\u2013939 (2016). The results of this study indicate that the development of ASD-like phenotypes in offspring in the murine model of maternal immune activation is mediated by TH17 cells. Shin Yim, Y. et al. Reversing behavioural abnormalities in mice exposed to maternal inflammation. Nature 549, 482\u2013487 (2017). Kim, S. et al. Maternal gut bacteria promote neurodevelopmental abnormalities in mouse offspring. Nature 549, 528\u2013532 (2017). This study shows that the production of IL-17 in the maternal immune activation model in mice depends on the composition of maternal intestinal bacteria, underlining the role of the gut\u2013immune\u2013brain axis. Cekanaviciute, E. et al. Gut bacteria from multiple sclerosis patients modulate human T cells and exacerbate symptoms in mouse models. Proc. Natl Acad. Sci. USA 114 , 10713\u201310718 (2017). de Magistris, L. et al. Alterations of the intestinal barrier in patients with autism spectrum disorders and in their first-degree relatives. J. Pediatr. Gastroenterol. Nutr. 51, 418\u2013424 (2010). Lombardo, M. V. et al. Maternal immune activation dysregulation of the fetal brain transcriptome and relevance to the pathophysiology of autism spectrum disorder. Mol. Psychiatry 23, 1001\u20131013 (2018). Zimmerman, A. W. et al. Maternal antibrain antibodies in autism. Brain Behav. Immun. 21, 351\u2013357 (2007). Braunschweig, D. et al. Autism-specific maternal autoantibodies recognize critical proteins in developing brain. Transl Psychiatry 3, e277 (2013). Bauman, M. D. et al. Maternal antibodies from mothers of children with autism alter brain growth and social behavior development in the rhesus monkey. Transl Psychiatry 3, e278 (2013). Brimberg, L. et al. Caspr2-reactive antibody cloned from a mother of an ASD child mediates an ASD-like phenotype in mice. Mol. Psychiatry 21, 1663\u20131671 (2016). This study describes the isolation and characterization of monoclonal brain-reactive antibodies from a mother of a child with ASD. Cross-Disorder Group of the Psychiatric Genomics Consortium. Identification of risk loci with shared effects on five major psychiatric disorders: a genome-wide analysis. Lancet 381, 1371\u20131379 (2013). Trowsdale, J. & Knight, J. C. Major histocompatibility complex genomics and human disease. Annu. Rev. Genomics Hum. Genet. 14, 301\u2013323 (2013). Dendrou, C. A., Petersen, J., Rossjohn, J. & Fugger, L. HLA variation and disease. Nat. Rev. Immunol. 18, 325\u2013339 (2018). Sekar, A. et al. Schizophrenia risk from complex variation of complement component 4. Nature 530, 177\u2013183 (2016). Prasad, K. M. et al. Neuropil contraction in relation to Complement C4 gene copy numbers in independent cohorts of adolescent-onset and young adult-onset schizophrenia patients-a pilot study. Transl Psychiatry 8, 134 (2018). Kury, P. et al. Human endogenous retroviruses in neurological diseases. Trends Mol. Med. 24, 379\u2013394 (2018). Perron, H. et al. Molecular characteristics of human endogenous retrovirus type-W in schizophrenia and bipolar disorder. Transl Psychiatry 2, e201 (2012). Melamed, I. R., Heffron, M., Testori, A. & Lipe, K. A pilot study of high-dose intravenous immunoglobulin 5% for autism: impact on autism spectrum and markers of neuroinflammation. Autism Res. 11, 421\u2013433 (2018). Connery, K. et al. Intravenous immunoglobulin for the treatment of autoimmune encephalopathy in children with autism. Transl Psychiatry 8, 148 (2018). This article and that by Melamed et al. (2018) describe pilot studies that provided the first evidence for a beneficial effect of intravenous immunoglobulin for the treatment of children with ASD and signs of inflammation. Swedo, S. E., Frankovich, J. & Murphy, T. K. Overview of treatment of pediatric acute-onset neuropsychiatric syndrome. J. Child Adolesc. Psychopharmacol. 27, 562\u2013565 (2017). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT03093064 (2017). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT02874573 (2018). US National Library of Medicine. ClinicalTrials.gov https://clinicaltrials.gov/ct2/show/NCT02034474 (2018). Miyaoka, T. et al. Remission of psychosis in treatment- resistant schizophrenia following bone marrow transplantation: a case report. Front. Psychiatry 8, 174 (2017). Dawson, G. et al. Autologous cord blood infusions are safe and feasible in young children with autism spectrum disorder: results of a single-center phase I open-label trial. Stem Cells Transl Med. 6, 1332\u20131339 (2017). Hannestad, J., DellaGioia, N. & Bloch, M. The effect of antidepressant medication treatment on serum levels of inflammatory cytokines: a meta-analysis. Neuropsychopharmacology 36, 2452\u20132459 (2011). Platten, M. et al. Treatment of autoimmune neuroinflammation with a synthetic tryptophan metabolite. Science 310, 850\u2013855 (2005). Kohler, O. et al. Effect of anti-inflammatory treatment on depression, depressive symptoms, and adverse effects: a systematic review and meta-analysis of randomized clinical trials. JAMA Psychiatry 71, 1381\u20131391 (2014). Molina-Hernandez, M., Tellez-Alcantara, N. P., Perez-Garcia, J., Olivera-Lopez, J. I. & Jaramillo-Jaimes, M. T. Antidepressant-like actions of minocycline combined with several glutamate antagonists. Prog. Neuropsychopharmacol. Biol. Psychiatry 32, 380\u2013386 (2008). Rosenblat, J. D. & McIntyre, R. S. Efficacy and tolerability of minocycline for depression: a systematic review and meta-analysis of clinical trials. J. Affect. Disord. 227, 219\u2013225 (2018). This systematic review and meta-analysis provides a proof of concept for the antidepressant effects of minocycline. Ghaleiha, A. et al. Minocycline as adjunctive treatment to risperidone in children with autistic disorder: a randomized, double-blind placebo-controlled trial. J. Child Adolesc. Psychopharmacol. 26, 784\u2013791 (2016). Pardo, C. A. et al. A pilot open-label trial of minocycline in patients with autism and regressive features. J. Neurodev. Disord. 5, 9 (2013). Raison, C. L. et al. A randomized controlled trial of the tumor necrosis factor antagonist infliximab for treatment-resistant depression: the role of baseline inflammatory biomarkers. JAMA Psychiatry 70, 31\u201341 (2013). Kalman, B. Autoimmune encephalitides: a broadening field of treatable conditions. Neurologist 22, 1\u201313 (2017). Howes, O. D. & McCutcheon, R. Inflammation and the neural diathesis-stress hypothesis of schizophrenia: a reconceptualization. Transl Psychiatry 7, e1024 (2017). Fonken, L. K., Frank, M. G., Gaudet, A. D. & Maier, S. F. Stress and aging act through common mechanisms to elicit neuroinflammatory priming. Brain Behav. Immun. 73, 133\u2013148 (2018). Baumeister, D., Akhtar, R., Ciufolini, S., Pariante, C. M. & Mondelli, V. Childhood trauma and adulthood inflammation: a meta-analysis of peripheral C-reactive protein, interleukin-6 and tumour necrosis factor-\u03b1. Mol. Psychiatry 21, 642\u2013649 (2016). Danese, A. et al. Elevated inflammation levels in depressed adults with a history of childhood maltreatment. Arch. Gen. Psychiatry 65, 409\u2013415 (2008). Menard, C. et al. Social stress induces neurovascular pathology promoting depression. Nat. Neurosci. 20, 1752\u20131760 (2017). This study reveals mechanistic links between psychosocial stress and depression via disruption of BBB integrity by downregulation of claudin 5 in mice. Wang, J. et al. Epigenetic modulation of inflammation and synaptic plasticity promotes resilience against stress in mice. Nat. Commun. 9, 477 (2018). Lewitus, G. M., Cohen, H. & Schwartz, M. Reducing post-traumatic anxiety by immunization. Brain Behav. Immun. 22, 1108\u20131114 (2008). Cohen, H. et al. Maladaptation to mental stress mitigated by the adaptive immune system via depletion of naturally occurring regulatory CD4+CD25+ cells. J. Neurobiol. 66, 552\u2013563 (2006). Lewitus, G. M. et al. Vaccination as a novel approach for treating depressive behavior. Biol. Psychiatry 65, 283\u2013288 (2009).","title":"10. References"},{"location":"190613_LiHongming_2019/","text":"19-06-13 \u00b6 Li, H., Habes, M., Wolk, D.A. and Fan, Y., 2019. A deep learning model for early prediction of Alzheimer's disease dementia based on hippocampal MRI. arXiv preprint arXiv:1904.07282. Website | PDF | [Mendeley] Contents \u00b6 00. Abstract 01. Background 02. Methods 02.01. Imaging and clinical data 02.02. Hippocampus extraction 02.03. Deep learning for informative feature extraction 02.04. Time-to-event prognostic model based on deep imaging features 02.05. Time-to-event prognostic model when deep imaging features meet clinical variables 02.06. Validation and comparisons 03. Results 04. Discussion 00. Abstract \u00b6 Introduction \u00b6 It is challenging at baseline to predict when and which individuals who meet criteria for mild cognitive impairment (MCI) will ultimately progress to Alzheimer\u2019s disease (AD) dementia. Methods \u00b6 A deep learning method is developed and validated based on MRI scans of 2146 subjects (803 for training and 1343 for validation) to predict MCI subjects\u2019 progression to AD dementia in a time- to-event analysis setting. Results \u00b6 The deep learning time-to-event model predicted individual subjects\u2019 progression to AD dementia with a concordance index (C-index) of 0.762 on 439 ADNI testing MCI subjects with follow-up duration from 6 to 78 months (quartiles: [24, 42, 54]) and a C-index of 0.781 on 40 AIBL testing MCI subjects with follow-up duration from 18-54 months (quartiles: [18, 36,54]). The predicted progression risk also clustered individual subjects into subgroups with significant differences in their progression time to AD dementia (p<0.0002). Improved performance for predicting progression to AD dementia (C- index=0.864) was obtained when the deep learning based progression risk was combined with baseline clinical measures. Conclusion \u00b6 Our method provides a cost effective and accurate means for prognosis and potentially to facilitate enrollment in clinical trials with individuals likely to progress within a specific temporal period. Keywords \u00b6 deep learning; hippocampus; time-to-event analysis; Alzheimer\u2019s disease 01. Background \u00b6 Individuals with mild cognitive impairment (MCI) are at a higher risk to develop dementia (usually due to Alzheimer\u2019s Disease (AD)), with an annual progression rate up to 10~20% [1]. Although clinical criteria for MCI and AD have been developed to formalize assessment of the gradual progression of the disease, it remains difficult at baseline to predict when and which individuals who meet criteria for MCI will ultimately progress to AD dementia. MCI pMCI (progressive) sMCI (stable) The early prediction of AD dementia has been typically modeled as a pattern classification problem. For instance, by dichotomizing MCI subjects into progressive MCIs ( pMCI s) and stable MCIs ( sMCI s) based on a cut-off threshold of follow-up duration, a binary classifier is then trained based on baseline data to distinguish pMCIs from sMCIs. To early predict AD dementia based on neuroimaging data, machine learning techniques have been adopted to build classifiers upon imaging data, and prominent brain structural differences have been identified between AD and cognitively normal (NC) subjects as well as between pMCI and sMCI subjects within the medial temporal lobe (MTL), including regions such as hippocampus and entorhinal cortex [2],[3],[4],[5],[6], 7 ,[8],[9],[10],[11] . The existing classification studies typically adopt relatively simple imaging measures, such as brain tissue density, volume, cortical thickness, and geometric characteristics of hippocampus. These hand-crafted measures might be less discriminative for the AD prognosis. For predicting MCI subjects\u2019 progression to AD dementia it is a suboptimal strategy to distinguish pMCI from sMCI subjects in a classification setting in that the classification performance is hindered on the cut-off threshold of follow-up duration to define pMCI and sMCI, and the cohorts of pMCI and sMCI subjects are highly heterogeneous regardless of the threshold used. More importantly, the classification based early prediction of AD dementia does not provide specific information about the timing of when MCI patients cross the threshold to AD dementia. Recent studies have moved the focus onto the prediction of timing of progression to AD over the follow-up duration using time-to-event analysis techniques [12],[13],[14],[15],[16] . Clinical and imaging based measures at baseline and their longitudinal change trajectory have been adopted for predicting MCI subjects\u2019 progression to AD dementia, and promising performance have been achieved. However, only simple imaging features or individual clinical measures have been investigated, which might be less discriminative for the prognosis. To better predict individual MCI subjects\u2019 progression to AD dementia based on baseline structural MRI data, we develop a deep learning framework to extract informative features from hippocampal MRI data, and build a prognostic model upon the extracted features to predict progression of MCI subjects in a time-to-event analysis setting. We have evaluated the proposed method using baseline structural MRI data of subjects from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) [17],[18],[19] (including ADNI-1, ADNI-GO&2), and the Australian Imaging Biomarkers and Lifestyle Study of Aging (AIBL) [20] . We also compared the deep learning based imaging features with conventionally hand-crafted imaging features. 02. Methods \u00b6 02.01. Imaging and clinical data \u00b6 We included data from the ADNI (http://adni.loni.usc.edu) and AIBL (www.aibl.csiro.au) cohorts, consisting of baseline MRI scans of 1711 ADNI subjects and 435 AIBL subjects. The data were downloaded on April 05, 2017. For up-to-date information, see www.adni-info.org. We used MRI data (n=803 scans) from the ADNI-1 to train the proposed prognostic model. Then we validated the proposed model with independent data from the ADNI-GO&2 and the AIBL. For the ADNI-GO cohort only new add-on subjects (no overlap with ADNI-1) were used for the validation. The present study included all MCI subjects with baseline MRI scans and at least one clinical follow-up data point, including those who converted back from MCI to Normal Cognition. The characteristics of the cohorts included in this study are summarized in tables S1, S2, and S3 of supplementary material. In the present study, the ADNI-1 scans were collected using 1.5T scanners, the ADNI-GO&2 scans were collected using 3T scanners, and the AIBL scans were collected using 3T scanners. Clinical variables include age, sex, education, APOE4 (Apolipoprotein E4), the 13-item version of the Alzheimer\u2019s Disease Assessment Scale-Cognitive subscale (ADAS-Cog13), Rey Auditory Verbal Learning Test (RAVLT) immediate, RAVLT learning, Functional Assessment Questionnaire (FAQ), and Mini-Mental State Examination (MMSE), were obtained for MCI subjects from the ADNI cohorts. Particularly, the ADAS-Cog13 consists of 11-item (word recall, commands, constructional praxis, naming, ideational praxis, orientation, word recognition, remembering test instructions, comprehension, word finding difficulty, spoken language ability) ADAS-cog plus 2 additional items (delayed word recall and number cancellation). We also analyzed MCI subjects of the ADNI-GO&2 cohorts in terms of their amyloid positive status. Particularly, amyloid positive subjects were defined as those with a CSF (cerebrospinal fluid) A\u03b242 (amyloid beta peptide 42) level below 192 pg/mL or with a summary AV-45 (Florbetapir-F18) cortical standardized uptake value ratio (SUVR) normalized by the whole cerebellum above 1.11 when CSF A\u03b242 level was not available. 02.02. Hippocampus extraction \u00b6 T1 MRI scans were registered to the MNI space using affine registration and resampled at a spatial resolution of 1 \u00d7 1 \u00d7 1 mm3. Bilateral hippocampal regions were segmented from the T1 images for each subject using the local label learning (LLL) [21] algorithm with 100 hippocampus atlases obtained from a preliminary release of the EADC-ADNI harmonized segmentation protocol project (www.hippocampal- protocol.net) [22]. A 3D bounding box of size 29 \u00d7 21 \u00d7 55 was then adopted to extract hippocampus regions from the T1 images using the segmentation labels. These hippocampal data were used to extract features and build the prognostic models. 02.03. Deep learning for informative feature extraction \u00b6 A deep learning model of convolutional neural networks (CNNs) with residual connections was trained to learning informative features for distinguishing AD from NC subjects. As illustrated in Fig. 1B, the left and right hippocampal images were inputs to the deep learning model with two streams, each for one hippocampus. Each stream of the deep learning model consisted of 1 convolutional layer (Conv), followed by 3 residual blocks (ResBlocks) [23], and 1 global average pooling (GAP) layer [24]. As illustrated in Fig. 1C, each of the ResBlocks had 2 Conv layers with a direct connection between its input and output. These two streams\u2019 outputs were then flattened and concatenated as input to a fully connected layer (FC) for building the classification model. Rectified linear units (ReLUs) were used as nonlinear activation functions and max pooling layers were adopted to learn features at multiple scales. Batch normalization (BN) was adopted in our deep learning model [25], and the GAP layers facilitated visualization of the learned features through AD-like relevance maps [24]. Specifically, each of the Conv layers contained 32 kernels, the ResBlocks 1, 2, and 3 contained 32, 64, and 128 kernels respectively, and all these kernels had the same size of 3 \u00d7 3 \u00d7 3. The max pooling layer had a stride of 2 \u00d7 2 \u00d7 2 and a kernel size of 2 \u00d7 2 \u00d7 2. Outputs of the GAP layers were concatenated and followed by a dropout operation with a ratio of 0.5 to form an input vector for the FC to make the diagnosis of the input data. Once the deep classification model was obtained, new image could be fed into the deep learning model and its deep learning features (1 \u00d7 256 feature vector in this study) were extracted as the output of the GAP layers and used as the input to time-to-event prognostic models. 02.04. Time-to-event prognostic model based on deep imaging features \u00b6 Based on the deep learning features, a prognostic model for predicting individual subject\u2019s timing of progression to AD dementia was built using LASSO regularized Cox regression model [26]. Particularly, the prognostic model was trained based on the ADNI-1 cohort, and its prognostic performance was evaluated based on the ADNI-GO&2 and AIBL cohorts. The overall training and validation procedures are illustrated in Fig. 1A. The LASSO model\u2019s regularization parameter was optimized using 10-fold cross-validation based on the training data. The time-to-event prognostic model estimates overall risk scores to progress to AD dementia for individuals. Individuals with higher risk scores will progress to AD sooner than those with lower risk scores. An annual probability value of progression to AD can be estimated based on the risk score, given a baseline hazard (progression to AD) function, which could be estimated based on training cohort [27]. Figure 1 \u00b6 Fig. 1. Schematic illustration of the deep learning based AD prognosis. (A) A general flowchart for training and validating the prognostic model. (B) Deep network architecture for data-driven hippocampus-based AD diagnosis. (C) residual block. L: left hippocampus; R: right hippocampus. (NC: cognitively normal control; AD: Alzheimer\u2019s disease; MCI: mild cognitive impairment; Conv: convolutional layer; BN: batch normalization; ReLU: rectified linear unit; ResBlock: residual block; Concat: concatenation layer; FC: fully connected layer). 02.05. Time-to-event prognostic model when deep imaging features meet clinical variables \u00b6 Based on the deep learning prognostic model, risk of progression to AD could be estimated for each MCI subject. The estimated risk was combined with clinical variables including age, sex, education, APOE4, ADAS-Cog13, RAVLT immediate, RAVLT learning, FAQ, and MMSE to build a second prognostic model using Cox regression [28]. The prognostic model was trained based on the ADNI-1 MCI subjects and evaluated based on the ADNI-GO&2 data. Since the AIBL did not provide all these clinical measures, we did not evaluate the prognostic model based on the AIBL data. To evaluate significance of the deep learning features when combined with the clinical measures for the prediction of AD dementia, we had also built another prognostic model of MCI subjects based on the AD score (probability) estimated by the deep AD/NC classification model and baseline clinical variables using Cox regression. The prognostic model was also trained on the ADNI-1 data and evaluated on the ADNI-GO&2 data. 02.06. Validation and comparisons \u00b6 We evaluated the proposed method and compared it with state-of-the-art alternative image feature extraction methods [29] based on the same training and validation datasets. All the models were trained using the ADNI-1 data, and validated using the ADNI-GO&2 and AIBL data. We compared the deep learning models with those built on conventional hippocampal imaging features, including shape features and texture features (Details in supplementary material). Particularly, two alternative models were built on shape features only and a combination of shape and texture features (denoted as shape & texture thereafter) respectively. In order to directly access reproducibility of our deep learning features across scanners with different magnetic field strengths, we obtained 1.5T and 3T scans of the same subjects of the ADNI-1 cohort (n=113, 37 NC, 50 MCI, and 26 AD), computed their deep learning features, and finally measured comparability between deep learning features of 1.5T and 3T scans and between their predicted risk scores for individual subjects. In order to evaluate the deep imaging features\u2019 discriminative power, we compared its performance for distinguishing AD from NC subjects with alternative classification models based on conventional hippocampal shape and texture features. Random forests (RF) [30] were adopted to build classification models upon the conventional features (Details in supplementary material). Classification accuracy, receiver operating characteristic (ROC) curve, and area under ROC curve (AUC) were used to evaluate all the models under comparison. We adopted Delong test to compare AUC measures obtained by different models. In order to investigate how different parts of the hippocampal imaging data contributed to the classification, class relevance maps [24] with respect to AD were obtained for all the subjects from the validation cohorts, and mean relevance maps of different subject groups including NC, sMCI, pMCI, and AD were obtained respectively. MCI subjects who progressed to AD from 0.5 to 3 years from the baseline scan were defined as pMCI (mean/standard deviation: 1.69/0.93 years, quartiles: [1, 2, 2.5] years), otherwise defined as sMCI for this visualization analysis. It is worth noting that we do not need to define pMCI and sMCI for the time-to-event analyses. We further investigated the associations between the deep hippocampal imaging features and clinical measures including MMSE, ADAS-cog13, RAVLT immediate, RAVLT learning, FAQ, A\ud835\udefd status (positive or negative), and APOE4, Pearson\u2019s correlation coefficients were adopted to measure the potential associations. In order to evaluate the performance of prognostic models built on different kinds of imaging features, concordance index (C-index) and time-dependent ROC curves were adopted to evaluate their accuracy. Particularly, the C-index measures proportion of all possible pairs of subjects, at least one of whom has progressed to AD dementia, in which the predicted progression risk (probability) is larger for the subjects who progressed to AD dementia in a shorter time [31], while the time-dependent ROC curves access prediction performance of progression of AD dementia at different observed times [32]. The time- dependent ROC curves were adopted so that the performance of prognostic models could be evaluated using ROC curves that are widely used to illustrate sensitivity and specificity of a continuous diagnostic marker for predicting a binary clinical outcome variable in classification studies. Since the status of progression of AD is dependent on the follow-up time, binary status of progression of AD could be obtained at different cut-off time points to compute their corresponding ROC curves, yielding time-dependent ROC curves [33]. A nonparametric approach [34] was adopted to estimate and compare C- index values, and nonparametric inverse probability of censoring weighting estimators [32] were adopted to obtain and compare time-dependent ROC based measures. The prognostic accuracy was also evaluated for amyloid positive MCI subjects alone. All the measures were calculated in R. We investigated the subject stratification results based on the prognostic risks of progression to AD dementia for MCI subjects. Specifically, all the MCI subjects were categorized into 3 sub-groups with low, middle, and high predicted risks of progression to AD dementia, and Kaplan-Meier plot was adopted to investigate their progression to AD dementia based on real follow-up duration information. 03. Results \u00b6 Mean of correlations between the deep learning features of 1.5T and 3T scans of the 113 ADNI-1 subjects was 0.955\u00b10.023 (quantiles: [0.930, 0.958, 0.979]), and mean of intra-class correlation coefficients (ICCs) across all the deep learning features was 0.950\u00b10.026 (quantiles: [0.925, 0.951, 0.978]), demonstrating that the deep learning features were robust to magnetic field strength differences. The ICC between the predicted risk scores of the 1.5T and 3T scans was 0.982, demonstrating that the prediction performance was also robust to the differences in magnetic field strength. The deep learning classifier\u2019s accuracy rates for distinguishing AD from NC subjects on the ADNI- GO&2 and AIBL cohorts were 0.900 and 0.929 respectively, and AUC values were 0.956 and 0.958 respectively. Delong test indicated that the deep learning classifier performed significantly better than the RF based classifiers in terms of their AUC measures (p<0.008). Fig. S2 (supplementary material) shows ROC curves of the classifiers under comparison. Fig. 2 illustrates the mean AD-like relevance maps for NC, sMCI, pMCI, and AD groups of the ADNI-GO&2 cohorts. We found that the relevance map of the AD subjects highlighted both the anterior and posterior hippocampus, the pMCI subjects\u2019 highlighted the anterior hippocampus, and maps of the NC and sMCI subjects had relatively weak relevance. Figure 2 \u00b6 ![Figure.2][fig02] Fig. 2. Mean AD-like relevance maps for different subject groups, demonstrating the discriminative sub- regions that distinguish AD from NC and characterizing the imaging patterns along the pathological progression of AD. Warmer color indicates severer brain degeneration and more relevant to the progression to AD and cool color indicates rare brain degeneration and irrelevant to the progression to AD. The hippocampus panel shows mean hippocampal segmentation maps across subjects in different views (A: anterior, P: posterior, M: medial, L: lateral, S: superior, I: inferior). The mean maps have values in [0,1], indicating each voxel\u2019s percentage for being located in the hippocampus across subjects whose brain images were registered to the MNI space. The segmentation maps serve as spatial references to the hippocampal locations for the AD relevance maps. NC: cognitively normal control; AD: Alzheimer\u2019s disease; MCI: mild cognitive impairment; sMCI: stable MCI; pMCI: progressive MCI. \u00b6 Fig. 3 shows that the correlation coefficients between top 50 deep learning features with largest weights in the deep learning classifier and cognitive measures and biomarkers across all subjects of the ADNI-GO&2 cohorts, indicating that the deep learning features were significantly correlated with clinical measures (Pearson\u2019s correlation, p<0.05). Fig. S3 (supplementary material) shows that amyloid negative MCI subjects had significantly lower progression risks compared with amyloid positive subjects (p<6.0 \u00d7 10\u221213, Wilcoxon rank sum tests). These results demonstrated that the deep imaging features were informative to capture the characteristics of AD related biomarkers and cognitive measures. Figure 3 \u00b6 ![Figure.3][fig03] Fig. 3. Top 50 features with largest weights in the deep classification model for AD/NC diagnosis were significantly correlated to cognitive/biological measures across all the subjects of the ADNI-GO&2 cohorts. The box plots show median/quartiles of correlation coefficients between pairs of one cognitive/biological measure and each of top 50 deep learning features. The outliers are plotted using the \u2018+\u2019 symbol. (MMSE: Mini\u2013Mental State Examination; ADAS-cog13: 13-item version of the Alzheimer\u2019s Disease Assessment Scale-Cognitive subscale; RAVLT: Rey Auditory Verbal Learning Test; FAQ: Functional Assessment Questionnaire; A\u03b2: amyloid beta peptide 42; APOE4: Apolipoprotein E4). \u00b6 The prediction accuracy of the prognostic models built upon hippocampal imaging features are summarized in Table 1. The deep learning model predicted the ADNI-GO&2 MCI subjects\u2019 progression to AD dementia with a C-index of 0.762, significantly better than those built upon conventional shape and texture image features (p<0.03 [34]). The deep leaning model predicted the AIBL MCI subjects\u2019 progression to AD dementia with a C-index of 0.781, better than those built upon conventional shape (p=0.037). However, the difference between prediction models built on the deep learning imaging features and the shape & texture image features was not statistically significant (p=0.694 [34]). As shown in Fig.4 (top row), the AUC measures of time-dependent ROC curves obtained by the deep learning based model on follow-up durations from year 1 to year 3 were 0.75, 0.778, and 0.813 respectively on the ADNI-GO&2 cohorts, better than the alternative models (the differences were significant when compared with prediction models built on the shape features at year 2, and on both the shape and shape & texture features at year 3, p<0.04 [32]). Table 1 \u00b6 ![Table.1][tbl01] Table 1. Prediction performance of prognostic models built upon different types of features. Figure 4 \u00b6 ![Figure.4][fig04] Fig. 4. Time-dependent ROC curves of prognostic models built upon different imaging features at follow- up durations from year 1 to year 3 on the ADNI-GO&2 cohorts. Top row: all the ADNI-GO&2 MCI subjects; Bottom row: Amyloid positive MCI subjects of the ADNI-GO&2 cohorts. \u00b6 The prediction model built on the deep learning imaging features predicted the amyloid positive subjects\u2019 progression to AD with a C-index of 0.733, better than those built on the conventional shape and shape & texture imaging features (with C-index of 0.656 and 0.680 respectively, p=0.004 and 0.06 [34]). As shown in Fig.4 (bottom row), the AUC measures of time-dependent ROC curves obtained by the deep imaging feature based model at follow-up durations from year 1 to year 3 were 0.731, 0.762, and 0.789 respectively, better than the alternative models (the differences were significant when compared with prediction models built on the shape features at year 2 and 3, p<0.05 [32]). The deep learning feature based model also performed better than prediction models built upon hippocampal volumes (supplementary material). The predicted progression risk to AD dementia also clustered MCI subjects from ADNI-GO&2 into subgroups with significant differences in their timing of progression (p<0.0002, Log-rank test) with age, sex, education and APOE4 status as covariates, as demonstrated by the Kaplan-Meier plots in Fig. 5. Figure 5 \u00b6 ![Figure.5][fig05] Fig. 5. Kaplan-Meier plots of MCI subgroups with different progression risks in terms of conversion to AD estimated by the deep learning prediction model on testing MCI subjects of the ADNI-GO&2 cohorts (Low: the 1st quartile, High: the 4th quartile, Middle: 2nd and 3rd quartiles). The subgroups were significantly different in their conversion timing to AD (Log-rank test, p<0.0002). \u00b6 The prediction model built upon the predicted AD dementia progression risks (based on deep learning features) and baseline clinical measures achieved improved performance (C-index=0.864) on the ADNI-GO&2 cohorts, significantly better than the model built on clinical measures alone (C- index=0.848, p=0.05). As summarized in Table S4 and S5, RAVLT_immediate, FAQ, and the deep learning features were top 3 predictors for predicting progression to AD dementia (p<1 \u00d7 10\u22125). We have further estimated performance of prediction models built on different combinations of imaging features, A\u03b2 measures, APOE4 status, and cognition measures, in addition to demographic data (age, sex, and education). As summarized in Tables S6 and S7 (supplementary material), the combination of the deep learning imaging features and demographic measures had significantly better prediction performance than the combination of MMSE and demographic measures (p=0.0004). Among the cognitive measures, ADAS-cog13 had the best prediction performance when combined with demographic measures, and the difference in terms of prediction performance between the deep learning imaging features and the ADAS- Cog13 was not statistically significant when combined with the demographic data (p=0.257). The deep learning imaging features had similar performance as A\ud835\udefd and APOE4 measures when combined with all the cognitive measures. 04. Discussion \u00b6 In this study, we proposed a deep learning framework for early prognosis of AD dementia based on the hippocampal MRI data. We trained a deep learning classifier based on the ADNI-1 cohort for extracting informative imaging features, and built a time-to-event prognostic model on these features to predict the progression to AD dementia for MCI subjects of the ADNI-GO&2 and AIBL cohorts. We demonstrated that the deep learning prediction model could achieve promising performance for predicting MCI subjects\u2019 progression to AD dementia and identifying subgroups of subjects with different progression patterns. Deep learning techniques have been explored for the prognosis of AD dementia [35-38]. These studies adopted a classification setting to predict MCI progression, and had to dichotomize the training data into progressive or stable MCIs based on certain cut-off threshold. Therefore, their prediction performance was dependent on their cut-off thresholds. Instead of formulating the early prediction of MCI subjects\u2019 progression to AD dementia as a binary classification problem, we built a prognostic model under a time-to-event analysis setting. The time-to-event prognostic model took into consideration the timing of progression to AD dementia, and could estimate the timing/risk of progression to AD dementia for each individual subject, which could be used to monitor their disease progression longitudinally. Our prediction results could also be analyzed using the conventional ROC curves, i.e., the time dependent ROC curves. On the other hand, the estimated risk could also facilitate stratification of MCI subjects to identify those with higher risk to progress to AD dementia as demonstrated in Fig. 5. A very striking finding in our study was the clear advantage of the use of deep learning hippocampal features in predicting progression to dementia compared with the hippocampal volume, which is frequently used in the literatures as a marker of neurodegeneration [39]. Maybe due to practicality in assessment, most of the attention has been given so far for the volumetric features of the hippocampus rather than other features in exploring dementia. However, even in structural MRI the dementia related changes in the hippocampus seem to be better exploited with the deep learning features we are proposing for more robust progression prediction. Several studies have specifically focused on the hippocampus for early diagnosis of AD and built predictive models upon anatomical features such as the hippocampal volume and shape based measures and image intensity texture features [5, 6, 40-44]. Although promising performance of the hippocampus shape features [45-47], texture features [48] and CNNs based features [43, 49] has been demonstrated for the classification of AD patients, their classification setting ignored the timing of progression to AD dementia. Moreover, it is challenging to define the pMCI and sMCI under a classification setting due to high heterogeneity of the AD continuum. Several recent studies [12-15] have focused on the prediction of time of progression to AD dementia under a time-to-event analysis setting; however, relatively simple features, e.g., volumetric and geometric measures, were included in the prognostic analysis. The discriminative power of these hand-crafted measures are relatively limited, especially when used for more complex prognostic tasks. As demonstrated in Fig. S2, the deep imaging features performed significantly better than conventional shape and texture features for distinguishing AD from NC subjects on both the ADNI-GO&2 and AIBL cohorts, indicating that the deep imaging features are more discriminative, and may have better potentials for characterizing the hippocampal changes related to AD dementia. It also demonstrated good generalization performance across different cohorts. Sub-regions of the hippocampus, as shown in Fig. 2, contributed differently to characterize the AD related differences. The relevance map of the AD subjects highlighted both the anterior and posterior hippocampus, the pMCI subjects\u2019 map highlighted the anterior hippocampus, and maps of the NC and sMCI subjects had relatively weak relevance. These results were largely consistent with patterns of MCI and AD subjects described in existing studies [50, 51]. It also suggested that the anterior hippocampus was involved along the progression to AD prior to the posterior part [52, 53]. These relevance patterns have demonstrated that the deep features were indeed extracted from the AD related hippocampus regions. As the CNNs was optimized to learn discriminative imaging features, representing an evolution of imaging features from low-level intensity contrast to high-level complex patterns, for better differentiating AD from NC, it is speculated that the learned imaging features might reflect the AD relevant microstructural alternations in the hippocampal regions, and different weights of sub-regions demonstrated their involvement in these changes. These imaging alternations might be results of pathophysiological changes such as neuronal loss [54]. As shown in Fig. 3, we also found that the deep learning features were significantly correlated with cognitive measures and AD related biomarkers. Moreover, we found that amyloid positive MCIs who have molecular evidence of prodromal AD had higher predicted AD dementia progression risks than amyloid negative MCIs [55]. Different quantitative evaluation measures, as shown in Table 1 and Fig. 4, have demonstrated that the deep imaging features\u2019 superiority for predicting the MCI subjects\u2019 progression to AD on different cohorts, compared with the conventional shape and texture features. The deep learning prediction model also performed significantly better than other prediction models built upon conventional imaging features for amyloid positive subjects. Fig. 4 shows that the AUC values of the A\ud835\udefd positive MCI subjects were smaller than those of all MCI subjects. This difference in AUC values was a result of the difference in the testing data sets, and the risk scores of individual A\ud835\udefd positive MCI subjects were the same regardless of the computation of AUC values. Comparisons in terms of AUC values should be based on the same data set since the AUC values are summary measures for certain testing data sets. Nonetheless, it is possible that measures of neurodegeneration in the hippocampus would have better predictive power than amyloid status alone, as the former measure is more tightly linked to disease progression. The prognostic performance of the prognostic model built on the combination of deep learning imaging features and clinical measures worked significantly better than that built on clinical measures alone. Particularly, RAVLT_immediate, FAQ, and the deep learning imaging features were top 3 predictors for predicting the MCI subjects\u2019 progression to AD dementia, as summarized in Table S4 and S5. The prognostic models built on a combination of demographic data (age, sex, education), cognitive measures, and deep learning features had better or similar prediction performance than prognostic models built on combinations of demographic data, cognitive measures, and APOE4 statuses or A\ud835\udefd measures as summarized in tables 1, S6, and S7, indicating that the deep learning imaging features could serve as a surrogate if APOE4 or A\ud835\udefd measures are not available. Recently studies have also demonstrated promising performance for predicting individual subjects\u2019 timing of progression to AD dementia using time-to-event analysis techniques [12-15]. Particularly, clinical and imaging based measures at baseline [12, 13, 15] and their longitudinal change trajectory [14] have been adopted for predicting MCI subjects\u2019 progression to AD dementia. In conjunction with these studies, our results further demonstrated that the hippocampal MRI data could provide informative measures for the prediction of MCI subjects\u2019 timing of progression to AD dementia. In the present study, the training imaging data were collected using 1.5T scanners and the testing imaging data were collected using 3T scanners. The deep learning features of 1.5 and 3T scans of the same ADNI 1 subjects highly correlated with each other and showed high comparability in their predicted risk scores, demonstrating that the deep learning features were robust to magnetic field strength differences. Therefore, the differences between the training data and testing data in the scanners\u2019 magnetic field strength minimally affected the prediction performance. We postulate that the image intensity normalization used in our study might minimize the difference caused by the scanner\u2019s magnetic field strength. Our deep learning models could be used to predict AD progression risks for individuals with MRI scans collected following the ADNI imaging protocol. The whole pipeline of the proposed prognostic model is automatic. It is not sensitive to hippocampus segmentation, as only a bounding box containing hippocampus is required. The deep learning feature extraction and prognosis is efficient on both modern GPU and CPU (within 1 second) once the trained model is obtained. Particularly, for each subject, it takes about 0.011 second on a GPU or about 0.463 second on a CPU to compute deep learning features, and it takes 0.1 millisecond to obtain a prognosis result on a CPU. This deep learning tool can be used across platforms, including cloud computing, once they are containerized using docker. The approach here offers a straightforward and computationally rapid means for a clinician to stratify MCI patients about the likelihood of progression within a particular timeframe. This could have significant impact on family and patient planning. In light of the fact that prediction accuracy is similar to prediction from A\u03b2 measures, this approach may obviate the need for some measures in the clinical setting with the advantage of being non-invasive, as opposed to lumbar puncture, and less expensive, as compared to amyloid PET. Although the proposed prognostic model has achieved better performance than state-of-the-art alternative imaging feature extraction methods for AD prognosis, further efforts are needed in following aspects. First, the current study focused on the hippocampus, it is expected to obtain improved prognostic performance when the deep learning method is applied to the whole brain MRI data. Second, the current study focused on predicting MCI subjects\u2019 progression to AD dementia, but the proposed framework could be applied to other clinical endpoints, such as predicting NC subject\u2019s progression to MCI [56] or cognitive decline, which could be useful in preclinical AD studies and facilitate subject screening in clinical trial. Third, only data at baseline were used in the current study, and we expect that the prognostic performance could be boosted if longitudinal data are incorporated into the model. Fourth, the current study focused on the prediction of boundaries between MCI and AD, which may not be well equipped to characterize the AD continuum [57]. The cognitive measures demonstrated better performance than the imaging measures for predicting the AD dementia progression as cognition is core component of the diagnosis of dementia, which results in circularity of using these measures in this type of prediction. Future work could focus more on predicting cognitive change which may be less susceptible to these circularity arguments. Additionally, the present study focused on the imaging features and did not fully explored other cognitive measures that might be even more informative for the prediction of MCI progression, such as RAVLT delayed recall, or impairment in other cognitive domains, such as executive function. Finally, changes in activities of daily living are also reflective progression from MCI to dementia and therefore could further add prediction. Although our method could build time-to-event prediction models with continuous timing information, our prediction model largely captured the AD progression on intervals of 6 months because the training subjects were followed every 6 months. Since most of the ADNI MCI subjects progressed to AD dementia within 36 months, our prediction model might be driven to focus more on the advanced MCI subjects and to identify information relevant to late MCI. Moreover, although quantitative results have demonstrated that the prognostic model was robust to imaging data collected by scanners with different magnetic field strength, other potential confounders regarding subject heterogeneity such as atypical forms of AD require further investigation using datasets in clinical settings. One source of comfort related to heterogeneity of imaging acquisition for the viability of this approach is that quantitative results have demonstrated that the deep learning imaging features were robust to imaging data collected using scanners with different magnetic field strengths. It should be noted also that ADNI was designed to mimic a clinical trial and that is an additional context, outside of clinical practice, in which the findings here are relevant for potentially determining inclusion in an intervention study. Finally, although our study showed that the deep hippocampal features have considerable advantages compared to conventional methods such as hippocampal volume, the hippocampus as a structure might be limited in sensitivity and specificity for AD prediction and future research should consider further relevant regions. In conclusion, the deep learning method for early prognosis of AD dementia could achieve promising performance, help distinguish MCI subjects with different progression patterns, and identify MCI subjects with higher risk to develop AD dementia, thus providing a cost effective and accurate means for prognosis and potentially to facilitate enrollment in clinical trials with individuals likely to progress within a specific temporal period. References \u00b6 [1] Langa K, Levine D. The diagnosis and management of mild cognitive impairment: a clinical review. JAMA. 2014;312. [2] Desikan RS, Cabral HJ, Settecase F, Hess CP, Dillon WP, Glastonbury CM, et al. Automated MRI measures predict progression to Alzheimer's disease. Neurobiology of aging. 2010;31:1364-74. [3] Filipovych R, Davatzikos C, Alzheimer's Disease Neuroimaging I. Semi-supervised pattern classification of medical images: application to mild cognitive impairment (MCI). NeuroImage. 2011;55:1109-19. [4] Moradi E, Pepe A, Gaser C, Huttunen H, Tohka J, Alzheimer's Disease Neuroimaging I. Machine learning framework for early MRI-based Alzheimer's conversion prediction in MCI subjects. NeuroImage. 2015;104:398-412. [5] de Vos F, Schouten TM, Hafkemeijer A, Dopper EG, van Swieten JC, de Rooij M, et al. Combining multiple anatomical MRI measures improves Alzheimer's disease classification. Human brain mapping. 2016;37:1920-9. [6] Hu K, Wang Y, Chen K, Hou L, Zhang X. Multi-scale features extraction from baseline structure MRI for MCI patient classification and AD early diagnosis. Neurocomputing. 2016;175, Part A:132-45. 7 Rathore S, Habes M, Iftikhar MA, Shacklett A, Davatzikos C. A review on neuroimaging-based classification studies and associated feature extraction methods for Alzheimer's disease and its prodromal stages. NeuroImage. 2017;155:530-48. [8] Fan Y, Resnick SM, Wu XY, Davatzikos C. Structural and functional biomarkers of prodromal Alzheimer's disease: A high-dimensional pattern classification study. NeuroImage. 2008;41:277-85. [9] Davatzikos C, Fan Y, Wu X, Shen D, Resnick SM. Detection of prodromal Alzheimer's disease via pattern classification of magnetic resonance imaging. Neurobiology of aging. 2008;29:514-23. [10] Fan Y, Batmanghelich N, Clark CM, Davatzikos C, Initia ADN. Spatial patterns of brain atrophy in MCI patients, identified via high-dimensional pattern classification, predict subsequent cognitive decline. NeuroImage. 2008;39:1731-43. [11] Misra C, Fan Y, Davatzikos C. Baseline and longitudinal patterns of brain atrophy in MCI patients, and their use in prediction of short-term conversion to AD: Results from ADNI. NeuroImage. 2009;44:1415-22. [12] Barnes DE, Cenzer IS, Yaffe K, Ritchie CS, Lee SJ, Alzheimer's Disease Neuroimaging I. A point- based tool to predict conversion from mild cognitive impairment to probable Alzheimer's disease. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2014;10:646-55. [13] Kong D, Giovanello KS, Wang Y, Lin W, Lee E, Fan Y, et al. Predicting Alzheimer\u2019s disease using combined imaging-whole genome SNP data. Journal of Alzheimer's Disease. 2015;46:695-702. [14] Li K, O'Brien R, Lutz M, Luo S, Alzheimer's Disease Neuroimaging I. A prognostic model of Alzheimer's disease relying on multiple longitudinal measures and time-to-event data. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2018;14:644-51. [15] Li S, Okonkwo O, Albert M, Wang M-C. Variation in variables that predict progression from MCI to AD dementia over duration of follow-up. American journal of Alzheimer's disease (Columbia, Mo). 2013;2:12-28. [16] Khachaturian AS, Corcoran CD, Mayer LS, Zandi PP, Breitner JS, Cache County Study I. Apolipoprotein e \u03b54 count affects age at onset of alzheimer disease,but not lifetime susceptibility: The cache county study. Archives of General Psychiatry. 2004;61:518-24. [17] Weiner MW, Veitch DP, Aisen PS, Beckett LA, Cairns NJ, Green RC, et al. Recent publications from the Alzheimer's Disease Neuroimaging Initiative: Reviewing progress toward improved AD clinical trials. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2017;13:e1-e85. [18] Weiner MW, Veitch DP, Aisen PS, Beckett LA, Cairns NJ, Green RC, et al. The Alzheimer's Disease Neuroimaging Initiative 3: Continued innovation for clinical trial improvement. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2017;13:561-71. [19] Weiner MW, Veitch DP, Aisen PS, Beckett LA, Cairns NJ, Cedarbaum J, et al. 2014 Update of the Alzheimer's Disease Neuroimaging Initiative: A review of papers published since its inception. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2015;11:e1-120. [20] Ellis KA, Bush AI, Darby D, De Fazio D, Foster J, Hudson P, et al. The Australian Imaging, Biomarkers and Lifestyle (AIBL) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of Alzheimer's disease. International psychogeriatrics. 2009;21:672- 87. [21] Hao Y, Wang T, Zhang X, Duan Y, Yu C, Jiang T, et al. Local label learning (LLL) for subcortical structure segmentation: Application to hippocampus segmentation. Human brain mapping. 2014;35:2674-97. [22] Boccardi M, Bocchetta M, Morency FC, Collins DL, Nishikawa M, Ganzola R, et al. Training labels for hippocampal segmentation based on the EADC-ADNI harmonized hippocampal protocol. Alzheimers & Dementia. 2015;11:175-83. [23] He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2016. p. 770-8. [24] Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A. Learning deep features for discriminative localization. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition2016. p. 2921-9. [25] Ioffe S, Szegedy C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In: Bach FR, Blei DM, editors. ICML: JMLR.org; 2015. p. 448-56. [26] Tibshirani R. The lasso method for variable selection in the Cox model. Statistics in medicine. 1997;16:385-95. [27] Royston P. Estimating a smooth baseline hazard function for the Cox model. London: Department of Statistical Science, University College London. 2011. [28] Cox DR. Regression models and life-tables. Breakthroughs in statistics: Springer; 1992. p. 527-41. [29] van Griethuysen JJM, Fedorov A, Parmar C, Hosny A, Aucoin N, Narayan V, et al. Computational Radiomics System to Decode the Radiographic Phenotype. Cancer research. 2017;77:e104-e7. [30] Barandiaran I. The random subspace method for constructing decision forests. IEEE transactions on pattern analysis and machine intelligence. 1998;20. [31] Harrell FE, Jr., Lee KL, Mark DB. Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Statistics in medicine. 1996;15:361-87. [32] Blanche P, Dartigues JF, Jacqmin-Gadda H. Estimating and comparing time-dependent areas under receiver operating characteristic curves for censored event times with competing risks. Statistics in medicine. 2013;32:5381-97. [33] Heagerty PJ, Lumley T, Pepe MS. Time-dependent ROC curves for censored survival data and a diagnostic marker. Biometrics. 2000;56:337-44. [34] Kang L, Chen W, Petrick NA, Gallas BD. Comparing two correlated C indices with right-censored survival outcome: a one-shot nonparametric approach. Statistics in medicine. 2015;34:685-703. [35] Li F, Tran L, Thung KH, Ji S, Shen D, Li J. A Robust Deep Model for Improved Classification of AD/MCI Patients. IEEE journal of biomedical and health informatics. 2015;19:1610-6. [36] Ithapu VK, Singh V, Okonkwo OC, Chappell RJ, Dowling NM, Johnson SC, et al. Imaging-based enrichment criteria using deep learning algorithms for efficient clinical trials in mild cognitive impairment. Alzheimer's & Dementia. 2015;11:1489-99. [37] Amoroso N, Diacono D, Fanizzi A, La Rocca M, Monaco A, Lombardi A, et al. Deep learning reveals Alzheimer's disease onset in MCI subjects: Results from an international challenge. Journal of neuroscience methods. 2018;302:3-9. [38] Suk HI, Lee SW, Shen D, Alzheimer's Disease Neuroimaging I. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014;101:569-82. [39] Wolk DA, Sadowsky C, Safirstein B, et al. Use of flutemetamol f 18\u2013labeled positron emission tomography and other biomarkers to assess risk of clinical progression in patients with amnestic mild cognitive impairment. JAMA Neurology. 2018. [40] Chupin M, Gerardin E, Cuingnet R, Boutet C, Lemieux L, Lehericy S, et al. Fully automatic hippocampus segmentation and classification in Alzheimer's disease and mild cognitive impairment applied on data from ADNI. Hippocampus. 2009;19:579-87. [41] Devanand DP, Bansal R, Liu J, Hao X, Pradhaban G, Peterson BS. MRI hippocampal and entorhinal cortex mapping in predicting conversion to Alzheimer's disease. NeuroImage. 2012;60:1622-9. [42] Ben Ahmed O, Benois-Pineau J, Allard M, Ben Amar C, Catheline G. Classification of Alzheimer\u2019s disease subjects from MRI using hippocampal visual features. Multimedia Tools and Applications. 2015;74:1249-66. [43] Aderghal K, Boissenin M, Benois-Pineau J, Catheline G, Afdel K. Classification of sMRI for AD Diagnosis with Convolutional Neuronal Networks: A Pilot 2-D+e Study on ADNI. In: Amsaleg L, Gu\u00f0mundsson G\u00de, Gurrin C, Jo\u0301nsson B\u00de, Satoh Si, editors. MultiMedia Modeling: 23rd International Conference, MMM 2017, Reykjavik, Iceland, January 4-6, 2017, Proceedings, Part I. Cham: Springer International Publishing; 2017. p. 690-701. [44] Tsao S, Gajawelli N, Zhou J, Shi J, Ye J, Wang Y, et al. Feature selective temporal prediction of Alzheimer's disease progression using hippocampus surface morphometry. Brain and behavior. 2017;7:e00733. [45] Li S, Shi F, Pu F, Li X, Jiang T, Xie S, et al. Hippocampal shape analysis of Alzheimer disease based on machine learning methods. AJNR Am J Neuroradiol. 2007;28:1339-45. [46] Costafreda SG, Dinov ID, Tu Z, Shi Y, Liu C-Y, Kloszewska I, et al. Automated hippocampal shape analysis predicts the onset of dementia in mild cognitive impairment. NeuroImage. 2011;56:212-9. [47] Gerardin E, Chetelat G, Chupin M, Cuingnet R, Desgranges B, Kim H-S, et al. Multidimensional classification of hippocampal shape features discriminates Alzheimer's disease and mild cognitive impairment from normal aging. NeuroImage. 2009;47:1476-86. [48] Sorensen L, Igel C, Liv Hansen N, Osler M, Lauritzen M, Rostrup E, et al. Early detection of Alzheimer's disease using MRI hippocampal texture. Human brain mapping. 2016;37:1148-61. [49] Li H, Habes M, Fan Y. Deep Ordinal Ranking for Multi-Category Diagnosis of Alzheimer's Disease using Hippocampal MRI data. arXiv: 170901599. 2017. [50] Adler DH, Wisse LE, Ittyerah R, Pluta JB, Ding S-L, Xie L, et al. Characterizing the human hippocampus in aging and Alzheimer\u2019s disease using a computational atlas derived from ex vivo MRI and histology. Proceedings of the National Academy of Sciences. 2018;115:4252-7. [51] Martin SB, Smith CD, Collins HR, Schmitt FA, Gold BT. Evidence that volume of anterior medial temporal lobe is reduced in seniors destined for mild cognitive impairment. Neurobiology of aging. 2010;31:1099-106. [52] Harrison TM, Burggren AC, Small GW, Bookheimer SY. Altered memory-related functional connectivity of the anterior and posterior hippocampus in older adults at increased genetic risk for Alzheimer's disease. Human brain mapping. 2016;37:366-80. [53] Das SR, Pluta J, Mancuso L, Kliot D, Yushkevich PA, Wolk DA. Anterior and posterior MTL networks in aging and MCI. Neurobiology of aging. 2015;36 Suppl 1:S141-50, S50 e1. [54] West MJ, Coleman PD, Flood DG, Troncoso JC. Differences in the pattern of hippocampal neuronal loss in normal ageing and Alzheimer's disease. Lancet. 1994;344:769-72. [55] Jack CR, Jr., Bennett DA, Blennow K, Carrillo MC, Dunn B, Haeberlein SB, et al. NIA-AA Research Framework: Toward a biological definition of Alzheimer's disease. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2018;14:535-62. [56] Albert M, Zhu Y, Moghekar A, Mori S, Miller MI, Soldan A, et al. Predicting progression from normal cognition to mild cognitive impairment for individuals at 5 years. Brain : a journal of neurology. 2018. [57] Aisen PS, Cummings J, Jack CR, Morris JC, Sperling R, Frolich L, et al. On the path to 2025: understanding the Alzheimer's disease continuum. Alzheimers Res Ther. 2017;9. \u00b6 img{width: 50%; float: right;}","title":"190613 Li, Hongming, 2019"},{"location":"190613_LiHongming_2019/#contents","text":"00. Abstract 01. Background 02. Methods 02.01. Imaging and clinical data 02.02. Hippocampus extraction 02.03. Deep learning for informative feature extraction 02.04. Time-to-event prognostic model based on deep imaging features 02.05. Time-to-event prognostic model when deep imaging features meet clinical variables 02.06. Validation and comparisons 03. Results 04. Discussion","title":"Contents"},{"location":"190613_LiHongming_2019/#00_abstract","text":"","title":"00. Abstract"},{"location":"190613_LiHongming_2019/#introduction","text":"It is challenging at baseline to predict when and which individuals who meet criteria for mild cognitive impairment (MCI) will ultimately progress to Alzheimer\u2019s disease (AD) dementia.","title":"Introduction"},{"location":"190613_LiHongming_2019/#methods","text":"A deep learning method is developed and validated based on MRI scans of 2146 subjects (803 for training and 1343 for validation) to predict MCI subjects\u2019 progression to AD dementia in a time- to-event analysis setting.","title":"Methods"},{"location":"190613_LiHongming_2019/#results","text":"The deep learning time-to-event model predicted individual subjects\u2019 progression to AD dementia with a concordance index (C-index) of 0.762 on 439 ADNI testing MCI subjects with follow-up duration from 6 to 78 months (quartiles: [24, 42, 54]) and a C-index of 0.781 on 40 AIBL testing MCI subjects with follow-up duration from 18-54 months (quartiles: [18, 36,54]). The predicted progression risk also clustered individual subjects into subgroups with significant differences in their progression time to AD dementia (p<0.0002). Improved performance for predicting progression to AD dementia (C- index=0.864) was obtained when the deep learning based progression risk was combined with baseline clinical measures.","title":"Results"},{"location":"190613_LiHongming_2019/#conclusion","text":"Our method provides a cost effective and accurate means for prognosis and potentially to facilitate enrollment in clinical trials with individuals likely to progress within a specific temporal period.","title":"Conclusion"},{"location":"190613_LiHongming_2019/#keywords","text":"deep learning; hippocampus; time-to-event analysis; Alzheimer\u2019s disease","title":"Keywords"},{"location":"190613_LiHongming_2019/#01_background","text":"Individuals with mild cognitive impairment (MCI) are at a higher risk to develop dementia (usually due to Alzheimer\u2019s Disease (AD)), with an annual progression rate up to 10~20% [1]. Although clinical criteria for MCI and AD have been developed to formalize assessment of the gradual progression of the disease, it remains difficult at baseline to predict when and which individuals who meet criteria for MCI will ultimately progress to AD dementia. MCI pMCI (progressive) sMCI (stable) The early prediction of AD dementia has been typically modeled as a pattern classification problem. For instance, by dichotomizing MCI subjects into progressive MCIs ( pMCI s) and stable MCIs ( sMCI s) based on a cut-off threshold of follow-up duration, a binary classifier is then trained based on baseline data to distinguish pMCIs from sMCIs. To early predict AD dementia based on neuroimaging data, machine learning techniques have been adopted to build classifiers upon imaging data, and prominent brain structural differences have been identified between AD and cognitively normal (NC) subjects as well as between pMCI and sMCI subjects within the medial temporal lobe (MTL), including regions such as hippocampus and entorhinal cortex [2],[3],[4],[5],[6], 7 ,[8],[9],[10],[11] . The existing classification studies typically adopt relatively simple imaging measures, such as brain tissue density, volume, cortical thickness, and geometric characteristics of hippocampus. These hand-crafted measures might be less discriminative for the AD prognosis. For predicting MCI subjects\u2019 progression to AD dementia it is a suboptimal strategy to distinguish pMCI from sMCI subjects in a classification setting in that the classification performance is hindered on the cut-off threshold of follow-up duration to define pMCI and sMCI, and the cohorts of pMCI and sMCI subjects are highly heterogeneous regardless of the threshold used. More importantly, the classification based early prediction of AD dementia does not provide specific information about the timing of when MCI patients cross the threshold to AD dementia. Recent studies have moved the focus onto the prediction of timing of progression to AD over the follow-up duration using time-to-event analysis techniques [12],[13],[14],[15],[16] . Clinical and imaging based measures at baseline and their longitudinal change trajectory have been adopted for predicting MCI subjects\u2019 progression to AD dementia, and promising performance have been achieved. However, only simple imaging features or individual clinical measures have been investigated, which might be less discriminative for the prognosis. To better predict individual MCI subjects\u2019 progression to AD dementia based on baseline structural MRI data, we develop a deep learning framework to extract informative features from hippocampal MRI data, and build a prognostic model upon the extracted features to predict progression of MCI subjects in a time-to-event analysis setting. We have evaluated the proposed method using baseline structural MRI data of subjects from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) [17],[18],[19] (including ADNI-1, ADNI-GO&2), and the Australian Imaging Biomarkers and Lifestyle Study of Aging (AIBL) [20] . We also compared the deep learning based imaging features with conventionally hand-crafted imaging features.","title":"01. Background"},{"location":"190613_LiHongming_2019/#02_methods","text":"","title":"02. Methods"},{"location":"190613_LiHongming_2019/#0201_imaging_and_clinical_data","text":"We included data from the ADNI (http://adni.loni.usc.edu) and AIBL (www.aibl.csiro.au) cohorts, consisting of baseline MRI scans of 1711 ADNI subjects and 435 AIBL subjects. The data were downloaded on April 05, 2017. For up-to-date information, see www.adni-info.org. We used MRI data (n=803 scans) from the ADNI-1 to train the proposed prognostic model. Then we validated the proposed model with independent data from the ADNI-GO&2 and the AIBL. For the ADNI-GO cohort only new add-on subjects (no overlap with ADNI-1) were used for the validation. The present study included all MCI subjects with baseline MRI scans and at least one clinical follow-up data point, including those who converted back from MCI to Normal Cognition. The characteristics of the cohorts included in this study are summarized in tables S1, S2, and S3 of supplementary material. In the present study, the ADNI-1 scans were collected using 1.5T scanners, the ADNI-GO&2 scans were collected using 3T scanners, and the AIBL scans were collected using 3T scanners. Clinical variables include age, sex, education, APOE4 (Apolipoprotein E4), the 13-item version of the Alzheimer\u2019s Disease Assessment Scale-Cognitive subscale (ADAS-Cog13), Rey Auditory Verbal Learning Test (RAVLT) immediate, RAVLT learning, Functional Assessment Questionnaire (FAQ), and Mini-Mental State Examination (MMSE), were obtained for MCI subjects from the ADNI cohorts. Particularly, the ADAS-Cog13 consists of 11-item (word recall, commands, constructional praxis, naming, ideational praxis, orientation, word recognition, remembering test instructions, comprehension, word finding difficulty, spoken language ability) ADAS-cog plus 2 additional items (delayed word recall and number cancellation). We also analyzed MCI subjects of the ADNI-GO&2 cohorts in terms of their amyloid positive status. Particularly, amyloid positive subjects were defined as those with a CSF (cerebrospinal fluid) A\u03b242 (amyloid beta peptide 42) level below 192 pg/mL or with a summary AV-45 (Florbetapir-F18) cortical standardized uptake value ratio (SUVR) normalized by the whole cerebellum above 1.11 when CSF A\u03b242 level was not available.","title":"02.01. Imaging and clinical data"},{"location":"190613_LiHongming_2019/#0202_hippocampus_extraction","text":"T1 MRI scans were registered to the MNI space using affine registration and resampled at a spatial resolution of 1 \u00d7 1 \u00d7 1 mm3. Bilateral hippocampal regions were segmented from the T1 images for each subject using the local label learning (LLL) [21] algorithm with 100 hippocampus atlases obtained from a preliminary release of the EADC-ADNI harmonized segmentation protocol project (www.hippocampal- protocol.net) [22]. A 3D bounding box of size 29 \u00d7 21 \u00d7 55 was then adopted to extract hippocampus regions from the T1 images using the segmentation labels. These hippocampal data were used to extract features and build the prognostic models.","title":"02.02. Hippocampus extraction"},{"location":"190613_LiHongming_2019/#0203_deep_learning_for_informative_feature_extraction","text":"A deep learning model of convolutional neural networks (CNNs) with residual connections was trained to learning informative features for distinguishing AD from NC subjects. As illustrated in Fig. 1B, the left and right hippocampal images were inputs to the deep learning model with two streams, each for one hippocampus. Each stream of the deep learning model consisted of 1 convolutional layer (Conv), followed by 3 residual blocks (ResBlocks) [23], and 1 global average pooling (GAP) layer [24]. As illustrated in Fig. 1C, each of the ResBlocks had 2 Conv layers with a direct connection between its input and output. These two streams\u2019 outputs were then flattened and concatenated as input to a fully connected layer (FC) for building the classification model. Rectified linear units (ReLUs) were used as nonlinear activation functions and max pooling layers were adopted to learn features at multiple scales. Batch normalization (BN) was adopted in our deep learning model [25], and the GAP layers facilitated visualization of the learned features through AD-like relevance maps [24]. Specifically, each of the Conv layers contained 32 kernels, the ResBlocks 1, 2, and 3 contained 32, 64, and 128 kernels respectively, and all these kernels had the same size of 3 \u00d7 3 \u00d7 3. The max pooling layer had a stride of 2 \u00d7 2 \u00d7 2 and a kernel size of 2 \u00d7 2 \u00d7 2. Outputs of the GAP layers were concatenated and followed by a dropout operation with a ratio of 0.5 to form an input vector for the FC to make the diagnosis of the input data. Once the deep classification model was obtained, new image could be fed into the deep learning model and its deep learning features (1 \u00d7 256 feature vector in this study) were extracted as the output of the GAP layers and used as the input to time-to-event prognostic models.","title":"02.03. Deep learning for informative feature extraction"},{"location":"190613_LiHongming_2019/#0204_time-to-event_prognostic_model_based_on_deep_imaging_features","text":"Based on the deep learning features, a prognostic model for predicting individual subject\u2019s timing of progression to AD dementia was built using LASSO regularized Cox regression model [26]. Particularly, the prognostic model was trained based on the ADNI-1 cohort, and its prognostic performance was evaluated based on the ADNI-GO&2 and AIBL cohorts. The overall training and validation procedures are illustrated in Fig. 1A. The LASSO model\u2019s regularization parameter was optimized using 10-fold cross-validation based on the training data. The time-to-event prognostic model estimates overall risk scores to progress to AD dementia for individuals. Individuals with higher risk scores will progress to AD sooner than those with lower risk scores. An annual probability value of progression to AD can be estimated based on the risk score, given a baseline hazard (progression to AD) function, which could be estimated based on training cohort [27].","title":"02.04. Time-to-event prognostic model based on deep imaging features"},{"location":"190613_LiHongming_2019/#figure_1","text":"Fig. 1. Schematic illustration of the deep learning based AD prognosis. (A) A general flowchart for training and validating the prognostic model. (B) Deep network architecture for data-driven hippocampus-based AD diagnosis. (C) residual block. L: left hippocampus; R: right hippocampus. (NC: cognitively normal control; AD: Alzheimer\u2019s disease; MCI: mild cognitive impairment; Conv: convolutional layer; BN: batch normalization; ReLU: rectified linear unit; ResBlock: residual block; Concat: concatenation layer; FC: fully connected layer).","title":"Figure 1"},{"location":"190613_LiHongming_2019/#0205_time-to-event_prognostic_model_when_deep_imaging_features_meet_clinical_variables","text":"Based on the deep learning prognostic model, risk of progression to AD could be estimated for each MCI subject. The estimated risk was combined with clinical variables including age, sex, education, APOE4, ADAS-Cog13, RAVLT immediate, RAVLT learning, FAQ, and MMSE to build a second prognostic model using Cox regression [28]. The prognostic model was trained based on the ADNI-1 MCI subjects and evaluated based on the ADNI-GO&2 data. Since the AIBL did not provide all these clinical measures, we did not evaluate the prognostic model based on the AIBL data. To evaluate significance of the deep learning features when combined with the clinical measures for the prediction of AD dementia, we had also built another prognostic model of MCI subjects based on the AD score (probability) estimated by the deep AD/NC classification model and baseline clinical variables using Cox regression. The prognostic model was also trained on the ADNI-1 data and evaluated on the ADNI-GO&2 data.","title":"02.05. Time-to-event prognostic model when deep imaging features meet clinical variables"},{"location":"190613_LiHongming_2019/#0206_validation_and_comparisons","text":"We evaluated the proposed method and compared it with state-of-the-art alternative image feature extraction methods [29] based on the same training and validation datasets. All the models were trained using the ADNI-1 data, and validated using the ADNI-GO&2 and AIBL data. We compared the deep learning models with those built on conventional hippocampal imaging features, including shape features and texture features (Details in supplementary material). Particularly, two alternative models were built on shape features only and a combination of shape and texture features (denoted as shape & texture thereafter) respectively. In order to directly access reproducibility of our deep learning features across scanners with different magnetic field strengths, we obtained 1.5T and 3T scans of the same subjects of the ADNI-1 cohort (n=113, 37 NC, 50 MCI, and 26 AD), computed their deep learning features, and finally measured comparability between deep learning features of 1.5T and 3T scans and between their predicted risk scores for individual subjects. In order to evaluate the deep imaging features\u2019 discriminative power, we compared its performance for distinguishing AD from NC subjects with alternative classification models based on conventional hippocampal shape and texture features. Random forests (RF) [30] were adopted to build classification models upon the conventional features (Details in supplementary material). Classification accuracy, receiver operating characteristic (ROC) curve, and area under ROC curve (AUC) were used to evaluate all the models under comparison. We adopted Delong test to compare AUC measures obtained by different models. In order to investigate how different parts of the hippocampal imaging data contributed to the classification, class relevance maps [24] with respect to AD were obtained for all the subjects from the validation cohorts, and mean relevance maps of different subject groups including NC, sMCI, pMCI, and AD were obtained respectively. MCI subjects who progressed to AD from 0.5 to 3 years from the baseline scan were defined as pMCI (mean/standard deviation: 1.69/0.93 years, quartiles: [1, 2, 2.5] years), otherwise defined as sMCI for this visualization analysis. It is worth noting that we do not need to define pMCI and sMCI for the time-to-event analyses. We further investigated the associations between the deep hippocampal imaging features and clinical measures including MMSE, ADAS-cog13, RAVLT immediate, RAVLT learning, FAQ, A\ud835\udefd status (positive or negative), and APOE4, Pearson\u2019s correlation coefficients were adopted to measure the potential associations. In order to evaluate the performance of prognostic models built on different kinds of imaging features, concordance index (C-index) and time-dependent ROC curves were adopted to evaluate their accuracy. Particularly, the C-index measures proportion of all possible pairs of subjects, at least one of whom has progressed to AD dementia, in which the predicted progression risk (probability) is larger for the subjects who progressed to AD dementia in a shorter time [31], while the time-dependent ROC curves access prediction performance of progression of AD dementia at different observed times [32]. The time- dependent ROC curves were adopted so that the performance of prognostic models could be evaluated using ROC curves that are widely used to illustrate sensitivity and specificity of a continuous diagnostic marker for predicting a binary clinical outcome variable in classification studies. Since the status of progression of AD is dependent on the follow-up time, binary status of progression of AD could be obtained at different cut-off time points to compute their corresponding ROC curves, yielding time-dependent ROC curves [33]. A nonparametric approach [34] was adopted to estimate and compare C- index values, and nonparametric inverse probability of censoring weighting estimators [32] were adopted to obtain and compare time-dependent ROC based measures. The prognostic accuracy was also evaluated for amyloid positive MCI subjects alone. All the measures were calculated in R. We investigated the subject stratification results based on the prognostic risks of progression to AD dementia for MCI subjects. Specifically, all the MCI subjects were categorized into 3 sub-groups with low, middle, and high predicted risks of progression to AD dementia, and Kaplan-Meier plot was adopted to investigate their progression to AD dementia based on real follow-up duration information.","title":"02.06. Validation and comparisons"},{"location":"190613_LiHongming_2019/#03_results","text":"Mean of correlations between the deep learning features of 1.5T and 3T scans of the 113 ADNI-1 subjects was 0.955\u00b10.023 (quantiles: [0.930, 0.958, 0.979]), and mean of intra-class correlation coefficients (ICCs) across all the deep learning features was 0.950\u00b10.026 (quantiles: [0.925, 0.951, 0.978]), demonstrating that the deep learning features were robust to magnetic field strength differences. The ICC between the predicted risk scores of the 1.5T and 3T scans was 0.982, demonstrating that the prediction performance was also robust to the differences in magnetic field strength. The deep learning classifier\u2019s accuracy rates for distinguishing AD from NC subjects on the ADNI- GO&2 and AIBL cohorts were 0.900 and 0.929 respectively, and AUC values were 0.956 and 0.958 respectively. Delong test indicated that the deep learning classifier performed significantly better than the RF based classifiers in terms of their AUC measures (p<0.008). Fig. S2 (supplementary material) shows ROC curves of the classifiers under comparison. Fig. 2 illustrates the mean AD-like relevance maps for NC, sMCI, pMCI, and AD groups of the ADNI-GO&2 cohorts. We found that the relevance map of the AD subjects highlighted both the anterior and posterior hippocampus, the pMCI subjects\u2019 highlighted the anterior hippocampus, and maps of the NC and sMCI subjects had relatively weak relevance.","title":"03. Results"},{"location":"190613_LiHongming_2019/#figure_2","text":"![Figure.2][fig02] Fig. 2. Mean AD-like relevance maps for different subject groups, demonstrating the discriminative sub- regions that distinguish AD from NC and characterizing the imaging patterns along the pathological progression of AD. Warmer color indicates severer brain degeneration and more relevant to the progression to AD and cool color indicates rare brain degeneration and irrelevant to the progression to AD. The hippocampus panel shows mean hippocampal segmentation maps across subjects in different views (A: anterior, P: posterior, M: medial, L: lateral, S: superior, I: inferior). The mean maps have values in [0,1], indicating each voxel\u2019s percentage for being located in the hippocampus across subjects whose brain images were registered to the MNI space. The segmentation maps serve as spatial references to the hippocampal locations for the AD relevance maps. NC: cognitively normal control; AD: Alzheimer\u2019s disease; MCI: mild cognitive impairment; sMCI: stable MCI; pMCI: progressive MCI.","title":"Figure 2"},{"location":"190613_LiHongming_2019/#figure_3","text":"![Figure.3][fig03] Fig. 3. Top 50 features with largest weights in the deep classification model for AD/NC diagnosis were significantly correlated to cognitive/biological measures across all the subjects of the ADNI-GO&2 cohorts. The box plots show median/quartiles of correlation coefficients between pairs of one cognitive/biological measure and each of top 50 deep learning features. The outliers are plotted using the \u2018+\u2019 symbol. (MMSE: Mini\u2013Mental State Examination; ADAS-cog13: 13-item version of the Alzheimer\u2019s Disease Assessment Scale-Cognitive subscale; RAVLT: Rey Auditory Verbal Learning Test; FAQ: Functional Assessment Questionnaire; A\u03b2: amyloid beta peptide 42; APOE4: Apolipoprotein E4).","title":"Figure 3"},{"location":"190613_LiHongming_2019/#table_1","text":"![Table.1][tbl01] Table 1. Prediction performance of prognostic models built upon different types of features.","title":"Table 1"},{"location":"190613_LiHongming_2019/#figure_4","text":"![Figure.4][fig04] Fig. 4. Time-dependent ROC curves of prognostic models built upon different imaging features at follow- up durations from year 1 to year 3 on the ADNI-GO&2 cohorts. Top row: all the ADNI-GO&2 MCI subjects; Bottom row: Amyloid positive MCI subjects of the ADNI-GO&2 cohorts.","title":"Figure 4"},{"location":"190613_LiHongming_2019/#figure_5","text":"![Figure.5][fig05] Fig. 5. Kaplan-Meier plots of MCI subgroups with different progression risks in terms of conversion to AD estimated by the deep learning prediction model on testing MCI subjects of the ADNI-GO&2 cohorts (Low: the 1st quartile, High: the 4th quartile, Middle: 2nd and 3rd quartiles). The subgroups were significantly different in their conversion timing to AD (Log-rank test, p<0.0002).","title":"Figure 5"},{"location":"190613_LiHongming_2019/#04_discussion","text":"In this study, we proposed a deep learning framework for early prognosis of AD dementia based on the hippocampal MRI data. We trained a deep learning classifier based on the ADNI-1 cohort for extracting informative imaging features, and built a time-to-event prognostic model on these features to predict the progression to AD dementia for MCI subjects of the ADNI-GO&2 and AIBL cohorts. We demonstrated that the deep learning prediction model could achieve promising performance for predicting MCI subjects\u2019 progression to AD dementia and identifying subgroups of subjects with different progression patterns. Deep learning techniques have been explored for the prognosis of AD dementia [35-38]. These studies adopted a classification setting to predict MCI progression, and had to dichotomize the training data into progressive or stable MCIs based on certain cut-off threshold. Therefore, their prediction performance was dependent on their cut-off thresholds. Instead of formulating the early prediction of MCI subjects\u2019 progression to AD dementia as a binary classification problem, we built a prognostic model under a time-to-event analysis setting. The time-to-event prognostic model took into consideration the timing of progression to AD dementia, and could estimate the timing/risk of progression to AD dementia for each individual subject, which could be used to monitor their disease progression longitudinally. Our prediction results could also be analyzed using the conventional ROC curves, i.e., the time dependent ROC curves. On the other hand, the estimated risk could also facilitate stratification of MCI subjects to identify those with higher risk to progress to AD dementia as demonstrated in Fig. 5. A very striking finding in our study was the clear advantage of the use of deep learning hippocampal features in predicting progression to dementia compared with the hippocampal volume, which is frequently used in the literatures as a marker of neurodegeneration [39]. Maybe due to practicality in assessment, most of the attention has been given so far for the volumetric features of the hippocampus rather than other features in exploring dementia. However, even in structural MRI the dementia related changes in the hippocampus seem to be better exploited with the deep learning features we are proposing for more robust progression prediction. Several studies have specifically focused on the hippocampus for early diagnosis of AD and built predictive models upon anatomical features such as the hippocampal volume and shape based measures and image intensity texture features [5, 6, 40-44]. Although promising performance of the hippocampus shape features [45-47], texture features [48] and CNNs based features [43, 49] has been demonstrated for the classification of AD patients, their classification setting ignored the timing of progression to AD dementia. Moreover, it is challenging to define the pMCI and sMCI under a classification setting due to high heterogeneity of the AD continuum. Several recent studies [12-15] have focused on the prediction of time of progression to AD dementia under a time-to-event analysis setting; however, relatively simple features, e.g., volumetric and geometric measures, were included in the prognostic analysis. The discriminative power of these hand-crafted measures are relatively limited, especially when used for more complex prognostic tasks. As demonstrated in Fig. S2, the deep imaging features performed significantly better than conventional shape and texture features for distinguishing AD from NC subjects on both the ADNI-GO&2 and AIBL cohorts, indicating that the deep imaging features are more discriminative, and may have better potentials for characterizing the hippocampal changes related to AD dementia. It also demonstrated good generalization performance across different cohorts. Sub-regions of the hippocampus, as shown in Fig. 2, contributed differently to characterize the AD related differences. The relevance map of the AD subjects highlighted both the anterior and posterior hippocampus, the pMCI subjects\u2019 map highlighted the anterior hippocampus, and maps of the NC and sMCI subjects had relatively weak relevance. These results were largely consistent with patterns of MCI and AD subjects described in existing studies [50, 51]. It also suggested that the anterior hippocampus was involved along the progression to AD prior to the posterior part [52, 53]. These relevance patterns have demonstrated that the deep features were indeed extracted from the AD related hippocampus regions. As the CNNs was optimized to learn discriminative imaging features, representing an evolution of imaging features from low-level intensity contrast to high-level complex patterns, for better differentiating AD from NC, it is speculated that the learned imaging features might reflect the AD relevant microstructural alternations in the hippocampal regions, and different weights of sub-regions demonstrated their involvement in these changes. These imaging alternations might be results of pathophysiological changes such as neuronal loss [54]. As shown in Fig. 3, we also found that the deep learning features were significantly correlated with cognitive measures and AD related biomarkers. Moreover, we found that amyloid positive MCIs who have molecular evidence of prodromal AD had higher predicted AD dementia progression risks than amyloid negative MCIs [55]. Different quantitative evaluation measures, as shown in Table 1 and Fig. 4, have demonstrated that the deep imaging features\u2019 superiority for predicting the MCI subjects\u2019 progression to AD on different cohorts, compared with the conventional shape and texture features. The deep learning prediction model also performed significantly better than other prediction models built upon conventional imaging features for amyloid positive subjects. Fig. 4 shows that the AUC values of the A\ud835\udefd positive MCI subjects were smaller than those of all MCI subjects. This difference in AUC values was a result of the difference in the testing data sets, and the risk scores of individual A\ud835\udefd positive MCI subjects were the same regardless of the computation of AUC values. Comparisons in terms of AUC values should be based on the same data set since the AUC values are summary measures for certain testing data sets. Nonetheless, it is possible that measures of neurodegeneration in the hippocampus would have better predictive power than amyloid status alone, as the former measure is more tightly linked to disease progression. The prognostic performance of the prognostic model built on the combination of deep learning imaging features and clinical measures worked significantly better than that built on clinical measures alone. Particularly, RAVLT_immediate, FAQ, and the deep learning imaging features were top 3 predictors for predicting the MCI subjects\u2019 progression to AD dementia, as summarized in Table S4 and S5. The prognostic models built on a combination of demographic data (age, sex, education), cognitive measures, and deep learning features had better or similar prediction performance than prognostic models built on combinations of demographic data, cognitive measures, and APOE4 statuses or A\ud835\udefd measures as summarized in tables 1, S6, and S7, indicating that the deep learning imaging features could serve as a surrogate if APOE4 or A\ud835\udefd measures are not available. Recently studies have also demonstrated promising performance for predicting individual subjects\u2019 timing of progression to AD dementia using time-to-event analysis techniques [12-15]. Particularly, clinical and imaging based measures at baseline [12, 13, 15] and their longitudinal change trajectory [14] have been adopted for predicting MCI subjects\u2019 progression to AD dementia. In conjunction with these studies, our results further demonstrated that the hippocampal MRI data could provide informative measures for the prediction of MCI subjects\u2019 timing of progression to AD dementia. In the present study, the training imaging data were collected using 1.5T scanners and the testing imaging data were collected using 3T scanners. The deep learning features of 1.5 and 3T scans of the same ADNI 1 subjects highly correlated with each other and showed high comparability in their predicted risk scores, demonstrating that the deep learning features were robust to magnetic field strength differences. Therefore, the differences between the training data and testing data in the scanners\u2019 magnetic field strength minimally affected the prediction performance. We postulate that the image intensity normalization used in our study might minimize the difference caused by the scanner\u2019s magnetic field strength. Our deep learning models could be used to predict AD progression risks for individuals with MRI scans collected following the ADNI imaging protocol. The whole pipeline of the proposed prognostic model is automatic. It is not sensitive to hippocampus segmentation, as only a bounding box containing hippocampus is required. The deep learning feature extraction and prognosis is efficient on both modern GPU and CPU (within 1 second) once the trained model is obtained. Particularly, for each subject, it takes about 0.011 second on a GPU or about 0.463 second on a CPU to compute deep learning features, and it takes 0.1 millisecond to obtain a prognosis result on a CPU. This deep learning tool can be used across platforms, including cloud computing, once they are containerized using docker. The approach here offers a straightforward and computationally rapid means for a clinician to stratify MCI patients about the likelihood of progression within a particular timeframe. This could have significant impact on family and patient planning. In light of the fact that prediction accuracy is similar to prediction from A\u03b2 measures, this approach may obviate the need for some measures in the clinical setting with the advantage of being non-invasive, as opposed to lumbar puncture, and less expensive, as compared to amyloid PET. Although the proposed prognostic model has achieved better performance than state-of-the-art alternative imaging feature extraction methods for AD prognosis, further efforts are needed in following aspects. First, the current study focused on the hippocampus, it is expected to obtain improved prognostic performance when the deep learning method is applied to the whole brain MRI data. Second, the current study focused on predicting MCI subjects\u2019 progression to AD dementia, but the proposed framework could be applied to other clinical endpoints, such as predicting NC subject\u2019s progression to MCI [56] or cognitive decline, which could be useful in preclinical AD studies and facilitate subject screening in clinical trial. Third, only data at baseline were used in the current study, and we expect that the prognostic performance could be boosted if longitudinal data are incorporated into the model. Fourth, the current study focused on the prediction of boundaries between MCI and AD, which may not be well equipped to characterize the AD continuum [57]. The cognitive measures demonstrated better performance than the imaging measures for predicting the AD dementia progression as cognition is core component of the diagnosis of dementia, which results in circularity of using these measures in this type of prediction. Future work could focus more on predicting cognitive change which may be less susceptible to these circularity arguments. Additionally, the present study focused on the imaging features and did not fully explored other cognitive measures that might be even more informative for the prediction of MCI progression, such as RAVLT delayed recall, or impairment in other cognitive domains, such as executive function. Finally, changes in activities of daily living are also reflective progression from MCI to dementia and therefore could further add prediction. Although our method could build time-to-event prediction models with continuous timing information, our prediction model largely captured the AD progression on intervals of 6 months because the training subjects were followed every 6 months. Since most of the ADNI MCI subjects progressed to AD dementia within 36 months, our prediction model might be driven to focus more on the advanced MCI subjects and to identify information relevant to late MCI. Moreover, although quantitative results have demonstrated that the prognostic model was robust to imaging data collected by scanners with different magnetic field strength, other potential confounders regarding subject heterogeneity such as atypical forms of AD require further investigation using datasets in clinical settings. One source of comfort related to heterogeneity of imaging acquisition for the viability of this approach is that quantitative results have demonstrated that the deep learning imaging features were robust to imaging data collected using scanners with different magnetic field strengths. It should be noted also that ADNI was designed to mimic a clinical trial and that is an additional context, outside of clinical practice, in which the findings here are relevant for potentially determining inclusion in an intervention study. Finally, although our study showed that the deep hippocampal features have considerable advantages compared to conventional methods such as hippocampal volume, the hippocampus as a structure might be limited in sensitivity and specificity for AD prediction and future research should consider further relevant regions. In conclusion, the deep learning method for early prognosis of AD dementia could achieve promising performance, help distinguish MCI subjects with different progression patterns, and identify MCI subjects with higher risk to develop AD dementia, thus providing a cost effective and accurate means for prognosis and potentially to facilitate enrollment in clinical trials with individuals likely to progress within a specific temporal period.","title":"04. Discussion"},{"location":"190613_LiHongming_2019/#references","text":"[1] Langa K, Levine D. The diagnosis and management of mild cognitive impairment: a clinical review. JAMA. 2014;312. [2] Desikan RS, Cabral HJ, Settecase F, Hess CP, Dillon WP, Glastonbury CM, et al. Automated MRI measures predict progression to Alzheimer's disease. Neurobiology of aging. 2010;31:1364-74. [3] Filipovych R, Davatzikos C, Alzheimer's Disease Neuroimaging I. Semi-supervised pattern classification of medical images: application to mild cognitive impairment (MCI). NeuroImage. 2011;55:1109-19. [4] Moradi E, Pepe A, Gaser C, Huttunen H, Tohka J, Alzheimer's Disease Neuroimaging I. Machine learning framework for early MRI-based Alzheimer's conversion prediction in MCI subjects. NeuroImage. 2015;104:398-412. [5] de Vos F, Schouten TM, Hafkemeijer A, Dopper EG, van Swieten JC, de Rooij M, et al. Combining multiple anatomical MRI measures improves Alzheimer's disease classification. Human brain mapping. 2016;37:1920-9. [6] Hu K, Wang Y, Chen K, Hou L, Zhang X. Multi-scale features extraction from baseline structure MRI for MCI patient classification and AD early diagnosis. Neurocomputing. 2016;175, Part A:132-45. 7 Rathore S, Habes M, Iftikhar MA, Shacklett A, Davatzikos C. A review on neuroimaging-based classification studies and associated feature extraction methods for Alzheimer's disease and its prodromal stages. NeuroImage. 2017;155:530-48. [8] Fan Y, Resnick SM, Wu XY, Davatzikos C. Structural and functional biomarkers of prodromal Alzheimer's disease: A high-dimensional pattern classification study. NeuroImage. 2008;41:277-85. [9] Davatzikos C, Fan Y, Wu X, Shen D, Resnick SM. Detection of prodromal Alzheimer's disease via pattern classification of magnetic resonance imaging. Neurobiology of aging. 2008;29:514-23. [10] Fan Y, Batmanghelich N, Clark CM, Davatzikos C, Initia ADN. Spatial patterns of brain atrophy in MCI patients, identified via high-dimensional pattern classification, predict subsequent cognitive decline. NeuroImage. 2008;39:1731-43. [11] Misra C, Fan Y, Davatzikos C. Baseline and longitudinal patterns of brain atrophy in MCI patients, and their use in prediction of short-term conversion to AD: Results from ADNI. NeuroImage. 2009;44:1415-22. [12] Barnes DE, Cenzer IS, Yaffe K, Ritchie CS, Lee SJ, Alzheimer's Disease Neuroimaging I. A point- based tool to predict conversion from mild cognitive impairment to probable Alzheimer's disease. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2014;10:646-55. [13] Kong D, Giovanello KS, Wang Y, Lin W, Lee E, Fan Y, et al. Predicting Alzheimer\u2019s disease using combined imaging-whole genome SNP data. Journal of Alzheimer's Disease. 2015;46:695-702. [14] Li K, O'Brien R, Lutz M, Luo S, Alzheimer's Disease Neuroimaging I. A prognostic model of Alzheimer's disease relying on multiple longitudinal measures and time-to-event data. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2018;14:644-51. [15] Li S, Okonkwo O, Albert M, Wang M-C. Variation in variables that predict progression from MCI to AD dementia over duration of follow-up. American journal of Alzheimer's disease (Columbia, Mo). 2013;2:12-28. [16] Khachaturian AS, Corcoran CD, Mayer LS, Zandi PP, Breitner JS, Cache County Study I. Apolipoprotein e \u03b54 count affects age at onset of alzheimer disease,but not lifetime susceptibility: The cache county study. Archives of General Psychiatry. 2004;61:518-24. [17] Weiner MW, Veitch DP, Aisen PS, Beckett LA, Cairns NJ, Green RC, et al. Recent publications from the Alzheimer's Disease Neuroimaging Initiative: Reviewing progress toward improved AD clinical trials. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2017;13:e1-e85. [18] Weiner MW, Veitch DP, Aisen PS, Beckett LA, Cairns NJ, Green RC, et al. The Alzheimer's Disease Neuroimaging Initiative 3: Continued innovation for clinical trial improvement. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2017;13:561-71. [19] Weiner MW, Veitch DP, Aisen PS, Beckett LA, Cairns NJ, Cedarbaum J, et al. 2014 Update of the Alzheimer's Disease Neuroimaging Initiative: A review of papers published since its inception. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2015;11:e1-120. [20] Ellis KA, Bush AI, Darby D, De Fazio D, Foster J, Hudson P, et al. The Australian Imaging, Biomarkers and Lifestyle (AIBL) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of Alzheimer's disease. International psychogeriatrics. 2009;21:672- 87. [21] Hao Y, Wang T, Zhang X, Duan Y, Yu C, Jiang T, et al. Local label learning (LLL) for subcortical structure segmentation: Application to hippocampus segmentation. Human brain mapping. 2014;35:2674-97. [22] Boccardi M, Bocchetta M, Morency FC, Collins DL, Nishikawa M, Ganzola R, et al. Training labels for hippocampal segmentation based on the EADC-ADNI harmonized hippocampal protocol. Alzheimers & Dementia. 2015;11:175-83. [23] He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2016. p. 770-8. [24] Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A. Learning deep features for discriminative localization. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition2016. p. 2921-9. [25] Ioffe S, Szegedy C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In: Bach FR, Blei DM, editors. ICML: JMLR.org; 2015. p. 448-56. [26] Tibshirani R. The lasso method for variable selection in the Cox model. Statistics in medicine. 1997;16:385-95. [27] Royston P. Estimating a smooth baseline hazard function for the Cox model. London: Department of Statistical Science, University College London. 2011. [28] Cox DR. Regression models and life-tables. Breakthroughs in statistics: Springer; 1992. p. 527-41. [29] van Griethuysen JJM, Fedorov A, Parmar C, Hosny A, Aucoin N, Narayan V, et al. Computational Radiomics System to Decode the Radiographic Phenotype. Cancer research. 2017;77:e104-e7. [30] Barandiaran I. The random subspace method for constructing decision forests. IEEE transactions on pattern analysis and machine intelligence. 1998;20. [31] Harrell FE, Jr., Lee KL, Mark DB. Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Statistics in medicine. 1996;15:361-87. [32] Blanche P, Dartigues JF, Jacqmin-Gadda H. Estimating and comparing time-dependent areas under receiver operating characteristic curves for censored event times with competing risks. Statistics in medicine. 2013;32:5381-97. [33] Heagerty PJ, Lumley T, Pepe MS. Time-dependent ROC curves for censored survival data and a diagnostic marker. Biometrics. 2000;56:337-44. [34] Kang L, Chen W, Petrick NA, Gallas BD. Comparing two correlated C indices with right-censored survival outcome: a one-shot nonparametric approach. Statistics in medicine. 2015;34:685-703. [35] Li F, Tran L, Thung KH, Ji S, Shen D, Li J. A Robust Deep Model for Improved Classification of AD/MCI Patients. IEEE journal of biomedical and health informatics. 2015;19:1610-6. [36] Ithapu VK, Singh V, Okonkwo OC, Chappell RJ, Dowling NM, Johnson SC, et al. Imaging-based enrichment criteria using deep learning algorithms for efficient clinical trials in mild cognitive impairment. Alzheimer's & Dementia. 2015;11:1489-99. [37] Amoroso N, Diacono D, Fanizzi A, La Rocca M, Monaco A, Lombardi A, et al. Deep learning reveals Alzheimer's disease onset in MCI subjects: Results from an international challenge. Journal of neuroscience methods. 2018;302:3-9. [38] Suk HI, Lee SW, Shen D, Alzheimer's Disease Neuroimaging I. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014;101:569-82. [39] Wolk DA, Sadowsky C, Safirstein B, et al. Use of flutemetamol f 18\u2013labeled positron emission tomography and other biomarkers to assess risk of clinical progression in patients with amnestic mild cognitive impairment. JAMA Neurology. 2018. [40] Chupin M, Gerardin E, Cuingnet R, Boutet C, Lemieux L, Lehericy S, et al. Fully automatic hippocampus segmentation and classification in Alzheimer's disease and mild cognitive impairment applied on data from ADNI. Hippocampus. 2009;19:579-87. [41] Devanand DP, Bansal R, Liu J, Hao X, Pradhaban G, Peterson BS. MRI hippocampal and entorhinal cortex mapping in predicting conversion to Alzheimer's disease. NeuroImage. 2012;60:1622-9. [42] Ben Ahmed O, Benois-Pineau J, Allard M, Ben Amar C, Catheline G. Classification of Alzheimer\u2019s disease subjects from MRI using hippocampal visual features. Multimedia Tools and Applications. 2015;74:1249-66. [43] Aderghal K, Boissenin M, Benois-Pineau J, Catheline G, Afdel K. Classification of sMRI for AD Diagnosis with Convolutional Neuronal Networks: A Pilot 2-D+e Study on ADNI. In: Amsaleg L, Gu\u00f0mundsson G\u00de, Gurrin C, Jo\u0301nsson B\u00de, Satoh Si, editors. MultiMedia Modeling: 23rd International Conference, MMM 2017, Reykjavik, Iceland, January 4-6, 2017, Proceedings, Part I. Cham: Springer International Publishing; 2017. p. 690-701. [44] Tsao S, Gajawelli N, Zhou J, Shi J, Ye J, Wang Y, et al. Feature selective temporal prediction of Alzheimer's disease progression using hippocampus surface morphometry. Brain and behavior. 2017;7:e00733. [45] Li S, Shi F, Pu F, Li X, Jiang T, Xie S, et al. Hippocampal shape analysis of Alzheimer disease based on machine learning methods. AJNR Am J Neuroradiol. 2007;28:1339-45. [46] Costafreda SG, Dinov ID, Tu Z, Shi Y, Liu C-Y, Kloszewska I, et al. Automated hippocampal shape analysis predicts the onset of dementia in mild cognitive impairment. NeuroImage. 2011;56:212-9. [47] Gerardin E, Chetelat G, Chupin M, Cuingnet R, Desgranges B, Kim H-S, et al. Multidimensional classification of hippocampal shape features discriminates Alzheimer's disease and mild cognitive impairment from normal aging. NeuroImage. 2009;47:1476-86. [48] Sorensen L, Igel C, Liv Hansen N, Osler M, Lauritzen M, Rostrup E, et al. Early detection of Alzheimer's disease using MRI hippocampal texture. Human brain mapping. 2016;37:1148-61. [49] Li H, Habes M, Fan Y. Deep Ordinal Ranking for Multi-Category Diagnosis of Alzheimer's Disease using Hippocampal MRI data. arXiv: 170901599. 2017. [50] Adler DH, Wisse LE, Ittyerah R, Pluta JB, Ding S-L, Xie L, et al. Characterizing the human hippocampus in aging and Alzheimer\u2019s disease using a computational atlas derived from ex vivo MRI and histology. Proceedings of the National Academy of Sciences. 2018;115:4252-7. [51] Martin SB, Smith CD, Collins HR, Schmitt FA, Gold BT. Evidence that volume of anterior medial temporal lobe is reduced in seniors destined for mild cognitive impairment. Neurobiology of aging. 2010;31:1099-106. [52] Harrison TM, Burggren AC, Small GW, Bookheimer SY. Altered memory-related functional connectivity of the anterior and posterior hippocampus in older adults at increased genetic risk for Alzheimer's disease. Human brain mapping. 2016;37:366-80. [53] Das SR, Pluta J, Mancuso L, Kliot D, Yushkevich PA, Wolk DA. Anterior and posterior MTL networks in aging and MCI. Neurobiology of aging. 2015;36 Suppl 1:S141-50, S50 e1. [54] West MJ, Coleman PD, Flood DG, Troncoso JC. Differences in the pattern of hippocampal neuronal loss in normal ageing and Alzheimer's disease. Lancet. 1994;344:769-72. [55] Jack CR, Jr., Bennett DA, Blennow K, Carrillo MC, Dunn B, Haeberlein SB, et al. NIA-AA Research Framework: Toward a biological definition of Alzheimer's disease. Alzheimer's & dementia : the journal of the Alzheimer's Association. 2018;14:535-62. [56] Albert M, Zhu Y, Moghekar A, Mori S, Miller MI, Soldan A, et al. Predicting progression from normal cognition to mild cognitive impairment for individuals at 5 years. Brain : a journal of neurology. 2018. [57] Aisen PS, Cummings J, Jack CR, Morris JC, Sperling R, Frolich L, et al. On the path to 2025: understanding the Alzheimer's disease continuum. Alzheimers Res Ther. 2017;9.","title":"References"},{"location":"190630_MasseNicolasY_2019/","text":"19-06-30 Masse, Nicolas Y. 2019 \u00b6 Circuit mechanisms for the maintenance and manipulation of information in working memory Original | [Mendeley] Contents \u00b6 00. Summary 01. Introduction 02. Results 02.01. Network model Figure 1 02.02. Maintaining information in short-term memory Figure 2 Figure 3 02.03. Manipulating information 02.04. Manipulating information during the WM delay period Figure 4 02.05. Controlling the representation of information Figure 5 02.06. Attending to specific memoranda Figure 6 02.07. Manipulating information and persistent neuronal activity 03. Discussion 03.01. Variation in persistent neuronal activity in vivo Figure 7 03.02. Comparison to other artificial neural network architectures 03.03. Understanding strategies employed by artificial neural networks 04. Methods 04.01. Network models 04.02. Network training 04.03. Short-term synaptic plasticity 04.04. Population decoding 04.05. Measuring the contribution of neuronal activity and synaptic plasticity towards solving the task 04.06. Tuning similarity index 04.07. Category tuning index 05. References Summary \u00b6 Recently it has been proposed that information in short-term memory may not always be stored in persistent neuronal activity, but can be maintained in \u201cactivity-silent\u201d hidden states such as synaptic efficacies endowed with short-term plasticity (STP). However, working memory involves manipulation as well as maintenance of information in the absence of external stimuli. In this work, we investigated working memory representation using recurrent neural network (RNN) models trained to perform several working memory dependent tasks. We found that STP can support the short-term maintenance of information provided that the memory delay period is sufficiently short. However, in tasks that require actively manipulating information, persistent neuronal activity naturally emerges from learning, and the amount of persistent neuronal activity scales with the degree of manipulation required. These results shed insight into the current debate on working memory encoding, and suggest that persistent neural activity can vary markedly between tasks used in different experiments. Introduction \u00b6 Working memory refers to our ability to temporarily maintain and manipulate information, and is a cornerstone of higher intelligence1. In order to understand the mechanisms underlying working memory (WM), we must resolve the substrate(s) in which information in working memory is maintained. It has been assumed that information in WM is maintained in persistent neuronal activity2\u20136, likely resulting from local recurrent connections7,8, and/or cortical to subcortical loops9. However, recent experiments reveal that the strength of persistent neuronal activity varies markedly between tasks10\u201316. This raises two related questions: 1) why does persistent neural activity vary between tasks, and 2) for those tasks with weak or non-existent persistent activity, where and how is information maintained? A possible answer to the second question is that information is not necessarily maintained in persistent activity, but can be maintained through short-term synaptic plasticity (STP). STP, which is distinct from more commonly known long-term depression (LTD) and potentiation (LTP), is the process in which pre-synaptic activity increases the calcium concentration in the presynaptic terminal but depletes neurotransmitter stores, altering synaptic efficacies on timescales of hundreds of milliseconds to seconds17. Importantly, modelling studies have suggested that STP can allow networks to maintain an \u201cactivity-silent\u201d memory trace of a stimulus, in which short-term information is maintained without persistent activity18. Recent work in human subjects suggests that information can be mnemonically encoded in a silent, or latent, state, and that information can be reactivated into neuronal activity by probing the circuit19,20. While STP might provide another mechanism for the maintenance of information, it does not in itself fully account for why the strength of persistent activity varies between tasks. To answer this, we highlight that WM involves not just the maintenance of information, but also its manipulation. Importantly, manipulating information in short-term memory appears to engage the frontoparietal network differently compared to simply maintaining information21,22. While STP can allow for activity-silent maintenance of information, it is unknown whether STP can support activity-silent manipulation of information without persistent activity. If it cannot, then it suggests that the strength of persistent activity reflects the degree of information manipulation required by the task. In this study, we examine whether STP can support the silent manipulation of information in WM, and whether it could explain the variability in the strength of persistent activity between tasks. Unfortunately, with current technology it would be technically daunting to measure synaptic efficacies in awake behaving mice, and next to impossible in non-human primates. However, recent advances in recurrent neural network (RNN) model algorithms have opened an entirely new avenue to study the putative neural mechanisms underlying various cognitive functions. Crucially, RNN models have successfully reproduced the patterns of neural activity and behavioral output that are observed in vivo, and have generated novel insights into neural circuit function that would otherwise be unattainable through direct experimental measurement23\u201329. Here, we train biologically inspired RNN models, consisting of excitatory and inhibitory like neurons30 and dynamic synapses governed by STP18, to solve a variety of widely studied WM based tasks. These tasks involved maintaining information (Figure 2), manipulating the contents of short-term memory (Figures 3&4), changing how information is represented (Figure 5), and attending to specific memoranda (Figure 6). We show that STP can support the activity-silent maintenance of information, but that it cannot support the silent manipulation of information without persistent activity. Furthermore, we show that the strength of persistent neural activity covaries with the degree of manipulation. This potentially explains the widely observed observation that persistent activity varies markedly between tasks. Results \u00b6 The goal of this study was 1) to determine whether STP can support activity-silent manipulation of information in WM (Figures 2\u20136), and 2) whether STP can explain the variability in persistent activity observed in different tasks10\u201316 (Figure 7). We trained RNN models to solve several widely studied WM tasks, which varied in their specific cognitive demands (see below). In all tasks, the stimuli were represented as coherent motion patterns moving in one of eight possible directions. However, the results of this study are not meant to be specific to motion, or even visual, inputs, and the use of motion patterns as stimuli was simply used to make our example tasks more concrete. Furthermore, similar to the metabolic constraints faced by real brains31, we added a penalty on high neuronal activity (see Methods) so that networks were encouraged to solve tasks using low levels of neuronal activity. Network model \u00b6 We constrained neurons in our network to be either excitatory or inhibitory30. The input layer of the network consisted of 36 excitatory, direction tuned neurons, whose responses across directions were modelled as a Von Mises function (see Methods). These input neurons projected onto a recurrently connected network of 80 excitatory and 20 inhibitory neurons (Figure 1A). Recurrent neurons never sent projections back onto themselves. Figure 1 \u00b6 Recurrent neural network design. (A) The core rate-based network model consisted of 36 motion direction tuned neurons that sent non-negative projections onto 100 recurrently connected neurons. Of these 100 neurons, 80 were excitatory (their output weights were non-negative) and 40 were inhibitory (their output weights were non-positive). The 80 excitatory neurons sent non-negative projections onto three decisions neurons. (B) For synapses that exhibited short-term synaptic depression (left panels), a 100 ms burst of presynaptic activity at 20 Hz (top panel) will increase the value representing the synaptic release probability (red trace, middle panel) and decrease the value representing the available neurotransmitter (blue trace). For these depressing synapses, the stronger and longer lasting decrease in release probability will lead to a decrease in synaptic efficacy (bottom panel) lasting thousands of milliseconds. For synapses that exhibited short-term synaptic depression (right panels), the same burst of presynaptic activity will lead to a stronger and longer lasting increase in release probability (middle panel), leading to an increase in synaptic efficacy lasting thousands of milliseconds. \u00b6 The connection weights between all recurrently connected neurons, were dynamically modulated by short-term synaptic plasticity (STP, see Methods) using a previously proposed model18. In this model, synaptic efficacy is proportional to the product of two terms: one representing the amount of available transmitter, and another representing the release probability. Connection weights from half of the neurons were depressing, such that presynaptic activity strongly decreases the amount of available neurotransmitter but only weakly increases release probability, leading to a decrease in synaptic efficacy lasting thousands of milliseconds (Figure 1B, left panel). In contrast, connection weights from the other half of the neurons were facilitating, such that presynaptic activity weakly decreases the amount of available neurotransmitter and strongly increases the release probability, leading to an increase in synaptic efficacy lasting thousands of milliseconds (right panel). For computational efficiency, these values will be identical for all synapses sharing the same presynaptic neuron. Maintaining information in short-term memory \u00b6 We first examined how networks endowed with STP maintain information in WM using either persistent neuronal activity or using STP. We trained 20 networks to solve the delayed match-to-sample task (DMS, Figure 2A), in which the networks had to indicate whether sequentially presented (500 ms presentation; 1000 ms delay) sample and test stimuli were an exact match. Task accuracy for all networks in this study was >90%. Figure 2 \u00b6 Neuronal and synaptic mnemonic encoding for the delayed match-to-sample task. (A) In the delayed match-to-sample (DMS) task, a 500 ms fixation period was followed by a 500 ms sample stimulus, which is represented as a coherent random dot motion pattern which could move in one of eight directions. This was followed by a 1000 ms delay period and finally a 500 ms test stimulus, which was also represented as motion in one of eight directions. The network was trained to indicate whether the motion directions of the sample and test stimuli matched. (B) The time course of the sample direction decoding accuracy, calculated using neuronal activity (green curves) or synaptic efficacy (magenta curves) are shown for all twenty networks that successfully solved the DMS task. The dashed vertical lines, from left to right, indicate the sample onset, offset, and end of the delay period. (C) Scatter plot showing the neuronal decoding accuracy measured at the end of the delay (x-axis) versus the behavioral accuracy (y-axis) for all 20 networks trained on this task (blue circles), the behavioral accuracy for the same 20 networks after neuronal activity was shuffled right before test onset (red circles) or synaptic efficacies were shuffled right before test onset (cyan circles). Thus, for each blue circle, there is a corresponding red and cyan circle with matching neuronal decoding accuracy. The dashed vertical line indicates chance level decoding accuracy. \u00b6 To measure how information was maintained, we linearly decoded the sample direction using support vector machines (SVM) that were trained and tested at 10 ms intervals, from 1) the population activity of the 100 recurrent neurons, and from 2) the 100 unique synaptic efficacy values dynamically modulated by STP. Classifiers were individually trained and tested at each 10 ms time step. If during the delay period we could decode sample direction from the synaptic efficacies, but not neuronal activity, it would indicate that STP allows for activity-silent maintenance of information. Sample decoding accuracy using synaptic efficacies (magenta curves, each curve represents decoding from one of twenty networks) was equal to one (perfect decoding) for most of the sample and the entire delay period across all trained networks. In contrast, decoding accuracy using neuronal activity (green curves) decreased to values below 0.4 for all 20 networks by the end of the delay, with 15 networks showing decoding accuracies near chance (1/8) levels (P>0.05, bootstrap, measured during the last 100ms of delay period, see Methods). Thus, for the DMS task, the sample stimulus was perfectly encoded by synaptic efficacies in all 20 networks, and either weakly encoded, or not encoded at all, in neuronal activity. Although the decoding accuracies provide us with a measure of how much information is stored in neuronal activity and synaptic efficacies, it does not address how the network is using either substrate to solve the task. We wanted to 1) measure how networks used information maintained in neuronal activity and synaptic efficacies to solve the task, and 2) how these contributions relate to the neuronal sample decoding accuracy. We answered these questions by disrupting network activity or synaptic efficacies during task performance. Specifically, we simulated each trial starting at test onset using the exact same input activity in three different ways: 1) we used the actual neuronal activity and synaptic efficacies taken at test onset as starting points, 2) same as 1, except that we then shuffled the neuronal activity between trials, and 3) same as 1, except that we then shuffled the synaptic efficacies between trials. In each of the three cases, we calculated whether the network output indicated the correct choice. These results are shown in Figure 2C, comparing neuronal decoding accuracy (x-axis) and task accuracy of the networks (y-axis). Blue, red and cyan circles represent results from the 20 networks using non-shuffled data, shuffled neuronal activity, and shuffled synaptic efficacies, respectively. This allows us to relate decoding accuracy of the sample stimulus (which one could measure in real neurophysiological data sets) with the causal contribution of neuronal and synaptic WM towards solving the task (which is relatively easy to measure in artificial neural network models, but not in neurophysiological experiments). Neuronal decoding accuracy calculated at the end of the delay was distributed between chance (0.125) and ~0.4, with task accuracy (calculated without shuffling data) consistently > 0.97 (blue circles, right panel, Figure 3A). Networks with strongest delay-period neuronal selectivity suffered the greatest performance loss when their neuronal activities were shuffled (correlation between neuronal decoding and accuracy after shuffling neuronal activity: Pearson correlation coefficient, R = \u22120.82, P < 10\u22126). Furthermore, networks with the strongest delay-period neuronal selectivity suffered the least performance loss when their synaptic activities were shuffled (correlation between neuronal decoding and accuracy after shuffling synaptic efficacies: R = 0.53, P = 0.015). Figure 3 \u00b6 Understanding how networks solve the delayed match-to-rotated sample (DMRS) task. (A) The DMRS task is similar to the DMS task (Figure 2A), except that the network was trained to indicate whether the test motion direction was rotated 90\u00b0 clockwise from the sample motion direction. (B) The time course of the sample direction decoding accuracy, calculated using neuronal activity (green curves) or synaptic efficacy (magenta curves) are shown for all twenty networks that successfully solved the DMRS task. The dashed vertical lines, from left to right, indicate the sample onset, offset, and end of the delay period. (C) Scatter plot showing the neuronal decoding accuracy measured at the end of the delay (x-axis) versus the behavioral accuracy (y-axis) for all 20 networks trained on this task (blue circles), the behavioral accuracy for the same 20 networks after neuronal activity was shuffled right before test onset (red circles) or synaptic efficacies were shuffled right before test onset (cyan circles). (D) The neuronal sample tuning curves were calculated for four groups of neurons (excitatory neurons with facilitating synapses, blue; excitatory neurons with depressing synapses, red; inhibitory neurons with facilitating synapses, green; inhibitory neurons with depressing synapses, orange) from all 20 networks that solved the task. Neuronal activity was averaged across the entire sample period, and the tuning curves were centered around the sample direction that generated the maximum response (i.e. the preferred direction). Error bars (which are small and difficult to see) indicate one SEM. (E) Same as (D), except that neuronal activity at the end of the delay period was used to calculate the tuning curves. In order to compare how neural activity evoked during the sample evolves across the delay period, tuning curves were aligned to the preferred directions calculated in (D). (F) Same as (D), except that synaptic efficacies at the end of the sample period were used to calculate the tuning curves. As above, the preferred directions are the same as those used in (D). (G) Same as (D), except that synaptic efficacies at the end of the delay period were used. (H) Task accuracy after shuffling synaptic efficacies at the end of the sample period for each of the four groups of neurons. (I) Scatter plot showing the neuronal decoding accuracy measured at the end of the delay period (x-axis) against the task accuracy after shuffling the synaptic efficacies of inhibitory neurons with depressing synapses at the end of the sample period (y-axis). Each dot represents the results of one of the 20 networks. (J) Tuning curves showing the activity received from the neurons in the input layer, which was calculated by multiplying the motion direction tuning of the neurons in the input layer with the input-to-recurrent weight matrix. Results were averaged across the 5 networks with the greatest neuronal decoding accuracy during the last 100 ms of the delay period. The four curves thus indicate the mean amount of input each group of recurrent neurons (excitatory or inhibitory, facilitating or depressing) receives from the input layer for each direction. As above, the preferred directions are the same as those used in (D). (K) Same as (J), except showing the input tuning curves averaged across the 5 networks with the lowest neuronal decoding accuracy during the last 100 ms of the delay period. \u00b6 For 14 out of the 15 networks that solved the task using activity-silent WM (in which neuronal decoding accuracy at the end of the delay was not significantly different than chance at the P>0.05 level), shuffling neuronal activity had no significant impact on behavior (P>0.05, bootstrap), and shuffling synaptic efficacies decreased behavioral accuracy to values not significantly greater than chance (P>0.05, bootstrap). Even more drastic perturbations to neuronal activity, such as setting activity for all recurrently connected neurons to zero for the last 100ms of the delay period, only had a marginal effect on performance (Figure S1), confirming that information maintained in synaptic efficacies, and not neuronal activity, was used to solve the DMS task. Thus, STP allows networks to silently maintain information for the DMS task. Intuitively, the ability of networks to silently maintain information in WM should depend upon the ratio between the delay duration and the values of STP time constants. We found that networks were unable to solve the DMS task in an activity-silent manner when we decreased the STP time constants to 100 and 500 ms (Figure S2A), or increased the delay to 2500 ms (Figure S2D). In contrast, some networks were still able to solve in activity-silent manner for delay periods of 1500 and 2000 ms (Figure S2B&C). This confirms that our networks are only capable of activity-silent mnemonic encoding when the delay period is not significantly greater than the slow STP time constant. Manipulating information \u00b6 Given that STP can allow networks to silently maintain information in WM, we examined whether it could also allow networks to silently manipulate information. Thus, we repeated the analysis from Figure 2 on 20 networks trained to solve a delayed match-to-rotated (DMRS) sample task, in which the target test motion direction was rotated 90\u00b0 clockwise from the sample (Figure 3A). Neuronal decoding accuracy for this task (green curves, Figure 3B) was greater than the DMS task (P < 10\u22127, two-sided t-test, measured during last 100 ms of delay period), suggesting that more information was maintained in neuronal activity compared to networks trained on the DMS task. Unlike the DMS task, all 20 networks maintained information in a hybrid manner, with significantly elevated synaptic decoding accuracy at the end of the delay compared to chance (P<0.05, bootstrap), and shuffling either neuronal activity or synaptic efficacies significantly decreased behavioral accuracy (P<0.05, bootstrap, Figure 3C). We note that persistent activity exists for this task despite the penalty on high neuronal activity (see Methods). Consistent with the DMS task, we found that networks with strongest delay-period neuronal selectivity suffered the greatest performance loss when their neuronal activities were shuffled (R = \u22120.65, P = 0.002, Figure 3C), and that networks with the strongest delay-period neuronal selectivity suffered the least performance loss when their synaptic activities were shuffled (R = 0.75, P < 0.001). Although all 20 networks solved the task using persistent activity during the delay, we wondered whether it was still possible that STP was responsible for manipulating sample information. Thus we sought to understand the specific strategy the networks used to solve this task. We examined the mean neuronal responses averaged across the sample for all 20 networks from 4 groups of neurons: excitatory or inhibitory neurons with facilitating or depressing synapses (Figure 3D). We found a striking asymmetry for inhibitory neurons with depressing synapses, in which the sample direction that produced the maximum response was 90\u00b0 clockwise from the sample direction that produced the weakest response. We can quantify this asymmetry as the difference between the neuronal response 90\u00b0 clockwise and 90\u00b0 counterclockwise from the preferred sample direction, and this difference was the greatest for inhibitory neurons with depressing synapses compared to the other three groups of neurons (P < 10\u22125 for all three comparisons, paired, two-sided t-test). This asymmetry in the neuronal activity tuning functions mostly disappeared by the end of the delay period (Figure 3E), but was present in the synaptic efficacies measured at the end of the sample period (Figure 3F), and at the end of the delay period (Figure 3G). Thus, synaptic efficacies for inhibitory neurons with depressing synapses were at their maximum by the start of the test period on trials in which the sample stimulus was 90\u00b0 counterclockwise from their preferred direction. If such a sample stimulus is followed by a target test stimulus (90\u00b0 clockwise from the sample), the total amount of \u201ccurrent\u201d (neuronal response multiplied by synaptic efficacy) these neurons will project upon the rest of the network will be at a maximum. This inhibitory signal can alter the pattern of activity that excitatory neurons project onto the output layer, potentially decreasing the input into the \u201cnon-match\u201d output neuron while increasing the input into the \u201cmatch\u201d output neuron. Shuffling the synaptic efficacies of inhibitory neurons with depressing synapses at the end of the sample resulted in the lowest task accuracy compared to shuffling efficacies form the other three groups of neurons (P < 10\u22129 for all three comparisons, paired, two-sided t-test, Figure 3G). Furthermore, networks that maintained the least amount of information in neuronal activity during the delay period were more adversely affected by shuffling their synaptic efficacies (R = 0.85, P < 10\u22125, Figure 3H). We hypothesized that the asymmetric tuning of inhibitory neurons with depressing synapses was the result of the connection weights from the input layer. Thus, we calculated the tuning curves for the current (neuronal activity times the connection weight) neurons receive from the input layer. For the five networks that maintained the greatest amount of information in neuronal activity during the delay (Figure 3J), their tuning curves were again asymmetric, but the direction producing the weakest input was not clearly 90\u00b0 counterclockwise from the direction producing the greatest input. In contrast, for the 5 networks that maintained the least amount of information in neuronal activity during the delay (Figure 3K), their tuning curves were strongly asymmetric with the direction producing the weakest input 90\u00b0 counterclockwise from the direction producing the greatest input. We confirmed that these results were not specific to this one task by repeating our analysis on 20 networks with a DMRS task using a 90\u00b0 counterclockwise rule (Figure S3). These results were the mirror image of the results shown in Figure 3. We also repeated our analysis on a delayed match-to-category (DMC) task, in which the networks had to indicate whether sample and test stimuli belonged to the same learned categories32. We found that the networks performed the manipulation (i.e. categorized the stimulus) by adjusting connection weights from the input layer (Figure S4). Given that we imposed an incentive for the networks to reduce neuronal activity, our results suggest that networks will attempt to perform the required manipulation of the sample stimulus by learning specific connection weights from the input layer if possible. In summary, STP was not directly involved in manipulating the sample stimulus in the DMRS task. Rather, networks performed the manipulation by adjusting connection weights from the input layer. However, this analysis reveals that STP can generate a prospective code33, facilitating the proper network response to upcoming test stimuli. Manipulating information during the WM delay period \u00b6 We considered whether STP can play a more active role manipulating information if the manipulation cannot be performed by adjusting input connection weights. This could be accomplished by forcing the manipulation to occur after the sample stimulus is extinguished\u2014when the input layer no longer remains active. We trained networks to solve a delayed cue task (Figure 4A), in which a cue was presented between 500 and 750 ms into the delay, instructing the network whether identical sample and test directions were a match (DMS), or whether a test direction rotated 90\u00b0 clockwise from a sample was the target (DMRS). Figure 4 \u00b6 Neuronal and synaptic mnemonic encoding for tasks that involve manipulating the contents of working memory. (A) This task was similar to the DMS and DMRS tasks, except that a rule cue from 500 to 750 ms into the delay period indicated to the network whether to perform the DMS or the DMRS task. (B) Similar to Figure 2B, except showing the decoding results for DMS trials for networks trained to solve the delayed rule task. The dashed vertical lines, from left to right, indicate the sample onset, offset, the rule cue onset, and end of the delay period. (B) Same as (A), except showing the decoding results for DMRS trials (in which matching sample and test stimuli are rotated by 90\u00b0). \u00b6 We found that none of the 20 networks silently-maintained information for either trial type as the sample neuronal decoding accuracy never decreased to chance levels (P > 0.05, bootstrap) during the delay for either DMS (green curves, Figure 4B) or DMRS trials (Figure 4D). Thus, these networks manipulate information in WM (at least) partly using persistent neuronal activity. We next examined the contribution of neuronal activity and synaptic efficacies toward solving the task by shuffling neuronal activity or synaptic efficacies immediately prior to test onset. Consistent with Figures 2&3, task accuracy after shuffling activity (red circles) was negatively correlated with neuronal decoding accuracy during the end of the delay (DMS: R = \u22120.51, P = 0. 021, Figure 4C; DMRS: R = \u22120.49, P = 0.028, Figure 4E), implying that for networks showing weak neuronal decoding, shuffling neuronal activity had little impact on behavior. Furthermore, behavioral accuracy after shuffling synaptic efficacies (cyan circles) was positively correlated with neuronal decoding accuracy during the end of the delay (DMS: R = 0.66, P = 0.002, Figure 4C; DMRS: R = 0.71, P < 0.001, Figure 4E), implying that networks showing weak neuronal decoding were strongly affected by shuffling synaptic efficacies. Furthermore, shuffling neuronal activity or synaptic efficacies significantly decreased behavioral accuracy (P < 0.05, bootstrap) in all 20 networks for both tasks. Thus, networks required neuronal activity to manipulate information maintained in WM. However, consistent with previous tasks, information was maintained in a hybrid state during the delay. Controlling the representation of information \u00b6 Although STP did not silently manipulate information during the tasks considered so far, we wondered if it could allow for subtler manipulations of information in a silent manner. For example, neural circuits in vivo are occasionally required to represent behaviorally-relevant information in a different format compared to irrelevant information34. Thus, we trained networks on a task that required networks to control how information is represented in the network: the A-B-B-A task35 (Figure 5A). Here, networks were presented with a sample stimulus followed by three sequential test stimuli, and had to indicate whether each test stimulus matched the sample. Importantly, if a test stimulus did not match the sample, there was a 50% probability that the test stimulus would be repeated on the subsequent test presentation. This forces the network to encode sample and test stimuli in different ways: if the sample and test were represented in similar manners, then the network could not distinguish between a test stimulus that matched the sample (match event), compared to a test stimulus that matched the previous test (non-match event). We provide evidence that the networks represent sample and test stimuli in different formats below. Figure 5 \u00b6 Neuronal and synaptic mnemonic encoding for tasks that require controlling how information is represented. (A) In the A-B-B-A and A-B-C-A task, the network was presented with a 400 ms motion direction stimulus, followed by three 400 ms motion direction test stimuli, in which all stimuli were separated by a 400 ms delay. The network was trained to indicate whether each test stimulus matched the motion direction of the sample stimulus. For test stimuli two and three in the A-B-B-A task, there was a 50% chance that a non-matching test stimulus would have the motion direction as the previous non-matching test stimulus. For the A-B-C-A task, non-matching test stimuli never matched any previous test stimuli. (B) Similar to Figure 2B, the time course of the sample direction decoding accuracy, calculated using neuronal activity (green curve) or synaptic efficacy (magenta curve) are shown for all 20 networks trained on the A-B-C-A task. The dashed lines indicate, from left to right, the sample onset, the sample offset, and the test onset and offset for the three sequential test stimuli. (C) Similar to (B), except showing the decoding accuracy of the first test stimulus. (D) The time course of the tuning similarity index (the weighted dot-product between the preferred sample motion direction and the preferred first test motion direction averaged across all synaptic efficacies), averaged across all 20 networks. (E-G) Similar to (B-D), except for the A-B-B-A task. (H) The mean tuning similarity index is shown for the A-B-B-A task, after suppressing neuronal activity for 200 ms before the first test onset, from four different groups of neurons (excitatory neurons with facilitating synapses, blue curve; excitatory neurons with depressing synapses, red curve; inhibitory neurons with facilitating synapses, green curve; inhibitory neurons with depressing synapses, cyan curve), and for the case when activity was not suppressed (black curve). (I) Behavioral task accuracy for the cases when no activity was suppressed (black bar), and after suppressing activity from the four groups of neurons described in (H). \u00b6 As a control, we also trained networks on an A-B-C-A version of the task, which was similar to the A-B-B-A task, except that non-matching test stimuli were never repeated during a single trial, so that the network is not forced to represent sample and test stimuli in a different manner. For the A-B-C-A task, most networks appeared to solve the task using information in synaptic efficacies, as few networks encoded sample information in neural activity throughout the entire trial. We found that sample decoding using neuronal activity decreased to chance levels (P>0.05, bootstrap) for 1 of 20 networks during the last 100 ms of the first delay, 15 of 20 networks during the second delay, and 18 of 20 networks for the third delay (green curves, Figure 5B). In contrast, sample decoding using synaptic efficacies (magenta curves) remained significantly above chance (P<0.05) throughout the entire trial for all 20 networks (values ranging from ~0.55 to 1.0). We note that decoding accuracy appeared relatively lower for this task because of how we performed the calculation (see Supplementary Information). Since networks were trained to compare each test direction to that of the previous sample, it made sense that sample information was represented throughout the trial. However, do networks also maintain test information in WM, which is only behaviorally relevant during test presentation? We found that decoding accuracy for the first test stimulus using neuronal activity (green curves) was perfect (1.0) for all networks during test presentation, before rapidly dropping to chance levels (P>0.05) for all 20 networks by the third delay period (Figure 5C). Test decoding using synaptic efficacies (magenta curves) was near perfect (~1.0) for all networks during the later stage of the first test presentation, the subsequent delay, and into the second test presentation. Thus, both the sample and first test motion directions were mnemonically encoded by the network during presentation of the second test stimulus. This could be problematic if the network had to distinguish between cases where the second test stimulus matched the sample vs the first test. However, this was not a problem for the A-B-C-A task, as non-matching test stimuli were never repeated. As discussed above, the network was under no pressure to represent sample and test stimuli differently during the A-B-C-A task, which we confirmed using a tuning similarity index10, which was the weighted dot-product between the preferred sample first test directions averaged across all synaptic efficacies (Figure 5D see Methods). An index value of +1 indicates that synaptic efficacies are identically tuned to sample and test stimuli, 0 indicates no correlation between the two, and \u22121 indicates that synaptic efficacies prefer opposite sample and test directions. As expected, the tuning similarity index was >0.9 for the first and second test periods. This implies that for each synapse, the preferred sample tended to be similar to the preferred test\u2014suggesting similar representation of sample and test information by synaptic efficacies. We then repeated these analyses for the A-B-B-A task, in which subsequent non-matching test stimuli were repeated 50% of the time. As with the previous task, we decoded the sample motion direction using neuronal activity (green curves, Figure 5E) and synaptic efficacies (magenta curves). In contrast to the A-B-C-A task, sample decoding accuracy using neuronal activity in the A-B-B-A task remained above chance (P<0.05, bootstrap) during all three delay periods for 19 out of 20 networks. Furthermore, sample decoding accuracy using synaptic efficacies remained close to 1.0 throughout the trial. Consistent with the A-B-C-A task, decoding accuracy for the first test stimulus using neuronal activity (green curves, Figure 5F) was perfect (1.0) during the first test presentation before rapidly falling to chance (P>0.05) levels after test stimulus offset. Also consistent with the A-B-C-A task, decoding accuracy for the first test stimulus using synaptic efficacies (magenta curves) was also near perfect (~1.0) for all networks during the later stages of the first test presentation, the subsequent delay, and into the second test presentation. As alluded to above, it is potentially problematic that the sample and the first test stimulus were both encoded by the network during the second test presentation, as the network must distinguish between cases where the second test matched the sample (match event), or the first test (non-match event). We hypothesized that the sample and the first test stimuli must be represented in different formats for networks to accurately solve the task. To confirm this, we calculated the tuning similarity index for all 20 networks. In contrast to the A-B-C-A task (Figure 5D), in which the mean tuning similarity index was 0.919 during the second test period, the similarity index for the A-B-B-A task decreased to 0.232 (Figure 5G, P < 10\u221217, unpaired two-sided t-test). Thus, for synapses in the A-B-B-A task, preferred sample stimuli tended to differ from preferred test stimuli, implying different encoding formats. By so doing, the network can theoretically distinguish between cases in which subsequent test stimuli match the sample (match event) vs earlier test stimuli (non-match event) We next asked how the network was able to represent the sample and first test stimuli in different formats. We hypothesized that delay-period persistent neuronal activity was necessary to encode the first test stimulus in a different format than the sample. Thus, we suppressed neuronal activity from four different groups of neurons for the 200 ms period prior to the first test, and re-calculated the tuning similarity index (Figure 5H). Suppressing activity from inhibitory neurons with facilitating synapses prior to the test stimulus significantly increased the mean tuning similarity index, measured during the second test period (no suppression = 0.232, suppression = 0.516, green curve, P < 10\u22124, paired, two-sided, t-test). Furthermore, suppressing the activity of inhibitory neurons with facilitating synapses decreased behavioral accuracy significantly more than suppressing the activity of any of the other three groups of neurons (Figure 5I, P < 10\u22124 for all three comparisons, paired, two-sided, t-test). Thus, neuronal activity from these neurons is required to manipulate information in short-term memory, increasing task performance. In summary, delay-period neuronal activity from inhibitory neurons with facilitating synapses was required to represent sample and test information in different formats, and suppressing this activity resulted in similar sample and test representations along with a decrease in behavioral accuracy. Interestingly, this contrasts with the results of the DMRS task (Figure 3), in which inhibitory neurons with depressing synapses played a critical role in representing task-relevant information. Thus, while sample information maintained in synaptic efficacies was primarily used by the networks to solve the A-B-C-A and A-B-B-A tasks, neuronal activity was required to control how information was represented. Attending to specific memoranda \u00b6 Recent studies suggest that silently-maintained information can be reactivated either by focusing attention towards the memorandum19 or by probing the neural circuits involved20. In a final experiment, we examined how STP supports short-term maintenance of information that is either attended or unattended. We trained networks on a dual-sample delayed matching task (Figure 6A) that roughly follows the study by Rose et al.19. The network was trained to maintain two sample directions (presented simultaneously in two different locations) in WM, followed by two successive cues and test stimuli, in which the cue indicated which of the two samples was relevant for the upcoming test. In this setup, it is possible that stimuli that were not cued as relevant for the first test stimulus, could still be cued as relevant for the second test stimuli. Thus, stimulus could switch from being unattended to attended. Figure 6 \u00b6 Neuronal and synaptic mnemonic encoding of multiple stimuli. (A) In the dual DMS task, two sample stimuli were simultaneously presented for 500 ms. This was followed by a 1000 ms delay period in which a cue appeared halfway through, and then two simultaneous test stimuli for 500 ms. The cue indicated which of the two sample/test pairs were task-relevant. Another 1000 ms delay and 500 ms test period was then repeated, in which a second cue again indicated which of the two sample/test pairs was task-relevant. (B) Neuronal decoding accuracy for the attended (blue curve) and unattended (red) stimuli, and the synaptic decoding accuracy for the attended (black) and unattended (yellow) stimuli, are shown from trial start through the first test period (left panel), the second delay and test periods (right panel). (C) Scatter plot showing the neuronal decoding accuracy, measured from 100 to 0 ms before the second cue (x-axis) against the neuronal decoding accuracy, measured from 400 to 500 ms after second cue (y-axis). Blue dots represent stimuli that were unattended after the first cue, and attended after the second cue, and red dots represent stimuli that were not attended to after the first and second cues. (D) The neuronal (green) and synaptic (magenta) rule decoding accuracy. The dashed lines indicate the decoding accuracy of the first cue, and the solid lines indicate the decoding accuracy of the second cue. \u00b6 Across the 20 networks, the mean sample decoding accuracy using neuronal activity was significantly greater when the sample was attended (blue curve) compared to unattended (red curve), during the last 100 ms of the first and second delay periods (first delay: Figure 6B, left panel, attended = 0.455, unattended = 0.354, P < 0.001, paired, two-sided t-test; second delay: right panel, attended = 0.335, unattended = 0.211, P < 10\u22125) Sample decoding accuracy using synaptic efficacies was near perfect (~1.0) for both the attended (black curves) and unattended (yellow curves) conditions. Thus, the attended memoranda were more strongly represented in neural activity compared to the unattended memoranda. The study by Rose et al. found that unattended information that was silently maintained could be reloaded back into neuronal activity after it was attended19. Similarly, we found that neural decoding significantly increased (P < 10\u22125, two-sided t-test) for stimuli that were unattended after the first cue became the focus of attention after the second cue (blue dots, Figure 6C), compared to stimuli that remained unattended to (red dots). Although sample decoding accuracy using neuronal activity was not significantly greater than chance for many networks, the rule cue indicating which stimulus to attend to was encoded and maintained in neuronal activity (neuronal decoding accuracy for rule cue #1 is indicated by the light, dashed green curve, and rule cue #2 is indicated by the dark, dashed green curve, Figure 6D). Thus, while sample information can be silently maintained, allocating attention to either memoranda requires neuronal activity. Manipulating information and persistent neuronal activity \u00b6 While STP can allow networks to silently maintain information, persistent neuronal activity during the delay period was observed in all the tasks that involved manipulating information (DMRS, delayed cue, ABBA, attention in the dual-sample task). We wondered if the level of manipulation required by the task was correlated with the level of persistent activity. This could be of special interest as varying levels of persistent activity have been observed between different tasks10\u201316. We reasoned that for tasks that did not require manipulation, the network should represent the sample stimulus in fundamentally the same way during all trial epochs (e.g. early sample vs late delay). One caveat is that networks trained on tasks that did not require manipulation (e.g. DMS) did not represent the sample in neuronal activity at the end of the delay. However, in all cases the sample stimulus was represented in synaptic efficacies, with high decoding accuracy throughout the delay. Thus, we quantified the level of manipulation for each task by computing the similarity (same as used in Figure 5, see Methods) between the neuronal response during the first 100 ms of sample onset, and the synaptic efficacies during the last 100 ms of the delay. We subtracted this value from 1.0, so that a manipulation near 0 implies that early-sample tuning was similar to late-delay synaptic tuning. To boost statistical power, we included two additional delayed match-to-rotated sample tasks in which the target test direction was 45\u00b0 clockwise from the sample direction (DMRS45) and 180\u00b0 from the sample direction (DMRS180). We found that the level of manipulation was correlated with the level of stimulus-selective persistent activity (i.e. neuronal decoding accuracy) measured during the end of the delay (Spearman correlation coefficient R = 0.79, P = 0.006). This suggests that tasks requiring greater manipulation require greater levels of persistent activity. Thus, different levels of manipulation could partly explain the observation that persistent neuronal activity varies markedly between tasks. Discussion \u00b6 We examined whether STP can support the activity-silent manipulation of information, and whether it could help explain previous observations that different tasks evoke different levels of persistent activity. We found that while STP can silently support the short-term maintenance of information, it cannot support manipulation of information without persistent neuronal activity. Furthermore, we found that tasks that required more manipulation also required more persistent activity, giving insight into why the strength of persistent neuronal activity varies markedly between different tasks. Variation in persistent neuronal activity in vivo \u00b6 Over the last several decades, monkey electrophysiology experiments2\u20136, and later human imaging studies36, have supported the idea that information in WM is maintained in stimulus-selective persistent neuronal activity during memory-delay periods of behavioral tasks. However, this viewpoint has evolved, as various studies now suggest that persistent neuronal activity might not always reflect information maintenance, but can reflect control processes required to manipulate remembered information into appropriate behavioral responses14. It is often unclear whether persistent neural activity reflects the maintenance or the manipulation of the stimulus. For example, neural activity in the frontal and parietal cortices mnemonically encode stimulus location in a memory delayed saccade task2,4. However, recent studies that have dissociated the stimulus location from the upcoming saccade location have shown that activity in frontal cortex initially encodes the location of the recent stimulus (retrospective code), before its representation shifts towards encoding the planned saccade target (prospective code) later in the delay37. In another example, prefrontal cortex (PFC) was shown to mnemonically encode color in a change-detection task when six, highly distinguishable, colors are used38, but color-selective persistent activity was not evident in PFC when the subject had to detect a change amongst a continuum of 20 colors12. This might suggest that PFC activity can encode a categorical representation of the stimulus, but not a precise representation of stimulus features. Similarly, delay-period direction encoding in the parietal cortex is weak during a delayed-matching task, but after the subject undergoes extensive categorization training using the same stimuli, delay-period categorical encoding becomes highly robust16. These results suggest that persistent neuronal activity emerges as a result of manipulating the stimulus representation into another format such as a categorical code or a planned behavioral response. In contrast, studies have found that persistent neuronal activity is weak39 to non-existent40,41 in sensory areas, which are expected to be more involved in stimulus representation rather than manipulation. Despite the relative lack of persistent neuronal activity, studies have found that stimulus-selective information can still be decoded from local-field potentials41 and from blood-oxygenation levels42, suggesting that subthreshold top-down signals from frontal areas may coordinate with posterior areas to maintain information in short-term memory. In summary, these studies suggest that tasks that require greater manipulation of the memoranda evoke greater levels of persistent neuronal activity across a greater number of cortical areas. Although other factors undoubtedly influence the level of persistent neuronal activity, such as circuit properties10,43 and other task-related variables44, we observed a similar correlation between the level of manipulation and the level of persistent neuronal activity in our network models (Figure 7). Thus, our results suggest that the variability in the level of persistent neuronal activity observed between different tasks is partially the result of varying levels of stimulus manipulation required between tasks. Figure 7 \u00b6 The relationship between the manipulation required by the task and the level of stimulus-selective persistent neuronal activity. Scatter plot shows the level of stimulus-selective persistent neuronal activity, measured as the neuronal decoding accuracy during the last 100 ms of the delay period (x-axis), versus the level of manipulation, which was equal to 1 minus the tuning similarity between the neuronal tuning during the first 100 ms of the sample period, and the synaptic tuning during the last 100 ms of the delay period (y-axis). Each dot represents the average values across 20 networks trained on one specific task, or the across one trial type from one specific task (i.e. the DMS and DMRS trials from the delayed rule task, and the attended and unattended trials from the dual-sample delayed matching task). Errors bars indicate one SEM. Dashed vertical line indicates chance level decoding. Comparison to other artificial neural network architectures \u00b6 Until recently, it was difficult to train RNNs to solve tasks involving long-term dependencies, in which the desired network outputs depend on inputs separated by long temporal delays. In the last several years, long short-term memory (LSTM)45 and more recently, gated recurrent unit (GRU)46 architectures have shot to prominence by allowing RNNs to solve a variety of tasks such as language modelling and translation that involve very long temporal delays. These architectures work by allowing networks to determine which information to maintain across time, and which information to update. We noticed that RNNs without STP either failed to accurately solve the task, or required many more training epochs. This was true even when setting the penalty on neuronal activity to zero (Figure S5). This difficulty in solving the tasks was partly because neurons in our networks never connected onto themselves, which could otherwise help neuron maintain information in short-term memory. That said, STP, with its relatively long time constants, greatly facilitated training on our set of WM based tasks. This highlights how adding network substrates with long time constants, without necessarily making these time constants flexible, can significantly decrease training time on tasks with long-term time dependencies. More generally, it also highlights how incorporating neurobiologically-inspired features to artificial neural networks is a promising strategy for advancing their capabilities. A recent study proposed a different recurrent network architecture in which activity was temporarily maintained in \u201cfast weights\u201d, which operated on a slower timescale than changes in neural activity, but faster than changes in synaptic weights47. Adding this different time constant allowed the network to more accurately solve tasks that required information maintenance across input sequences. These fast weights are reminiscent of the dynamic synapses regulated by STP considered in this study, and further suggest how adding a range of effective time constants to RNN models can improve performance on tasks that require maintaining information across different time scales. Understanding strategies employed by artificial neural networks \u00b6 A key differentiating feature of RNNs compared to biological networks is that all connection weights and activity is known, facilitating any attempt to understand how the network solves various tasks. This has allowed researchers to describe how delayed association in RNNs can be performed by transient dynamics25, how simple dynamical systems consisting of line attractors and a selection vector can underlie flexible decision-making23, how RNNs can encode temporally-invariant signals27,26, and how functional clustering develops when RNNs learn multiple tasks48. While modelling studies cannot directly replace experimental work, they can be highly advantageous when obtaining the necessary experimental data is not feasible. Such was the case with our study, in which current technology does not allow us to directly measure the information maintained in synaptic efficacies or their contribution towards the behavioral decision. Thus, these modelling studies can serve as an excellent complement to experimental work, allowing researchers to rapidly generate novel hypothesis regarding cortical function that can later be tested when technology better allows for experimental verification. Lastly, the discovery of novel mechanisms found in silico can be fed back into the design of network models, potentially accelerating the development of machine learning algorithms and architectures. We believe that this synergy between experimental neuroscience and neural network modelling will only strengthen in the future. Methods \u00b6 Neural networks were trained and simulated using the Python machine learning framework TensorFlow49. The code used to train, simulate and analyze network activity is available at https://github.com/nmasse/Short-term-plasticity-RNN. All parameters used to define the network architecture and training are given in Table 1. Table 1 \u00b6 Network models \u00b6 All networks consisted of 36 or 48 input neurons (whose firing rates are represented as u(t) that projected onto 100 recurrently connected neurons (whose firing rates are represented as r(t)), which in turn projected onto 3 output neurons (whose firing rates are represented as z(t)) (Figure 2A). The activity of the recurrent neurons was modelled to follow the dynamical system48: \\tau \\frac{dr}{dt} = - r + f(W^{rec} r + W^{in} u + b^{rec} + \\sqrt{2\\tau} \\sigma_{rec} \\zeta) where \u03c4 is the neuron\u2019s time constant, f (\u00b7) is the activation function, Wrec and Win are the synaptic weights between recurrent neurons, and between input and recurrent neurons, respectively, b is a bias term, \u03b6 is independent Gaussian white noise with zero mean and unit variance applied to all recurrent neurons, and \u03c3rec is the strength of the noise. To ensure that neuron\u2019s firing rates were non-negative and non-saturating, we chose the rectified linear function as our activation function: f (x) = max(0, x) The recurrent neurons project linearly to the output neurons. The activity of the output neurons, z, was then normalized by a softmax function such that their total activity at any given time point was one: z = g(W^{out} r + b^{out}) where Wout are the synaptic weights between the recurrent and output neurons, and g is the softmax function: g(x_i) = \\frac{\\exp(x_i)}{\\sum_j{\\exp(x_j)}} To simulate the network, we used a first-order Euler approximation with time step \u0394t: r_t = (1 - \\alpha) r_{t-1} + \\alpha f(W^{rec} r_{t-1} + W^{in} u_t + b + \\sqrt{\\frac{2}{\\alpha}} \\sigma_{rec} N(0,1)) where \u03b1 = \u0394t/\u03c4 and N(0.1) indicates the standard normal distribution. To maintain separate populations of excitatory and inhibitory neurons, we decomposed the recurrent weight matrix, Wrec as the product between a matrix for which all entries are non-negative, Wrec,+ whose values were trained, and a fixed diagonal matrix, D, composed of 1s and \u22121s, corresponding to excitatory and inhibitory neurons, respectively30: W^{rec} = W^{rec, +} D D = \\begin{bmatrix} 1 & & \\\\ & \\ddots & \\\\ & & -1 \\\\ \\end{bmatrix} Initial connection weight values were randomly sampled from a Gamma distribution with shape parameter of 0.25 and scale parameter of 130. We note that training networks to accurately solve the tasks appeared somewhat faster when initializing connection weights from a gamma distribution compared to a uniform distribution (data not shown). Initial bias values were set to 0. Networks consisted of 36 motion direction tuned input neurons, and for the rule switching tasks (i.e. DMS+DMRS and dualDMS tasks), an additional 12 rule tuned neurons. The tuning of the motion direction selective neurons followed a Von Mises\u2019 distribution, such that the activity of the input neuron i at time t was u_t^i = A \\exp(\\kappa \\cos{(\\theta - \\theta_{pre\\, f})}) + \\sqrt{\\frac{2}{\\alpha}} \\sigma_{in} N(0,1) where \u03b8 is the direction of the stimulus, Embedded Image is the preferred direction of input neuron i, k was set to 2, and A was set to 4/exp(k) when a stimulus present (i.e. during the sample and test periods), and was set to zero when no stimulus was present (i.e. during the fixation and delay periods). For the dualDMS task, 18 motion direction tuned input neurons represented the first stimulus position, and the second represented the second stimulus position. The 12 rule tuned neurons for the DMS+DMRS and dualDMS tasks had binary tuning, in which their activity was set to 2 (plus the Gaussian noise term) for their preferred rule cue, and 0 (plus the Gaussian noise term) for all other times. The number of rule tuned neurons was arbitrarily chosen, and has little impact on network training. Network training \u00b6 RNNs were trained based on techniques previously described30,48. Specifically, the goal of training was to minimize 1) the cross-entropy between the output activity and the target output, and 2) the mean L2-norm of the recurrent neurons\u2019 spike rate. The target output was a length 3 one-hot vector, in which the first element was equal to one for all times except the test period, the second element was one when the test stimulus matched the sample, and the third element was one when the test stimulus did not match the sample. Specifically, the loss function at time t during trial i is L_{i, t} = - \\sigma_{n=1}^{N_{out}}{m^i(t) z_n^{\\text{target}, i}}(t) \\log{z_n^i}(t) + \\frac{\\beta}{N_{rec}} \\sum_{n=1}^{N_{rec}}{r_n^2(t)} where \u03b2 controls how much to penalize neuronal activity of the recurrent neurons, and mi(t) is a mask function, with values of zero or one, that determine which portions of the trial to include in the loss function. To give the network adequate time to form a match or non-match decision, we set the mask function to zero for the first 50 ms of the test period, and one for all other times. The total loss function is then L = \\frac{1}{N_{\\text{trials}} N_{\\text{time}}} \\sum_i^{N_{\\text{trials}}} \\sum_t^{N_{\\text{time}}} L_{i, t} During training, we adjusted all parameters (Win,Wrec,+,Wout,brec,bout) using the Adam50 version of stochastic gradient descent. The decay rate of the 1st and 2nd moments were set to their default values (0.9 and 0.999, respectively). Short-term synaptic plasticity \u00b6 The synaptic efficacy between all recurrently connected neurons was dynamically modulated through short-term synaptic plasticity (STP). For half of the recurrent neurons (40 excitatory and 10 inhibitory), all projecting synapses were facilitating, and for the other half of recurrent neurons, all projecting synapses were depressing. Following the conventions of Mongillo et al.18, we modelled STP as the interaction between two values: x, representing the fraction of neurotransmitter available, and u, representing the calcium concentration in the presynaptic terminal. Presynaptic activity acts to increase the calcium concentration inside the presynaptic terminal, increasing synaptic efficacy. However, presynaptic activity decreases the fraction of neurotransmitter available, leading to decreasing efficacy. These two values evolve according to: \\begin{align*} \\frac{dx(t)}{dt} &= \\frac{1 - x(t)}{\\tau_x} - u(t) x(t) r(t) \\\\ \\frac{du(t)}{dt} &= \\frac{U - u(t)}{\\tau_u} + U(1 - u(t)) r(t) \\end{align*} where r(t) is the presynaptic activity at time t, \u03c4x is the neurotransmitter recovery time constant, and \u03c4u is the calcium concentration time constant. The amount of input the postsynaptic neuron receives through this one synapses at time t is then I(t) = W x(t) u(t) r(t) where W is the synaptic efficacy before STP is applied. For depressing synapses, the neurotransmitter recovery time constant was much longer compared to the calcium concentration time constant, whereas the opposite was true for facilitating synapses. Population decoding \u00b6 Similar to our previous studies10,16, we quantified the strength of stimulus encoding by measuring how accurately we could decode the motion direction using multiclass support vector machines (SVMs). In this approach, we trained linear, multiclass SVMs to classify the motion direction using the neuronal activity of the 100 recurrent neurons, or the synaptic efficacies from the same 100 recurrent neurons, at each time point. The synaptic efficacy values were the product x(t)u(t) where x(t) and u(t) are the time varying values representing the amount of neurotransmitter available, and the calcium concentration, respectively, as described above. We measured the classification accuracy using cross-validation, in which we randomly selected 75% of trials for training the decoder and the remaining 25% for testing the decoder. For each of the eight motion directions, we randomly sampled, with replacement, 20 trials to train the decoder (from the 75% of trials set aside for training), and 20 trials to test the decoder (from the 25% of trials set aside for testing). From the 160 trials in the test set (20 time 8 directions), the accuracy was the fraction of trials in which the predicted motion direction matched the actual motion direction. We then repeated this sampling procedure 100 times to create a decoder accuracy distribution for each time point. The difference was deemed significantly greater than chance if 98 values were greater than chance (equivalent to P < 0.05 for a two-sided test). For each network, we calculated decoding accuracies using a batch of 1024 trials in which the test motion directions were randomly sampled independently of the sample motion direction. This was in contrast to how we trained the network and measured behavioral accuracy, in which there was always a 50% chance that a test stimulus would match the sample. We note that the pattern of neural and synaptic activity generated by a sample stimulus will be similar to the pattern generated by a matching test stimulus. Thus, if matching test stimuli occur more frequently than chance, our sample decoding accuracy during and after the test stimuli would be artificially elevated. Measuring the contribution of neuronal activity and synaptic plasticity towards solving the task \u00b6 To measure how network models used information maintained in neuronal activity and in dynamic synaptic efficacies to solve the task, we used a shuffling procedure as follows. At the time point right before test onset (or right before the third test onset for the A-B-C-A/A-B-B-A tasks), we either 1) did not shuffle any activity, 2) shuffled the neuronal activity between trials, or 3) shuffled the synaptic efficacies between trials. We shuffled between trials as opposed to between neurons because neurons can have different baseline activity levels, and shuffling this activity can significantly perturb the network and degrade performance, even if no information is maintained in their activity. We then simulated the network activity for the remainder of the trial using the saved input activity, and measured the performance accuracy by comparing the activity of the three output neurons to the target output activity during the test period. We performed this random shuffling 100 times, and measured the mean performance accuracy for all three conditions. The rationale behind this analysis is that if the network was exclusively using information maintained in neuronal activity to solve the task, then shuffling neuronal activity between trials should devastate performance, while shuffling synaptic efficacies should have little effect. Similarly, if the network was exclusively using information maintained in synaptic efficacies to solve the task, then shuffling synaptic efficacies between trials should devastate performance, while shuffling neuronal activity should have little effect. If the network was using information maintained in both neuronal activity and synaptic efficacies, then shuffling either should lead to significant decreases in performance. Tuning similarity index \u00b6 We measured the similarity between sample and test stimuli encoding in the A-B-C-A and A-B-B-A tasks (Figure 5) using a similarity index we previously employed to study the relation between functional clustering and mnemonic encoding10. To calculate this index, we first modelled the synaptic efficacy for all neurons as a linear function of the sample or test motion direction, represented by the unit vector d: z_i(t) = dH_i(t) + \\epsilon_i(t) where \\epsilon_i(t) is a Gaussian noise term and the vector Hi(t) relates the stimulus direction to the synaptic efficacy. The angle of Hi(t) is the preferred direction of the neuron at time t, and its magnitude indicates the change in synaptic efficacy from baseline when the stimulus matches the preferred direction of the synapse. Thus, the preferred direction of a synapse, represented as a unit vector, is PD_i(t) = \\frac{H_i(t)}{\\| H_i(t) \\|} We can calculate how well this linear model fit the data for each synapse i and time point t indicated by wi(t), by comparing the variance in the residuals with the variance in the synaptic efficacy: w_i(t) = 1 - \\frac{var(\\hat{z_i}(t) - z_i(t))}{var(z_i(t))} where the fitted synaptic efficacy is determined by the linear model: \\hat{z_i}(t) = \\text{baseline} + dH_i(t) For each synapses, we calculated these values for both the sample and first test motion direction, and then calculated the tuning similarity as the dot-product between their preferred sample and test motion directions directions, weighted by the geometric mean of their normalized linear model fits: s_i(t) = \\sqrt{w_i^{\\text{sample}}(t) w_i^{\\text{test}}(t)} PD_i^{\\text{sample}}(t) PD_i^{\\text{test}}(t) Finally, we calculated the similarity tuning index as the sum of the similarity scores for all synapses, divided by the sum of the geometric means of their respective linear model fits: \\text{similarity}(t) = \\frac{\\sum_i{s_i(t)}}{\\sum_i{\\sqrt{w_i^{\\text{sample}}(t) w_i^{\\text{test}}(t)}}} For the analysis examining whether preferred sample motion directions rotate during the DMRS task (Figure S2), we calculated how preferred sample motion directions changed across time based on the calculations above. To do so, we first represented the selectivity and preferred sample direction of neuronal activity or synaptic efficacies in complex form: z_i(t) = \\sqrt{w_i(t)} \\exp{(i PD_i(t))} We then calculated the weighted change in preferred sample directions between synapses at different times: \\Delta PD_i^{\\text{synaptic-synaptic}}(t1, t2) = z_i^{\\text{synaptic}}(t1) \\text{conj}(z_i^{\\text{synaptic}}(t2)) or the weighted change in preferred sample directions between neurons and their associated synapses at different times: \\Delta PD_i^{\\text{neuronal-synaptic}}(t1, t2) = z_i^{\\text{neuronal}}(t1) \\text{conj}(z_i^{\\text{synaptic}}(t2)) We then calculated the weighted histogram by binning the angular differences (angle(\u0394PD)) weighted by their absolute value (abs(\u0394PD)) When calculating the tuning similarity index for Figure 7, we note that the delay period and the number of test stimuli differed between tasks. Thus, to calculate the metric in a standard way across all tasks, we tested all networks with a sample stimulus lasting 500 ms followed by a 1000 ms delay. Category tuning index \u00b6 The category tuning index (CTI) measured the difference in synaptic efficacy (averaged across all trials for each direction) for each neuron between pairs of directions in different categories (a between category difference or BCD) and the difference in activity between pairs of directions in the same category (a within category difference or WCD)51. The CTI was defined as the difference between BCD and WCD divided by their sum. Values of the index could vary from +1 (which indicates strong binary-like differences in activity to directions in the two categories) to \u22121 (which indicates large activity differences between directions in the same category, no difference between categories). A CTI value of 0 indicates the same difference in firing rate between and within categories. References \u00b6 \u00b6 img{width: 50%; float: right;}","title":"190630 Masse, Nicolas Y., 2019"},{"location":"190630_MasseNicolasY_2019/#contents","text":"00. Summary 01. Introduction 02. Results 02.01. Network model Figure 1 02.02. Maintaining information in short-term memory Figure 2 Figure 3 02.03. Manipulating information 02.04. Manipulating information during the WM delay period Figure 4 02.05. Controlling the representation of information Figure 5 02.06. Attending to specific memoranda Figure 6 02.07. Manipulating information and persistent neuronal activity 03. Discussion 03.01. Variation in persistent neuronal activity in vivo Figure 7 03.02. Comparison to other artificial neural network architectures 03.03. Understanding strategies employed by artificial neural networks 04. Methods 04.01. Network models 04.02. Network training 04.03. Short-term synaptic plasticity 04.04. Population decoding 04.05. Measuring the contribution of neuronal activity and synaptic plasticity towards solving the task 04.06. Tuning similarity index 04.07. Category tuning index 05. References","title":"Contents"},{"location":"190630_MasseNicolasY_2019/#summary","text":"Recently it has been proposed that information in short-term memory may not always be stored in persistent neuronal activity, but can be maintained in \u201cactivity-silent\u201d hidden states such as synaptic efficacies endowed with short-term plasticity (STP). However, working memory involves manipulation as well as maintenance of information in the absence of external stimuli. In this work, we investigated working memory representation using recurrent neural network (RNN) models trained to perform several working memory dependent tasks. We found that STP can support the short-term maintenance of information provided that the memory delay period is sufficiently short. However, in tasks that require actively manipulating information, persistent neuronal activity naturally emerges from learning, and the amount of persistent neuronal activity scales with the degree of manipulation required. These results shed insight into the current debate on working memory encoding, and suggest that persistent neural activity can vary markedly between tasks used in different experiments.","title":"Summary"},{"location":"190630_MasseNicolasY_2019/#introduction","text":"Working memory refers to our ability to temporarily maintain and manipulate information, and is a cornerstone of higher intelligence1. In order to understand the mechanisms underlying working memory (WM), we must resolve the substrate(s) in which information in working memory is maintained. It has been assumed that information in WM is maintained in persistent neuronal activity2\u20136, likely resulting from local recurrent connections7,8, and/or cortical to subcortical loops9. However, recent experiments reveal that the strength of persistent neuronal activity varies markedly between tasks10\u201316. This raises two related questions: 1) why does persistent neural activity vary between tasks, and 2) for those tasks with weak or non-existent persistent activity, where and how is information maintained? A possible answer to the second question is that information is not necessarily maintained in persistent activity, but can be maintained through short-term synaptic plasticity (STP). STP, which is distinct from more commonly known long-term depression (LTD) and potentiation (LTP), is the process in which pre-synaptic activity increases the calcium concentration in the presynaptic terminal but depletes neurotransmitter stores, altering synaptic efficacies on timescales of hundreds of milliseconds to seconds17. Importantly, modelling studies have suggested that STP can allow networks to maintain an \u201cactivity-silent\u201d memory trace of a stimulus, in which short-term information is maintained without persistent activity18. Recent work in human subjects suggests that information can be mnemonically encoded in a silent, or latent, state, and that information can be reactivated into neuronal activity by probing the circuit19,20. While STP might provide another mechanism for the maintenance of information, it does not in itself fully account for why the strength of persistent activity varies between tasks. To answer this, we highlight that WM involves not just the maintenance of information, but also its manipulation. Importantly, manipulating information in short-term memory appears to engage the frontoparietal network differently compared to simply maintaining information21,22. While STP can allow for activity-silent maintenance of information, it is unknown whether STP can support activity-silent manipulation of information without persistent activity. If it cannot, then it suggests that the strength of persistent activity reflects the degree of information manipulation required by the task. In this study, we examine whether STP can support the silent manipulation of information in WM, and whether it could explain the variability in the strength of persistent activity between tasks. Unfortunately, with current technology it would be technically daunting to measure synaptic efficacies in awake behaving mice, and next to impossible in non-human primates. However, recent advances in recurrent neural network (RNN) model algorithms have opened an entirely new avenue to study the putative neural mechanisms underlying various cognitive functions. Crucially, RNN models have successfully reproduced the patterns of neural activity and behavioral output that are observed in vivo, and have generated novel insights into neural circuit function that would otherwise be unattainable through direct experimental measurement23\u201329. Here, we train biologically inspired RNN models, consisting of excitatory and inhibitory like neurons30 and dynamic synapses governed by STP18, to solve a variety of widely studied WM based tasks. These tasks involved maintaining information (Figure 2), manipulating the contents of short-term memory (Figures 3&4), changing how information is represented (Figure 5), and attending to specific memoranda (Figure 6). We show that STP can support the activity-silent maintenance of information, but that it cannot support the silent manipulation of information without persistent activity. Furthermore, we show that the strength of persistent neural activity covaries with the degree of manipulation. This potentially explains the widely observed observation that persistent activity varies markedly between tasks.","title":"Introduction"},{"location":"190630_MasseNicolasY_2019/#results","text":"The goal of this study was 1) to determine whether STP can support activity-silent manipulation of information in WM (Figures 2\u20136), and 2) whether STP can explain the variability in persistent activity observed in different tasks10\u201316 (Figure 7). We trained RNN models to solve several widely studied WM tasks, which varied in their specific cognitive demands (see below). In all tasks, the stimuli were represented as coherent motion patterns moving in one of eight possible directions. However, the results of this study are not meant to be specific to motion, or even visual, inputs, and the use of motion patterns as stimuli was simply used to make our example tasks more concrete. Furthermore, similar to the metabolic constraints faced by real brains31, we added a penalty on high neuronal activity (see Methods) so that networks were encouraged to solve tasks using low levels of neuronal activity.","title":"Results"},{"location":"190630_MasseNicolasY_2019/#network_model","text":"We constrained neurons in our network to be either excitatory or inhibitory30. The input layer of the network consisted of 36 excitatory, direction tuned neurons, whose responses across directions were modelled as a Von Mises function (see Methods). These input neurons projected onto a recurrently connected network of 80 excitatory and 20 inhibitory neurons (Figure 1A). Recurrent neurons never sent projections back onto themselves.","title":"Network model"},{"location":"190630_MasseNicolasY_2019/#figure_1","text":"Recurrent neural network design. (A) The core rate-based network model consisted of 36 motion direction tuned neurons that sent non-negative projections onto 100 recurrently connected neurons. Of these 100 neurons, 80 were excitatory (their output weights were non-negative) and 40 were inhibitory (their output weights were non-positive). The 80 excitatory neurons sent non-negative projections onto three decisions neurons. (B) For synapses that exhibited short-term synaptic depression (left panels), a 100 ms burst of presynaptic activity at 20 Hz (top panel) will increase the value representing the synaptic release probability (red trace, middle panel) and decrease the value representing the available neurotransmitter (blue trace). For these depressing synapses, the stronger and longer lasting decrease in release probability will lead to a decrease in synaptic efficacy (bottom panel) lasting thousands of milliseconds. For synapses that exhibited short-term synaptic depression (right panels), the same burst of presynaptic activity will lead to a stronger and longer lasting increase in release probability (middle panel), leading to an increase in synaptic efficacy lasting thousands of milliseconds.","title":"Figure 1"},{"location":"190630_MasseNicolasY_2019/#maintaining_information_in_short-term_memory","text":"We first examined how networks endowed with STP maintain information in WM using either persistent neuronal activity or using STP. We trained 20 networks to solve the delayed match-to-sample task (DMS, Figure 2A), in which the networks had to indicate whether sequentially presented (500 ms presentation; 1000 ms delay) sample and test stimuli were an exact match. Task accuracy for all networks in this study was >90%.","title":"Maintaining information in short-term memory"},{"location":"190630_MasseNicolasY_2019/#figure_2","text":"Neuronal and synaptic mnemonic encoding for the delayed match-to-sample task. (A) In the delayed match-to-sample (DMS) task, a 500 ms fixation period was followed by a 500 ms sample stimulus, which is represented as a coherent random dot motion pattern which could move in one of eight directions. This was followed by a 1000 ms delay period and finally a 500 ms test stimulus, which was also represented as motion in one of eight directions. The network was trained to indicate whether the motion directions of the sample and test stimuli matched. (B) The time course of the sample direction decoding accuracy, calculated using neuronal activity (green curves) or synaptic efficacy (magenta curves) are shown for all twenty networks that successfully solved the DMS task. The dashed vertical lines, from left to right, indicate the sample onset, offset, and end of the delay period. (C) Scatter plot showing the neuronal decoding accuracy measured at the end of the delay (x-axis) versus the behavioral accuracy (y-axis) for all 20 networks trained on this task (blue circles), the behavioral accuracy for the same 20 networks after neuronal activity was shuffled right before test onset (red circles) or synaptic efficacies were shuffled right before test onset (cyan circles). Thus, for each blue circle, there is a corresponding red and cyan circle with matching neuronal decoding accuracy. The dashed vertical line indicates chance level decoding accuracy.","title":"Figure 2"},{"location":"190630_MasseNicolasY_2019/#figure_3","text":"Understanding how networks solve the delayed match-to-rotated sample (DMRS) task. (A) The DMRS task is similar to the DMS task (Figure 2A), except that the network was trained to indicate whether the test motion direction was rotated 90\u00b0 clockwise from the sample motion direction. (B) The time course of the sample direction decoding accuracy, calculated using neuronal activity (green curves) or synaptic efficacy (magenta curves) are shown for all twenty networks that successfully solved the DMRS task. The dashed vertical lines, from left to right, indicate the sample onset, offset, and end of the delay period. (C) Scatter plot showing the neuronal decoding accuracy measured at the end of the delay (x-axis) versus the behavioral accuracy (y-axis) for all 20 networks trained on this task (blue circles), the behavioral accuracy for the same 20 networks after neuronal activity was shuffled right before test onset (red circles) or synaptic efficacies were shuffled right before test onset (cyan circles). (D) The neuronal sample tuning curves were calculated for four groups of neurons (excitatory neurons with facilitating synapses, blue; excitatory neurons with depressing synapses, red; inhibitory neurons with facilitating synapses, green; inhibitory neurons with depressing synapses, orange) from all 20 networks that solved the task. Neuronal activity was averaged across the entire sample period, and the tuning curves were centered around the sample direction that generated the maximum response (i.e. the preferred direction). Error bars (which are small and difficult to see) indicate one SEM. (E) Same as (D), except that neuronal activity at the end of the delay period was used to calculate the tuning curves. In order to compare how neural activity evoked during the sample evolves across the delay period, tuning curves were aligned to the preferred directions calculated in (D). (F) Same as (D), except that synaptic efficacies at the end of the sample period were used to calculate the tuning curves. As above, the preferred directions are the same as those used in (D). (G) Same as (D), except that synaptic efficacies at the end of the delay period were used. (H) Task accuracy after shuffling synaptic efficacies at the end of the sample period for each of the four groups of neurons. (I) Scatter plot showing the neuronal decoding accuracy measured at the end of the delay period (x-axis) against the task accuracy after shuffling the synaptic efficacies of inhibitory neurons with depressing synapses at the end of the sample period (y-axis). Each dot represents the results of one of the 20 networks. (J) Tuning curves showing the activity received from the neurons in the input layer, which was calculated by multiplying the motion direction tuning of the neurons in the input layer with the input-to-recurrent weight matrix. Results were averaged across the 5 networks with the greatest neuronal decoding accuracy during the last 100 ms of the delay period. The four curves thus indicate the mean amount of input each group of recurrent neurons (excitatory or inhibitory, facilitating or depressing) receives from the input layer for each direction. As above, the preferred directions are the same as those used in (D). (K) Same as (J), except showing the input tuning curves averaged across the 5 networks with the lowest neuronal decoding accuracy during the last 100 ms of the delay period.","title":"Figure 3"},{"location":"190630_MasseNicolasY_2019/#manipulating_information","text":"Given that STP can allow networks to silently maintain information in WM, we examined whether it could also allow networks to silently manipulate information. Thus, we repeated the analysis from Figure 2 on 20 networks trained to solve a delayed match-to-rotated (DMRS) sample task, in which the target test motion direction was rotated 90\u00b0 clockwise from the sample (Figure 3A). Neuronal decoding accuracy for this task (green curves, Figure 3B) was greater than the DMS task (P < 10\u22127, two-sided t-test, measured during last 100 ms of delay period), suggesting that more information was maintained in neuronal activity compared to networks trained on the DMS task. Unlike the DMS task, all 20 networks maintained information in a hybrid manner, with significantly elevated synaptic decoding accuracy at the end of the delay compared to chance (P<0.05, bootstrap), and shuffling either neuronal activity or synaptic efficacies significantly decreased behavioral accuracy (P<0.05, bootstrap, Figure 3C). We note that persistent activity exists for this task despite the penalty on high neuronal activity (see Methods). Consistent with the DMS task, we found that networks with strongest delay-period neuronal selectivity suffered the greatest performance loss when their neuronal activities were shuffled (R = \u22120.65, P = 0.002, Figure 3C), and that networks with the strongest delay-period neuronal selectivity suffered the least performance loss when their synaptic activities were shuffled (R = 0.75, P < 0.001). Although all 20 networks solved the task using persistent activity during the delay, we wondered whether it was still possible that STP was responsible for manipulating sample information. Thus we sought to understand the specific strategy the networks used to solve this task. We examined the mean neuronal responses averaged across the sample for all 20 networks from 4 groups of neurons: excitatory or inhibitory neurons with facilitating or depressing synapses (Figure 3D). We found a striking asymmetry for inhibitory neurons with depressing synapses, in which the sample direction that produced the maximum response was 90\u00b0 clockwise from the sample direction that produced the weakest response. We can quantify this asymmetry as the difference between the neuronal response 90\u00b0 clockwise and 90\u00b0 counterclockwise from the preferred sample direction, and this difference was the greatest for inhibitory neurons with depressing synapses compared to the other three groups of neurons (P < 10\u22125 for all three comparisons, paired, two-sided t-test). This asymmetry in the neuronal activity tuning functions mostly disappeared by the end of the delay period (Figure 3E), but was present in the synaptic efficacies measured at the end of the sample period (Figure 3F), and at the end of the delay period (Figure 3G). Thus, synaptic efficacies for inhibitory neurons with depressing synapses were at their maximum by the start of the test period on trials in which the sample stimulus was 90\u00b0 counterclockwise from their preferred direction. If such a sample stimulus is followed by a target test stimulus (90\u00b0 clockwise from the sample), the total amount of \u201ccurrent\u201d (neuronal response multiplied by synaptic efficacy) these neurons will project upon the rest of the network will be at a maximum. This inhibitory signal can alter the pattern of activity that excitatory neurons project onto the output layer, potentially decreasing the input into the \u201cnon-match\u201d output neuron while increasing the input into the \u201cmatch\u201d output neuron. Shuffling the synaptic efficacies of inhibitory neurons with depressing synapses at the end of the sample resulted in the lowest task accuracy compared to shuffling efficacies form the other three groups of neurons (P < 10\u22129 for all three comparisons, paired, two-sided t-test, Figure 3G). Furthermore, networks that maintained the least amount of information in neuronal activity during the delay period were more adversely affected by shuffling their synaptic efficacies (R = 0.85, P < 10\u22125, Figure 3H). We hypothesized that the asymmetric tuning of inhibitory neurons with depressing synapses was the result of the connection weights from the input layer. Thus, we calculated the tuning curves for the current (neuronal activity times the connection weight) neurons receive from the input layer. For the five networks that maintained the greatest amount of information in neuronal activity during the delay (Figure 3J), their tuning curves were again asymmetric, but the direction producing the weakest input was not clearly 90\u00b0 counterclockwise from the direction producing the greatest input. In contrast, for the 5 networks that maintained the least amount of information in neuronal activity during the delay (Figure 3K), their tuning curves were strongly asymmetric with the direction producing the weakest input 90\u00b0 counterclockwise from the direction producing the greatest input. We confirmed that these results were not specific to this one task by repeating our analysis on 20 networks with a DMRS task using a 90\u00b0 counterclockwise rule (Figure S3). These results were the mirror image of the results shown in Figure 3. We also repeated our analysis on a delayed match-to-category (DMC) task, in which the networks had to indicate whether sample and test stimuli belonged to the same learned categories32. We found that the networks performed the manipulation (i.e. categorized the stimulus) by adjusting connection weights from the input layer (Figure S4). Given that we imposed an incentive for the networks to reduce neuronal activity, our results suggest that networks will attempt to perform the required manipulation of the sample stimulus by learning specific connection weights from the input layer if possible. In summary, STP was not directly involved in manipulating the sample stimulus in the DMRS task. Rather, networks performed the manipulation by adjusting connection weights from the input layer. However, this analysis reveals that STP can generate a prospective code33, facilitating the proper network response to upcoming test stimuli.","title":"Manipulating information"},{"location":"190630_MasseNicolasY_2019/#manipulating_information_during_the_wm_delay_period","text":"We considered whether STP can play a more active role manipulating information if the manipulation cannot be performed by adjusting input connection weights. This could be accomplished by forcing the manipulation to occur after the sample stimulus is extinguished\u2014when the input layer no longer remains active. We trained networks to solve a delayed cue task (Figure 4A), in which a cue was presented between 500 and 750 ms into the delay, instructing the network whether identical sample and test directions were a match (DMS), or whether a test direction rotated 90\u00b0 clockwise from a sample was the target (DMRS).","title":"Manipulating information during the WM delay period"},{"location":"190630_MasseNicolasY_2019/#figure_4","text":"Neuronal and synaptic mnemonic encoding for tasks that involve manipulating the contents of working memory. (A) This task was similar to the DMS and DMRS tasks, except that a rule cue from 500 to 750 ms into the delay period indicated to the network whether to perform the DMS or the DMRS task. (B) Similar to Figure 2B, except showing the decoding results for DMS trials for networks trained to solve the delayed rule task. The dashed vertical lines, from left to right, indicate the sample onset, offset, the rule cue onset, and end of the delay period. (B) Same as (A), except showing the decoding results for DMRS trials (in which matching sample and test stimuli are rotated by 90\u00b0).","title":"Figure 4"},{"location":"190630_MasseNicolasY_2019/#controlling_the_representation_of_information","text":"Although STP did not silently manipulate information during the tasks considered so far, we wondered if it could allow for subtler manipulations of information in a silent manner. For example, neural circuits in vivo are occasionally required to represent behaviorally-relevant information in a different format compared to irrelevant information34. Thus, we trained networks on a task that required networks to control how information is represented in the network: the A-B-B-A task35 (Figure 5A). Here, networks were presented with a sample stimulus followed by three sequential test stimuli, and had to indicate whether each test stimulus matched the sample. Importantly, if a test stimulus did not match the sample, there was a 50% probability that the test stimulus would be repeated on the subsequent test presentation. This forces the network to encode sample and test stimuli in different ways: if the sample and test were represented in similar manners, then the network could not distinguish between a test stimulus that matched the sample (match event), compared to a test stimulus that matched the previous test (non-match event). We provide evidence that the networks represent sample and test stimuli in different formats below.","title":"Controlling the representation of information"},{"location":"190630_MasseNicolasY_2019/#figure_5","text":"Neuronal and synaptic mnemonic encoding for tasks that require controlling how information is represented. (A) In the A-B-B-A and A-B-C-A task, the network was presented with a 400 ms motion direction stimulus, followed by three 400 ms motion direction test stimuli, in which all stimuli were separated by a 400 ms delay. The network was trained to indicate whether each test stimulus matched the motion direction of the sample stimulus. For test stimuli two and three in the A-B-B-A task, there was a 50% chance that a non-matching test stimulus would have the motion direction as the previous non-matching test stimulus. For the A-B-C-A task, non-matching test stimuli never matched any previous test stimuli. (B) Similar to Figure 2B, the time course of the sample direction decoding accuracy, calculated using neuronal activity (green curve) or synaptic efficacy (magenta curve) are shown for all 20 networks trained on the A-B-C-A task. The dashed lines indicate, from left to right, the sample onset, the sample offset, and the test onset and offset for the three sequential test stimuli. (C) Similar to (B), except showing the decoding accuracy of the first test stimulus. (D) The time course of the tuning similarity index (the weighted dot-product between the preferred sample motion direction and the preferred first test motion direction averaged across all synaptic efficacies), averaged across all 20 networks. (E-G) Similar to (B-D), except for the A-B-B-A task. (H) The mean tuning similarity index is shown for the A-B-B-A task, after suppressing neuronal activity for 200 ms before the first test onset, from four different groups of neurons (excitatory neurons with facilitating synapses, blue curve; excitatory neurons with depressing synapses, red curve; inhibitory neurons with facilitating synapses, green curve; inhibitory neurons with depressing synapses, cyan curve), and for the case when activity was not suppressed (black curve). (I) Behavioral task accuracy for the cases when no activity was suppressed (black bar), and after suppressing activity from the four groups of neurons described in (H).","title":"Figure 5"},{"location":"190630_MasseNicolasY_2019/#attending_to_specific_memoranda","text":"Recent studies suggest that silently-maintained information can be reactivated either by focusing attention towards the memorandum19 or by probing the neural circuits involved20. In a final experiment, we examined how STP supports short-term maintenance of information that is either attended or unattended. We trained networks on a dual-sample delayed matching task (Figure 6A) that roughly follows the study by Rose et al.19. The network was trained to maintain two sample directions (presented simultaneously in two different locations) in WM, followed by two successive cues and test stimuli, in which the cue indicated which of the two samples was relevant for the upcoming test. In this setup, it is possible that stimuli that were not cued as relevant for the first test stimulus, could still be cued as relevant for the second test stimuli. Thus, stimulus could switch from being unattended to attended.","title":"Attending to specific memoranda"},{"location":"190630_MasseNicolasY_2019/#figure_6","text":"Neuronal and synaptic mnemonic encoding of multiple stimuli. (A) In the dual DMS task, two sample stimuli were simultaneously presented for 500 ms. This was followed by a 1000 ms delay period in which a cue appeared halfway through, and then two simultaneous test stimuli for 500 ms. The cue indicated which of the two sample/test pairs were task-relevant. Another 1000 ms delay and 500 ms test period was then repeated, in which a second cue again indicated which of the two sample/test pairs was task-relevant. (B) Neuronal decoding accuracy for the attended (blue curve) and unattended (red) stimuli, and the synaptic decoding accuracy for the attended (black) and unattended (yellow) stimuli, are shown from trial start through the first test period (left panel), the second delay and test periods (right panel). (C) Scatter plot showing the neuronal decoding accuracy, measured from 100 to 0 ms before the second cue (x-axis) against the neuronal decoding accuracy, measured from 400 to 500 ms after second cue (y-axis). Blue dots represent stimuli that were unattended after the first cue, and attended after the second cue, and red dots represent stimuli that were not attended to after the first and second cues. (D) The neuronal (green) and synaptic (magenta) rule decoding accuracy. The dashed lines indicate the decoding accuracy of the first cue, and the solid lines indicate the decoding accuracy of the second cue.","title":"Figure 6"},{"location":"190630_MasseNicolasY_2019/#manipulating_information_and_persistent_neuronal_activity","text":"While STP can allow networks to silently maintain information, persistent neuronal activity during the delay period was observed in all the tasks that involved manipulating information (DMRS, delayed cue, ABBA, attention in the dual-sample task). We wondered if the level of manipulation required by the task was correlated with the level of persistent activity. This could be of special interest as varying levels of persistent activity have been observed between different tasks10\u201316. We reasoned that for tasks that did not require manipulation, the network should represent the sample stimulus in fundamentally the same way during all trial epochs (e.g. early sample vs late delay). One caveat is that networks trained on tasks that did not require manipulation (e.g. DMS) did not represent the sample in neuronal activity at the end of the delay. However, in all cases the sample stimulus was represented in synaptic efficacies, with high decoding accuracy throughout the delay. Thus, we quantified the level of manipulation for each task by computing the similarity (same as used in Figure 5, see Methods) between the neuronal response during the first 100 ms of sample onset, and the synaptic efficacies during the last 100 ms of the delay. We subtracted this value from 1.0, so that a manipulation near 0 implies that early-sample tuning was similar to late-delay synaptic tuning. To boost statistical power, we included two additional delayed match-to-rotated sample tasks in which the target test direction was 45\u00b0 clockwise from the sample direction (DMRS45) and 180\u00b0 from the sample direction (DMRS180). We found that the level of manipulation was correlated with the level of stimulus-selective persistent activity (i.e. neuronal decoding accuracy) measured during the end of the delay (Spearman correlation coefficient R = 0.79, P = 0.006). This suggests that tasks requiring greater manipulation require greater levels of persistent activity. Thus, different levels of manipulation could partly explain the observation that persistent neuronal activity varies markedly between tasks.","title":"Manipulating information and persistent neuronal activity"},{"location":"190630_MasseNicolasY_2019/#discussion","text":"We examined whether STP can support the activity-silent manipulation of information, and whether it could help explain previous observations that different tasks evoke different levels of persistent activity. We found that while STP can silently support the short-term maintenance of information, it cannot support manipulation of information without persistent neuronal activity. Furthermore, we found that tasks that required more manipulation also required more persistent activity, giving insight into why the strength of persistent neuronal activity varies markedly between different tasks.","title":"Discussion"},{"location":"190630_MasseNicolasY_2019/#variation_in_persistent_neuronal_activity_in_vivo","text":"Over the last several decades, monkey electrophysiology experiments2\u20136, and later human imaging studies36, have supported the idea that information in WM is maintained in stimulus-selective persistent neuronal activity during memory-delay periods of behavioral tasks. However, this viewpoint has evolved, as various studies now suggest that persistent neuronal activity might not always reflect information maintenance, but can reflect control processes required to manipulate remembered information into appropriate behavioral responses14. It is often unclear whether persistent neural activity reflects the maintenance or the manipulation of the stimulus. For example, neural activity in the frontal and parietal cortices mnemonically encode stimulus location in a memory delayed saccade task2,4. However, recent studies that have dissociated the stimulus location from the upcoming saccade location have shown that activity in frontal cortex initially encodes the location of the recent stimulus (retrospective code), before its representation shifts towards encoding the planned saccade target (prospective code) later in the delay37. In another example, prefrontal cortex (PFC) was shown to mnemonically encode color in a change-detection task when six, highly distinguishable, colors are used38, but color-selective persistent activity was not evident in PFC when the subject had to detect a change amongst a continuum of 20 colors12. This might suggest that PFC activity can encode a categorical representation of the stimulus, but not a precise representation of stimulus features. Similarly, delay-period direction encoding in the parietal cortex is weak during a delayed-matching task, but after the subject undergoes extensive categorization training using the same stimuli, delay-period categorical encoding becomes highly robust16. These results suggest that persistent neuronal activity emerges as a result of manipulating the stimulus representation into another format such as a categorical code or a planned behavioral response. In contrast, studies have found that persistent neuronal activity is weak39 to non-existent40,41 in sensory areas, which are expected to be more involved in stimulus representation rather than manipulation. Despite the relative lack of persistent neuronal activity, studies have found that stimulus-selective information can still be decoded from local-field potentials41 and from blood-oxygenation levels42, suggesting that subthreshold top-down signals from frontal areas may coordinate with posterior areas to maintain information in short-term memory. In summary, these studies suggest that tasks that require greater manipulation of the memoranda evoke greater levels of persistent neuronal activity across a greater number of cortical areas. Although other factors undoubtedly influence the level of persistent neuronal activity, such as circuit properties10,43 and other task-related variables44, we observed a similar correlation between the level of manipulation and the level of persistent neuronal activity in our network models (Figure 7). Thus, our results suggest that the variability in the level of persistent neuronal activity observed between different tasks is partially the result of varying levels of stimulus manipulation required between tasks.","title":"Variation in persistent neuronal activity in vivo"},{"location":"190630_MasseNicolasY_2019/#figure_7","text":"The relationship between the manipulation required by the task and the level of stimulus-selective persistent neuronal activity. Scatter plot shows the level of stimulus-selective persistent neuronal activity, measured as the neuronal decoding accuracy during the last 100 ms of the delay period (x-axis), versus the level of manipulation, which was equal to 1 minus the tuning similarity between the neuronal tuning during the first 100 ms of the sample period, and the synaptic tuning during the last 100 ms of the delay period (y-axis). Each dot represents the average values across 20 networks trained on one specific task, or the across one trial type from one specific task (i.e. the DMS and DMRS trials from the delayed rule task, and the attended and unattended trials from the dual-sample delayed matching task). Errors bars indicate one SEM. Dashed vertical line indicates chance level decoding.","title":"Figure 7"},{"location":"190630_MasseNicolasY_2019/#comparison_to_other_artificial_neural_network_architectures","text":"Until recently, it was difficult to train RNNs to solve tasks involving long-term dependencies, in which the desired network outputs depend on inputs separated by long temporal delays. In the last several years, long short-term memory (LSTM)45 and more recently, gated recurrent unit (GRU)46 architectures have shot to prominence by allowing RNNs to solve a variety of tasks such as language modelling and translation that involve very long temporal delays. These architectures work by allowing networks to determine which information to maintain across time, and which information to update. We noticed that RNNs without STP either failed to accurately solve the task, or required many more training epochs. This was true even when setting the penalty on neuronal activity to zero (Figure S5). This difficulty in solving the tasks was partly because neurons in our networks never connected onto themselves, which could otherwise help neuron maintain information in short-term memory. That said, STP, with its relatively long time constants, greatly facilitated training on our set of WM based tasks. This highlights how adding network substrates with long time constants, without necessarily making these time constants flexible, can significantly decrease training time on tasks with long-term time dependencies. More generally, it also highlights how incorporating neurobiologically-inspired features to artificial neural networks is a promising strategy for advancing their capabilities. A recent study proposed a different recurrent network architecture in which activity was temporarily maintained in \u201cfast weights\u201d, which operated on a slower timescale than changes in neural activity, but faster than changes in synaptic weights47. Adding this different time constant allowed the network to more accurately solve tasks that required information maintenance across input sequences. These fast weights are reminiscent of the dynamic synapses regulated by STP considered in this study, and further suggest how adding a range of effective time constants to RNN models can improve performance on tasks that require maintaining information across different time scales.","title":"Comparison to other artificial neural network architectures"},{"location":"190630_MasseNicolasY_2019/#understanding_strategies_employed_by_artificial_neural_networks","text":"A key differentiating feature of RNNs compared to biological networks is that all connection weights and activity is known, facilitating any attempt to understand how the network solves various tasks. This has allowed researchers to describe how delayed association in RNNs can be performed by transient dynamics25, how simple dynamical systems consisting of line attractors and a selection vector can underlie flexible decision-making23, how RNNs can encode temporally-invariant signals27,26, and how functional clustering develops when RNNs learn multiple tasks48. While modelling studies cannot directly replace experimental work, they can be highly advantageous when obtaining the necessary experimental data is not feasible. Such was the case with our study, in which current technology does not allow us to directly measure the information maintained in synaptic efficacies or their contribution towards the behavioral decision. Thus, these modelling studies can serve as an excellent complement to experimental work, allowing researchers to rapidly generate novel hypothesis regarding cortical function that can later be tested when technology better allows for experimental verification. Lastly, the discovery of novel mechanisms found in silico can be fed back into the design of network models, potentially accelerating the development of machine learning algorithms and architectures. We believe that this synergy between experimental neuroscience and neural network modelling will only strengthen in the future.","title":"Understanding strategies employed by artificial neural networks"},{"location":"190630_MasseNicolasY_2019/#methods","text":"Neural networks were trained and simulated using the Python machine learning framework TensorFlow49. The code used to train, simulate and analyze network activity is available at https://github.com/nmasse/Short-term-plasticity-RNN. All parameters used to define the network architecture and training are given in Table 1.","title":"Methods"},{"location":"190630_MasseNicolasY_2019/#table_1","text":"","title":"Table 1"},{"location":"190630_MasseNicolasY_2019/#network_models","text":"All networks consisted of 36 or 48 input neurons (whose firing rates are represented as u(t) that projected onto 100 recurrently connected neurons (whose firing rates are represented as r(t)), which in turn projected onto 3 output neurons (whose firing rates are represented as z(t)) (Figure 2A). The activity of the recurrent neurons was modelled to follow the dynamical system48: \\tau \\frac{dr}{dt} = - r + f(W^{rec} r + W^{in} u + b^{rec} + \\sqrt{2\\tau} \\sigma_{rec} \\zeta) where \u03c4 is the neuron\u2019s time constant, f (\u00b7) is the activation function, Wrec and Win are the synaptic weights between recurrent neurons, and between input and recurrent neurons, respectively, b is a bias term, \u03b6 is independent Gaussian white noise with zero mean and unit variance applied to all recurrent neurons, and \u03c3rec is the strength of the noise. To ensure that neuron\u2019s firing rates were non-negative and non-saturating, we chose the rectified linear function as our activation function: f (x) = max(0, x) The recurrent neurons project linearly to the output neurons. The activity of the output neurons, z, was then normalized by a softmax function such that their total activity at any given time point was one: z = g(W^{out} r + b^{out}) where Wout are the synaptic weights between the recurrent and output neurons, and g is the softmax function: g(x_i) = \\frac{\\exp(x_i)}{\\sum_j{\\exp(x_j)}} To simulate the network, we used a first-order Euler approximation with time step \u0394t: r_t = (1 - \\alpha) r_{t-1} + \\alpha f(W^{rec} r_{t-1} + W^{in} u_t + b + \\sqrt{\\frac{2}{\\alpha}} \\sigma_{rec} N(0,1)) where \u03b1 = \u0394t/\u03c4 and N(0.1) indicates the standard normal distribution. To maintain separate populations of excitatory and inhibitory neurons, we decomposed the recurrent weight matrix, Wrec as the product between a matrix for which all entries are non-negative, Wrec,+ whose values were trained, and a fixed diagonal matrix, D, composed of 1s and \u22121s, corresponding to excitatory and inhibitory neurons, respectively30: W^{rec} = W^{rec, +} D D = \\begin{bmatrix} 1 & & \\\\ & \\ddots & \\\\ & & -1 \\\\ \\end{bmatrix} Initial connection weight values were randomly sampled from a Gamma distribution with shape parameter of 0.25 and scale parameter of 130. We note that training networks to accurately solve the tasks appeared somewhat faster when initializing connection weights from a gamma distribution compared to a uniform distribution (data not shown). Initial bias values were set to 0. Networks consisted of 36 motion direction tuned input neurons, and for the rule switching tasks (i.e. DMS+DMRS and dualDMS tasks), an additional 12 rule tuned neurons. The tuning of the motion direction selective neurons followed a Von Mises\u2019 distribution, such that the activity of the input neuron i at time t was u_t^i = A \\exp(\\kappa \\cos{(\\theta - \\theta_{pre\\, f})}) + \\sqrt{\\frac{2}{\\alpha}} \\sigma_{in} N(0,1) where \u03b8 is the direction of the stimulus, Embedded Image is the preferred direction of input neuron i, k was set to 2, and A was set to 4/exp(k) when a stimulus present (i.e. during the sample and test periods), and was set to zero when no stimulus was present (i.e. during the fixation and delay periods). For the dualDMS task, 18 motion direction tuned input neurons represented the first stimulus position, and the second represented the second stimulus position. The 12 rule tuned neurons for the DMS+DMRS and dualDMS tasks had binary tuning, in which their activity was set to 2 (plus the Gaussian noise term) for their preferred rule cue, and 0 (plus the Gaussian noise term) for all other times. The number of rule tuned neurons was arbitrarily chosen, and has little impact on network training.","title":"Network models"},{"location":"190630_MasseNicolasY_2019/#network_training","text":"RNNs were trained based on techniques previously described30,48. Specifically, the goal of training was to minimize 1) the cross-entropy between the output activity and the target output, and 2) the mean L2-norm of the recurrent neurons\u2019 spike rate. The target output was a length 3 one-hot vector, in which the first element was equal to one for all times except the test period, the second element was one when the test stimulus matched the sample, and the third element was one when the test stimulus did not match the sample. Specifically, the loss function at time t during trial i is L_{i, t} = - \\sigma_{n=1}^{N_{out}}{m^i(t) z_n^{\\text{target}, i}}(t) \\log{z_n^i}(t) + \\frac{\\beta}{N_{rec}} \\sum_{n=1}^{N_{rec}}{r_n^2(t)} where \u03b2 controls how much to penalize neuronal activity of the recurrent neurons, and mi(t) is a mask function, with values of zero or one, that determine which portions of the trial to include in the loss function. To give the network adequate time to form a match or non-match decision, we set the mask function to zero for the first 50 ms of the test period, and one for all other times. The total loss function is then L = \\frac{1}{N_{\\text{trials}} N_{\\text{time}}} \\sum_i^{N_{\\text{trials}}} \\sum_t^{N_{\\text{time}}} L_{i, t} During training, we adjusted all parameters (Win,Wrec,+,Wout,brec,bout) using the Adam50 version of stochastic gradient descent. The decay rate of the 1st and 2nd moments were set to their default values (0.9 and 0.999, respectively).","title":"Network training"},{"location":"190630_MasseNicolasY_2019/#short-term_synaptic_plasticity","text":"The synaptic efficacy between all recurrently connected neurons was dynamically modulated through short-term synaptic plasticity (STP). For half of the recurrent neurons (40 excitatory and 10 inhibitory), all projecting synapses were facilitating, and for the other half of recurrent neurons, all projecting synapses were depressing. Following the conventions of Mongillo et al.18, we modelled STP as the interaction between two values: x, representing the fraction of neurotransmitter available, and u, representing the calcium concentration in the presynaptic terminal. Presynaptic activity acts to increase the calcium concentration inside the presynaptic terminal, increasing synaptic efficacy. However, presynaptic activity decreases the fraction of neurotransmitter available, leading to decreasing efficacy. These two values evolve according to: \\begin{align*} \\frac{dx(t)}{dt} &= \\frac{1 - x(t)}{\\tau_x} - u(t) x(t) r(t) \\\\ \\frac{du(t)}{dt} &= \\frac{U - u(t)}{\\tau_u} + U(1 - u(t)) r(t) \\end{align*} where r(t) is the presynaptic activity at time t, \u03c4x is the neurotransmitter recovery time constant, and \u03c4u is the calcium concentration time constant. The amount of input the postsynaptic neuron receives through this one synapses at time t is then I(t) = W x(t) u(t) r(t) where W is the synaptic efficacy before STP is applied. For depressing synapses, the neurotransmitter recovery time constant was much longer compared to the calcium concentration time constant, whereas the opposite was true for facilitating synapses.","title":"Short-term synaptic plasticity"},{"location":"190630_MasseNicolasY_2019/#population_decoding","text":"Similar to our previous studies10,16, we quantified the strength of stimulus encoding by measuring how accurately we could decode the motion direction using multiclass support vector machines (SVMs). In this approach, we trained linear, multiclass SVMs to classify the motion direction using the neuronal activity of the 100 recurrent neurons, or the synaptic efficacies from the same 100 recurrent neurons, at each time point. The synaptic efficacy values were the product x(t)u(t) where x(t) and u(t) are the time varying values representing the amount of neurotransmitter available, and the calcium concentration, respectively, as described above. We measured the classification accuracy using cross-validation, in which we randomly selected 75% of trials for training the decoder and the remaining 25% for testing the decoder. For each of the eight motion directions, we randomly sampled, with replacement, 20 trials to train the decoder (from the 75% of trials set aside for training), and 20 trials to test the decoder (from the 25% of trials set aside for testing). From the 160 trials in the test set (20 time 8 directions), the accuracy was the fraction of trials in which the predicted motion direction matched the actual motion direction. We then repeated this sampling procedure 100 times to create a decoder accuracy distribution for each time point. The difference was deemed significantly greater than chance if 98 values were greater than chance (equivalent to P < 0.05 for a two-sided test). For each network, we calculated decoding accuracies using a batch of 1024 trials in which the test motion directions were randomly sampled independently of the sample motion direction. This was in contrast to how we trained the network and measured behavioral accuracy, in which there was always a 50% chance that a test stimulus would match the sample. We note that the pattern of neural and synaptic activity generated by a sample stimulus will be similar to the pattern generated by a matching test stimulus. Thus, if matching test stimuli occur more frequently than chance, our sample decoding accuracy during and after the test stimuli would be artificially elevated.","title":"Population decoding"},{"location":"190630_MasseNicolasY_2019/#measuring_the_contribution_of_neuronal_activity_and_synaptic_plasticity_towards_solving_the_task","text":"To measure how network models used information maintained in neuronal activity and in dynamic synaptic efficacies to solve the task, we used a shuffling procedure as follows. At the time point right before test onset (or right before the third test onset for the A-B-C-A/A-B-B-A tasks), we either 1) did not shuffle any activity, 2) shuffled the neuronal activity between trials, or 3) shuffled the synaptic efficacies between trials. We shuffled between trials as opposed to between neurons because neurons can have different baseline activity levels, and shuffling this activity can significantly perturb the network and degrade performance, even if no information is maintained in their activity. We then simulated the network activity for the remainder of the trial using the saved input activity, and measured the performance accuracy by comparing the activity of the three output neurons to the target output activity during the test period. We performed this random shuffling 100 times, and measured the mean performance accuracy for all three conditions. The rationale behind this analysis is that if the network was exclusively using information maintained in neuronal activity to solve the task, then shuffling neuronal activity between trials should devastate performance, while shuffling synaptic efficacies should have little effect. Similarly, if the network was exclusively using information maintained in synaptic efficacies to solve the task, then shuffling synaptic efficacies between trials should devastate performance, while shuffling neuronal activity should have little effect. If the network was using information maintained in both neuronal activity and synaptic efficacies, then shuffling either should lead to significant decreases in performance.","title":"Measuring the contribution of neuronal activity and synaptic plasticity towards solving the task"},{"location":"190630_MasseNicolasY_2019/#tuning_similarity_index","text":"We measured the similarity between sample and test stimuli encoding in the A-B-C-A and A-B-B-A tasks (Figure 5) using a similarity index we previously employed to study the relation between functional clustering and mnemonic encoding10. To calculate this index, we first modelled the synaptic efficacy for all neurons as a linear function of the sample or test motion direction, represented by the unit vector d: z_i(t) = dH_i(t) + \\epsilon_i(t) where \\epsilon_i(t) is a Gaussian noise term and the vector Hi(t) relates the stimulus direction to the synaptic efficacy. The angle of Hi(t) is the preferred direction of the neuron at time t, and its magnitude indicates the change in synaptic efficacy from baseline when the stimulus matches the preferred direction of the synapse. Thus, the preferred direction of a synapse, represented as a unit vector, is PD_i(t) = \\frac{H_i(t)}{\\| H_i(t) \\|} We can calculate how well this linear model fit the data for each synapse i and time point t indicated by wi(t), by comparing the variance in the residuals with the variance in the synaptic efficacy: w_i(t) = 1 - \\frac{var(\\hat{z_i}(t) - z_i(t))}{var(z_i(t))} where the fitted synaptic efficacy is determined by the linear model: \\hat{z_i}(t) = \\text{baseline} + dH_i(t) For each synapses, we calculated these values for both the sample and first test motion direction, and then calculated the tuning similarity as the dot-product between their preferred sample and test motion directions directions, weighted by the geometric mean of their normalized linear model fits: s_i(t) = \\sqrt{w_i^{\\text{sample}}(t) w_i^{\\text{test}}(t)} PD_i^{\\text{sample}}(t) PD_i^{\\text{test}}(t) Finally, we calculated the similarity tuning index as the sum of the similarity scores for all synapses, divided by the sum of the geometric means of their respective linear model fits: \\text{similarity}(t) = \\frac{\\sum_i{s_i(t)}}{\\sum_i{\\sqrt{w_i^{\\text{sample}}(t) w_i^{\\text{test}}(t)}}} For the analysis examining whether preferred sample motion directions rotate during the DMRS task (Figure S2), we calculated how preferred sample motion directions changed across time based on the calculations above. To do so, we first represented the selectivity and preferred sample direction of neuronal activity or synaptic efficacies in complex form: z_i(t) = \\sqrt{w_i(t)} \\exp{(i PD_i(t))} We then calculated the weighted change in preferred sample directions between synapses at different times: \\Delta PD_i^{\\text{synaptic-synaptic}}(t1, t2) = z_i^{\\text{synaptic}}(t1) \\text{conj}(z_i^{\\text{synaptic}}(t2)) or the weighted change in preferred sample directions between neurons and their associated synapses at different times: \\Delta PD_i^{\\text{neuronal-synaptic}}(t1, t2) = z_i^{\\text{neuronal}}(t1) \\text{conj}(z_i^{\\text{synaptic}}(t2)) We then calculated the weighted histogram by binning the angular differences (angle(\u0394PD)) weighted by their absolute value (abs(\u0394PD)) When calculating the tuning similarity index for Figure 7, we note that the delay period and the number of test stimuli differed between tasks. Thus, to calculate the metric in a standard way across all tasks, we tested all networks with a sample stimulus lasting 500 ms followed by a 1000 ms delay.","title":"Tuning similarity index"},{"location":"190630_MasseNicolasY_2019/#category_tuning_index","text":"The category tuning index (CTI) measured the difference in synaptic efficacy (averaged across all trials for each direction) for each neuron between pairs of directions in different categories (a between category difference or BCD) and the difference in activity between pairs of directions in the same category (a within category difference or WCD)51. The CTI was defined as the difference between BCD and WCD divided by their sum. Values of the index could vary from +1 (which indicates strong binary-like differences in activity to directions in the two categories) to \u22121 (which indicates large activity differences between directions in the same category, no difference between categories). A CTI value of 0 indicates the same difference in firing rate between and within categories.","title":"Category tuning index"},{"location":"190630_MasseNicolasY_2019/#references","text":"","title":"References"},{"location":"190723_TononiGiulio_2016/","text":"19-07-24 Integrated information theory: from consciousness to its physical substrate \u00b6 Tononi, Giulio, Melanie Boly, Marcello Massimini, and Christof Koch. \"Integrated information theory: from consciousness to its physical substrate.\" Nature Reviews Neuroscience 17, no. 7 (2016): 450. Original | Mendeley | pdf Contents \u00b6 00. Abstract 01. Introduction 02. From phenomenology to physics Figure 1. An experience is a conceptual structure 03. The PSC within the brain Figure 2. Identifying the elements, timescale and states of the physical substrate of consciousness (PSC) from first principles 03.01. Elements of the PSC 03.02. Timescale 03.03. State of the elements 03.04. Constitution of the PSC Figure 3. Identifying the physical substrate of consciousness (PSC) from first principles 03.05. Can the PSC change? 03.06. Multiple complexes 04. Information capacity of consciousness Figure 4. Phenomenal content and access content Box 1. Consciousness, integrated information and Shannon information 05. Explanations 06. Predictions 07. Conclusions 00. Abstract \u00b6 In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause\u2013effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients. 01. Introduction \u00b6 consci is subjective EX \u00b6 Consciousness is subjective experience \u2014 \u2018what it is like\u2019, for example, to perceive a scene, to endure pain, to entertain a thought or to reflect on the experience itself 1 , 2 , 3 . When consciousness fades, as it does in dreamless sleep, from the intrinsic perspective of the experiencing subject, the entire world vanishes. Qs about consci \u00b6 Consciousness depends on the integrity of certain brain regions and the particular content of an experience depends on the activity of neurons in parts of the cerebral cortex 4 . However, despite increasingly refined clinical and experimental studies, a proper understanding of the relationship between consciousness and the brain has yet to be established 5 , 6 . For example, it is not known why the cortex supports consciousness when the cerebellum does not, despite having four times as many neurons 7 , 8 , or why consciousness fades during deep sleep while the cerebral cortex remains active. There are also many other difficult questions about consciousness. Are patients with a functional island of cortex surrounded by widespread damage conscious, and if so, of what? Are newborn infants conscious? Are animals that display complex behaviours, but have brains very different from humans, conscious6? Can intelligent machines be conscious 9 ? hard problem & IIT \u00b6 start from phenomenal properties of experience, or axioms and inder postulates To answer these questions, the empirical study of consciousness should be complemented by a theoretical approach. The reason why some neural mechanisms, but not others, should be associated with consciousness has been called \u2018 the hard problem \u2019 because it seems to defy the possibility of a scientific explanation 10 . In this Opinion article, we provide an overview of the integrated information theory (IIT) of consciousness, which has been developed over the past few years 1 , 2 , 3 , 11 , 12 . IIT addresses the hard problem in a new way. It does not start from the brain and ask how it could give rise to experience; instead, it starts from the essential phenomenal properties of experience, or axioms , and infers postulates about the characteristics that are required of its physical substrate. Moreover, IIT presents a mathematical framework for evaluating the quality and quantity of consciousness 1 , 2 , 3 , 9 . We begin by providing a summary of the axioms and corresponding postulates of IIT and show how they can be used, in principle, to identify the physical substrate of consciousness (PSC) . We then discuss how IIT explains in a parsimonious manner a variety of facts about the relationship between consciousness and the brain, leads to testable predictions, and allows inferences and extrapolations about consciousness. Glossary \u00b6 [Achromatopsia] A condition in which a person is unable to perceive colours. [Anosognosia] A condition in which a person has a neurological deficit, but is unaware of it. Axioms Properties that are self-evident and essential; in integrated information theory, those that are true of every possible experience \u2014 namely, intrinsic existence, composition, information, integration and exclusion. Background conditions [background condition] Factors that enable consciousness, such as neuromodulators and external inputs that maintain adequate excitability. Cause\u2013effect repertoire The probability distribution of potential past and future states of a system that is specified by a mechanism in its current state. [Cause\u2013effect space] A space with each axis representing the probability of each possible past and future state of a system. Cause\u2013effect structure The set of cause\u2013effect repertoires specified by all the mechanisms of a system in its current state. [Complex] A set of elements in a state that specifies a conceptual structure corresponding to a maximum of integrated information ( \\Phi^{\\max} ). A complex is thus a physical substrate of consciousness. [Concepts][concept] The cause\u2013effect repertoires specified by a mechanism that is maximally irreducible ( \\phi^{\\max} ). Conceptual structure The set of all concepts specified by a system of elements in a state with their respective \\phi^{\\max} values, which can be plotted as a set of points in cause\u2013effect space. Content-specific NCC Neural elements, the activity of which determines a particular content of experience Elements The minimum constituents of a system that have at least two different states (for example, being on or off), inputs that can affect those states and outputs that depend on them. Full NCC The neural elements constituting the physical substrate of consciousness, irrespective of its specific content. Integrated information (Denoted \\Phi ). Information that is specified by a system that is irreducible to that specified by its parts. It is calculated as the distance between the conceptual structure specified by the intact system and that specified by its minimum information partition. Mechanism Any subset of elements within a system that has cause\u2013effect power on it (that is, that constrains its cause\u2013effect space). Neural correlates of consciousness (NCC). The minimum neuronal mechanisms jointly sufficient for any one specific conscious experience. Postulates Properties of experience that are derived from the axioms of integrated information theory and that must be satisfied by the physical substrate of consciousness \u2014 namely, to be a maximum of irreducible, specific, compositional, intrinsic cause\u2013effect power (intrinsic cause\u2013effect power for short). [Purviews][purview] The subsets of elements of a complex, the past and future states of which are constrained by a mechanism specifying a concept. Qualia The qualitative feeling of phenomenal distinctions within an experience (for example, seeing a colour, hearing a sound or feeling a pain). Relations Maximally irreducible overlaps among the purviews of two or more concepts 02. From phenomenology to physics \u00b6 \u00b6 every EX intrinsically structured specific unitary definite The axioms of IIT state that every experience exists intrinsically and is structured, specific, unitary and definite. IIT then postulates that, for each essential property of experience, there must be a corresponding causal property of the PSC. The postulates of IIT state that the PSC must have intrinsic cause\u2013effect power; its parts must also have cause\u2013effect power within the PSC and they must specify a cause\u2013effect structure that is specific, unitary and definite. Below, we discuss the axioms and postulates of IIT (see Supplementary information S1 , S2 (figure, box)) and describe the fundamental identity \u2014 between an experience and a conceptual structure \u2014 that it proposes ( FIG. 1 ). \u00b6 The first axiom of IIT states that experience exists intrinsically. As recognized by Descartes 13 , my own experience is the only thing whose existence is immediately and absolutely evident, and it exists for myself, from my own intrinsic perspective. The corresponding postulate states that the PSC must also exist intrinsically. For something to exist in a physical sense, it must have cause\u2013effect power \u2014 that is, it must be possible to make a difference to it (that is, change its state) and it must be able to make a difference to something. Moreover, the PSC must exist intrinsically \u2014 that is, it must have cause\u2013 effect power for itself, from its own intrinsic perspective. A neuron in the brain, for example, satisfies the criterion for existence because it has two or more internal states (such as active and inactive) that can be affected by inputs (causes) and its output can make a difference to other neurons (effects). A minimal system consisting of two interconnected neurons satisfies the criterion of intrinsic existence because, through their reciprocal interactions, the system can make a difference to itself. \u00b6 The axiom of composition states that experience is structured, being composed of several phenomenal distinctions that exist within it. For example, within an experience, I may distinguish a piano, a blue colour, a book, countless spatial locations, and so on ( FIG. 1 ). Based on this axiom, IIT postulates that the elements that constitute the PSC must also have cause\u2013effect power within the PSC, either alone or in combination (composing first-order and higher-order mechanisms, respectively). The axiom of information states that experience is specific, being composed of a particular set of phenomenal distinctions ( qualia ), which make it what it is and different from other experiences. In the example shown in FIG. 1 , the content of my current experience might be composed of seeing a book (rather than seeing no book), which is blue (rather than not blue), and so on for all other possible contents of consciousness. The corresponding postulate states that the PSC must specify a cause\u2013effect structure of a specific form, which makes it different from other possible forms. A cause\u2013effect structure is defined as the set of cause\u2013effect repertoires specified by all the mechanisms of a system. A cause\u2013effect repertoire specifies how a mechanism in its current state affects the probability distribution of past and future states of the system. integrated information ( \\Phi ): info The axiom of integration states that experience is unitary, meaning that it is composed of a set of phenomenal distinctions, bound together in various ways, that is irreducible to non-interdependent subsets. For example, I experience a whole visual scene and that experience cannot be subdivided into independent experiences of the left and right sides of the visual field. In other words, the content of an experience (information) is integrated within a unitary consciousness. The corresponding postulate states that the cause\u2013effect structure specified by the PSC must also be unitary \u2014 that is, it must be irreducible to the cause\u2013effect structure specified by non-interdependent subsystems. Note that, from the intrinsic perspective of the system, integration requires that every part of the system has both causes and effects within the rest of the system, which implies bidirectional interactions. The irreducibility of a conceptual structure is measured as integrated information (denoted \\Phi , the minimum distance between an intact and a partitioned cause\u2013effect structure). The integration postulate also requires the irreducibility of each cause\u2013effect repertoire (denoted \\phi , the minimum distance between an intact and a partitioned cause\u2013effect repertoire) and the irreducibility of relations among overlapping cause\u2013effect repertoires. The axiom of exclusion states that an experience is definite in its content and spatio-temporal grain. For example, in the scene depicted in FIG. 1 , the content of my present experience includes seeing my hands on the piano, the books on the piano, one of which is blue, and so on, but I am not having an experience with less content (for example, the same scene in black and white, lacking the phenomenal distinction between coloured and not coloured) or with more content (for example, including the additional phenomenal distinction of feeling one\u2019s blood pressure as high or low). The duration of the instant of consciousness is also definite, ranging from a few tens of milliseconds to a few hundred milliseconds, rather than lasting a few microseconds or a few minutes 14 , 15 , 16 . The corresponding postulate states that the cause\u2013effect structure specified by the PSC must also be definite. It must specify a definite set of cause\u2013effect repertoires over a definite set of elements, neither less nor more, at a definite spatio-temporal grain, neither finer nor coarser. Because a prerequisite for intrinsic existence is having irreducible cause\u2013effect power, the cause\u2013effect structure that actually exists, over a set of elements and spatio-temporal grains, is that which is maximally irreducible ( \\Phi^{\\max} ), called a conceptual structure. As a consequence, any cause\u2013effect structure overlapping over the same set of elements and spatio-temporal grain is excluded. The exclusion postulate also requires the maximum irreducibility of cause\u2013effect repertoires (denoted \\phi^{\\max} ), called concepts, and of relations among overlapping concepts. A set of elements in a state that satisfies all the postulates of IIT constitutes the PSC and is referred to as a complex ( FIG. 1 ). Thus a complex specifies a conceptual structure composed of concepts, which can be represented as a set of points (shown as a constellation of stars in FIG. 1 ) in cause\u2013effect space, in which each axis corresponds to a possible past and future state of the system and each star corresponds to a concept1 ( FIG. 1 ). With these notions at hand, the fundamental identity of IIT can be stated as follows 2 : an experience is identical to a conceptual structure, meaning that every property of the experience must correspond to a property of the conceptual structure and vice versa. Note that the postulated identity is between an experience and the conceptual structure specified by the PSC, not between an experience and the set of elements in a state constituting the PSC ( FIG. 1 ). The quality or content of consciousness \u2014 which particular way the system exists for itself \u2014 corresponds to the form of the conceptual structure. The quantity of consciousness \u2014 how much the system exists for itself \u2014 corresponds to its irreducibility \\Phi^{\\max} . Figure 1 | An experience is a conceptual structure \u00b6 According to inte- grated information theory (IIT), a particular experience (illustrated here from the point of view of the subject) is identical to a conceptual structure spec- ified by a physical substrate. The true physical substrate of the depicted experience (seeing one\u2019s hands on the piano) and the associated conceptual structure are highly complex. To allow a complete analysis of conceptual structures, the physical substrate illustrated here was chosen to be extremely simple1,2: four logic gates (labelled A, B, C and D, where A is a Majority (MAJ) gate, B is an OR gate, and C and D are AND gates; the straight arrows indicate connections among the logic gates, the curved arrows indi- cate self-connections) are shown in a particular state (ON or OFF). The anal- ysis of this system, performed according to the postulates of IIT, identifies a conceptual structure supported by a complex constituted of the elements A, B and C in their current ON states. The borders of the complex, which include elements A, B, and C but exclude element D, are indicated by the green circle. According to IIT, such a complex would be a physical substrate of consciousness (Supplementary information S1 (figure)). The conceptual structure is represented as a set of stars and, equivalently, as a set of histo- grams. The green circle represents the fact that experience is definite (it has borders). Each histogram illustrates the cause\u2013effect repertoire of a concept: how a particular mechanism constrains the probability of past and future states of its maximally irreducible purview within the complex ABC. The bins on the horizontal axis at the bottom of the histograms rep- resent the 16-dimensional cause\u2013effect space of the complex \u2014 all its eight possible past states (p; in blue) and eight possible future states (f; in red; ON is 1 and OFF is 0). The vertical axis rNepatruesrenRtesvthiewpsro| Nbaebuirliotsycoief enacceh state (for consistency, the probability values shown are over the states of the entire complex and not just over the subset of elements constituting the purview). In this example, five of seven possible concepts exist, specified by the mechanisms A, B, C, AB, AC (all with \\phi^{\\max}>0 ) in their current state (which are labelled as Ac, Bc, etc.). The subsets BC and ABC do not specify any con- cept because their cause\u2013effect repertoire is reducible by partitions ( \\phi^{\\max}=0 ). In the middle, the 16-dimensional cause\u2013effect space of the com- plex is represented as a circle, where each of the 16 axes corresponds to one of the eight possible past (p; blue arrows) and eight possible future states (f; red arrows) of the complex, and the position along the axis represents the probability of that state. Each concept is depicted as a star, the position of which in cause\u2013effect space represents how the concept specifies the probability of past and future states of the complex, and the size of which measures how irreducible the concept is ( \\phi^{\\max} ). Relations between two concepts (overlaps in their purviews) are represented as lines between the stars. The fundamental identity postulated by IIT claims that the set of con- cepts and their relations that compose the conceptual structure are identi- cal to the quality of the experience. This is how the experience feels \u2014 what it is like to be the complex ABC in its current state 111. The intrinsic irreduc- ibility of the entire conceptual structure ( \\Phi^{\\max} , a non-negative number) reflects how much consciousness there is (the quantity of the experience). The irreducibility of each concept ( \\phi^{\\max} ) reflects how much each phenomenal distinction exists within the experience. Different experiences correspond to different conceptual structures. 03. The PSC within the brain \u00b6 NCC located in cortico-thalamic system NOT known which areas / layers / neuronal populations whether units are neuron / groups of neurons which aspects of activity neural substrate of consci is anatomically fixed / can shrink IIT content-specific NCC : in paticular state (activity pattern) full NCC : irrespcetive of particular state [background condition]: factors enable consci Experimental evidence currently suggests that the neural correlates of consciousness (NCC) are likely to be located in certain parts of the cortico-thalamic system 5 , but it is not known specifically which cortical areas, layers or neuronal populations are involved, whether the relevant units are neurons or groups of neurons, and which aspects of their activity matter 5 . It is also not known whether the neural substrate of consciousness is anatomically fixed or can shrink, expand and move. IIT offers theoretical clarity on the empirical notion of the NCC 5 . Specifically, it states that the content-specific NCC correspond to the neural elements of the PSC in a particular state (activity pattern), which specify a particular phenomenal content; the full NCC correspond to the neural elements constituting the PSC irrespective of their particular state; the background conditions [background condition] are factors that enable consciousness, such as neuromodulators and external inputs that maintain adequate excitability, which are kept fixed when evaluating the \\Phi value of the PSC. Most importantly, the axioms and postulates of IIT can be used to provide a single, general principle for identifying the PSC in the brain \u2014 namely that the PSC must correspond to a complex of neural elements with maximum intrinsic cause\u2013effect power. Figure 2 | Identifying the elements, timescale and states of the physical substrate of conscious- ness (PSC) from first principles \u00b6 It is possible to determine maxima of cause\u2013effect power within the central nervous system by perturbing and observing neural elements at various micro- and macro-levels 18 . High cause\u2013effect power is reflected in deterministic responses and low cause\u2013 effect power is reflected in responses that vary randomly across trials. a | To identify the spatial grain of the elements of the PSC supporting consciousness, a schematic example shows how optogenetic perturbation and unit recording could be applied to a subset of neurons (here, 3 out of 36 neurons) to establish maxima of cause\u2013effect power. For each of three trials, the left panel shows the effects of the perturbation on the entire system at the micro-level. Grey neurons are unaffected, blue neu- rons decrease their firing rates, red neurons increase their firing rates and purple neurons respond with burst firing. The right-hand panel shows the effects of the perturbation at the macro-level after coarse-graining of the 36 neurons into nine groups of four cells each. Macro-states are defined according to the rule that if \u226550% of the neurons in the group are in a given micro-state (such as low firing, high firing or bursting), then the group is considered to be in that state at the macro-level. In this example, the macro-level (groups of neurons) has higher cause\u2013effect power than the micro- level (single neurons), because the response is deterministic at the macro-level (as evidenced by the consistent colour scheme), whereas there are variations between trials at the micro-level (incon- sistent colours). b | To identify the temporal grain of neuronal activity supporting consciousness, a possible experimental setup would be one in which one neuron (the top trace) is optogenetically excited while recording from other neurons (labelled N1\u2013N4) across three trials, shown in the upper panel at the 10 ms timescale (micro-scale). Grey shading indicates no effects on neuron firing in the 10 ms following the stimulation compared with the 10 ms before the stimulation, blue shading indi- cates decreased firing and red shading indicates increased firing. The lower panel shows the same data after temporal coarse-graining over 100 ms intervals. Macro-states are defined according to the rule that if a neuron increases (or decreases) its firing rate by >50% within 100 ms post-stimulus compared with the baseline, the macro state is considered to be high (or low) firing. In this example, the macro-level (100 ms intervals) has higher cause\u2013effect power (more deterministic responses) than the micro-level (10 ms intervals). c | To identify the neural states that support consciousness, optoge- netic perturbations could be used to drive one neuron to fire either at low frequency, high tonic frequency or bursting (top trace) resulting in spectral peaks at 2 Hz (green), 50 Hz (red) and 150 Hz (yellow) for neurons N1\u2013N4 (data are shown as a firing rate histogram). For each trial, the upper panel shows the responses of the other four neurons to each stimulation frequency at the micro-scale level in the spectral domain (micro-bins, only a few of which are represented). The coloured bars indicate coincidence, within a micro-bin, between the frequency of stimulation and the spectral peak of the responses. The lower panel of each trial shows the effect of the perturbation at the corresponding macro-level after spectral coarse-graining. Macro-states map into micro-states as indicated below the frequency bins. Here, spectral coarse-graining (binning firing rates into three levels, low, high and burst firing) results in higher cause\u2013effect power (responses that are more deterministic) than at the micro-level. 03.01. Elements of the PSC \u00b6 What is the spatial scale of the neural elements that support consciousness: synapses, neurons, neuronal groups, local fields or perhaps all of these? According to IIT, the neural elements of the PSC are those, and only those, that support a maximum of cause\u2013 effect power, as determined from the intrinsic perspective of the system itself. Importantly, and contrary to common reductionist assumptions 17 , cause\u2013effect power can be higher at a macro-level than at a micro-level 18 . For example, a system of neuron-like micro-elements may have less cause\u2013effect power than the same system coarse-grained at the macro-level of neuronal groups ( FIG. 2a ). In general, whether the macro or micro grain size has higher cause\u2013effect power depends on how intra- and inter-group connections are organized and the amount of indeterminism (noise) and degeneracy (multiple ways of obtaining the same effect 18 ). An exhaustive evaluation of cause\u2013 effect power at multiple levels is only possible in small simulated networks 19 . In a real network 20 , we could start by assessing the cause\u2013effect repertoire of individual neurons. For example, if a neuron is firing a burst of spikes, its cause repertoire is the probability distribution of past network states that would have caused it to burst (for example, firing patterns of its afferent neurons within the previous 100 ms). Similarly, its effect repertoire is the probability distribution of future network states given that the neuron is bursting. Experimentally, we could obtain an estimate of such cause\u2013 effect repertoires by stimulating one or more neurons optogenetically while simultaneously recording the firing activity of a population of neurons via two-photon calcium imaging (keeping the background conditions constant, such as the level of arousal and sensory input) ( FIG. 2a ). Next, we would need to test for the irreducibility of the cause\u2013effect repertoires, which can be achieved by noising connections (that is, enforcing firing at chance levels) across a partition of the network. Doing so would establish which subset of incoming connections makes the most irreducible difference ( \\phi^{\\max} ) to the firing of the observed neuron1 (and this could be carried out analogously for outgoing connections). A similar procedure should then be repeated for subsets of two neurons, three neurons, and so on, because combinations of neurons can also have irreducible cause\u2013effect repertoires (defined as higher order mechanisms). Such experiments would provide an estimate of maximally irreducible cause\u2013effect repertoires at the level of neurons. To evaluate cause\u2013effect power at the macro-level, we could then repeat the same stimulation\u2013recording\u2013noising procedure by considering subsets of neurons as distinct macro-groups and mapping micro-states onto macro-states. For example, we could take all pyramidal neurons in each mini-column as a distinct group and define the group state as low firing, high firing or bursting, depending on the overall firing rate of the neurons over 100 ms. By estimating the \\phi^{\\max} value of cause\u2013effect repertoires at the level of both individual neurons and groups of neurons, an experimenter could thus assess at which grain size the network has most cause\u2013effect power from its own intrinsic perspective \u2014 that is, at which level it makes the most difference to itself. IIT predicts that the elements of the PSC are to be found at exactly that level and not at any finer or coarser grain, a prediction that is empirically testable: does the firing of a single neuron make a difference21 to the content of experience, or only the average activity of a cortical mini-column22? 03.02. Timescale \u00b6 Which timescale of neuronal activity is important for consciousness: a few milliseconds, tens of milliseconds, hundreds of milliseconds, or perhaps all of these? Again, IIT predicts that the relevant time interval should be that which makes the most difference to the system, as determined from its intrinsic perspective. Once more, depending on the specific mechanisms of a system, some macro-temporal grain may have a higher cause\u2013effect power than both finer and coarser grains ( FIG. 2b ). Whatever timescale turns out to have the maximum cause\u2013effect power within the relevant brain regions, it should be consistent with estimates of the timescale of experience14\u201316. 03.03. State of the elements \u00b6 An external observer can choose to analyse brain states at any level of detail. For example, some neu- rophysiologists may be interested in the effects of the timing of individual neuronal spikes on brain function, others in the effects of broad fluctuations in the activity of populations of neurons. In fact, it is likely that almost any change in the state of any neurobiological variable will have some effect somewhere in the brain21. According to IIT, the neural states that are important for consciousness are only those that have maximum cause\u2013effect power on the system itself. For example, assume that, from the intrinsic perspective of the system, maximum cause\u2013effect power was achieved when coarse-graining firing states into low, high and burst firing ( FIG. 2c ). In this case, IIT predicts that finer grained neural states, despite their demonstrable neuro- physiological effects, make no difference to the content of experience. Note that spatio-temporal grain and the relevant activity states of the elements specifying the PSC could change according to brain region, developmental period, species, neuromodulatory milieu and even the task being performed. 03.04. Constitution of the PSC \u00b6 Figure 3 | Identifying the physical substrate of consciousness (PSC) from first principles \u00b6 The complex of neural elements that constitutes the PSC can be identified by searching for maxima of intrinsic cause\u2013effect power. a | For example, assume that the elements, timescale and states at which intrinsic cause\u2013effect power reaches a maximum have been identified using optogenetic and unit recording tools ( FIG. 2 ). Here, the elements are groups of neurons, the timescale is over 100 ms and there are three states (low, high and burst firing). b | In a healthy, awake participant, the set of neural elements specifying the conceptual structure with the highest \\Phi^{\\max} is assumed, based on current evidence, to be a complex of neuronal groups distributed over the posterior cortex and portions of the anterior cortex 5 . Empirical studies can, in principle, establish whether the full neural corre- lates of consciousness 5 correspond to the maximum of intrinsic cause\u2013effect power, thereby corroborating or falsifying a key prediction of integrated information theory. c | The boundaries of the PSC (green line) may change after cortical lesions, such as those causing absolute achromatopsia, result- ing in a smaller PSC. d | The PSC boundaries may also move as a result of changes in excitability and effective connectivity, as might occur during pure thought that is devoid of sensory content. e | The PSC could also split into two large local maxima of cause\u2013effect power (represented here by green and blue boundaries) as a result of anatomical disconnections, such as in split-brain patients, in which instance each hemisphere would have its own consciousness. f | The PSC may also split as a result of functional disconnec- tions, which may occur in some psychiatric disorders and perhaps under certain dual-task conditions \u2014 for example while driving and talking at the same time. g | The coexistence of a large major complex with one or more minor complexes that may support sophisticated, seemingly unconscious performance could be a common occurrence in everyday life. \u00b6 Assume that we have determined that the elementary units of the PSC are local groups of cortical neurons, over a time interval of ~100 ms, with three relevant states (low, high and burst firing) ( FIG. 3a ). Next we must determine, at the system level, which particular subset of neuronal groups constitutes the PSC for a particular experience. IIT addresses this question from first principles \u2014 it predicts that the PSC is the set of neuronal groups that has maximally irreducible cause\u2013effect power on itself, specifying a conceptual structure with the highest value of \\Phi 1 ( FIG. 3b ). Ideally, systematic manipulation and recording of this particular set of neuronal groups would show that it has the maximum value of \\Phi , whereas any other assortment of neuronal groups in the brain has a lower value of \\Phi . Although such an exhaustive evaluation of \\Phi is not currently feasible, neuroimaging studies can evaluate two key requirements for a high \\Phi value: information, using measures that reflect the size of the repertoire of neural states the system can have (that is, neurophysiological differentiation) [23] ; and integration, using measures of functional or effective connectivity among brain regions [24],[25] . In addition, large-scale computer simulations based on the known anatomy and physiology of cortical circuits26 can be used to assess cause\u2013effect repertoires, test their irreducibility and estimate conceptual structures. Crucially, if the evidence thus obtained indicates that the PSC does not correspond to a maximum of intrinsic cause\u2013effect power, IIT would be invalidated. A related prediction is that any perturbation of the PSC at the appropriate spatio-temporal grain should produce a change in experience, whereas any perturbation that does not alter the PSC should not. 03.05. Can the PSC change? \u00b6 An important issue is the extent to which the set of neural elements that constitute the PSC is fixed. Clearly, if a cortical area is inactivated (by a lesion, for example) it will no longer be part of the PSC and the phenomenal distinctions contributed by that area will no longer be available. For example, if cortical areas responding to colour are inactivated ( FIG. 3c ), experiences will not only lack colour, but patients would not even understand what is lacking (as reported in cases of achromatopsia with anosognosia [27] ). It is an open question whether the PSC can shrink, expand or move during normal wakefulness, possibly through attentional modulation of excitability and functional connectivity. For example, when we are engrossed in an action movie and not engaged in self-reflection, the activity in prefrontal areas decreases [28] . Does this mean that the PSC shrinks, like when colour areas are inactivated, or that brain regions supporting self-reflection remain inside the PSC but are inactive, in the same way that colour areas are inactive when watching a black and white movie? The location and size of the PSC is likely to change during sleep, during seizures, in patients with conversion and dissociative disorders, and possibly during hypnosis. During slow wave sleep, for example, neurons are bistable and show off-periods during which they become hyperpolarized (down-states) and silent [29] . However, these off-periods are usually not global, but affect local subsets of brain areas at different times [30] . Hence it is possible that during slow wave sleep the PSC may become smaller and reconfigure substantially. Sustained inactivation of certain areas during sleep may make dreaming patients incapable of reflective thought. Similarly, experiences of pure thought that have minimal perceptual content may be caused by slow waves that inactivate the posterior cortex, and be specified by a PSC that is considerably different from the PSC for purely perceptual experiences [31] ( FIG. 3d ). At other times, transient, local slow waves (indicative of an off-period) in colour areas may cause the PSC to shrink and lead to brief episodes of achromatopsia. Novel methods that allow the transient inactivation of specific cortical areas in humans, such as transcranial magnetic stimulation or focused ultrasound, would be ideal for evaluating the contribution of those areas to conscious content. 03.06. Multiple complexes \u00b6 According to IIT, two or more non-overlapping complexes may coexist as discrete PSCs within a single brain1, each with its own definite borders and value of \\Phi^{\\max} . The complex that specifies a person\u2019s day-to-day stream of consciousness should have the highest value of \\Phi^{\\max} \u2014 that is, it should be the \u2018major\u2019 complex. In some conditions, for example after a split-brain operation, the major complex may split ( FIG. 3e ). In such instances, one consciousness, supported by a complex in the dominant hemisphere and with privileged access to Broca\u2019s area, would be able to speak about the experience, but would remain unaware of the presence of another consciousness, supported by a complex in the other hemisphere, which can be revealed by carefully designed experiments [32],[33] . An intriguing possibility is that splitting of the PSC may also occur in healthy people during long-lasting dual-task conditions \u2014 for example, when driving in an auto-pilot like manner on a familiar road while listening to an engaging conversation ( FIG. 3f ). Splitting into separate maxima may also occur through functional disconnections caused by pathological conditions, such as conversion and dissociative disorders [34] . Another intriguing possibility is that multiple conscious streams may coexist within a single brain in daily life. For example, the grid-like architectures in the colliculus and related mesencephalic regions, which are adept at multimodal integration within a spatial framework, may support a separate minor complex. Some examples of high-level cognitive performance such as judging whether a scene is congruous or incongruous [35],[36] \u2014 that appear to be carried out unconsciously from the perspective of the major complex \u2014 may support a separate minor complex ( FIG. 3e,g ). Alternatively, some of these functions may be mediated by feedforward circuits [37] that have \\Phi^{\\max}=0 because they lack integration and therefore are strictly unconscious 1 . An important question for the future is whether automatic, unconscious behaviours are mediated by specific cell types within the cortex, such as subcortical projection neurons of layer 5B [38] , that are different from other cell types that support consciousness. 04. Information capacity of consciousness \u00b6 The information-processing approach common in psychology estimates the information capacity of human consciousness to be at around 7 \u00b1 2 items [39] or \u226440 bits per second [39],[40] . In the classic Sperling task [41] , for example, participants are presented with a set of 12 letters for 300 ms, of which, after a mask and a delay, they can report at most four ( FIG. 4 ). The inference from such experiments is that the information content of consciousness is extremely limited, as is also suggested by the attentional blink and related psychophysical paradigms [42],[43] . For example, in change blindness, a major modification in a visual scene may go undetected if a blank is interposed between the two images [44] . In this view, the content of consciousness is limited to what can be accessed and reported, despite our phenomenal impression of richer content [42],[45],[46] . By contrast, others argue that phenomenal consciousness (what it is like to have an experience) has far greater capacity than access consciousness (what can be reported) [47],[48],[49] . For example, if participants are cued to a particular row of the Sperling display during the delay period, they can report three letters of any row; moreover, they can report the colour diversity of unattended letters at no cost to the identification of the cued letters [50] . Likewise, change blindness may be due not to a failure to experience, but to a lack of memory for the experience [51] . Similarly, low-level phenomenal features may be difficult to report because they vary rapidly and may be forgotten before they can be accessed from top-down mechanisms; pre-categorical stimuli, such as irregular scribbles, may be phenomenally salient but hard to describe in words. IIT claims that human consciousness has a high capacity for integrated information ( BOX 1 ). Even for a simple experience, such as seeing the Sperling display, the elements of the PSC specify a rich conceptual structure (high \\Phi^{\\max} ) composed of a very large number of concepts and relations. These correspond to all the phenomenal distinctions that make that experience what it is and thereby different from countless others 11 ( FIG. 4 ). It is useful to distinguish between low- and high-order concepts, depending on how many PSC elements are contained in their purviews. For example, a concept specifying the presence of an oriented edge at a particular location in the visual field has a low-order purview, whereas a concept specifying the extent of the entire visual field has a high-order purview. Concepts can also have low- and high invariance; for example, the concept for the letter A has high invariance because its purview specifies a high-order disjunction of states of the PSC elements (a specific arrangement of oriented edges in any of a large number of possible locations). Mechanisms specifying invariant concepts form a hierarchy going from low- to high-level areas of the cerebral cortex, as indicated by experimental data [52] and consistent with computational models for the recognition of objects [53] , places, events54 and spatial reference frames [55] . A concept can have low or high selectivity, depending on how strongly the state of its mechanism constrains its cause\u2013effect repertoire. In the brain, the adaptive bias towards sparse firing makes it likely that the neurons would fire strongly when specifying a high invariance, high selectivity concept, such as the presence of the letter A (that is, a positive concept), and be silent when specifying its low selectivity counterpart, such as the absence of the letter A (that is, a negative concept) ( FIG. 4 ). In experimental settings, the content of experience is typically probed by asking the participant about high invariance, positive concepts, such as letters in the Sperling paradigm. However, we could undoubtedly report many more concepts than just the identity of a few letters. For example, we could report that there are many black symbols, that they are arranged in three rows and four columns, in a rectangular array, within a rectangular display, over a white homogeneous background that is spatially extended, being composed of a multitude of distinguishable locations, each with its specific neighbours, and so on. We can also report many negative concepts \u2014 for example, that the Sperling display did not include a face, a tree, an animal, a house, and so on \u2014 for the thousands of high invariance concepts we possess that happen to be negative for this particular image. Finally, we can report how all these concepts are bound together within the same experience in a complex pattern of relations \u2014 for example, we see the letter A as an invariant that is nevertheless located at a particular spatial location, that is composed of two oblique edges and a horizontal edge in between, that is capital, printed in black and located on the rightmost column in the upper row of the array, and so on. According to IIT, this dynamic binding of phenomenal attributes56 occurs if, and only if, in cause\u2013effect space the corresponding concept purviews are related, meaning that they refer to an overlapping set of PSC elements and jointly constrain their past or future states. In short, the information that specifies an experience is much larger than the purported limited capacity of consciousness [57] . Although we are accustomed to summarizing what we see by referring to a few positive, high invariance concepts (for example, in FIG. 4 bottom panel, a participant may state: \u201cI see the letters O, S and A\u201d), we would not see what we see without the contribution of a large number of other concepts \u2014 low and high order, low and high invariance, positive and negative \u2014 and relations, which make the experience what it is (information) and thereby different from others (differentiation; FIG. 4 ). Consider what it would be like to look at the Sperling display not as a human, but as a machine implementing an efficient feedforward algorithm for letter recognition. The machine could certainly report three letters (in fact, all 12). However, such a machine could not see the scene and would understand virtually nothing because it has no other concept apart from the letters, not for the letter combination OSA, the array, the display, a face, an animal, and so on. Indeed, if there were a face, an animal, or anything else in the middle of the display, it would do its best to categorize it as a letter. Figure 4 | Phenomenal content and access content \u00b6 The content NofaatunrexRpevriewncse| Nisemuruocshcileanrgcer than what can be reported by a subject at any point in time. The left-hand panel illustrates the Sperling task [41] , which involves the brief presentation of a three by four array of letters on a screen, and a particular row being cued by a tone. Out of the 12 letters shown on the display, participants correctly report only three or four letters \u2014 the letters cued by the tone \u2014 reflecting limited access. The top middle panel illustrates a highly simplified conceptual structure that corresponds to seeing the Sperling display, using the same conventions as outlined in FIG. 1. The myriad of positive and negative, first- and high-order, low- and high invariance concepts (represented by stars) that specify the content of this particular experience (seeing the Sperling display and having to report which letters were seen) make it what it is and different from countless other experiences (rich phenomenal content). The bottom panel schematically illustrates the physical substrate of consciousness (PSC) that might cor- respond to this particular conceptual structure (its boundary is represented by a green line). The PSC consists of neuronal groups that can be in a low firing state, a high firing state or a bursting state. Alone and in combination, these neuronal groups specify all the concepts that compose the conceptual structure. Stars that are linked to the PSC by grey dashed lines represent a small subset of these con- cepts. The PSC is synaptically connected to neurons in Broca\u2019s area by means of a limited capacity channel (dashed black arrow) that is dynamically gated by top-down connections (shown as solid black arrows) originating in the prefrontal cortex to carry out the instruction (that is, to report the observed letters \u2018OSA\u2019). Box 1 | Consciousness, integrated information and Shannon information \u00b6 The term information is used very differently in integrated information theory (IIT) and in Shannon\u2019s theory of communication1, and confusing the two meanings can cause misunderstandings80. The word information derives from the Latin verb informare, which means \u2018to give form\u2019. In IIT the information content of an experience is specified by the form of the associated conceptual structure (the quality of the integrated information) and quantified by \u03a6max (the quantity of integrated information). In IIT, information is causal and intrinsic: it is assessed from the intrinsic perspective of a system based on how its mechanisms and present state affect the probability of its own past and future states (cause\u2013effect power). It is also compositional, in that different combinations of elements can simultaneously specify different probability distributions within the system. Moreover, it is qualitative, as it determines not only how much a system of mechanisms in a state constrains its past and future states, but also how it does so. Crucially, in IIT, information must be integrated. This means that if partitioning a system makes no difference to it, there is no system to begin with. Information in IIT is exclusive \u2014 only the maxima of integrated information are considered. By contrast, Shannon information is observational and extrinsic \u2014 it is assessed from the extrinsic perspective of an observer and it quantifies how accurately input signals can be decoded from the output signals transmitted across a noisy channel. It is not compositional nor qualitative, and it does not require integration or exclusion1. When averaged over many different states of the physical substrate of consciousness (PSC), we can think of the integrated information \u03a6max as a measure of the intrinsic phenomenal capacity of the conceptual structures specified by the PSC. By contrast, Shannon information can be used to measure the extrinsic access capacity of a channel that runs from a subset of elements of the PSC to Broca\u2019s area and from there to the motor neurons that ultimately convey the report (FIG. 4). In IIT, the experience of seeing the Sperling display is identical to a particular conceptual structure \u2014 it is a form in cause\u2013effect space with a high value of integrated information \u03a6max, as specified by its PSC (FIG. 4). The average value of \u03a6max for different states of the PSC measures its intrinsic phenomenal capacity. The figure also shows a neural information channel from the PSC to Broca\u2019s area, formed dynamically by top-down attentional mechanisms located in the prefrontal cortex, which select which subset of elements of the PSC should drive the report (FIG. 4). This channel conveys extrinsic information and has a low Shannon capacity (only four letters at a time can be reported), which corresponds to the mutual information between its inputs and outputs. Seen in this way, it becomes obvious that the extrinsic information that can be selected through attention, kept in working memory and channelled out for report is only a partial read-out of the intrinsic information that is specified by the PSC over its own cause\u2013effect space. Although at any given time we can access and report the state of a few elements of the PSC, and that of some other elements at another time, it is not possible to dump the state of all elements through a limited capacity channel. It is certainly not possible to transmit a conceptual structure (intrinsic information) through a channel (extrinsic information)\u2014phenomenal capacity, properly understood, truly exceeds access capacity. Likewise, conscious information is not something that is transmitted or broadcast from one part of the brain to another77,78 (Supplementary information S5 (box)). 05. Explanations \u00b6 IIT provides a principled explanation for several seemingly disparate facts about the PSC. For example, IIT can explain why the cerebral cortex is important for consciousness, but the cerebellum is not. In general, the coexistence of functional specialization and integration in the cerebral cortex is ideally suited to integrating information (Supplementary information S3 (figure)). Specifically, the grid-like horizontal connectivity among neurons in topographically organized areas in the posterior cortex, augmented by converging\u2013diverging vertical connectivity linking neurons along sensory hierarchies, should yield high values of \\Phi^{\\max} . By contrast, cerebellar micro-zones that process inputs and produce outputs that are feedforward and largely independent of each other cannot form a large complex; nor can they be incorporated into a cortical high \\Phi^{\\max} complex, even though each cerebellar micro-zone may be functionally connected with a portion of the cerebral cortex (Supplementary information S3 (figure)) 1 . In principle, these differences in organization can explain why lesions of the cerebellum, which has four times more neurons than the cerebral cortex [58] , do not seem to affect consciousness 7 , 8 . Furthermore, circuits providing inputs and outputs to a major complex may not contribute to consciousness directly. This seems to be true with neural activity in the peripheral sensory and motor pathways, as well as within circuits looping out and back into the cortex through the basal ganglia [59],[60],[61] , despite their manifest ability to affect cortical activity and thereby to influence the content of experience indirectly (Supplementary information S3 (figure)). IIT also accounts for the fading of consciousness during slow wave sleep when cortical neurons fire but, as a result of changes in neuromodulation, become bistable \u2014 that is, any input quickly triggers a stereotypical neuronal down-state, after which neurons enter an up-state and activity resumes stochastically [29] . Bistability implies a generalized loss of both selectivity (causal convergence or degeneracy) and effectiveness (causal divergence or indeterminism) 18 that results in a breakdown of information integration (Supplementary information S3 (figure)). Findings from a study that used intracranial stimulation and recordings in patients with epilepsy are consistent with this account (Supplementary information S4 (box)) [62] . During wakefulness, electrical stimulation of the cortex triggered a chain of deterministic phase-locked activations, whereas during slow wave sleep the same input induced a stereotyped slow wave that was associated with a cortical down-state (that is, a suppression of power \u226520 Hz). The cortical activity resumed to wakefulness-like levels after the down-state, but the phase-locking to the stimulus was lost, indicative of a break in the cause\u2013effect chain (Supplementary information [S4] (box)). Similar considerations would explain why information integration is impaired when consciousness fades despite the increased level of activity and synchronization that occurs early during generalized seizures63. IIT also provides a plausible account as to why conscious brains might have evolved. The world is immensely complex, at multiple spatial and temporal scales, and organisms with brains that can incorporate statistical regularities that reflect the causal structure of the environment into their own causal structure have an adaptive advantage for prediction and control 2 . The IIT framework, which emphasizes the information matching between intrinsic and extrinsic causal structures, has both similarities and differences with Bayesian approaches (for example, see REF. [64]). According to IIT, given the constraints on energy and space, organisms with brains of high \\Phi^{\\max} should have an adaptive advantage over less integrated competitors because they can fit more concepts (that is, functions) within a given number of neurons and connections. Simulated organisms (known as animats), whose \u2018brains\u2019 evolve by natural selection, show a monotonic relationship between integrated information and adaptation when placed in a maze [65] . Similarly, in the brain of animats that evolved to catch falling blocks in a simulated two-dimensional environment, both \\Phi^{\\max} and the number of concepts increased as a function of how well the animats performed on the task. Although in simpler environments animats with modular feedforward brains can catch blocks just as well, only animats with a high \\Phi^{\\max} evolve to adapt to more complex environments [66] . 06. Predictions \u00b6 At the most general level, IIT predicts that the PSC in the brain \u2014 that is, the major complex \u2014 must be a maximum of intrinsic cause\u2013effect power, regardless of the particular set of neurons that constitute it ( FIG. 3 ). IIT also predicts that the spatio-temporal grain of the physical elements specifying consciousness is that yielding the maximum \\phi \u03a6 ( FIG. 2 ). Testing these predictions experimentally is challenging but not impossible. During the initial formulation of IIT, a systematic set of experiments was designed to test its specific prediction that consciousness requires both integration and differentiation [67] . An empirical measure, the perturbational complexity index (PCI), which can gauge the intrinsic cause\u2013effect power of the cortex, has been introduced as a practical proxy for \\Phi^{\\max} (REF. [68]). Calculating the PCI involves two steps: perturbing the cerebral cortex using transcranial magnetic stimulation to engage deterministic interactions among distributed groups of cortical neurons (integration) and measuring the incompressibility (algorithmic complexity) of the resulting responses (information). The PCI is high only if brain responses are both integrated and differentiated, corresponding to a distributed spatio-temporal pattern of causal interactions that is complex and hence not very compressible. So far, studies using PCI have confirmed the prediction of IIT that the loss and recovery of consciousness is associated with the breakdown and recovery of the capacity for information integration. This relationship holds true across different states of sleep69 and anaesthesia (using different agents with various mechanisms of action) [70] and in patients with brain damage, at the level of single subjects [68] . Importantly, once PCI is validated in participants that can report on whether they were conscious or not, the index can be used to assess the capacity for information integration in patients who are unresponsive (such as those in a vegetative state) or cannot report (such as newborn infants and non-human species). Another approach to estimating differentiation and integration in practice is to investigate the average properties of neural interactions based on a representative sample of neural states that span many regions of cause\u2013effect space, such as those triggered by a movie sequence [23] . The data from a candidate set of neural elements (for example, functional MRI blood oxygen level-dependent values) can then be analysed using measures of differentiation and integration based on the postulates of IIT [23] . It is also possible to obtain an indication of information capacity from the dynamics of spontaneous activity [26],[71],[72] . Some studies in rats [73] , monkeys [74] and humans [75] have confirmed that the differentiation of blood oxygen level-dependent activity patterns decreases when consciousness is lost. A similar approach can be used to evaluate information matching \u2014 how well the intrinsic cause\u2013effect structures specified by the brain fit the causal structure of the environment 2 ,[23] . Similar approaches could also be used to test the prediction that consciousness should split if a single major complex splits into two or more complexes, and that the split should happen precisely when two maxima of integrated information supplant a single maximum. For example, we could progressively reduce the efficacy of transmission in the callosal fibres by cooling or by the use of optogenetics. IIT predicts that there would be a moment at which, as a result of a minor change in the traffic of neural impulses across the callosum, a single consciousness would suddenly split into two. As discussed earlier, a split from a single major complex into two or more might also be observed in functional blindness (when a patient claims to be blind but may purposefully avoid obstacles) and other dissociative disorders, perhaps even in healthy participants under certain circumstances (such as during autopilot-like driving while having a conversation) ( FIG. 3f ). Turning to the contents of consciousness, the fundamental identity of IIT implies that all qualitative features of experience correspond to features of the conceptual structure specified by the PSC. For example, the organization of experience into distinct modalities (such as sight, hearing and touch) and submodalities (such as colour, shape and motion within the modality of sight) should correspond to the presence, within a conceptual structure, of distinct sets of concepts with extensively overlapping purviews within each set, but much less across sets 2 . IIT further predicts that the binding [56] of phenomenal distinctions, such as seeing a blue book on the piano on the left, should correspond, in the conceptual structure, to an overlap in the purview of the respective concepts (a relation). Also, differences between experiences should correspond to distances among conceptual structures in cause\u2013effect space and dissimilarities among phenomenal distinctions within an experience should correspond to distances between concepts. The refinement of experience that occurs through learning (for example, learning to discriminate the taste of different wines) should be reflected in a refinement of shapes in cause\u2013effect space as a result of the addition and splitting of concepts. IIT also predicts that the spatial structure that characterizes much of our daily experience should be reflected in features of conceptual structures that are specified by connections among neurons arranged in two-dimensional grids. For example, horizontal connections within topographically organized visual areas would be needed to experience visual space from the intrinsic perspective, rather than merely serving to mediate modulatory contextual effects. This also implies that local strengthening or weakening of such horizontal connections in topographic areas should lead to a local distortion of experienced visual space, even though the feedforward mapping of visual inputs from the world remains unchanged. More generally, IIT predicts that changes in the efficacy of the connections among elements of the PSC should lead to changes in experience even when these changes are not accompanied by changes in activity. A counterintuitive consequence of this prediction is that a brain area could contribute to an experience even if it is inactive but not if its connections or neurons are inactivated. Thus topographic visual areas would create visual space even in the absence of spiking activity but not if the horizontal connections within those areas are inactivated. Similarly, if the connections of neurons in colour areas are intact, the neurons would contribute to experience even if they are silent, by specifying negative colour concepts, such as when seeing a picture in black and white. However, if the connections are damaged, they would not specify any colour concepts, as with certain achromatopsic patients who do not even understand that the picture is missing colour [27] ( FIG. 3c ). Similarly, IIT predicts that the cerebral cortex as a whole may support experience even if it is almost silent, a state which may perhaps be reached through meditative practices designed to achieve \u2018naked awareness\u2019 without content76. This contrasts with the common assumption that neurons only contribute to consciousness if they are active and \u2018broadcast\u2019 the information they represent77,78 (Supplementary information S5 (box)). States of naked awareness could be compared with states of unawareness that occur, for example, during deep sleep or anaesthesia, when the cause\u2013effect repertoires of cortical neurons, regardless of the level of neuronal activity, are disrupted as a result of bistability [79] . 07. Conclusions \u00b6 In summary, IIT is a theory of consciousness that starts from the self-evident, essential properties (axioms) of experience and translates them into the necessary and sufficient conditions (postulates) for the PSC. The axioms are intrinsic existence (my experience exists from my own intrinsic perspective); composition (it has structure), information (it is specific), integration (it is unitary) and exclusion (it is definite). The corresponding postulates state that the physical substrate of an experience must have cause\u2013effect power upon itself (intrinsic existence); its parts must have cause\u2013effect power within the whole (composition); and the cause\u2013effect power of the PSC must be specific (information), irreducible (integration) and maximally so (exclusion). The fundamental identity of IIT states that the quality or content of consciousness is identical to the form of the conceptual structure specified by the PSC, and the quantity or level of consciousness corresponds to its irreducibility (integrated information \\Phi ). The assessment of the identity between experiences and conceptual structures as proposed by IIT is clearly a demanding task, not only experimentally, but also mathematically and computationally. Evaluating maxima of intrinsic cause\u2013effect power systematically requires going through many levels of organization, at multiple temporal scales, in many sets of brain regions, while performing an extraordinary number of perturbations and observations. Hopefully, heuristic approaches will be sufficient to make a strong case that the PSC is constituted of some particular neural elements, timescales and activity states. It will then be essential to test the prediction that any manipulation that affects the PSC at the spatio-temporal grain of maximum intrinsic cause\u2013effect power should affect experience. Conversely, similar manipulations that do not affect the PSC, or that affect it at the wrong spatio-temporal grain, should leave experience unchanged. These and other predictions, especially those that are coun- terintuitive, will also help in assessing the validity of IIT in relation to other proposals about the neural basis of consciousness (Supplementary information S5 (box)). Importantly, the more convincingly IIT can be validated under conditions in which it is relatively easy to assess how consciousness changes, the more it will help to make inferences about consciousness in hard examples, such as brain-damaged patients with residual areas of cortical activity, fetuses, infants, animals and machines. If it is validated, IIT may also prompt a reconsideration of how widespread consciousness is in nature and at what physical scale it may occur 9 . Intriguingly, IIT allows for certain simple systems such as grid-like architectures, similar to topographically organized areas in the human posterior cortex, to be highly conscious even when not engaging in any intelligent behaviour. Conversely, digital computers running complex programs based on a von Neumann architecture would not be conscious, even though they may perform highly intelligent functions and simulate human cognition. IIT offers a principled, empirically testable and clinically useful account of how three pounds of organized, excitable matter support the central fact of our existence \u2014 subjective experience. Time will tell whether this account is anywhere near the mark. \u00b6","title":"190723 Tononi, Giulio, 2016"},{"location":"190723_TononiGiulio_2016/#contents","text":"00. Abstract 01. Introduction 02. From phenomenology to physics Figure 1. An experience is a conceptual structure 03. The PSC within the brain Figure 2. Identifying the elements, timescale and states of the physical substrate of consciousness (PSC) from first principles 03.01. Elements of the PSC 03.02. Timescale 03.03. State of the elements 03.04. Constitution of the PSC Figure 3. Identifying the physical substrate of consciousness (PSC) from first principles 03.05. Can the PSC change? 03.06. Multiple complexes 04. Information capacity of consciousness Figure 4. Phenomenal content and access content Box 1. Consciousness, integrated information and Shannon information 05. Explanations 06. Predictions 07. Conclusions","title":"Contents"},{"location":"190723_TononiGiulio_2016/#00_abstract","text":"In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause\u2013effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.","title":"00. Abstract"},{"location":"190723_TononiGiulio_2016/#01_introduction","text":"","title":"01. Introduction"},{"location":"190723_TononiGiulio_2016/#consci_is_subjective_ex","text":"Consciousness is subjective experience \u2014 \u2018what it is like\u2019, for example, to perceive a scene, to endure pain, to entertain a thought or to reflect on the experience itself 1 , 2 , 3 . When consciousness fades, as it does in dreamless sleep, from the intrinsic perspective of the experiencing subject, the entire world vanishes.","title":"consci is subjective EX"},{"location":"190723_TononiGiulio_2016/#qs_about_consci","text":"Consciousness depends on the integrity of certain brain regions and the particular content of an experience depends on the activity of neurons in parts of the cerebral cortex 4 . However, despite increasingly refined clinical and experimental studies, a proper understanding of the relationship between consciousness and the brain has yet to be established 5 , 6 . For example, it is not known why the cortex supports consciousness when the cerebellum does not, despite having four times as many neurons 7 , 8 , or why consciousness fades during deep sleep while the cerebral cortex remains active. There are also many other difficult questions about consciousness. Are patients with a functional island of cortex surrounded by widespread damage conscious, and if so, of what? Are newborn infants conscious? Are animals that display complex behaviours, but have brains very different from humans, conscious6? Can intelligent machines be conscious 9 ?","title":"Qs about consci"},{"location":"190723_TononiGiulio_2016/#hard_problem_iit","text":"start from phenomenal properties of experience, or axioms and inder postulates To answer these questions, the empirical study of consciousness should be complemented by a theoretical approach. The reason why some neural mechanisms, but not others, should be associated with consciousness has been called \u2018 the hard problem \u2019 because it seems to defy the possibility of a scientific explanation 10 . In this Opinion article, we provide an overview of the integrated information theory (IIT) of consciousness, which has been developed over the past few years 1 , 2 , 3 , 11 , 12 . IIT addresses the hard problem in a new way. It does not start from the brain and ask how it could give rise to experience; instead, it starts from the essential phenomenal properties of experience, or axioms , and infers postulates about the characteristics that are required of its physical substrate. Moreover, IIT presents a mathematical framework for evaluating the quality and quantity of consciousness 1 , 2 , 3 , 9 . We begin by providing a summary of the axioms and corresponding postulates of IIT and show how they can be used, in principle, to identify the physical substrate of consciousness (PSC) . We then discuss how IIT explains in a parsimonious manner a variety of facts about the relationship between consciousness and the brain, leads to testable predictions, and allows inferences and extrapolations about consciousness.","title":"hard problem &amp; IIT"},{"location":"190723_TononiGiulio_2016/#glossary","text":"[Achromatopsia] A condition in which a person is unable to perceive colours. [Anosognosia] A condition in which a person has a neurological deficit, but is unaware of it. Axioms Properties that are self-evident and essential; in integrated information theory, those that are true of every possible experience \u2014 namely, intrinsic existence, composition, information, integration and exclusion. Background conditions [background condition] Factors that enable consciousness, such as neuromodulators and external inputs that maintain adequate excitability. Cause\u2013effect repertoire The probability distribution of potential past and future states of a system that is specified by a mechanism in its current state. [Cause\u2013effect space] A space with each axis representing the probability of each possible past and future state of a system. Cause\u2013effect structure The set of cause\u2013effect repertoires specified by all the mechanisms of a system in its current state. [Complex] A set of elements in a state that specifies a conceptual structure corresponding to a maximum of integrated information ( \\Phi^{\\max} ). A complex is thus a physical substrate of consciousness. [Concepts][concept] The cause\u2013effect repertoires specified by a mechanism that is maximally irreducible ( \\phi^{\\max} ). Conceptual structure The set of all concepts specified by a system of elements in a state with their respective \\phi^{\\max} values, which can be plotted as a set of points in cause\u2013effect space. Content-specific NCC Neural elements, the activity of which determines a particular content of experience Elements The minimum constituents of a system that have at least two different states (for example, being on or off), inputs that can affect those states and outputs that depend on them. Full NCC The neural elements constituting the physical substrate of consciousness, irrespective of its specific content. Integrated information (Denoted \\Phi ). Information that is specified by a system that is irreducible to that specified by its parts. It is calculated as the distance between the conceptual structure specified by the intact system and that specified by its minimum information partition. Mechanism Any subset of elements within a system that has cause\u2013effect power on it (that is, that constrains its cause\u2013effect space). Neural correlates of consciousness (NCC). The minimum neuronal mechanisms jointly sufficient for any one specific conscious experience. Postulates Properties of experience that are derived from the axioms of integrated information theory and that must be satisfied by the physical substrate of consciousness \u2014 namely, to be a maximum of irreducible, specific, compositional, intrinsic cause\u2013effect power (intrinsic cause\u2013effect power for short). [Purviews][purview] The subsets of elements of a complex, the past and future states of which are constrained by a mechanism specifying a concept. Qualia The qualitative feeling of phenomenal distinctions within an experience (for example, seeing a colour, hearing a sound or feeling a pain). Relations Maximally irreducible overlaps among the purviews of two or more concepts","title":"Glossary"},{"location":"190723_TononiGiulio_2016/#02_from_phenomenology_to_physics","text":"","title":"02. From phenomenology to physics"},{"location":"190723_TononiGiulio_2016/#figure_1_an_experience_is_a_conceptual_structure","text":"According to inte- grated information theory (IIT), a particular experience (illustrated here from the point of view of the subject) is identical to a conceptual structure spec- ified by a physical substrate. The true physical substrate of the depicted experience (seeing one\u2019s hands on the piano) and the associated conceptual structure are highly complex. To allow a complete analysis of conceptual structures, the physical substrate illustrated here was chosen to be extremely simple1,2: four logic gates (labelled A, B, C and D, where A is a Majority (MAJ) gate, B is an OR gate, and C and D are AND gates; the straight arrows indicate connections among the logic gates, the curved arrows indi- cate self-connections) are shown in a particular state (ON or OFF). The anal- ysis of this system, performed according to the postulates of IIT, identifies a conceptual structure supported by a complex constituted of the elements A, B and C in their current ON states. The borders of the complex, which include elements A, B, and C but exclude element D, are indicated by the green circle. According to IIT, such a complex would be a physical substrate of consciousness (Supplementary information S1 (figure)). The conceptual structure is represented as a set of stars and, equivalently, as a set of histo- grams. The green circle represents the fact that experience is definite (it has borders). Each histogram illustrates the cause\u2013effect repertoire of a concept: how a particular mechanism constrains the probability of past and future states of its maximally irreducible purview within the complex ABC. The bins on the horizontal axis at the bottom of the histograms rep- resent the 16-dimensional cause\u2013effect space of the complex \u2014 all its eight possible past states (p; in blue) and eight possible future states (f; in red; ON is 1 and OFF is 0). The vertical axis rNepatruesrenRtesvthiewpsro| Nbaebuirliotsycoief enacceh state (for consistency, the probability values shown are over the states of the entire complex and not just over the subset of elements constituting the purview). In this example, five of seven possible concepts exist, specified by the mechanisms A, B, C, AB, AC (all with \\phi^{\\max}>0 ) in their current state (which are labelled as Ac, Bc, etc.). The subsets BC and ABC do not specify any con- cept because their cause\u2013effect repertoire is reducible by partitions ( \\phi^{\\max}=0 ). In the middle, the 16-dimensional cause\u2013effect space of the com- plex is represented as a circle, where each of the 16 axes corresponds to one of the eight possible past (p; blue arrows) and eight possible future states (f; red arrows) of the complex, and the position along the axis represents the probability of that state. Each concept is depicted as a star, the position of which in cause\u2013effect space represents how the concept specifies the probability of past and future states of the complex, and the size of which measures how irreducible the concept is ( \\phi^{\\max} ). Relations between two concepts (overlaps in their purviews) are represented as lines between the stars. The fundamental identity postulated by IIT claims that the set of con- cepts and their relations that compose the conceptual structure are identi- cal to the quality of the experience. This is how the experience feels \u2014 what it is like to be the complex ABC in its current state 111. The intrinsic irreduc- ibility of the entire conceptual structure ( \\Phi^{\\max} , a non-negative number) reflects how much consciousness there is (the quantity of the experience). The irreducibility of each concept ( \\phi^{\\max} ) reflects how much each phenomenal distinction exists within the experience. Different experiences correspond to different conceptual structures.","title":"Figure 1 | An experience is a conceptual structure"},{"location":"190723_TononiGiulio_2016/#03_the_psc_within_the_brain","text":"NCC located in cortico-thalamic system NOT known which areas / layers / neuronal populations whether units are neuron / groups of neurons which aspects of activity neural substrate of consci is anatomically fixed / can shrink IIT content-specific NCC : in paticular state (activity pattern) full NCC : irrespcetive of particular state [background condition]: factors enable consci Experimental evidence currently suggests that the neural correlates of consciousness (NCC) are likely to be located in certain parts of the cortico-thalamic system 5 , but it is not known specifically which cortical areas, layers or neuronal populations are involved, whether the relevant units are neurons or groups of neurons, and which aspects of their activity matter 5 . It is also not known whether the neural substrate of consciousness is anatomically fixed or can shrink, expand and move. IIT offers theoretical clarity on the empirical notion of the NCC 5 . Specifically, it states that the content-specific NCC correspond to the neural elements of the PSC in a particular state (activity pattern), which specify a particular phenomenal content; the full NCC correspond to the neural elements constituting the PSC irrespective of their particular state; the background conditions [background condition] are factors that enable consciousness, such as neuromodulators and external inputs that maintain adequate excitability, which are kept fixed when evaluating the \\Phi value of the PSC. Most importantly, the axioms and postulates of IIT can be used to provide a single, general principle for identifying the PSC in the brain \u2014 namely that the PSC must correspond to a complex of neural elements with maximum intrinsic cause\u2013effect power.","title":"03. The PSC within the brain"},{"location":"190723_TononiGiulio_2016/#figure_2_identifying_the_elements_timescale_and_states_of_the_physical_substrate_of_conscious-_ness_psc_from_first_principles","text":"It is possible to determine maxima of cause\u2013effect power within the central nervous system by perturbing and observing neural elements at various micro- and macro-levels 18 . High cause\u2013effect power is reflected in deterministic responses and low cause\u2013 effect power is reflected in responses that vary randomly across trials. a | To identify the spatial grain of the elements of the PSC supporting consciousness, a schematic example shows how optogenetic perturbation and unit recording could be applied to a subset of neurons (here, 3 out of 36 neurons) to establish maxima of cause\u2013effect power. For each of three trials, the left panel shows the effects of the perturbation on the entire system at the micro-level. Grey neurons are unaffected, blue neu- rons decrease their firing rates, red neurons increase their firing rates and purple neurons respond with burst firing. The right-hand panel shows the effects of the perturbation at the macro-level after coarse-graining of the 36 neurons into nine groups of four cells each. Macro-states are defined according to the rule that if \u226550% of the neurons in the group are in a given micro-state (such as low firing, high firing or bursting), then the group is considered to be in that state at the macro-level. In this example, the macro-level (groups of neurons) has higher cause\u2013effect power than the micro- level (single neurons), because the response is deterministic at the macro-level (as evidenced by the consistent colour scheme), whereas there are variations between trials at the micro-level (incon- sistent colours). b | To identify the temporal grain of neuronal activity supporting consciousness, a possible experimental setup would be one in which one neuron (the top trace) is optogenetically excited while recording from other neurons (labelled N1\u2013N4) across three trials, shown in the upper panel at the 10 ms timescale (micro-scale). Grey shading indicates no effects on neuron firing in the 10 ms following the stimulation compared with the 10 ms before the stimulation, blue shading indi- cates decreased firing and red shading indicates increased firing. The lower panel shows the same data after temporal coarse-graining over 100 ms intervals. Macro-states are defined according to the rule that if a neuron increases (or decreases) its firing rate by >50% within 100 ms post-stimulus compared with the baseline, the macro state is considered to be high (or low) firing. In this example, the macro-level (100 ms intervals) has higher cause\u2013effect power (more deterministic responses) than the micro-level (10 ms intervals). c | To identify the neural states that support consciousness, optoge- netic perturbations could be used to drive one neuron to fire either at low frequency, high tonic frequency or bursting (top trace) resulting in spectral peaks at 2 Hz (green), 50 Hz (red) and 150 Hz (yellow) for neurons N1\u2013N4 (data are shown as a firing rate histogram). For each trial, the upper panel shows the responses of the other four neurons to each stimulation frequency at the micro-scale level in the spectral domain (micro-bins, only a few of which are represented). The coloured bars indicate coincidence, within a micro-bin, between the frequency of stimulation and the spectral peak of the responses. The lower panel of each trial shows the effect of the perturbation at the corresponding macro-level after spectral coarse-graining. Macro-states map into micro-states as indicated below the frequency bins. Here, spectral coarse-graining (binning firing rates into three levels, low, high and burst firing) results in higher cause\u2013effect power (responses that are more deterministic) than at the micro-level.","title":"Figure 2 | Identifying the elements, timescale and states of the physical substrate of conscious- ness (PSC) from first principles"},{"location":"190723_TononiGiulio_2016/#0301_elements_of_the_psc","text":"What is the spatial scale of the neural elements that support consciousness: synapses, neurons, neuronal groups, local fields or perhaps all of these? According to IIT, the neural elements of the PSC are those, and only those, that support a maximum of cause\u2013 effect power, as determined from the intrinsic perspective of the system itself. Importantly, and contrary to common reductionist assumptions 17 , cause\u2013effect power can be higher at a macro-level than at a micro-level 18 . For example, a system of neuron-like micro-elements may have less cause\u2013effect power than the same system coarse-grained at the macro-level of neuronal groups ( FIG. 2a ). In general, whether the macro or micro grain size has higher cause\u2013effect power depends on how intra- and inter-group connections are organized and the amount of indeterminism (noise) and degeneracy (multiple ways of obtaining the same effect 18 ). An exhaustive evaluation of cause\u2013 effect power at multiple levels is only possible in small simulated networks 19 . In a real network 20 , we could start by assessing the cause\u2013effect repertoire of individual neurons. For example, if a neuron is firing a burst of spikes, its cause repertoire is the probability distribution of past network states that would have caused it to burst (for example, firing patterns of its afferent neurons within the previous 100 ms). Similarly, its effect repertoire is the probability distribution of future network states given that the neuron is bursting. Experimentally, we could obtain an estimate of such cause\u2013 effect repertoires by stimulating one or more neurons optogenetically while simultaneously recording the firing activity of a population of neurons via two-photon calcium imaging (keeping the background conditions constant, such as the level of arousal and sensory input) ( FIG. 2a ). Next, we would need to test for the irreducibility of the cause\u2013effect repertoires, which can be achieved by noising connections (that is, enforcing firing at chance levels) across a partition of the network. Doing so would establish which subset of incoming connections makes the most irreducible difference ( \\phi^{\\max} ) to the firing of the observed neuron1 (and this could be carried out analogously for outgoing connections). A similar procedure should then be repeated for subsets of two neurons, three neurons, and so on, because combinations of neurons can also have irreducible cause\u2013effect repertoires (defined as higher order mechanisms). Such experiments would provide an estimate of maximally irreducible cause\u2013effect repertoires at the level of neurons. To evaluate cause\u2013effect power at the macro-level, we could then repeat the same stimulation\u2013recording\u2013noising procedure by considering subsets of neurons as distinct macro-groups and mapping micro-states onto macro-states. For example, we could take all pyramidal neurons in each mini-column as a distinct group and define the group state as low firing, high firing or bursting, depending on the overall firing rate of the neurons over 100 ms. By estimating the \\phi^{\\max} value of cause\u2013effect repertoires at the level of both individual neurons and groups of neurons, an experimenter could thus assess at which grain size the network has most cause\u2013effect power from its own intrinsic perspective \u2014 that is, at which level it makes the most difference to itself. IIT predicts that the elements of the PSC are to be found at exactly that level and not at any finer or coarser grain, a prediction that is empirically testable: does the firing of a single neuron make a difference21 to the content of experience, or only the average activity of a cortical mini-column22?","title":"03.01. Elements of the PSC"},{"location":"190723_TononiGiulio_2016/#0302_timescale","text":"Which timescale of neuronal activity is important for consciousness: a few milliseconds, tens of milliseconds, hundreds of milliseconds, or perhaps all of these? Again, IIT predicts that the relevant time interval should be that which makes the most difference to the system, as determined from its intrinsic perspective. Once more, depending on the specific mechanisms of a system, some macro-temporal grain may have a higher cause\u2013effect power than both finer and coarser grains ( FIG. 2b ). Whatever timescale turns out to have the maximum cause\u2013effect power within the relevant brain regions, it should be consistent with estimates of the timescale of experience14\u201316.","title":"03.02. Timescale"},{"location":"190723_TononiGiulio_2016/#0303_state_of_the_elements","text":"An external observer can choose to analyse brain states at any level of detail. For example, some neu- rophysiologists may be interested in the effects of the timing of individual neuronal spikes on brain function, others in the effects of broad fluctuations in the activity of populations of neurons. In fact, it is likely that almost any change in the state of any neurobiological variable will have some effect somewhere in the brain21. According to IIT, the neural states that are important for consciousness are only those that have maximum cause\u2013effect power on the system itself. For example, assume that, from the intrinsic perspective of the system, maximum cause\u2013effect power was achieved when coarse-graining firing states into low, high and burst firing ( FIG. 2c ). In this case, IIT predicts that finer grained neural states, despite their demonstrable neuro- physiological effects, make no difference to the content of experience. Note that spatio-temporal grain and the relevant activity states of the elements specifying the PSC could change according to brain region, developmental period, species, neuromodulatory milieu and even the task being performed.","title":"03.03. State of the elements"},{"location":"190723_TononiGiulio_2016/#0304_constitution_of_the_psc","text":"","title":"03.04. Constitution of the PSC"},{"location":"190723_TononiGiulio_2016/#figure_3_identifying_the_physical_substrate_of_consciousness_psc_from_first_principles","text":"The complex of neural elements that constitutes the PSC can be identified by searching for maxima of intrinsic cause\u2013effect power. a | For example, assume that the elements, timescale and states at which intrinsic cause\u2013effect power reaches a maximum have been identified using optogenetic and unit recording tools ( FIG. 2 ). Here, the elements are groups of neurons, the timescale is over 100 ms and there are three states (low, high and burst firing). b | In a healthy, awake participant, the set of neural elements specifying the conceptual structure with the highest \\Phi^{\\max} is assumed, based on current evidence, to be a complex of neuronal groups distributed over the posterior cortex and portions of the anterior cortex 5 . Empirical studies can, in principle, establish whether the full neural corre- lates of consciousness 5 correspond to the maximum of intrinsic cause\u2013effect power, thereby corroborating or falsifying a key prediction of integrated information theory. c | The boundaries of the PSC (green line) may change after cortical lesions, such as those causing absolute achromatopsia, result- ing in a smaller PSC. d | The PSC boundaries may also move as a result of changes in excitability and effective connectivity, as might occur during pure thought that is devoid of sensory content. e | The PSC could also split into two large local maxima of cause\u2013effect power (represented here by green and blue boundaries) as a result of anatomical disconnections, such as in split-brain patients, in which instance each hemisphere would have its own consciousness. f | The PSC may also split as a result of functional disconnec- tions, which may occur in some psychiatric disorders and perhaps under certain dual-task conditions \u2014 for example while driving and talking at the same time. g | The coexistence of a large major complex with one or more minor complexes that may support sophisticated, seemingly unconscious performance could be a common occurrence in everyday life.","title":"Figure 3 | Identifying the physical substrate of consciousness (PSC) from first principles"},{"location":"190723_TononiGiulio_2016/#0305_can_the_psc_change","text":"An important issue is the extent to which the set of neural elements that constitute the PSC is fixed. Clearly, if a cortical area is inactivated (by a lesion, for example) it will no longer be part of the PSC and the phenomenal distinctions contributed by that area will no longer be available. For example, if cortical areas responding to colour are inactivated ( FIG. 3c ), experiences will not only lack colour, but patients would not even understand what is lacking (as reported in cases of achromatopsia with anosognosia [27] ). It is an open question whether the PSC can shrink, expand or move during normal wakefulness, possibly through attentional modulation of excitability and functional connectivity. For example, when we are engrossed in an action movie and not engaged in self-reflection, the activity in prefrontal areas decreases [28] . Does this mean that the PSC shrinks, like when colour areas are inactivated, or that brain regions supporting self-reflection remain inside the PSC but are inactive, in the same way that colour areas are inactive when watching a black and white movie? The location and size of the PSC is likely to change during sleep, during seizures, in patients with conversion and dissociative disorders, and possibly during hypnosis. During slow wave sleep, for example, neurons are bistable and show off-periods during which they become hyperpolarized (down-states) and silent [29] . However, these off-periods are usually not global, but affect local subsets of brain areas at different times [30] . Hence it is possible that during slow wave sleep the PSC may become smaller and reconfigure substantially. Sustained inactivation of certain areas during sleep may make dreaming patients incapable of reflective thought. Similarly, experiences of pure thought that have minimal perceptual content may be caused by slow waves that inactivate the posterior cortex, and be specified by a PSC that is considerably different from the PSC for purely perceptual experiences [31] ( FIG. 3d ). At other times, transient, local slow waves (indicative of an off-period) in colour areas may cause the PSC to shrink and lead to brief episodes of achromatopsia. Novel methods that allow the transient inactivation of specific cortical areas in humans, such as transcranial magnetic stimulation or focused ultrasound, would be ideal for evaluating the contribution of those areas to conscious content.","title":"03.05. Can the PSC change?"},{"location":"190723_TononiGiulio_2016/#0306_multiple_complexes","text":"According to IIT, two or more non-overlapping complexes may coexist as discrete PSCs within a single brain1, each with its own definite borders and value of \\Phi^{\\max} . The complex that specifies a person\u2019s day-to-day stream of consciousness should have the highest value of \\Phi^{\\max} \u2014 that is, it should be the \u2018major\u2019 complex. In some conditions, for example after a split-brain operation, the major complex may split ( FIG. 3e ). In such instances, one consciousness, supported by a complex in the dominant hemisphere and with privileged access to Broca\u2019s area, would be able to speak about the experience, but would remain unaware of the presence of another consciousness, supported by a complex in the other hemisphere, which can be revealed by carefully designed experiments [32],[33] . An intriguing possibility is that splitting of the PSC may also occur in healthy people during long-lasting dual-task conditions \u2014 for example, when driving in an auto-pilot like manner on a familiar road while listening to an engaging conversation ( FIG. 3f ). Splitting into separate maxima may also occur through functional disconnections caused by pathological conditions, such as conversion and dissociative disorders [34] . Another intriguing possibility is that multiple conscious streams may coexist within a single brain in daily life. For example, the grid-like architectures in the colliculus and related mesencephalic regions, which are adept at multimodal integration within a spatial framework, may support a separate minor complex. Some examples of high-level cognitive performance such as judging whether a scene is congruous or incongruous [35],[36] \u2014 that appear to be carried out unconsciously from the perspective of the major complex \u2014 may support a separate minor complex ( FIG. 3e,g ). Alternatively, some of these functions may be mediated by feedforward circuits [37] that have \\Phi^{\\max}=0 because they lack integration and therefore are strictly unconscious 1 . An important question for the future is whether automatic, unconscious behaviours are mediated by specific cell types within the cortex, such as subcortical projection neurons of layer 5B [38] , that are different from other cell types that support consciousness.","title":"03.06. Multiple complexes"},{"location":"190723_TononiGiulio_2016/#04_information_capacity_of_consciousness","text":"The information-processing approach common in psychology estimates the information capacity of human consciousness to be at around 7 \u00b1 2 items [39] or \u226440 bits per second [39],[40] . In the classic Sperling task [41] , for example, participants are presented with a set of 12 letters for 300 ms, of which, after a mask and a delay, they can report at most four ( FIG. 4 ). The inference from such experiments is that the information content of consciousness is extremely limited, as is also suggested by the attentional blink and related psychophysical paradigms [42],[43] . For example, in change blindness, a major modification in a visual scene may go undetected if a blank is interposed between the two images [44] . In this view, the content of consciousness is limited to what can be accessed and reported, despite our phenomenal impression of richer content [42],[45],[46] . By contrast, others argue that phenomenal consciousness (what it is like to have an experience) has far greater capacity than access consciousness (what can be reported) [47],[48],[49] . For example, if participants are cued to a particular row of the Sperling display during the delay period, they can report three letters of any row; moreover, they can report the colour diversity of unattended letters at no cost to the identification of the cued letters [50] . Likewise, change blindness may be due not to a failure to experience, but to a lack of memory for the experience [51] . Similarly, low-level phenomenal features may be difficult to report because they vary rapidly and may be forgotten before they can be accessed from top-down mechanisms; pre-categorical stimuli, such as irregular scribbles, may be phenomenally salient but hard to describe in words. IIT claims that human consciousness has a high capacity for integrated information ( BOX 1 ). Even for a simple experience, such as seeing the Sperling display, the elements of the PSC specify a rich conceptual structure (high \\Phi^{\\max} ) composed of a very large number of concepts and relations. These correspond to all the phenomenal distinctions that make that experience what it is and thereby different from countless others 11 ( FIG. 4 ). It is useful to distinguish between low- and high-order concepts, depending on how many PSC elements are contained in their purviews. For example, a concept specifying the presence of an oriented edge at a particular location in the visual field has a low-order purview, whereas a concept specifying the extent of the entire visual field has a high-order purview. Concepts can also have low- and high invariance; for example, the concept for the letter A has high invariance because its purview specifies a high-order disjunction of states of the PSC elements (a specific arrangement of oriented edges in any of a large number of possible locations). Mechanisms specifying invariant concepts form a hierarchy going from low- to high-level areas of the cerebral cortex, as indicated by experimental data [52] and consistent with computational models for the recognition of objects [53] , places, events54 and spatial reference frames [55] . A concept can have low or high selectivity, depending on how strongly the state of its mechanism constrains its cause\u2013effect repertoire. In the brain, the adaptive bias towards sparse firing makes it likely that the neurons would fire strongly when specifying a high invariance, high selectivity concept, such as the presence of the letter A (that is, a positive concept), and be silent when specifying its low selectivity counterpart, such as the absence of the letter A (that is, a negative concept) ( FIG. 4 ). In experimental settings, the content of experience is typically probed by asking the participant about high invariance, positive concepts, such as letters in the Sperling paradigm. However, we could undoubtedly report many more concepts than just the identity of a few letters. For example, we could report that there are many black symbols, that they are arranged in three rows and four columns, in a rectangular array, within a rectangular display, over a white homogeneous background that is spatially extended, being composed of a multitude of distinguishable locations, each with its specific neighbours, and so on. We can also report many negative concepts \u2014 for example, that the Sperling display did not include a face, a tree, an animal, a house, and so on \u2014 for the thousands of high invariance concepts we possess that happen to be negative for this particular image. Finally, we can report how all these concepts are bound together within the same experience in a complex pattern of relations \u2014 for example, we see the letter A as an invariant that is nevertheless located at a particular spatial location, that is composed of two oblique edges and a horizontal edge in between, that is capital, printed in black and located on the rightmost column in the upper row of the array, and so on. According to IIT, this dynamic binding of phenomenal attributes56 occurs if, and only if, in cause\u2013effect space the corresponding concept purviews are related, meaning that they refer to an overlapping set of PSC elements and jointly constrain their past or future states. In short, the information that specifies an experience is much larger than the purported limited capacity of consciousness [57] . Although we are accustomed to summarizing what we see by referring to a few positive, high invariance concepts (for example, in FIG. 4 bottom panel, a participant may state: \u201cI see the letters O, S and A\u201d), we would not see what we see without the contribution of a large number of other concepts \u2014 low and high order, low and high invariance, positive and negative \u2014 and relations, which make the experience what it is (information) and thereby different from others (differentiation; FIG. 4 ). Consider what it would be like to look at the Sperling display not as a human, but as a machine implementing an efficient feedforward algorithm for letter recognition. The machine could certainly report three letters (in fact, all 12). However, such a machine could not see the scene and would understand virtually nothing because it has no other concept apart from the letters, not for the letter combination OSA, the array, the display, a face, an animal, and so on. Indeed, if there were a face, an animal, or anything else in the middle of the display, it would do its best to categorize it as a letter.","title":"04. Information capacity of consciousness"},{"location":"190723_TononiGiulio_2016/#figure_4_phenomenal_content_and_access_content","text":"The content NofaatunrexRpevriewncse| Nisemuruocshcileanrgcer than what can be reported by a subject at any point in time. The left-hand panel illustrates the Sperling task [41] , which involves the brief presentation of a three by four array of letters on a screen, and a particular row being cued by a tone. Out of the 12 letters shown on the display, participants correctly report only three or four letters \u2014 the letters cued by the tone \u2014 reflecting limited access. The top middle panel illustrates a highly simplified conceptual structure that corresponds to seeing the Sperling display, using the same conventions as outlined in FIG. 1. The myriad of positive and negative, first- and high-order, low- and high invariance concepts (represented by stars) that specify the content of this particular experience (seeing the Sperling display and having to report which letters were seen) make it what it is and different from countless other experiences (rich phenomenal content). The bottom panel schematically illustrates the physical substrate of consciousness (PSC) that might cor- respond to this particular conceptual structure (its boundary is represented by a green line). The PSC consists of neuronal groups that can be in a low firing state, a high firing state or a bursting state. Alone and in combination, these neuronal groups specify all the concepts that compose the conceptual structure. Stars that are linked to the PSC by grey dashed lines represent a small subset of these con- cepts. The PSC is synaptically connected to neurons in Broca\u2019s area by means of a limited capacity channel (dashed black arrow) that is dynamically gated by top-down connections (shown as solid black arrows) originating in the prefrontal cortex to carry out the instruction (that is, to report the observed letters \u2018OSA\u2019).","title":"Figure 4 | Phenomenal content and access content"},{"location":"190723_TononiGiulio_2016/#box_1_consciousness_integrated_information_and_shannon_information","text":"The term information is used very differently in integrated information theory (IIT) and in Shannon\u2019s theory of communication1, and confusing the two meanings can cause misunderstandings80. The word information derives from the Latin verb informare, which means \u2018to give form\u2019. In IIT the information content of an experience is specified by the form of the associated conceptual structure (the quality of the integrated information) and quantified by \u03a6max (the quantity of integrated information). In IIT, information is causal and intrinsic: it is assessed from the intrinsic perspective of a system based on how its mechanisms and present state affect the probability of its own past and future states (cause\u2013effect power). It is also compositional, in that different combinations of elements can simultaneously specify different probability distributions within the system. Moreover, it is qualitative, as it determines not only how much a system of mechanisms in a state constrains its past and future states, but also how it does so. Crucially, in IIT, information must be integrated. This means that if partitioning a system makes no difference to it, there is no system to begin with. Information in IIT is exclusive \u2014 only the maxima of integrated information are considered. By contrast, Shannon information is observational and extrinsic \u2014 it is assessed from the extrinsic perspective of an observer and it quantifies how accurately input signals can be decoded from the output signals transmitted across a noisy channel. It is not compositional nor qualitative, and it does not require integration or exclusion1. When averaged over many different states of the physical substrate of consciousness (PSC), we can think of the integrated information \u03a6max as a measure of the intrinsic phenomenal capacity of the conceptual structures specified by the PSC. By contrast, Shannon information can be used to measure the extrinsic access capacity of a channel that runs from a subset of elements of the PSC to Broca\u2019s area and from there to the motor neurons that ultimately convey the report (FIG. 4). In IIT, the experience of seeing the Sperling display is identical to a particular conceptual structure \u2014 it is a form in cause\u2013effect space with a high value of integrated information \u03a6max, as specified by its PSC (FIG. 4). The average value of \u03a6max for different states of the PSC measures its intrinsic phenomenal capacity. The figure also shows a neural information channel from the PSC to Broca\u2019s area, formed dynamically by top-down attentional mechanisms located in the prefrontal cortex, which select which subset of elements of the PSC should drive the report (FIG. 4). This channel conveys extrinsic information and has a low Shannon capacity (only four letters at a time can be reported), which corresponds to the mutual information between its inputs and outputs. Seen in this way, it becomes obvious that the extrinsic information that can be selected through attention, kept in working memory and channelled out for report is only a partial read-out of the intrinsic information that is specified by the PSC over its own cause\u2013effect space. Although at any given time we can access and report the state of a few elements of the PSC, and that of some other elements at another time, it is not possible to dump the state of all elements through a limited capacity channel. It is certainly not possible to transmit a conceptual structure (intrinsic information) through a channel (extrinsic information)\u2014phenomenal capacity, properly understood, truly exceeds access capacity. Likewise, conscious information is not something that is transmitted or broadcast from one part of the brain to another77,78 (Supplementary information S5 (box)).","title":"Box 1 | Consciousness, integrated information and Shannon information"},{"location":"190723_TononiGiulio_2016/#05_explanations","text":"IIT provides a principled explanation for several seemingly disparate facts about the PSC. For example, IIT can explain why the cerebral cortex is important for consciousness, but the cerebellum is not. In general, the coexistence of functional specialization and integration in the cerebral cortex is ideally suited to integrating information (Supplementary information S3 (figure)). Specifically, the grid-like horizontal connectivity among neurons in topographically organized areas in the posterior cortex, augmented by converging\u2013diverging vertical connectivity linking neurons along sensory hierarchies, should yield high values of \\Phi^{\\max} . By contrast, cerebellar micro-zones that process inputs and produce outputs that are feedforward and largely independent of each other cannot form a large complex; nor can they be incorporated into a cortical high \\Phi^{\\max} complex, even though each cerebellar micro-zone may be functionally connected with a portion of the cerebral cortex (Supplementary information S3 (figure)) 1 . In principle, these differences in organization can explain why lesions of the cerebellum, which has four times more neurons than the cerebral cortex [58] , do not seem to affect consciousness 7 , 8 . Furthermore, circuits providing inputs and outputs to a major complex may not contribute to consciousness directly. This seems to be true with neural activity in the peripheral sensory and motor pathways, as well as within circuits looping out and back into the cortex through the basal ganglia [59],[60],[61] , despite their manifest ability to affect cortical activity and thereby to influence the content of experience indirectly (Supplementary information S3 (figure)). IIT also accounts for the fading of consciousness during slow wave sleep when cortical neurons fire but, as a result of changes in neuromodulation, become bistable \u2014 that is, any input quickly triggers a stereotypical neuronal down-state, after which neurons enter an up-state and activity resumes stochastically [29] . Bistability implies a generalized loss of both selectivity (causal convergence or degeneracy) and effectiveness (causal divergence or indeterminism) 18 that results in a breakdown of information integration (Supplementary information S3 (figure)). Findings from a study that used intracranial stimulation and recordings in patients with epilepsy are consistent with this account (Supplementary information S4 (box)) [62] . During wakefulness, electrical stimulation of the cortex triggered a chain of deterministic phase-locked activations, whereas during slow wave sleep the same input induced a stereotyped slow wave that was associated with a cortical down-state (that is, a suppression of power \u226520 Hz). The cortical activity resumed to wakefulness-like levels after the down-state, but the phase-locking to the stimulus was lost, indicative of a break in the cause\u2013effect chain (Supplementary information [S4] (box)). Similar considerations would explain why information integration is impaired when consciousness fades despite the increased level of activity and synchronization that occurs early during generalized seizures63. IIT also provides a plausible account as to why conscious brains might have evolved. The world is immensely complex, at multiple spatial and temporal scales, and organisms with brains that can incorporate statistical regularities that reflect the causal structure of the environment into their own causal structure have an adaptive advantage for prediction and control 2 . The IIT framework, which emphasizes the information matching between intrinsic and extrinsic causal structures, has both similarities and differences with Bayesian approaches (for example, see REF. [64]). According to IIT, given the constraints on energy and space, organisms with brains of high \\Phi^{\\max} should have an adaptive advantage over less integrated competitors because they can fit more concepts (that is, functions) within a given number of neurons and connections. Simulated organisms (known as animats), whose \u2018brains\u2019 evolve by natural selection, show a monotonic relationship between integrated information and adaptation when placed in a maze [65] . Similarly, in the brain of animats that evolved to catch falling blocks in a simulated two-dimensional environment, both \\Phi^{\\max} and the number of concepts increased as a function of how well the animats performed on the task. Although in simpler environments animats with modular feedforward brains can catch blocks just as well, only animats with a high \\Phi^{\\max} evolve to adapt to more complex environments [66] .","title":"05. Explanations"},{"location":"190723_TononiGiulio_2016/#06_predictions","text":"At the most general level, IIT predicts that the PSC in the brain \u2014 that is, the major complex \u2014 must be a maximum of intrinsic cause\u2013effect power, regardless of the particular set of neurons that constitute it ( FIG. 3 ). IIT also predicts that the spatio-temporal grain of the physical elements specifying consciousness is that yielding the maximum \\phi \u03a6 ( FIG. 2 ). Testing these predictions experimentally is challenging but not impossible. During the initial formulation of IIT, a systematic set of experiments was designed to test its specific prediction that consciousness requires both integration and differentiation [67] . An empirical measure, the perturbational complexity index (PCI), which can gauge the intrinsic cause\u2013effect power of the cortex, has been introduced as a practical proxy for \\Phi^{\\max} (REF. [68]). Calculating the PCI involves two steps: perturbing the cerebral cortex using transcranial magnetic stimulation to engage deterministic interactions among distributed groups of cortical neurons (integration) and measuring the incompressibility (algorithmic complexity) of the resulting responses (information). The PCI is high only if brain responses are both integrated and differentiated, corresponding to a distributed spatio-temporal pattern of causal interactions that is complex and hence not very compressible. So far, studies using PCI have confirmed the prediction of IIT that the loss and recovery of consciousness is associated with the breakdown and recovery of the capacity for information integration. This relationship holds true across different states of sleep69 and anaesthesia (using different agents with various mechanisms of action) [70] and in patients with brain damage, at the level of single subjects [68] . Importantly, once PCI is validated in participants that can report on whether they were conscious or not, the index can be used to assess the capacity for information integration in patients who are unresponsive (such as those in a vegetative state) or cannot report (such as newborn infants and non-human species). Another approach to estimating differentiation and integration in practice is to investigate the average properties of neural interactions based on a representative sample of neural states that span many regions of cause\u2013effect space, such as those triggered by a movie sequence [23] . The data from a candidate set of neural elements (for example, functional MRI blood oxygen level-dependent values) can then be analysed using measures of differentiation and integration based on the postulates of IIT [23] . It is also possible to obtain an indication of information capacity from the dynamics of spontaneous activity [26],[71],[72] . Some studies in rats [73] , monkeys [74] and humans [75] have confirmed that the differentiation of blood oxygen level-dependent activity patterns decreases when consciousness is lost. A similar approach can be used to evaluate information matching \u2014 how well the intrinsic cause\u2013effect structures specified by the brain fit the causal structure of the environment 2 ,[23] . Similar approaches could also be used to test the prediction that consciousness should split if a single major complex splits into two or more complexes, and that the split should happen precisely when two maxima of integrated information supplant a single maximum. For example, we could progressively reduce the efficacy of transmission in the callosal fibres by cooling or by the use of optogenetics. IIT predicts that there would be a moment at which, as a result of a minor change in the traffic of neural impulses across the callosum, a single consciousness would suddenly split into two. As discussed earlier, a split from a single major complex into two or more might also be observed in functional blindness (when a patient claims to be blind but may purposefully avoid obstacles) and other dissociative disorders, perhaps even in healthy participants under certain circumstances (such as during autopilot-like driving while having a conversation) ( FIG. 3f ). Turning to the contents of consciousness, the fundamental identity of IIT implies that all qualitative features of experience correspond to features of the conceptual structure specified by the PSC. For example, the organization of experience into distinct modalities (such as sight, hearing and touch) and submodalities (such as colour, shape and motion within the modality of sight) should correspond to the presence, within a conceptual structure, of distinct sets of concepts with extensively overlapping purviews within each set, but much less across sets 2 . IIT further predicts that the binding [56] of phenomenal distinctions, such as seeing a blue book on the piano on the left, should correspond, in the conceptual structure, to an overlap in the purview of the respective concepts (a relation). Also, differences between experiences should correspond to distances among conceptual structures in cause\u2013effect space and dissimilarities among phenomenal distinctions within an experience should correspond to distances between concepts. The refinement of experience that occurs through learning (for example, learning to discriminate the taste of different wines) should be reflected in a refinement of shapes in cause\u2013effect space as a result of the addition and splitting of concepts. IIT also predicts that the spatial structure that characterizes much of our daily experience should be reflected in features of conceptual structures that are specified by connections among neurons arranged in two-dimensional grids. For example, horizontal connections within topographically organized visual areas would be needed to experience visual space from the intrinsic perspective, rather than merely serving to mediate modulatory contextual effects. This also implies that local strengthening or weakening of such horizontal connections in topographic areas should lead to a local distortion of experienced visual space, even though the feedforward mapping of visual inputs from the world remains unchanged. More generally, IIT predicts that changes in the efficacy of the connections among elements of the PSC should lead to changes in experience even when these changes are not accompanied by changes in activity. A counterintuitive consequence of this prediction is that a brain area could contribute to an experience even if it is inactive but not if its connections or neurons are inactivated. Thus topographic visual areas would create visual space even in the absence of spiking activity but not if the horizontal connections within those areas are inactivated. Similarly, if the connections of neurons in colour areas are intact, the neurons would contribute to experience even if they are silent, by specifying negative colour concepts, such as when seeing a picture in black and white. However, if the connections are damaged, they would not specify any colour concepts, as with certain achromatopsic patients who do not even understand that the picture is missing colour [27] ( FIG. 3c ). Similarly, IIT predicts that the cerebral cortex as a whole may support experience even if it is almost silent, a state which may perhaps be reached through meditative practices designed to achieve \u2018naked awareness\u2019 without content76. This contrasts with the common assumption that neurons only contribute to consciousness if they are active and \u2018broadcast\u2019 the information they represent77,78 (Supplementary information S5 (box)). States of naked awareness could be compared with states of unawareness that occur, for example, during deep sleep or anaesthesia, when the cause\u2013effect repertoires of cortical neurons, regardless of the level of neuronal activity, are disrupted as a result of bistability [79] .","title":"06. Predictions"},{"location":"190723_TononiGiulio_2016/#07_conclusions","text":"In summary, IIT is a theory of consciousness that starts from the self-evident, essential properties (axioms) of experience and translates them into the necessary and sufficient conditions (postulates) for the PSC. The axioms are intrinsic existence (my experience exists from my own intrinsic perspective); composition (it has structure), information (it is specific), integration (it is unitary) and exclusion (it is definite). The corresponding postulates state that the physical substrate of an experience must have cause\u2013effect power upon itself (intrinsic existence); its parts must have cause\u2013effect power within the whole (composition); and the cause\u2013effect power of the PSC must be specific (information), irreducible (integration) and maximally so (exclusion). The fundamental identity of IIT states that the quality or content of consciousness is identical to the form of the conceptual structure specified by the PSC, and the quantity or level of consciousness corresponds to its irreducibility (integrated information \\Phi ). The assessment of the identity between experiences and conceptual structures as proposed by IIT is clearly a demanding task, not only experimentally, but also mathematically and computationally. Evaluating maxima of intrinsic cause\u2013effect power systematically requires going through many levels of organization, at multiple temporal scales, in many sets of brain regions, while performing an extraordinary number of perturbations and observations. Hopefully, heuristic approaches will be sufficient to make a strong case that the PSC is constituted of some particular neural elements, timescales and activity states. It will then be essential to test the prediction that any manipulation that affects the PSC at the spatio-temporal grain of maximum intrinsic cause\u2013effect power should affect experience. Conversely, similar manipulations that do not affect the PSC, or that affect it at the wrong spatio-temporal grain, should leave experience unchanged. These and other predictions, especially those that are coun- terintuitive, will also help in assessing the validity of IIT in relation to other proposals about the neural basis of consciousness (Supplementary information S5 (box)). Importantly, the more convincingly IIT can be validated under conditions in which it is relatively easy to assess how consciousness changes, the more it will help to make inferences about consciousness in hard examples, such as brain-damaged patients with residual areas of cortical activity, fetuses, infants, animals and machines. If it is validated, IIT may also prompt a reconsideration of how widespread consciousness is in nature and at what physical scale it may occur 9 . Intriguingly, IIT allows for certain simple systems such as grid-like architectures, similar to topographically organized areas in the human posterior cortex, to be highly conscious even when not engaging in any intelligent behaviour. Conversely, digital computers running complex programs based on a von Neumann architecture would not be conscious, even though they may perform highly intelligent functions and simulate human cognition. IIT offers a principled, empirically testable and clinically useful account of how three pounds of organized, excitable matter support the central fact of our existence \u2014 subjective experience. Time will tell whether this account is anywhere near the mark.","title":"07. Conclusions"},{"location":"190816_OsawaMasachi_2019/","text":"Osawa, Masachi, 2019, History of sociology \u00b6 Contents \u00b6 0. \u793e\u4f1a\u5b66\u306b\u56fa\u6709\u306e\u4e3b\u984c I. \u793e\u4f1a\u5b66\u306e\u8a95\u751f - \u8fd1\u4ee3\u306e\u81ea\u5df1\u610f\u8b58\u3068\u3057\u3066 1. \u53e4\u4ee3\u306e\u793e\u4f1a\u7406\u8ad6\u30a2\u30ea\u30b9\u30c8\u30c6\u30ec\u30b9 \u4eba\u9593\u306f\u653f\u6cbb\u7684\u52d5\u7269\u3067\u3042\u308b \u53cb\u611b\u95a2\u4fc2\u3068\u6575\u5bfe\u95a2\u4fc2 \u5e8f\u5217\u5316\u3055\u308c\u305f\u793e\u4f1a \u307e\u3060\u793e\u4f1a\u5b66\u306f\u306a\u3044 2. \u793e\u4f1a\u5951\u7d04\u306e\u601d\u60f3\u793e\u4f1a\u5b66\u524d\u591c 2.1. \u81ea\u7136\u6cd5\u306e\u7406\u8ad6 \u81ea\u7136\u6cd5 \"\u795e\u304c\u5b58\u5728\u3057\u306a\u3044\u3068\u3057\u3066\u3082\" \u30d1\u30b9\u30ab\u30eb\u306e\u8ced\u3051 2.2. \u30db\u30c3\u30d6\u30ba\u306e\u793e\u4f1a\u5951\u7d04 \"\u4e07\u4eba\u306e\u4e07\u4eba\u306b\u5bfe\u3059\u308b\u95d8\u4e89\" [3. \u793e\u4f1a\u79d1\u5b66\u306e\u8a95\u751f][0103] [4. \u30de\u30eb\u30af\u30b9 - \u5b97\u6559\u3068\u3057\u3066\u306e\u8cc7\u672c\u4e3b\u7fa9][0104] [II. \u793e\u4f1a\u306e\u767a\u898b][02] [1. \u30d5\u30ed\u30a4\u30c8\u7121\u610f\u8b58\u306e\u767a\u898b][0201] [2. \u30c7\u30e5\u30eb\u30b1\u30fc\u30e0\u793e\u4f1a\u306e\u767a\u898b][0202] [3. \u30b8\u30f3\u30e1\u30eb\u76f8\u4e92\u884c\u70ba\u3068\u3057\u3066\u306e\u793e\u4f1a][0203] [4. \u30f4\u30a7\u30fc\u30d0\u30fc\u306e\u5408\u7406\u5316\u306e\u9006\u8aac][0204] [III. \u30b7\u30b9\u30c6\u30e0\u3068\u610f\u5473][03] [1. \u30d1\u30fc\u30bd\u30f3\u30ba\u6a5f\u80fd\u4e3b\u7fa9\u306e\u5b9a\u5f0f\u5316][0301] [2. \u610f\u5473\u306e\u793e\u4f1a\u5b66][0302] [3. \u610f\u5473\u69cb\u6210\u7684\u306a\u30b7\u30b9\u30c6\u30e0\u306e\u7406\u8ad6\u30eb\u30fc\u30de\u30f3\u3068\u30d5\u30fc\u30b3\u30fc][0303] [4. \u793e\u4f1a\u5b66\u306e\u672a\u6765\u306b\u5411\u3051\u3066][0304] [IV. \u304a\u308f\u308a\u306b][04] I. \u793e\u4f1a\u5b66\u306e\u8a95\u751f - \u8fd1\u4ee3\u306e\u81ea\u5df1\u610f\u8b58\u3068\u3057\u3066 \u00b6 01.01. \u53e4\u4ee3\u306e\u793e\u4f1a\u7406\u8ad6\u30a2\u30ea\u30b9\u30c8\u30c6\u30ec\u30b9 \u00b6 z\u014don politikon \u00b6 philia \u00b6 stratification \u00b6 01.01.01.04. \u00b6 01.02. \u793e\u4f1a\u5951\u7d04\u306e\u601d\u60f3\u793e\u4f1a\u5b66\u524d\u591c \u00b6 01.02.01. \u81ea\u7136\u6cd5\u306e\u7406\u8ad6 \u30b0\u30ed\u30c6\u30a3\u30a6\u30b9\u3068\u30d1\u30b9\u30ab\u30eb \u00b6 natural law \u00b6 natural law etsi Deus non daretur \u00b6 Hugo Grotius \" De jure belli ac pacis \" \"etsi Deus non daretur\" \"even when God were assumed not to exist\" Pari de Pascal \u00b6 Blaise Pascal \"Pari de Pascal (Pascal's Gambit)\" 01.02.02. \u30db\u30c3\u30d6\u30ba\u306e\u793e\u4f1a\u5951\u7d04 \u00b6 bellum omnium contra omnes \u00b6 Thomas Hobbes \"bellum omnium contra omnes (the war of all against all)\" \u00b6 img{width: 50%; float: right;}","title":"190816"},{"location":"190816_OsawaMasachi_2019/#contents","text":"0. \u793e\u4f1a\u5b66\u306b\u56fa\u6709\u306e\u4e3b\u984c I. \u793e\u4f1a\u5b66\u306e\u8a95\u751f - \u8fd1\u4ee3\u306e\u81ea\u5df1\u610f\u8b58\u3068\u3057\u3066 1. \u53e4\u4ee3\u306e\u793e\u4f1a\u7406\u8ad6\u30a2\u30ea\u30b9\u30c8\u30c6\u30ec\u30b9 \u4eba\u9593\u306f\u653f\u6cbb\u7684\u52d5\u7269\u3067\u3042\u308b \u53cb\u611b\u95a2\u4fc2\u3068\u6575\u5bfe\u95a2\u4fc2 \u5e8f\u5217\u5316\u3055\u308c\u305f\u793e\u4f1a \u307e\u3060\u793e\u4f1a\u5b66\u306f\u306a\u3044 2. \u793e\u4f1a\u5951\u7d04\u306e\u601d\u60f3\u793e\u4f1a\u5b66\u524d\u591c 2.1. \u81ea\u7136\u6cd5\u306e\u7406\u8ad6 \u81ea\u7136\u6cd5 \"\u795e\u304c\u5b58\u5728\u3057\u306a\u3044\u3068\u3057\u3066\u3082\" \u30d1\u30b9\u30ab\u30eb\u306e\u8ced\u3051 2.2. \u30db\u30c3\u30d6\u30ba\u306e\u793e\u4f1a\u5951\u7d04 \"\u4e07\u4eba\u306e\u4e07\u4eba\u306b\u5bfe\u3059\u308b\u95d8\u4e89\" [3. \u793e\u4f1a\u79d1\u5b66\u306e\u8a95\u751f][0103] [4. \u30de\u30eb\u30af\u30b9 - \u5b97\u6559\u3068\u3057\u3066\u306e\u8cc7\u672c\u4e3b\u7fa9][0104] [II. \u793e\u4f1a\u306e\u767a\u898b][02] [1. \u30d5\u30ed\u30a4\u30c8\u7121\u610f\u8b58\u306e\u767a\u898b][0201] [2. \u30c7\u30e5\u30eb\u30b1\u30fc\u30e0\u793e\u4f1a\u306e\u767a\u898b][0202] [3. \u30b8\u30f3\u30e1\u30eb\u76f8\u4e92\u884c\u70ba\u3068\u3057\u3066\u306e\u793e\u4f1a][0203] [4. \u30f4\u30a7\u30fc\u30d0\u30fc\u306e\u5408\u7406\u5316\u306e\u9006\u8aac][0204] [III. \u30b7\u30b9\u30c6\u30e0\u3068\u610f\u5473][03] [1. \u30d1\u30fc\u30bd\u30f3\u30ba\u6a5f\u80fd\u4e3b\u7fa9\u306e\u5b9a\u5f0f\u5316][0301] [2. \u610f\u5473\u306e\u793e\u4f1a\u5b66][0302] [3. \u610f\u5473\u69cb\u6210\u7684\u306a\u30b7\u30b9\u30c6\u30e0\u306e\u7406\u8ad6\u30eb\u30fc\u30de\u30f3\u3068\u30d5\u30fc\u30b3\u30fc][0303] [4. \u793e\u4f1a\u5b66\u306e\u672a\u6765\u306b\u5411\u3051\u3066][0304] [IV. \u304a\u308f\u308a\u306b][04]","title":"Contents"},{"location":"190816_OsawaMasachi_2019/#i_-","text":"","title":"I. \u793e\u4f1a\u5b66\u306e\u8a95\u751f - \u8fd1\u4ee3\u306e\u81ea\u5df1\u610f\u8b58\u3068\u3057\u3066"},{"location":"190816_OsawaMasachi_2019/#0101","text":"","title":"01.01. \u53e4\u4ee3\u306e\u793e\u4f1a\u7406\u8ad6\u30a2\u30ea\u30b9\u30c8\u30c6\u30ec\u30b9"},{"location":"190816_OsawaMasachi_2019/#zoon_politikon","text":"","title":"z\u014don politikon"},{"location":"190816_OsawaMasachi_2019/#philia","text":"","title":"philia"},{"location":"190816_OsawaMasachi_2019/#stratification","text":"","title":"stratification"},{"location":"190816_OsawaMasachi_2019/#01010104","text":"","title":"01.01.01.04."},{"location":"190816_OsawaMasachi_2019/#0102","text":"","title":"01.02. \u793e\u4f1a\u5951\u7d04\u306e\u601d\u60f3\u793e\u4f1a\u5b66\u524d\u591c"},{"location":"190816_OsawaMasachi_2019/#010201","text":"","title":"01.02.01. \u81ea\u7136\u6cd5\u306e\u7406\u8ad6 \u30b0\u30ed\u30c6\u30a3\u30a6\u30b9\u3068\u30d1\u30b9\u30ab\u30eb"},{"location":"190816_OsawaMasachi_2019/#natural_law","text":"natural law","title":"natural law"},{"location":"190816_OsawaMasachi_2019/#etsi_deus_non_daretur","text":"Hugo Grotius \" De jure belli ac pacis \" \"etsi Deus non daretur\" \"even when God were assumed not to exist\"","title":"etsi Deus non daretur"},{"location":"190816_OsawaMasachi_2019/#pari_de_pascal","text":"Blaise Pascal \"Pari de Pascal (Pascal's Gambit)\"","title":"Pari de Pascal"},{"location":"190816_OsawaMasachi_2019/#010202","text":"","title":"01.02.02. \u30db\u30c3\u30d6\u30ba\u306e\u793e\u4f1a\u5951\u7d04"},{"location":"190816_OsawaMasachi_2019/#bellum_omnium_contra_omnes","text":"Thomas Hobbes \"bellum omnium contra omnes (the war of all against all)\"","title":"bellum omnium contra omnes"},{"location":"190920_SizemoreAnnE_2019/","text":"19-09-20 Sizemore, Ann E., 2019 \u00b6 Sizemore, Ann E., Jennifer E. Phillips-Cremins, Robert Ghrist, and Danielle S. Bassett. \"The importance of the whole: topological data analysis for the network neuroscientist.\" Network Neuroscience 3, no. 3 (2019): 656-673. Original | [Mendeley] ToC \u00b6 00. Abstract 01. Intro Figure 1 02. When should we use topological data analysis? [03. Pieces and parts of the simplicical complex][03] [03.01. Chain groups and boudaries][0301] [03.02. Homological algebra][0302] [04. Homology from complex to complex: persistebt homology][04] [04.01. Filtrations][0401] [04.02. Persistent homology][0402] [04.03. Extracting topological feautres][0403] [05. Conclusion][05] 00. Abstract \u00b6 01. Intro \u00b6 01 \u00b601 \u00b6 algebraic topology 01 \u00b602 \u00b6 neteork topology Figure 1 \u00b6 01 \u00b603 \u00b6 persistent homology 01 \u00b604 \u00b6 01 \u00b605 \u00b6 02. When should we use topological data analysis? \u00b6 02 \u00b601 \u00b6 02 \u00b602 \u00b6 03. Pieces and parts of the simplicical complex \u00b6 03 \u00b601 \u00b6 simplicial complex unit of k + 1 nodes is k -simplex k -simplex nodes \\{v_0, \\cdots, v_k\\} rule: if s is simplex in K and s' \\subseteq s , then s' \\in K k -skeleton ( X^k ): collection of all cimplices w dim at most k k -cycle: looped patterns of k -simplices 03 \u00b602 \u00b6 simplex dist 03 \u00b603 \u00b6 face : subset of a simplex e.g., if \\{v_1, v_2, v_3\\} is 2-simplex, it contains 1-simplex \\{v_1, v_2\\} , where \\{v_i, v_j\\} is face of \\{v_i, v_j, v_k\\} Figure 2 \u00b6 03.01. Chain groups and boudaries \u00b6 03.01 \u00b601 \u00b6 cavities w/i simplicial complexes as akin to bubbles under water 03.01 \u00b602 \u00b6 \\text{#nodes} \\times \\text{#edges} binary mat ( \\partial_1 ) first / zeroth chain group s ( C_1 , C_0 ), 1- / 0-chains 03.01. \u00b603 \u00b6 Figure 3 \u00b6 Figure 4 \u00b6 03.02. Homological algebra \u00b6 04. Homology from complex to complex: persistebt homology \u00b6 04.01. Filtrations \u00b6 Figure 5 \u00b6 04.02. Persistent homology \u00b6 04.03. Extracting topological feautres \u00b6 Figure 6 \u00b6 05. Conclusion \u00b6 \u00b6 img{width: 50%; float: right;}","title":"190920 Sizemore Ann E., 2019"},{"location":"190920_SizemoreAnnE_2019/#toc","text":"00. Abstract 01. Intro Figure 1 02. When should we use topological data analysis? [03. Pieces and parts of the simplicical complex][03] [03.01. Chain groups and boudaries][0301] [03.02. Homological algebra][0302] [04. Homology from complex to complex: persistebt homology][04] [04.01. Filtrations][0401] [04.02. Persistent homology][0402] [04.03. Extracting topological feautres][0403] [05. Conclusion][05]","title":"ToC"},{"location":"190920_SizemoreAnnE_2019/#00_abstract","text":"","title":"00. Abstract"},{"location":"190920_SizemoreAnnE_2019/#01_intro","text":"","title":"01. Intro"},{"location":"190920_SizemoreAnnE_2019/#01_01","text":"algebraic topology","title":"01 \u00b601"},{"location":"190920_SizemoreAnnE_2019/#01_02","text":"neteork topology","title":"01 \u00b602"},{"location":"190920_SizemoreAnnE_2019/#figure_1","text":"","title":"Figure 1"},{"location":"190920_SizemoreAnnE_2019/#01_03","text":"persistent homology","title":"01 \u00b603"},{"location":"190920_SizemoreAnnE_2019/#01_04","text":"","title":"01 \u00b604"},{"location":"190920_SizemoreAnnE_2019/#01_05","text":"","title":"01 \u00b605"},{"location":"190920_SizemoreAnnE_2019/#02_when_should_we_use_topological_data_analysis","text":"","title":"02. When should we use topological data analysis?"},{"location":"190920_SizemoreAnnE_2019/#02_01","text":"","title":"02 \u00b601"},{"location":"190920_SizemoreAnnE_2019/#02_02","text":"","title":"02 \u00b602"},{"location":"190920_SizemoreAnnE_2019/#03_pieces_and_parts_of_the_simplicical_complex","text":"","title":"03. Pieces and parts of the simplicical complex"},{"location":"190920_SizemoreAnnE_2019/#03_01","text":"simplicial complex unit of k + 1 nodes is k -simplex k -simplex nodes \\{v_0, \\cdots, v_k\\} rule: if s is simplex in K and s' \\subseteq s , then s' \\in K k -skeleton ( X^k ): collection of all cimplices w dim at most k k -cycle: looped patterns of k -simplices","title":"03 \u00b601"},{"location":"190920_SizemoreAnnE_2019/#03_02","text":"simplex dist","title":"03 \u00b602"},{"location":"190920_SizemoreAnnE_2019/#03_03","text":"face : subset of a simplex e.g., if \\{v_1, v_2, v_3\\} is 2-simplex, it contains 1-simplex \\{v_1, v_2\\} , where \\{v_i, v_j\\} is face of \\{v_i, v_j, v_k\\}","title":"03 \u00b603"},{"location":"190920_SizemoreAnnE_2019/#figure_2","text":"","title":"Figure 2"},{"location":"190920_SizemoreAnnE_2019/#0301_chain_groups_and_boudaries","text":"","title":"03.01. Chain groups and boudaries"},{"location":"190920_SizemoreAnnE_2019/#0301_01","text":"cavities w/i simplicial complexes as akin to bubbles under water","title":"03.01 \u00b601"},{"location":"190920_SizemoreAnnE_2019/#0301_02","text":"\\text{#nodes} \\times \\text{#edges} binary mat ( \\partial_1 ) first / zeroth chain group s ( C_1 , C_0 ), 1- / 0-chains","title":"03.01 \u00b602"},{"location":"190920_SizemoreAnnE_2019/#0301_03","text":"","title":"03.01. \u00b603"},{"location":"190920_SizemoreAnnE_2019/#figure_3","text":"","title":"Figure 3"},{"location":"190920_SizemoreAnnE_2019/#figure_4","text":"","title":"Figure 4"},{"location":"190920_SizemoreAnnE_2019/#0302_homological_algebra","text":"","title":"03.02. Homological algebra"},{"location":"190920_SizemoreAnnE_2019/#04_homology_from_complex_to_complex_persistebt_homology","text":"","title":"04. Homology from complex to complex: persistebt homology"},{"location":"190920_SizemoreAnnE_2019/#0401_filtrations","text":"","title":"04.01. Filtrations"},{"location":"190920_SizemoreAnnE_2019/#figure_5","text":"","title":"Figure 5"},{"location":"190920_SizemoreAnnE_2019/#0402_persistent_homology","text":"","title":"04.02. Persistent homology"},{"location":"190920_SizemoreAnnE_2019/#0403_extracting_topological_feautres","text":"","title":"04.03. Extracting topological feautres"},{"location":"190920_SizemoreAnnE_2019/#figure_6","text":"","title":"Figure 6"},{"location":"190920_SizemoreAnnE_2019/#05_conclusion","text":"","title":"05. Conclusion"}]}