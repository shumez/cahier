{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cahier \u00b6","title":"Home"},{"location":"160523_okada_y/","text":"16-05-23 Okada, Yukinori \u00b6 HP genome \u3092\u7d10\u89e3\u304f --> genome \u3092\u6d3b\u7528\u3059\u308b GWAS drug repositioning Ref. \u00b6 Genetics of rheumatoid arthritis contributes to biology and drug discovery Japanese Population Structure, Based on SNP Genotypes from 7003 Individuals Compared to Other Ethnic Groups: Effects on Population-Based Association Studies \u7d71\u8a08\u5b66\u304c\u6700\u5f37\u306e\u5b66\u554f\u3067\u3042\u308b genome.gov/GWASstudies/ Genetic studies of body mass index yield new insights for obesity biology Construction of a population-specific HLA imputation reference panel and its application to Graves' disease risk in Japanese New data and an old puzzle: the negative association between schizophrenia and rheumatoid arthritis Lessons learned from the fate of AstraZeneca\u2019s drug pipeline: a five-dimensional framework","title":"160523 Okada, Y."},{"location":"160523_okada_y/#ref","text":"Genetics of rheumatoid arthritis contributes to biology and drug discovery Japanese Population Structure, Based on SNP Genotypes from 7003 Individuals Compared to Other Ethnic Groups: Effects on Population-Based Association Studies \u7d71\u8a08\u5b66\u304c\u6700\u5f37\u306e\u5b66\u554f\u3067\u3042\u308b genome.gov/GWASstudies/ Genetic studies of body mass index yield new insights for obesity biology Construction of a population-specific HLA imputation reference panel and its application to Graves' disease risk in Japanese New data and an old puzzle: the negative association between schizophrenia and rheumatoid arthritis Lessons learned from the fate of AstraZeneca\u2019s drug pipeline: a five-dimensional framework","title":"Ref."},{"location":"160526_osumi_semi/","text":"16-05-26 Osumi Semi \u00b6 Rapid Quantification of Adult and Developing Mouse Spatial Vision Using a Virtual Optomotor System 80% basic \u60c5\u5831\u3092\u8133\u306f\u3069\u3046\u3084\u3063\u3066\u51e6\u7406\u3057\u3066\u3044\u308b\u306e\u304b\uff1f Refractive state and visual acuity in the hooded rat \u7d14\u5316\u3059\u308b\u306b\u306f\u6642\u9593\u304c\u304b\u304b\u308b \u5b66\u7fd2 =>\u3000\u8996\u529b\u6e2c\u5b9a\u306b\u306f\u6642\u9593\u304c\u304b\u304b\u308b \u5b50\u4f9b\u306e\u30de\u30a6\u30b9\u306f\u6e2c\u5b9a\u3067\u304d\u306a\u3044 \u767a\u751f\u306f\u6e2c\u5b9a\u3067\u304d\u306a\u3044 optomotor response\u3000\u8996\u904b\u52d5\u53cd\u5c04 \u3092\u5229\u7528\u3057\u3088\u3046 visual acuity (2\u70b9\u306e\u8b58\u5225) contrast sensitivity ref. 1. Refractive state and visual acuity in the hooded rat \u4eba\u3078\u306e\u5fdc\u7528\u306f \u8996\u529b\u3000 \u57fa\u6e96\u306f\u4e21\u773c\u306e\u9593\u3067\u3044\u3044\u306e\uff1f","title":"160526 Osumi semi"},{"location":"160604_kano_m/","text":"16-06-04 Kano, Michiko \u00b6 Kano, Michiko Ref. 1. An fMRI-Based Neurologic Signature of Physical Pain 2. Multivariate morphological brain signatures predict patients with chronic abdominal pain from healthy control subjects Ref. 1. Resting-state functional MRI in depression unmasks increased connectivity between networks via the dorsal nexus 2. shoaner 2016 cognitive behavioral therapy 3. http://search.proquest.com/openview/c7681dda220035093b9858bbcd7f0095/1?pq-origsite=gscholar","title":"160604 Kano, M."},{"location":"170216_hasegawa_t/","text":"17-02-16 Hasegawa, Takafumi \u00b6 Hasegwa, T Parkinson, James. 1817. \"Essay on Shaking Palsy\" Reselpin Carisson SN Striate DA 20% => Sx 0-3 y: \u30cf\u30cd\u30e0\u30fc\u30f3 3-5 y: Wearing off 5-10 y: Dyskinesia 10-15 y: DA resistance 15-20 y: Hallucination +3-4 y : Death 1: Olf, Med 2: RBD 3-4: Motor Progression inh Marker Prion Hypothesis \u00b6 Oshima. 2016 SSRI Kanno, 2012 RBD \u00b6 Synnucleinopathy 70-80% 12-15y Olf \u00b6 \"Sniffing out dementia\" MIBG HCM ratio Prinon \u00b6 Kikneh, 2016 Sampsom, 2016, Gut microbiota","title":"170216 Hasegawa, T."},{"location":"170216_hasegawa_t/#prion_hypothesis","text":"Oshima. 2016 SSRI Kanno, 2012","title":"Prion Hypothesis"},{"location":"170216_hasegawa_t/#rbd","text":"Synnucleinopathy 70-80% 12-15y","title":"RBD"},{"location":"170216_hasegawa_t/#olf","text":"\"Sniffing out dementia\" MIBG HCM ratio","title":"Olf"},{"location":"170216_hasegawa_t/#prinon","text":"Kikneh, 2016 Sampsom, 2016, Gut microbiota","title":"Prinon"},{"location":"170314_yoshimori_t/","text":"17-03-14 Yoshimori T \u00b6 Yoshimori Lab 1-2% of prot --> lysis 80% of AA --> biosynthesis of protein selective atg 1-14 core atg autophagosome lc3 (atg8) Kabeya, 2000, Embo J from where ? \u00b6 Hayashi, 2009, Nat Cell Biol cradle model er vs. mito contact site mito er ca, lipid Hamasaki, 2013, Nature iii pi3 k xenophagy \u00b6 Nakagawa, 2004, Sci gas selectivity Kageyama, 2011, Mol Biol Cell enodosme Fujita, 2013, J Cell Biol autophagy recognize damaged endosome lysophagy \u00b6 Maejima, 2013, Embo J damaged lysosome Hr-UAemia nephliitis lysosome fusion w/ autophagosome \u00b6 inpp5e Hasegawa, 2016, Embo J Joubert synd Bielas, 2009, Nat Genet cd, pd senda synd viei synd , cancer, joubert synd Fatty liver \u00b6 Tanaka, 2016, Hepato rubicon Matsumaga, 2009, J Cell Biol fatty diet incrased rubicon fatty liver acquired Melanosome \u00b6 Murase, 2013, J Invest Derm QA \u00b6 Virus infexn \u00b6 polio autophagosome PD \u00b6 pink1 park","title":"170314 Yoshimori, T."},{"location":"170314_yoshimori_t/#from_where","text":"Hayashi, 2009, Nat Cell Biol cradle model er vs. mito contact site mito er ca, lipid Hamasaki, 2013, Nature iii pi3 k","title":"from where ?"},{"location":"170314_yoshimori_t/#xenophagy","text":"Nakagawa, 2004, Sci gas selectivity Kageyama, 2011, Mol Biol Cell enodosme Fujita, 2013, J Cell Biol autophagy recognize damaged endosome","title":"xenophagy"},{"location":"170314_yoshimori_t/#lysophagy","text":"Maejima, 2013, Embo J damaged lysosome Hr-UAemia nephliitis","title":"lysophagy"},{"location":"170314_yoshimori_t/#lysosome_fusion_w_autophagosome","text":"inpp5e Hasegawa, 2016, Embo J Joubert synd Bielas, 2009, Nat Genet cd, pd senda synd viei synd , cancer, joubert synd","title":"lysosome fusion w/ autophagosome"},{"location":"170314_yoshimori_t/#fatty_liver","text":"Tanaka, 2016, Hepato rubicon Matsumaga, 2009, J Cell Biol fatty diet incrased rubicon fatty liver acquired","title":"Fatty liver"},{"location":"170314_yoshimori_t/#melanosome","text":"Murase, 2013, J Invest Derm","title":"Melanosome"},{"location":"170314_yoshimori_t/#qa","text":"","title":"QA"},{"location":"170314_yoshimori_t/#virus_infexn","text":"polio autophagosome","title":"Virus infexn"},{"location":"170314_yoshimori_t/#pd","text":"pink1 park","title":"PD"},{"location":"170317_bassett_d_s_2017/","text":"Bassett, D.S. 2017 \u00b6 Bassett, D.S. and Mattar, M.G., 2017. A Network Neuroscience of Human Learning: Potential to Inform Quantitative Theories of Brain and Behavior. Trends in Cognitive Sciences. in Mendeley \u00a71 Learning as a Network Phenomenone \u00b6 \u00b61 \u00b62 a new literature is emerging that focuses on the effects of learning at a coarser level \u2013 between entire brain regions \u00b63 broad-scale changes in neurophysi-ological dynamics across distributed neural circuits or networks When learning produces such distributed network changes\u2013refl ected either in anatomy or function\u2013 it is useful to consider quantitative methods that can not only describe that network architecture but also predict its dynamics \u00b64 In this review... unique approach to describing neural systems in terms of associations in the brain the reconfigurations underlying its adaptive processe how network neuroscience could provide a quantitative framework that complements existing models of learning by cohesively accounting for network structure in neurophysiological and behavioral data \u00a72 Network Neuroscience \u00b6 \u00b61 Network science: a subfield of complex systems science \u00b62 \"network neuroscinece\" emcompass: the use of network science in understanding connectivity patterns the impact of these connectivity patterns on animal behavior describe the architecture of relational data \u00b63 E, V, forming a dyad , dyads can change in their strength over time \u00b64 mesurement of local structure measurement of mesoscale & global structure local : e.g., clustering coeeficient of graph mesoscale : e.g., modularity of G (measures the presence/strength of local clusters of interconnected nodes) \u00b65 G structure can have distincet implications for how the system functions For e.g, Random G : N has equal prob of connecting to any other Ns : transmit info quickly, NOT local info integration Regular G : N connetcts to equal # of neighbor : local info integration, NOT global transmission of info Modular G : evolvabiloty : contain groups of Ns that can change / adapt their func w/o perturbin other groups Hierachial modular network : similat to human brain : specialization of nested func & adaptability \u00b66 functional brain network : fMRI, MEG, EEG : Ns - region/voxel; Es - functional connectivity infer circuits \u00b67 DWI Structural network : WM microstructure : Vs - ; Es - estimated strength of WM tracts \u00a73 Dynamic Networks During Learning \u00b6 \u00b68 Temporal network : each G represents interaction pattern in single time-window : ensemble of Gs - evlution of interaction patterns N in one time-window is connected to iteself in neighboring time-windows using interlayer link traditional stat : adjacency matrix new stat : adjacency tensor \u00b69 Within-scan : functional imaging Across-scan : functiona & structural imaging \u00b610 Base on Hebbian learning, \u0394 in struc & func: driven by patterns of activity and connectivity functional connectivity e.g., 1. detect memory rehabilitation trainning intervention following stroke 2. predict future abilty of individual to learn motor skills 3. words of artificial language \u00b611 motor skill learning network reconfigulation (learning) struc \u0394: func \u0394 : \u00a74 Reconfigulation of Network Modulates During Learning \u00b6 \u00b612 in motor skill learning, network stat \u0394: \u2191 clustering coefficients \u2191 # of network connections \u2191 connection strength \u2193 communication distances * \u0394 in network centrality learning requires \u0394 in modular organization Modular network : contains local clusters of densely interconnected Ns recruitments of & integration between systems is altered \u00b613 reconfigulation are charcteristics of a flexible brain networks organizaition individual \u0394 predicts future learning rate \u00b6 14 modularity plays role in behavioral adaptability \u00b615 modularity is marker of high congnitive func modularity \u221d working memory flexible rearrangement \u221d \u0394 memory acuracy & cognitive flexibility \u00a75 Challenges & Oppotunities \u00b6 \u00b616 ? States / Traits ? depends on the type of learning ? Can predict ? brains flexible ? modulate w/ mood / pharmochological intervention ? modulated by NMDA / NE / other \u00b617 Dynamic Network Drivers of Seizure Generation, Propagation and Termination in Human Neocortical Epilepsy \u00a76 Torward a Network-Based Theory of Learning \u00b6 \u00a77 Conculuding Remarks & Future Directions \u00b6","title":"170317 Bassett, D.S. 2017"},{"location":"170317_bassett_d_s_2017/#1_learning_as_a_network_phenomenone","text":"\u00b61 \u00b62 a new literature is emerging that focuses on the effects of learning at a coarser level \u2013 between entire brain regions \u00b63 broad-scale changes in neurophysi-ological dynamics across distributed neural circuits or networks When learning produces such distributed network changes\u2013refl ected either in anatomy or function\u2013 it is useful to consider quantitative methods that can not only describe that network architecture but also predict its dynamics \u00b64 In this review... unique approach to describing neural systems in terms of associations in the brain the reconfigurations underlying its adaptive processe how network neuroscience could provide a quantitative framework that complements existing models of learning by cohesively accounting for network structure in neurophysiological and behavioral data","title":"\u00a71 Learning as a Network Phenomenone"},{"location":"170317_bassett_d_s_2017/#2_network_neuroscience","text":"\u00b61 Network science: a subfield of complex systems science \u00b62 \"network neuroscinece\" emcompass: the use of network science in understanding connectivity patterns the impact of these connectivity patterns on animal behavior describe the architecture of relational data \u00b63 E, V, forming a dyad , dyads can change in their strength over time \u00b64 mesurement of local structure measurement of mesoscale & global structure local : e.g., clustering coeeficient of graph mesoscale : e.g., modularity of G (measures the presence/strength of local clusters of interconnected nodes) \u00b65 G structure can have distincet implications for how the system functions For e.g, Random G : N has equal prob of connecting to any other Ns : transmit info quickly, NOT local info integration Regular G : N connetcts to equal # of neighbor : local info integration, NOT global transmission of info Modular G : evolvabiloty : contain groups of Ns that can change / adapt their func w/o perturbin other groups Hierachial modular network : similat to human brain : specialization of nested func & adaptability \u00b66 functional brain network : fMRI, MEG, EEG : Ns - region/voxel; Es - functional connectivity infer circuits \u00b67 DWI Structural network : WM microstructure : Vs - ; Es - estimated strength of WM tracts","title":"\u00a72 Network Neuroscience"},{"location":"170317_bassett_d_s_2017/#3_dynamic_networks_during_learning","text":"\u00b68 Temporal network : each G represents interaction pattern in single time-window : ensemble of Gs - evlution of interaction patterns N in one time-window is connected to iteself in neighboring time-windows using interlayer link traditional stat : adjacency matrix new stat : adjacency tensor \u00b69 Within-scan : functional imaging Across-scan : functiona & structural imaging \u00b610 Base on Hebbian learning, \u0394 in struc & func: driven by patterns of activity and connectivity functional connectivity e.g., 1. detect memory rehabilitation trainning intervention following stroke 2. predict future abilty of individual to learn motor skills 3. words of artificial language \u00b611 motor skill learning network reconfigulation (learning) struc \u0394: func \u0394 :","title":"\u00a73 Dynamic Networks During Learning"},{"location":"170317_bassett_d_s_2017/#4_reconfigulation_of_network_modulates_during_learning","text":"\u00b612 in motor skill learning, network stat \u0394: \u2191 clustering coefficients \u2191 # of network connections \u2191 connection strength \u2193 communication distances * \u0394 in network centrality learning requires \u0394 in modular organization Modular network : contains local clusters of densely interconnected Ns recruitments of & integration between systems is altered \u00b613 reconfigulation are charcteristics of a flexible brain networks organizaition individual \u0394 predicts future learning rate \u00b6 14 modularity plays role in behavioral adaptability \u00b615 modularity is marker of high congnitive func modularity \u221d working memory flexible rearrangement \u221d \u0394 memory acuracy & cognitive flexibility","title":"\u00a74 Reconfigulation of Network Modulates During Learning"},{"location":"170317_bassett_d_s_2017/#5_challenges_oppotunities","text":"\u00b616 ? States / Traits ? depends on the type of learning ? Can predict ? brains flexible ? modulate w/ mood / pharmochological intervention ? modulated by NMDA / NE / other \u00b617 Dynamic Network Drivers of Seizure Generation, Propagation and Termination in Human Neocortical Epilepsy","title":"\u00a75 Challenges &amp; Oppotunities"},{"location":"170317_bassett_d_s_2017/#6_torward_a_network-based_theory_of_learning","text":"","title":"\u00a76 Torward a Network-Based Theory of Learning"},{"location":"170317_bassett_d_s_2017/#7_conculuding_remarks_future_directions","text":"","title":"\u00a77 Conculuding Remarks &amp; Future Directions"},{"location":"170319_tni_v88n11/","text":"NeuroImages vol.88 \u00b6 A spiritual visual hallucination from a right parieto-occipital seizure Ictal guardian angel. Wong, V.S. and Kellogg, M.A., 2017. Neurology, 88(11), pp.e101-e102.","title":"170319 TNI v88n11"},{"location":"170329_bassett_2017/","text":"17-03-29 Bassett, D.S. 2017 \u00b6 Bassett, D.S. and Sporns, O., 2017. Network neuroscience. Nature Neuroscience, 20(3), pp.353-364. Mendeley \u00a71 Intro \u00b6 \u00a72 Network mapping and obervation \u00b6 \u00b62 dense reconstructure : \u00a73 Network analysis and modeling \u00b6 \u00a74 Current frontiers \u00b6 \u00a74-1 Network dynamics \u00b6 \u00a74-2 Prediction \u00b6 \u00a74-3 Perturbation, manipulation and control \u00b6 \u00a74-4 Crossing levels \u00b6 \u00a7 5 Conclusion \u00b6 01 01:","title":"170329 Bassett 2017"},{"location":"170329_bassett_2017/#1_intro","text":"","title":"\u00a71 Intro"},{"location":"170329_bassett_2017/#2_network_mapping_and_obervation","text":"\u00b62 dense reconstructure :","title":"\u00a72 Network mapping and obervation"},{"location":"170329_bassett_2017/#3_network_analysis_and_modeling","text":"","title":"\u00a73 Network analysis and modeling"},{"location":"170329_bassett_2017/#4_current_frontiers","text":"","title":"\u00a74 Current frontiers"},{"location":"170329_bassett_2017/#4-1_network_dynamics","text":"","title":"\u00a74-1 Network dynamics"},{"location":"170329_bassett_2017/#4-2_prediction","text":"","title":"\u00a74-2 Prediction"},{"location":"170329_bassett_2017/#4-3_perturbation_manipulation_and_control","text":"","title":"\u00a74-3 Perturbation, manipulation and control"},{"location":"170329_bassett_2017/#4-4_crossing_levels","text":"","title":"\u00a74-4 Crossing levels"},{"location":"170329_bassett_2017/#5_conclusion","text":"01 01:","title":"\u00a7 5 Conclusion"},{"location":"170616_cogmtg_hamamoto/","text":"killen 1996 dissatisfaction fairbuen 1993 relapsing tanja 2014 dankanalis 2016 vooks 2013; mohr 2010 looking own body other rating body katzman 1996 holland 2013 nevonen 2001 EDI2 - control of mind & - body image EBA Extrastriate FBA Fusiform","title":"170616 CogMTG Hamamoto"},{"location":"170704_logan_cross/","text":"17-07-04 Logan Cross \u00b6 evernote Presenter: Logan Cross His Boss: John Doherty About Deep Q-Network \u00b6 Q-network DQN\u00a4\u00ce\u00c9\u00fa\u00a4\u00a4\u00c1\u00a2\u00a4\u00c1\u00a1\u00a1\u00a3\u00ab\u00a1\u00a1Deep Q-Network\u00a4\u00f2Chainer\u00a4\u00c7\u0095\u00f8\u00a4\u00a4\u00a4\u00bf Convolutional neural network \u00b6 Wiki Fig: DQN Fig: Google DeepMind Suzuki Qs: Why not use Representational similarity analysis MVPA ... \u00b6 r01...","title":"170704 Logan Cross"},{"location":"170704_logan_cross/#about_deep_q-network","text":"Q-network DQN\u00a4\u00ce\u00c9\u00fa\u00a4\u00a4\u00c1\u00a2\u00a4\u00c1\u00a1\u00a1\u00a3\u00ab\u00a1\u00a1Deep Q-Network\u00a4\u00f2Chainer\u00a4\u00c7\u0095\u00f8\u00a4\u00a4\u00a4\u00bf","title":"About Deep Q-Network"},{"location":"170704_logan_cross/#convolutional_neural_network","text":"Wiki Fig: DQN Fig: Google DeepMind Suzuki Qs: Why not use Representational similarity analysis MVPA","title":"Convolutional neural network"},{"location":"170704_logan_cross/#_1","text":"r01...","title":"..."},{"location":"171124_NIPS/","text":"17-11-24 NIPS \u00b6 171124_NIPS.m4a Mushiake \u00b6 Sadato \u00b6 Sadato lab .. .. .. Koike 2012 Takahiro Koike Saito 2010 Isoda \u00b6 turn taking .. Isoda 2011 .. Yoshida 2012 mirror or mentalizing Eifuku \u00b6 lab Eifuku 2004 sao 2008 perception identity semantic","title":"171124 NIPS"},{"location":"171124_NIPS/#mushiake","text":"","title":"Mushiake"},{"location":"171124_NIPS/#sadato","text":"Sadato lab .. .. .. Koike 2012 Takahiro Koike Saito 2010","title":"Sadato"},{"location":"171124_NIPS/#isoda","text":"turn taking .. Isoda 2011 .. Yoshida 2012 mirror or mentalizing","title":"Isoda"},{"location":"171124_NIPS/#eifuku","text":"lab Eifuku 2004 sao 2008 perception identity semantic","title":"Eifuku"},{"location":"171125_NIPS/","text":"17-11-25 NIPS \u00b6 Sugiura \u00b6 .. .. Kikuchi, Sugiura Suzuki \u00b6 .. .. .. \u5f62\u614b \u7d71\u5408\u5931\u8a8d \u9023\u5408 \u8272\u5f69 \u8272\u899a\u5931\u8a8d \u8272\u5f69\u5931\u8a8d \u5931\u540d\u8f9e cavian r01...","title":"171125 NIPS"},{"location":"171125_NIPS/#sugiura","text":".. .. Kikuchi, Sugiura","title":"Sugiura"},{"location":"171125_NIPS/#suzuki","text":".. .. .. \u5f62\u614b \u7d71\u5408\u5931\u8a8d \u9023\u5408 \u8272\u5f69 \u8272\u899a\u5931\u8a8d \u8272\u5f69\u5931\u8a8d \u5931\u540d\u8f9e cavian r01...","title":"Suzuki"},{"location":"180220_anzellotti_2018/","text":"18-02-20 Anzellotti, 2018 \u00b6 ![cover] Anzellotti, Stefano , Coutanche, Marc N. , 2018 T1 - Beyond Functional Connectivity: Investigating Networks of Multivariate Representations AU - Anzellotti, Stefano AU - Coutanche, Marc N. JF - Trends in Cognitive Sciences UR - http://dx.doi.org/10.1016/j.tics.2017.12.002 Origin Mendeley Contents \u00b6 1 . Incorporation Multivariate Information into Our Understanding of Brain Networks 2 . From Univariate to Multivariate Signal 2.1 . Multivoxel Pattern Analysis 3 . Moving beyound Univariate Approaches to Connectivity 4 . Modeling Regions' Multivariate Responses 4.1 . Informational Connectivity 4.2 . Multivariate 4.3 . Multivariate 5 . Modeling Interactiona 5.1 . Informational 5.2 . Structural 5.3 . Multivariate Pattern 5.4 . Nonlinear 6 . From Studying 7 . Concluding 1 Incorporation Multivariate Information into Our Understanding of Brain Networks \u00b6 \u00b601 \u00b6 \u00b602 \u00b6 ? what is the relationship between teh information encoded in one brain reginon and the information encoded in another Functional Connectivity -> Multivariate analysis outline: advances / how this information is neglected recent tech significance of the change of perspective 2 From Univariate to Multivariate Signal \u00b6 \u00b603 \u00b6 univariate diff can reflect a wide variety of neural computations 2.1 Multivoxel Pattern Analysis \u00b6 \u00b604 \u00b6 MVPA : ... \u00b605 \u00b6 3 Moving beyond Univariate Approaches to Connectivity \u00b6 \u00b606 \u00b6 traditional FC NOT measure fluctuations in info that is represented in multivox pattern fig1 \u00b6 ![fig1] \u00b607 \u00b6 2 approaches: 1. Informational connectivity 2. MVPA they examine the relationship between patterns of multiple brain regions , such as using the patterns from one region to predict patterns in another region \u00b608 \u00b6 diff multivariate connectivity techs vary in: - how describe multivariate signal - classifier - points in multidim - model regional interaction - correlation - linear regression - nonlinear model 4 Modeling Regions' Multivariate Responses 4.1 Informational Connectivity \u00b6 \u00b609 \u00b6 Informational connectivity : ref.24 4.2 . Multivariate Pattern Dependence \u00b6","title":"180220 Anzellotti 2018"},{"location":"180220_anzellotti_2018/#contents","text":"1 . Incorporation Multivariate Information into Our Understanding of Brain Networks 2 . From Univariate to Multivariate Signal 2.1 . Multivoxel Pattern Analysis 3 . Moving beyound Univariate Approaches to Connectivity 4 . Modeling Regions' Multivariate Responses 4.1 . Informational Connectivity 4.2 . Multivariate 4.3 . Multivariate 5 . Modeling Interactiona 5.1 . Informational 5.2 . Structural 5.3 . Multivariate Pattern 5.4 . Nonlinear 6 . From Studying 7 . Concluding","title":"Contents"},{"location":"180220_anzellotti_2018/#1_incorporation_multivariate_information_into_our_understanding_of_brain_networks","text":"","title":"1 Incorporation Multivariate Information into Our Understanding of Brain Networks"},{"location":"180220_anzellotti_2018/#01","text":"","title":"\u00b601"},{"location":"180220_anzellotti_2018/#02","text":"? what is the relationship between teh information encoded in one brain reginon and the information encoded in another Functional Connectivity -> Multivariate analysis outline: advances / how this information is neglected recent tech significance of the change of perspective","title":"\u00b602"},{"location":"180220_anzellotti_2018/#2_from_univariate_to_multivariate_signal","text":"","title":"2 From Univariate to Multivariate Signal"},{"location":"180220_anzellotti_2018/#03","text":"univariate diff can reflect a wide variety of neural computations","title":"\u00b603"},{"location":"180220_anzellotti_2018/#21_multivoxel_pattern_analysis","text":"","title":"2.1 Multivoxel Pattern Analysis"},{"location":"180220_anzellotti_2018/#04","text":"MVPA : ...","title":"\u00b604"},{"location":"180220_anzellotti_2018/#05","text":"","title":"\u00b605"},{"location":"180220_anzellotti_2018/#3_moving_beyond_univariate_approaches_to_connectivity","text":"","title":"3 Moving beyond Univariate Approaches to Connectivity"},{"location":"180220_anzellotti_2018/#06","text":"traditional FC NOT measure fluctuations in info that is represented in multivox pattern","title":"\u00b606"},{"location":"180220_anzellotti_2018/#fig1","text":"![fig1]","title":"fig1"},{"location":"180220_anzellotti_2018/#07","text":"2 approaches: 1. Informational connectivity 2. MVPA they examine the relationship between patterns of multiple brain regions , such as using the patterns from one region to predict patterns in another region","title":"\u00b607"},{"location":"180220_anzellotti_2018/#08","text":"diff multivariate connectivity techs vary in: - how describe multivariate signal - classifier - points in multidim - model regional interaction - correlation - linear regression - nonlinear model 4 Modeling Regions' Multivariate Responses","title":"\u00b608"},{"location":"180220_anzellotti_2018/#41_informational_connectivity","text":"","title":"4.1 Informational Connectivity"},{"location":"180220_anzellotti_2018/#09","text":"Informational connectivity : ref.24","title":"\u00b609"},{"location":"180220_anzellotti_2018/#42_multivariate_pattern_dependence","text":"","title":"4.2. Multivariate Pattern Dependence"},{"location":"180307_avena-koenigsberger_2017/","text":"2018-03-07 Avena-Koenigsberger A, 2017 \u00b6 AU - Avena-Koenigsberger, Andrea AU - Misic, Bratislav AU - Sporns, Olaf TI - Communication dynamics in complex brain networks JO - Nature Reviews Neuroscience UR - http://dx.doi.org/10.1038/nrn.2017.149 https://www.nature.com/articles/nrn.2017.149 0. Intro \u00b6 1. Conceptual framework \u00b6 Structural Communicational Statistical (Functional) : time-averaged 2. Network topology and communication \u00b6 2.1. Routing communication \u00b6 2.2. Parallel communication \u00b6 2.3. Communication efficienct \u00b6 3. Network dynamics and communication \u00b6 3.1. Flow-based communication models \u00b6 3.2. Integration and segregation through routing and diffusion \u00b6 3.3. Communication process as a function of time \u00b6 4. Network computation and communication \u00b6 4.1. Communication dynamics as effective connectivity \u00b6 4.2. computation by networks \u00b6 Conclusion \u00b6","title":"180307 Avena Koenigsberger 2017"},{"location":"180307_avena-koenigsberger_2017/#0_intro","text":"","title":"0. Intro"},{"location":"180307_avena-koenigsberger_2017/#1_conceptual_framework","text":"Structural Communicational Statistical (Functional) : time-averaged","title":"1. Conceptual framework"},{"location":"180307_avena-koenigsberger_2017/#2_network_topology_and_communication","text":"","title":"2. Network topology and communication"},{"location":"180307_avena-koenigsberger_2017/#21_routing_communication","text":"","title":"2.1. Routing communication"},{"location":"180307_avena-koenigsberger_2017/#22_parallel_communication","text":"","title":"2.2. Parallel communication"},{"location":"180307_avena-koenigsberger_2017/#23_communication_efficienct","text":"","title":"2.3. Communication efficienct"},{"location":"180307_avena-koenigsberger_2017/#3_network_dynamics_and_communication","text":"","title":"3. Network dynamics and communication"},{"location":"180307_avena-koenigsberger_2017/#31_flow-based_communication_models","text":"","title":"3.1. Flow-based communication models"},{"location":"180307_avena-koenigsberger_2017/#32_integration_and_segregation_through_routing_and_diffusion","text":"","title":"3.2. Integration and segregation through routing and diffusion"},{"location":"180307_avena-koenigsberger_2017/#33_communication_process_as_a_function_of_time","text":"","title":"3.3. Communication process as a function of time"},{"location":"180307_avena-koenigsberger_2017/#4_network_computation_and_communication","text":"","title":"4. Network computation and communication"},{"location":"180307_avena-koenigsberger_2017/#41_communication_dynamics_as_effective_connectivity","text":"","title":"4.1. Communication dynamics as effective connectivity"},{"location":"180307_avena-koenigsberger_2017/#42_computation_by_networks","text":"","title":"4.2. computation by networks"},{"location":"180307_avena-koenigsberger_2017/#conclusion","text":"","title":"Conclusion"},{"location":"180326_harari_2011/","text":"2018-03-26 Harari 2011 \u00b6 TY - BOOK T1 - Sapiens: A Brief History of Humankind A1 - Harari, Y.N. SN - 9780062316103 UR - https://books.google.co.jp/books?id=FmyBAwAAQBAJ Y1 - 2015 PB - HarperCollins ER - Sapiens: A Brief History of Humankind Yuval Noah Harari 2011 Table of Contents \u00b6 1 : The cognitive revolution : 1.01 : An animal of no significance 1.02 : The tree of knowledge 1.03 : A day in the life of Adam and Eve 1.04 : The flood 2 : The agricultural revolution : 2.05 : History's biggest fraud 2.06 : Building pyramids 2.07 : Memory overload 2.08 : There is no justice in history 3 : The unification of humankind : 3.09 : The arrow of history 3.10 : The scent of money 3.11 : Imperial visions 3.12 : The law of religion 3.13 : The secret of success 4 : The scientific revolution : 4.14 : The discovery of ignorance 4.15 : The marriage of science and empire 4.16 : The capitalist creed 4.17 : The wheels of industry 4.18 : A permanent revolution 4.19 : And they lived happily ever after 4.20 : The end of Homo sapiens 5 : Afterword: The animal that became a god. 5.21 : 1 The cognitive revolution \u00b6 1.01 An animal of no significance \u00b6 Ann Gibbons, 2007, Food for thought https://scholar.google.co.jp/scholar?hl=en&num=20&as_sdt=0%2C5&q=denisova+genome+interbreeding+melanesia+review&btnG=&oq=denisova+genome+interbreeding+melanesia+revie 1.02 The tree of knowledge \u00b6 circumstance v. Gossip \u00b6 Grooming, Gossip, and the Evolution of Language, Robin Dunbar fiction \u00b6 believe \"as many as six impossible things before breakfast\" Peugeot \u00b6 Chimpanzee Politics: Power and Sex among Apes corporation <- corpus fiction, social construct (\u793e\u4f1a\u7684\u69cb\u6210\u6982\u5ff5), Genome \u00b6 . History & biology \u00b6 prehistory : ... 1.03 A day in the life of Adam and Eve \u00b6 http://science.sciencemag.org/content/sci/316/5831/1558.full.pdf","title":"180326 Harari 2011"},{"location":"180326_harari_2011/#table_of_contents","text":"1 : The cognitive revolution : 1.01 : An animal of no significance 1.02 : The tree of knowledge 1.03 : A day in the life of Adam and Eve 1.04 : The flood 2 : The agricultural revolution : 2.05 : History's biggest fraud 2.06 : Building pyramids 2.07 : Memory overload 2.08 : There is no justice in history 3 : The unification of humankind : 3.09 : The arrow of history 3.10 : The scent of money 3.11 : Imperial visions 3.12 : The law of religion 3.13 : The secret of success 4 : The scientific revolution : 4.14 : The discovery of ignorance 4.15 : The marriage of science and empire 4.16 : The capitalist creed 4.17 : The wheels of industry 4.18 : A permanent revolution 4.19 : And they lived happily ever after 4.20 : The end of Homo sapiens 5 : Afterword: The animal that became a god. 5.21 :","title":"Table of Contents"},{"location":"180326_harari_2011/#1_the_cognitive_revolution","text":"","title":"1 The cognitive revolution"},{"location":"180326_harari_2011/#101_an_animal_of_no_significance","text":"Ann Gibbons, 2007, Food for thought https://scholar.google.co.jp/scholar?hl=en&num=20&as_sdt=0%2C5&q=denisova+genome+interbreeding+melanesia+review&btnG=&oq=denisova+genome+interbreeding+melanesia+revie","title":"1.01 An animal of no significance"},{"location":"180326_harari_2011/#102_the_tree_of_knowledge","text":"","title":"1.02 The tree of knowledge"},{"location":"180326_harari_2011/#circumstance_v_gossip","text":"Grooming, Gossip, and the Evolution of Language, Robin Dunbar","title":"circumstance v. Gossip"},{"location":"180326_harari_2011/#fiction","text":"believe \"as many as six impossible things before breakfast\"","title":"fiction"},{"location":"180326_harari_2011/#peugeot","text":"Chimpanzee Politics: Power and Sex among Apes corporation <- corpus fiction, social construct (\u793e\u4f1a\u7684\u69cb\u6210\u6982\u5ff5),","title":"Peugeot"},{"location":"180326_harari_2011/#genome","text":".","title":"Genome"},{"location":"180326_harari_2011/#history_biology","text":"prehistory : ...","title":"History &amp; biology"},{"location":"180326_harari_2011/#103_a_day_in_the_life_of_adam_and_eve","text":"http://science.sciencemag.org/content/sci/316/5831/1558.full.pdf","title":"1.03 A day in the life of Adam and Eve"},{"location":"180719_betzel2017/","text":"18-07-19 Multi-scale brain networks \u00b6 Betzel, R. F., & Bassett, D. S. (2017). Multi-scale brain networks. Neuroimage, 160, 73-83. @article{betzel2017multi, title = {Multi-scale brain networks}, author = {Betzel, Richard F and Bassett, Danielle S}, journal = {Neuroimage}, volume = {160}, pages = {73--83}, year = {2017}, publisher = {Elsevier} } Original | Mendeley Overview 1 Intro 2 Functional and structural brain networks 3 Multi-scale network analysis 3.1 Multi-scale community structure 3.1.1 Multi-scale community structure in the neuroimaging literature 3.1.2 Implementation and practical considerations 3.1.3 Selecting the resolution parameter 3.1.4 Consensus community structure and communities of interest 3.2 Multi-scale rich club and core\u2013periphery organization 3.3 Multi-scale temporal networks 3.3.1 Multi-scale, multi-layer network analysis 3.3.2 Practical considerations 3.4 Multi-scale spatial networks 4 Conclusion and future directions 1. Intro \u00b6 \"scale\": spatial cell /synapse brain region / large-scale fiber tract temporal sub-millisecond entire lifespan / evolutionary changes across different species topological individual nodes network focus on... networks algorithms (e.g., community detection) multi-scale temporal networks & multi-layer techniques 2. Functional and structural brain networks \u00b6 connectivity: structured / anatomical connectivity (SC) functional (FC) connectivity mat: \\mathbf{A} element: A_{i,j} 3. Multi-scale network analysis \u00b6 Simpson 2016 topological degree Takeuchi 2015 spectrum path length : average # of steps Santernecchi 2014 mesoscale: - community structure Fortunato 2010 - cores & peripheries - rich clubs 3.1 Multi-scale community structure \u00b6 \\begin{align*} \\tag{eq.1} Q = \\sum_{ij} \\big[ A_{ij} - P_{ij} \\big] \\delta(\\sigma_i\\sigma_j) \\end{align*} Q : modularity, i , j : node index A_{ij} : weight of observed connection between nodes i & j P_{ij} : weight of expected connection \\sigma_i \\in [1, \\cdots, K] : communities node i is assigned \\delta(\\sigma_i\\sigma_j) : Kronecker delta fn \\begin{align*} \\tag{app.1} \\delta(\\sigma_i \\sigma_j) = \\begin{cases} 1 \\quad \\text{if arguments are the same}\\\\ 0 \\quad \\text{if NOT} \\end{cases} \\end{align*} \\begin{align*} \\tag{eq.2} Q(\\gamma) = \\sum_{ij} \\big[ A_{ij} - \\gamma P_{ij} \\big] \\delta(\\sigma_i \\sigma_j) \\end{align*} 4. Conclusion and future directions \u00b6 img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; } /*img{width: 50%; float: right;}*/","title":"180719 Betzel 2017"},{"location":"180719_betzel2017/#1_intro","text":"\"scale\": spatial cell /synapse brain region / large-scale fiber tract temporal sub-millisecond entire lifespan / evolutionary changes across different species topological individual nodes network focus on... networks algorithms (e.g., community detection) multi-scale temporal networks & multi-layer techniques","title":"1. Intro"},{"location":"180719_betzel2017/#2_functional_and_structural_brain_networks","text":"connectivity: structured / anatomical connectivity (SC) functional (FC) connectivity mat: \\mathbf{A} element: A_{i,j}","title":"2. Functional and structural brain networks"},{"location":"180719_betzel2017/#3_multi-scale_network_analysis","text":"Simpson 2016 topological degree Takeuchi 2015 spectrum path length : average # of steps Santernecchi 2014 mesoscale: - community structure Fortunato 2010 - cores & peripheries - rich clubs","title":"3. Multi-scale network analysis"},{"location":"180719_betzel2017/#31_multi-scale_community_structure","text":"\\begin{align*} \\tag{eq.1} Q = \\sum_{ij} \\big[ A_{ij} - P_{ij} \\big] \\delta(\\sigma_i\\sigma_j) \\end{align*} Q : modularity, i , j : node index A_{ij} : weight of observed connection between nodes i & j P_{ij} : weight of expected connection \\sigma_i \\in [1, \\cdots, K] : communities node i is assigned \\delta(\\sigma_i\\sigma_j) : Kronecker delta fn \\begin{align*} \\tag{app.1} \\delta(\\sigma_i \\sigma_j) = \\begin{cases} 1 \\quad \\text{if arguments are the same}\\\\ 0 \\quad \\text{if NOT} \\end{cases} \\end{align*} \\begin{align*} \\tag{eq.2} Q(\\gamma) = \\sum_{ij} \\big[ A_{ij} - \\gamma P_{ij} \\big] \\delta(\\sigma_i \\sigma_j) \\end{align*}","title":"3.1 Multi-scale community structure"},{"location":"180719_betzel2017/#4_conclusion_and_future_directions","text":"img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; } /*img{width: 50%; float: right;}*/","title":"4. Conclusion and future directions"},{"location":"180728_schellekens2018/","text":"18-07-28 schellekens2018 \u00b6 Schellekens, W., Petridou, N., & Ramsey, N. F. (2018). Detailed somatotopy in primary motor and somatosensory cortex revealed by Gaussian population receptive fields. NeuroImage, 179, 337-347. @article{schellekens2018detailed, title = \"Detailed somatotopy in primary motor and somatosensory cortex revealed by Gaussian population receptive fields\", journal = \"NeuroImage\", volume = \"179\", pages = \"337 - 347\", year = \"2018\", issn = \"1053-8119\", doi = \"https://doi.org/10.1016/j.neuroimage.2018.06.062\", url = \"http://www.sciencedirect.com/science/article/pii/S105381191830569X\", author = \"Wouter Schellekens and Natalia Petridou and Nick F. Ramsey\", keywords = \"Somatotopy, Motor cortex, Somatosensory cortex, Population receptive fields, Sensorimotor integration, High-field fMRI\" } Original | Mendeley 1. \u00b6 ref img{width: 400px; float: right;}","title":"180728 Schellekens 2018"},{"location":"180805_xia2018/","text":"18-08-05 \u00b6 Linked dimensions of psychopathology and connectivity in functional brain networks Xia, C. H., Ma, Z., Ciric, R., Gu, S., Betzel, R. F., Kaczkurkin, A. N., ... & Cui, Z. (2018). Linked dimensions of psychopathology and connectivity in functional brain networks. Nature Communications, 9(1), 3003. Original | Mendeley | Github Overview \u00b6 Intro Results 2.1 Linked dimensions of psychopathology and connectivity 2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories 2.3 Common and dissociable patterns of connectivity 2.4 Developmental effects and sex differences 2.5 Linked dimensions are replicated in an independent sample Discussion Methods 4.1 Participants 4.2 Psychiatric assessment 4.3 Image acquisition 4.4 Structural preprocessing 4.5 Functional preprocessing 4.6 Network construction 4.7 Dimensionality reduction 4.8 Sparse canonical correlation analysis 4.9 Grid search for regularization parameters 4.10 Permutation testing 4.11 Resampling procedure 4.12 Network module analysis 4.13 Analysis of common connectivity features across dimensions 4.14 Analysis of age effects and sex differences 1. Intro \u00b6 Insel 2015 Bassett 2017 module-specific: Gordon 2014 Bassett 2018 between modules Satterthwaite 2013 \"neurodeve-lopmental connectopathies\" Paus 2005 limitations: 1. case-ctrl approach 2. driven by cov in the clinical symptomatolgy, rather than brain & behavior feat 3. youth studies: small samples Bzdok 2017 sCCA Witten 2009 Drysdale 2017 2. Result \u00b6 2.1 Linked dimensions of psychopathology and connectivity \u00b6 2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories \u00b6 2.3 Common and dissociable patterns of connectivity \u00b6 2.4 Developmental effects and sex differences \u00b6 2.5 Linked dimensions are replicated in an independent sample \u00b6 3. Discussion \u00b6 4. Online methods \u00b6 4.1 Participants \u00b6 4.2 Psychiatric assessment \u00b6 4.3 Image acq \u00b6 4.4 Structural prep \u00b6 4.5 Functional prep \u00b6 magnetic field inhomogeneities 4 initial vol realignmnet intensity outliers demeaning & removal of any linear / quatdric trends 4.6 Network construction \u00b6 4.7 Dimensinality reduction \u00b6 MAD (Median Absolute Deviation) Def: MAD (| \\mathbf{X}_i - \\text{median}(\\mathbf{X})|) 4.8 sCCA \u00b6 \\mathbf{X}_{n \\times p} , \\mathbf{Y}_{n \\times q} : mat n : num of obs (participants) p , q : num of var (feat) \\mathbf{u} , \\mathbf{v} c_1 , c_2 : constant \\begin{align*} \\tag{1} \\text{maximize } \\mathbf{u}^T\\mathbf{X}^T\\mathbf{Yv}, \\\\ \\text{subject to } ||\\mathbf{u}||^2_2 \u2264 1, ||\\mathbf{v}||^2_2 \u2264 1, \\\\ ||\\mathbf{u}||_1 \u2264 c_1, ||\\mathbf{v}||_1 \u2264 c_2 \\end{align*} \\begin{align*} L_1\\text{-norm:} \\quad ||\\mathbf{x}||_1 &:= \\sum_{i=1}^n |x_i| \\\\ L_2\\text{-norm:} \\quad ||\\mathbf{x}||_2 &:= \\sqrt{ \\sum_{i=1}^n{x_i^2}} \\end{align*} 4.9 Grid search for regularization parameters \u00b6 4.10 Permutation testing \u00b6 4.11 Resampling procedure \u00b6 4.12 Network module analysis \u00b6 4.13 Analysis of common connectivity features across dimensions \u00b6 4.14 Analysis of age effects and sex differences \u00b6 img{width: 50%; float: right;}","title":"180805 Xia 2018"},{"location":"180805_xia2018/#overview","text":"Intro Results 2.1 Linked dimensions of psychopathology and connectivity 2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories 2.3 Common and dissociable patterns of connectivity 2.4 Developmental effects and sex differences 2.5 Linked dimensions are replicated in an independent sample Discussion Methods 4.1 Participants 4.2 Psychiatric assessment 4.3 Image acquisition 4.4 Structural preprocessing 4.5 Functional preprocessing 4.6 Network construction 4.7 Dimensionality reduction 4.8 Sparse canonical correlation analysis 4.9 Grid search for regularization parameters 4.10 Permutation testing 4.11 Resampling procedure 4.12 Network module analysis 4.13 Analysis of common connectivity features across dimensions 4.14 Analysis of age effects and sex differences","title":"Overview"},{"location":"180805_xia2018/#1_intro","text":"Insel 2015 Bassett 2017 module-specific: Gordon 2014 Bassett 2018 between modules Satterthwaite 2013 \"neurodeve-lopmental connectopathies\" Paus 2005 limitations: 1. case-ctrl approach 2. driven by cov in the clinical symptomatolgy, rather than brain & behavior feat 3. youth studies: small samples Bzdok 2017 sCCA Witten 2009 Drysdale 2017","title":"1. Intro"},{"location":"180805_xia2018/#2_result","text":"","title":"2. Result"},{"location":"180805_xia2018/#21_linked_dimensions_of_psychopathology_and_connectivity","text":"","title":"2.1 Linked dimensions of psychopathology and connectivity"},{"location":"180805_xia2018/#22_brain-guided_dimensions_of_psychopathology_cross_clinical_diagnostic_categories","text":"","title":"2.2 Brain-guided dimensions of psychopathology cross clinical diagnostic categories"},{"location":"180805_xia2018/#23_common_and_dissociable_patterns_of_connectivity","text":"","title":"2.3 Common and dissociable patterns of connectivity"},{"location":"180805_xia2018/#24_developmental_effects_and_sex_differences","text":"","title":"2.4 Developmental effects and sex differences"},{"location":"180805_xia2018/#25_linked_dimensions_are_replicated_in_an_independent_sample","text":"","title":"2.5 Linked dimensions are replicated in an independent sample"},{"location":"180805_xia2018/#3_discussion","text":"","title":"3. Discussion"},{"location":"180805_xia2018/#4_online_methods","text":"","title":"4. Online methods"},{"location":"180805_xia2018/#41_participants","text":"","title":"4.1 Participants"},{"location":"180805_xia2018/#42_psychiatric_assessment","text":"","title":"4.2 Psychiatric assessment"},{"location":"180805_xia2018/#43_image_acq","text":"","title":"4.3 Image acq"},{"location":"180805_xia2018/#44_structural_prep","text":"","title":"4.4 Structural prep"},{"location":"180805_xia2018/#45_functional_prep","text":"magnetic field inhomogeneities 4 initial vol realignmnet intensity outliers demeaning & removal of any linear / quatdric trends","title":"4.5 Functional prep"},{"location":"180805_xia2018/#46_network_construction","text":"","title":"4.6 Network construction"},{"location":"180805_xia2018/#47_dimensinality_reduction","text":"MAD (Median Absolute Deviation) Def: MAD (| \\mathbf{X}_i - \\text{median}(\\mathbf{X})|)","title":"4.7 Dimensinality reduction"},{"location":"180805_xia2018/#48_scca","text":"\\mathbf{X}_{n \\times p} , \\mathbf{Y}_{n \\times q} : mat n : num of obs (participants) p , q : num of var (feat) \\mathbf{u} , \\mathbf{v} c_1 , c_2 : constant \\begin{align*} \\tag{1} \\text{maximize } \\mathbf{u}^T\\mathbf{X}^T\\mathbf{Yv}, \\\\ \\text{subject to } ||\\mathbf{u}||^2_2 \u2264 1, ||\\mathbf{v}||^2_2 \u2264 1, \\\\ ||\\mathbf{u}||_1 \u2264 c_1, ||\\mathbf{v}||_1 \u2264 c_2 \\end{align*} \\begin{align*} L_1\\text{-norm:} \\quad ||\\mathbf{x}||_1 &:= \\sum_{i=1}^n |x_i| \\\\ L_2\\text{-norm:} \\quad ||\\mathbf{x}||_2 &:= \\sqrt{ \\sum_{i=1}^n{x_i^2}} \\end{align*}","title":"4.8 sCCA"},{"location":"180805_xia2018/#49_grid_search_for_regularization_parameters","text":"","title":"4.9 Grid search for regularization parameters"},{"location":"180805_xia2018/#410_permutation_testing","text":"","title":"4.10 Permutation testing"},{"location":"180805_xia2018/#411_resampling_procedure","text":"","title":"4.11 Resampling procedure"},{"location":"180805_xia2018/#412_network_module_analysis","text":"","title":"4.12 Network module analysis"},{"location":"180805_xia2018/#413_analysis_of_common_connectivity_features_across_dimensions","text":"","title":"4.13 Analysis of common connectivity features across dimensions"},{"location":"180805_xia2018/#414_analysis_of_age_effects_and_sex_differences","text":"img{width: 50%; float: right;}","title":"4.14 Analysis of age effects and sex differences"},{"location":"180904/","text":"18-09-04 \u00b6 Albert L\u00e1szl\u00f3 Barab\u00e1si, Network Science Original 0 Personal Intro \u00b6 1 Intro \u00b6 1. 1 Vulnerability Due to Interconnectivity \u00b6 1. 2 Networks at the Heart of Complex Systems \u00b6 1. 3 Two Forces Helped the Emergence of Network Science \u00b6 The Emergence of Network Maps \u00b6 The Universality of Network Characteristics \u00b6 1. 4 The Characteristics of Network Science \u00b6 Interdisciplinary Nature \u00b6 Empirical, Data Driven Nature \u00b6 Quantitative and Mathematical Nature \u00b6 Computational Nature \u00b6 1. 5 Societal Impact \u00b6 Economic Impact: From Web Search to Social Networking \u00b6 Health: From Drug Design to Metabolic Engineering \u00b6 Security: Fighting Terrorism \u00b6 Epidemics: from Forecasting to Halting Deadly Viruses \u00b6 Neuroscience: Mapping the Brain \u00b6 Management: Uncovering the Internal Structure of an Organization \u00b6 1. 6 Scientific Impact \u00b6 1. 7 Summary \u00b6 1. 8 Homework \u00b6 1. 9 \u00b6 2. Graph Theory \u00b6 3. Random Networks \u00b6 4. The Scale-Free Property \u00b6 5. The Barab\u00e1si-Albert Model \u00b6 6. Evolving Networks \u00b6 7. Degree Correlations \u00b6 8. Network Robustness \u00b6 9. Communities \u00b6 10. Spreading Phenomena \u00b6 ref img.right{ width: 50%; float: right; } img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; }","title":"180904"},{"location":"180904/#0_personal_intro","text":"","title":"0 Personal Intro"},{"location":"180904/#1_intro","text":"","title":"1 Intro"},{"location":"180904/#1_1_vulnerability_due_to_interconnectivity","text":"","title":"1. 1 Vulnerability Due to Interconnectivity"},{"location":"180904/#1_2_networks_at_the_heart_of_complex_systems","text":"","title":"1. 2 Networks at the Heart of Complex Systems"},{"location":"180904/#1_3_two_forces_helped_the_emergence_of_network_science","text":"","title":"1. 3 Two Forces Helped the Emergence of Network Science"},{"location":"180904/#the_emergence_of_network_maps","text":"","title":"The Emergence of Network Maps"},{"location":"180904/#the_universality_of_network_characteristics","text":"","title":"The Universality of Network Characteristics"},{"location":"180904/#1_4_the_characteristics_of_network_science","text":"","title":"1. 4 The Characteristics of Network Science"},{"location":"180904/#interdisciplinary_nature","text":"","title":"Interdisciplinary Nature"},{"location":"180904/#empirical_data_driven_nature","text":"","title":"Empirical, Data Driven Nature"},{"location":"180904/#quantitative_and_mathematical_nature","text":"","title":"Quantitative and Mathematical Nature"},{"location":"180904/#computational_nature","text":"","title":"Computational Nature"},{"location":"180904/#1_5_societal_impact","text":"","title":"1. 5 Societal Impact"},{"location":"180904/#economic_impact_from_web_search_to_social_networking","text":"","title":"Economic Impact: From Web Search to Social Networking"},{"location":"180904/#health_from_drug_design_to_metabolic_engineering","text":"","title":"Health: From Drug Design to Metabolic Engineering"},{"location":"180904/#security_fighting_terrorism","text":"","title":"Security: Fighting Terrorism"},{"location":"180904/#epidemics_from_forecasting_to_halting_deadly_viruses","text":"","title":"Epidemics: from Forecasting to Halting Deadly Viruses"},{"location":"180904/#neuroscience_mapping_the_brain","text":"","title":"Neuroscience: Mapping the Brain"},{"location":"180904/#management_uncovering_the_internal_structure_of_an_organization","text":"","title":"Management: Uncovering the Internal Structure of an Organization"},{"location":"180904/#1_6_scientific_impact","text":"","title":"1. 6 Scientific Impact"},{"location":"180904/#1_7_summary","text":"","title":"1. 7 Summary"},{"location":"180904/#1_8_homework","text":"","title":"1. 8 Homework"},{"location":"180904/#1_9","text":"","title":"1. 9"},{"location":"180904/#2_graph_theory","text":"","title":"2. Graph Theory"},{"location":"180904/#3_random_networks","text":"","title":"3. Random Networks"},{"location":"180904/#4_the_scale-free_property","text":"","title":"4. The Scale-Free Property"},{"location":"180904/#5_the_barabasi-albert_model","text":"","title":"5. The Barab\u00e1si-Albert Model"},{"location":"180904/#6_evolving_networks","text":"","title":"6. Evolving Networks"},{"location":"180904/#7_degree_correlations","text":"","title":"7. Degree Correlations"},{"location":"180904/#8_network_robustness","text":"","title":"8. Network Robustness"},{"location":"180904/#9_communities","text":"","title":"9. Communities"},{"location":"180904/#10_spreading_phenomena","text":"ref img.right{ width: 50%; float: right; } img#cover{ display: block; margin-left: auto; margin-right: auto; width: 50%; }","title":"10. Spreading Phenomena"},{"location":"190311_BassettDS_BullmoreET2017/","text":"19-03-11 Small-World Brain Networks Revisited \u00b6 Bassett, D.S. and Bullmore, E.T., 2017. Small-world brain networks revisited. The Neuroscientist, 23(5), pp.499-516. Original | Mendeley Contents \u00b6 Abstract Small Worlds, Watts and Strogatz Small-World Brain Graphs What Have We (Not) Learnt Since 2006? Universality Economical Small-World Networks Small-Worldness Is Not the Whole Story Challenges to Small-Worldness Binary Graphs Weighted Graphs The Small-World Propensity Twenty-First Century Tract-Tracing Small-Worldness of Binary Tract-Tracing Networks Small-Worldness of Weighted Tract-Tracing Networks Weighted Small-Worldness and the Role of Edge Weights The Utility of Weak Connections Conclusions 0. Abstract \u00b6 It is nearly 20 years since the concept of a small-world network was first quantitatively defined, by a combination of high clustering and short path length; and about 10 years since this metric of complex network topology began to be widely applied to analysis of neuroimaging and other neuroscience data as part of the rapid growth of the new field of connectomics. Here, we review briefly the foundational concepts of graph theoretical estimation and generation of small-world networks. We take stock of some of the key developments in the field in the past decade and we consider in some detail the implications of recent studies using high-resolution tract-tracing methods to map the anatomical networks of the macaque and the mouse. In doing so, we draw attention to the important methodological distinction between topological analysis of binary or unweighted graphs, which have provided a popular but simple approach to brain network analysis in the past, and the topology of weighted graphs, which retain more biologically relevant information and are more appropriate to the increasingly sophisticated data on brain connectivity emerging from contemporary tract-tracing and other imaging studies. We conclude by highlighting some possible future trends in the further development of weighted small-worldness as part of a deeper and broader understanding of the topology and the functional value of the strong and weak links between areas of mammalian cortex. 1. Small Worlds, Watts and Strogatz \u00b6 Small-worldness now seems to be a ubiquitous characteristic of many complex systems; but its first, and still most familiar, appearance was in the form of social networks. We know that as individual agents (nodes) in a social network, we are connected by strong familial and friendship ties (edges) to a relatively few people who are likely also strongly connected to each other, forming a social clique, family or tribe. Yet we also know that we can travel far away from our tribal network, to physically remote cultures and places, and sometimes be surprised there to meet people\u2014often \u201cfriends-of-friends\u201d\u2014who are quite closely connected to our home tribe: \u201cit\u2019s a small world,\u201d we say. This common intuition was experimentally investigated by Milgram (1967) , who asked people in the Midwest of the United States (Omaha, Nebraska) to forward a letter addressed to an unknown individual in Boston by posting it to the friend or acquaintance in their social network that they thought might know someone else who would know the addressee ( Fig. 1 ). It was discovered, on average over multiple trials of this procedure, that the letters successfully reaching Boston had been passed through 6 intermediate postings, which was considered much less than expected given the geographical distance between source and target addresses. In the language of graph theory, the characteristic path length of Milgram\u2019s social networks was short. Figure 1. An illustration of the shortest path between Omaha and Boston in Milgram\u2019s social network experiment, published in Psychology Today in 1967 . Here, the results of multiple experiments are represented as a composite shortest path between the source (a person in Omaha) and the target (a person in Boston). A letter addressed to the target was given to the source, who was asked to send it on (with the same instructions) to the friend or acquaintance that they thought was most likely to know the target, or someone else who might know the target personally. It was found that most letters that eventually reached the correct address in Boston passed through six intermediaries between source and target (denoted 1st remove, 2nd remove, etc.), popularizing the notion that each of us is separated by no more than \u201csix degrees of freedom\u201d from any other individual in a geographically distributed social network. Reproduced with permission from Milgram (1967) . Famously, Watts and Strogatz (1998) combined this concept of path length (the minimum number of edges needed to make a connection between nodes) with a measure of topological clustering or cliquishness of edges between nodes ( Fig. 2 ). More formally, clustering measures the probability that the nodes j and k which are both directly connected to node i are also directly connected to each other; this is equivalent to measuring the proportion of closed triangular three-node motifs in a network ( Sporns and K\u00f6tter 2004 ). Watts and Strogatz (WS) explored the behavior of path length and clustering in a simple generative model (henceforth the WS model ) ( Fig. 3 ). Starting with a binary lattice network of N nodes each connected to the same number of nearest neighbors, by edges of identical weight (unity), the WS model iteratively rewires the lattice by randomly deleting an existing edge, between nodes i and j and replacing it by a new edge between node i and any node k \\neq j . They found that as the probability of random rewiring was incrementally increased from zero, so that the original lattice was progressively randomized, sparsely rewired networks demonstrated both high clustering (like a lattice) and short path length (like a random graph). By analogy to social networks, these algorithmically generated graphs were called small-world networks. Figure 2. Diagrams of clustering and path length in binary and weighted networks. (A) In a binary network, all edges have the same weight, and that is a weight equal to unity. In this example of a binary graph, if one wishes to walk along the shortest path from the orange node to the green node, then one would choose to walk along the edges highlighted in red, rather than along the edges highlighted in blue. We also note that the clustering coefficient of the green node is equal to 1 (all neighbors are also connected to each other to form a closed triangular motif), while the clustering coefficient of the orange node is =1 (only three out of five neighbors are also connected to each other). (B) In a weighted graph, edges can have different weights. In this example, edges have weights of \\frac{3}{3} = 1 , \\frac{2}{3} = 0.66 , and \\frac{1}{3} = 0.33 . If one wishes to traverse the graph from the orange node to the green node along the shortest path, one would choose to follow the path along the edges with weight equal to unity (stronger weights are equivalent to shorter topological distance). Note also that because the edges are now weighted, neither the orange nor the green nodes has a clustering coefficient equal to unity. Figure 3. The Watts\u2013Strogatz model and the generation of small-world networks. The canonical model of a small-world network is that described by Duncan Watts and Steve Strogatz in their 1998 article in Nature. The model begins with a regular lattice network in which each node is placed along the circumference of a circle, and is connected to its k nearest neighbors on that circle. Then, with probability p edges are rewired uniformly at random such that (1) at p = 0 the network is a lattice and (2) at p = 1 the network is random. Interestingly, at intermediate values of p the network has so-called \u201csmall-world\u201d characteristics with significant local clustering (from the lattice model) and short average path length facilitated by the topological short-cuts created during the random rewiring procedure. Because this architecture can be defined mathematically, small-world graphs have proven fundamental in understanding game theory ( Li and Cao 2009 ) and even testing analytical results in subfields of mathematics ( Konishi and Hara 2011 ). Yet, while this work provided a qualitative model of a small-world graph, it did not give a statistic to measure the degree of small-worldness in a particular data set. As a simple scalar measure of \u201csmall-worldness,\u201d Humphries and colleagues defined the small-world index, \u03c3 to be the ratio of the clustering coefficient (normalized by that expected in a random graph) to the average shortest path length (also normalized by that expected in a random graph) ( Humphries and others 2006 ). The intuition here is that this index should be large (in particular, \\sigma > 1 when the clustering coefficient is much greater than expected in the random graph, and the average shortest path length is comparable to that expected in a random graph. Since this initial definition, other extensions have been proposed and utilized ( Telesford and others 2011 ; Toppi and others 2012 ), building on the same general notions. In addition to introducing this generative model, Watts and Strogatz (1998) also showed how small-worldness could be estimated in naturally occurring networks. The hybrid combination of high clustering and short path length that emerged in sparsely rewired WS networks was proposed as a general quantitative measure of small-worldness in other networks. It was shown immediately that a nervous system was among the real-world networks that shared the small-world pattern of topological organization. Using data on the synaptic and gap junction connectivity between all N = 302 neurons in the nervous system of Caenorhabditis elegans ( White and others 1986 ), a binary undirected graph was constructed representing each neuron as an identical node and each synapse (~5000) or gap junction (~600) as an identical, unweighted and undirected edge between nodes. This graph of about 5600 edges between 302 nodes was sparsely connected: only about 12% of the maximum possible number of synaptic connections, \\frac{N^2-N}{2} = 45451 , actually existed. Compared with a random graph of \\frac{N^2-N}{2} = 45451 nodes, C. elegans had high clustering \\Gamma \\sim 5.6 and short path length \\Lambda \\sim 1.18 . Thus the C. elegans connectome was small-world, in the same quantitative sense as the networks generated by the WS model at low rewiring probabilities, less than 10%. But note that does not necessarily mean that the C. elegans connectome was biologically generated by the WS algorithm of random rewiring of established connections (axonal projections) between neurons. To put it another way, the WS model can generate small-world networks but not all small-world networks were generated by a WS model. (And the WS model does not seem like a biologically plausible generative model for brain networks ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]).) Small-World Brain Graphs \u00b6 Following the small-world analysis of C. elegans, pioneering topological studies of mammalian cortical networks used databases of tract-tracing experiments to demonstrate that the cat and macaque interareal anatomical networks shared similar small-world properties of short path length and high clustering ([Hilgetag and Kaiser 2004][2004KaiserM_HilgetagCC]; [Sporns and Zwi 2004][2004ZwiJD_SpornsO]). The first graph theoretical studies of neuroimaging data demonstrated that large-scale interareal networks of functional and structural connectivity in the human brain also had small-world properties ([Bassett and others 2006][2006BullmoreE_BassettDS]; [Salvador and others 2005][2005BullmoreE_SalvadorR]; [Vaessen and others 2010][2010BackesWH_VaessenMJ]). These and other seminal discoveries were central to the emergence of connectomics as a major growth point of network neuroscience ([Sporns and others 2005][2005K\u00f6tterR_SpornsO]). About 10 years ago, we reviewed these and other data in support of the idea that the brain is a small world network ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). Here, we aim to take another look at the concept of small-worldness, one or two decades since it was first formulated quantitatively and applied to brain network analysis at microscopic and macroscopic scales of anatomical resolution. First, we review some of the key questions about small-worldness that have been a focus of work in the period 2006\u20132016; then we review the technical evidence for small-worldness in high resolution tract-tracing data from the macaque and the mouse; finally, we highlight some likely trends in the further evolution of small-worldness as part of a deeper understanding of the topology of weighted brain graphs. 2. What Have We (Not) Learnt Since 2006? \u00b6 We have learnt a lot about complex topological organization of nervous systems since 2006, as evidenced by rapid growth in research articles, reviews, and citations related to \u201cbrain graphs\u201d and \u201cconnectomes\u201d ([Bullmore and Bassett 2011][2011BassettDS_Bullmore]; [Bullmore and Sporns 2009][2009SpornsO_BullmoreE]; [Pessoa 2014][2014PessoaL]); by the publication of several textbooks ([Fornito and others 2016][2016BullmoreET_FornitoA]; Sporns 2011 ); and by the recent launch of new specialist journals for network neuroscience. This emerging field of brain topology has grown much bigger than the foundational concept of small-worldness. But what have we learnt more specifically about brain small-worldness since 2006, and what do we still have to learn? Universality \u00b6 There is no doubt that small-worldness\u2014the combination of non-random clustering with near-random path length\u2014has been very frequently reported across a wide range of neuroscience studies. Small-world topology has been highly replicated across multiple species and scales from structural and functional MRI studies of large-scale brain networks in humans to multielectrode array recordings of cellular networks in cultures ([Bettencourt and others 2007][2007GrossGW_BettencourtLM]) and intact animals ([van den Heuvel and others 2016][2016SpornsO_VanDenHeuvelMP]). It seems reasonable to conclude that small-worldness is at least very common in network neuroscience; but is it a universal property of nervous systems? Universality is a strong claim and difficult to affirm conclusively. As Popper noted in his philosophy of science by hypothetical refutation ([Popper 1963][1963PopperKR]), the universal hypothesis that \u201call swans are white\u201d can only be affirmed conclusively by a complete survey of every swan in the world. Whereas it can be immediately and decisively refuted by the observation of a single black swan. Similarly, the claim that all brains have small-world topology has not yet been (and never will be) affirmed by a complete connectomic mapping of every brain in the world. Some apparent counter-examples of brain networks that do not have small-world topology have been reported and deserve careful consideration as possible Popperian black swans ([see below][]). However, we can provisionally conclude that enough evidence has amassed to judge that small-worldness is a nearly universal property of nervous systems. Indeed, it seems likely that brains are only one of a large \u201cuniversality class\u201d of small-world networks comprising also many other non-neural or non-biological complex systems. Such near-universality of small-worldness, or any other brain network parameter, has a number of implications. First, near-universality implies self-similarity . If the macroscale interareal network of the human brain is small-world, as is the microscale interneuronal network of the worm or the fly, then we should expect also that the microscale interneuronal network of the human brain is small-world. Self-similarity of small-worldness would be indexed by scale invariance of network path length and clustering parameters as the anatomical resolution \u201czooms in\u201d from macro- to microscales. Although there is abundant evidence for scaling, fractal or self-similar statistics in many aspects of brain network topology ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Klimm and others 2014][2014MuchaPJ_KlimmF]), experimental data do not yet exist that could support a multiscale, macro-to-micro analysis of small-worldness (and other network properties) in the same (human or mammalian) nervous system ( Bassett and Siebenhuhner 2013 ). Second, near-universality suggests some very general selection pressures might be operative on the evolution and development of nervous systems across scales and species. This line of thinking has led to the formulation of generative models that can simulate brain networks by some probabilistic growth rule or genetic algorithm. It has been found that simple generative models, that add edges to a network based on the spatial distance and the topological relationships between nodes, can recapitulate small-worldness and many other properties of the connectome on the basis of two (spatial and topological) parameters ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). This serves as a reminder that the network phenotype of small-worldness can be generated by many different mechanisms and the biological mechanisms controlling formation of small-world properties in brain networks currently remain unknown. Third, and from a somewhat more controversial perspective, universality might seem tantamount to triviality . If the brain is everywhere small-world, and so are almost all other complex systems in real life ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Gaiteri and others 2014][2014SibilleE_GaiteriC]; [Moslonka-Lefebvre and others 2011][2011PautassoM_Moslonka-LefebvreM]; [Sizemore and others 2016][2016BassettDS_SizemoreA]) (for a few exceptions, see [Koschutzki and others 2010][2010SchreiberF_KoschutzkiD]), then what is the small-worldness of the brain telling us that\u2019s of any interest specifically to neuroscience? There are two main answers to this important question, as we discuss in more detail below: (1) studies have recently succeeded in linking network topological metrics to biological concepts, like wiring cost ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bassett and others 2011a][2011GraftonST_BassettDS]; [Bullmore and Sporns 2012][2012SpornsO_BullomoreET]; [Rubinov and others 2015][2015BullmoreET_RubinovM]), and to biological phenotypes, like neuronal density ([A\u0107imovi\u0107 and others 2015][2015LinneML_A\u0107imovi\u0107J]; [van den Heuvel and others 2015][2015deReusMA_VanDenHeuvel]) or gene expression ([Fulcher and Fornito 2016][2016FornitoA_FulcherBD]) and (2) small-worldness is not the whole story of brain network organization ([Wang and Kennedy 2016][2016KennedyH_WangXJ]). Economical Small-World Networks \u00b6 At the risk of stating the obvious, small-worldness is a purely topological quantity that tells us nothing about the physical layout of the nodes or edges that constitute the graph ([Bassett and others 2010][2010BassettDS]; [Pessoa 2014][2014PessoaL]). However, it is equally obvious that brain networks are embedded in anatomical space ([Bassett and others 2011a][2011BassettDS]; [Klimm and others 2014][2014KlimmF]; [Lohse and others 2014][2014LohseC]). Somehow the abstract, dimensionless topology of small-worldness must be reconciled to the anatomy of the brain. It turns out that the small-world topology of brain networks is (almost) always economically embedded in physical space ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Kaiser and Hilgetag 2006][2006HilgetagCC_KaiserM]). For both clustering and path length, the two topological metrics combined in the hybrid small-world estimator, there is a strong relationship with brain anatomical space ([Bassett and others 2010][2010BassettDS]; [Bassett and others 2011a][2011BassettDS]; [Rubinov and others 2015][2015RubinovM]). The edges between clustered nodes tend to be shorter distance whereas the edges that mediate topological short cuts tend to traverse longer anatomical distances. Interpreting the Euclidean distance between brain regional nodes or neurons as a proxy for the wiring cost, that is, the total biological cost of building a physical connection and maintaining communication between nodes, it has been argued that the brain is an economical small-world network ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Latora and Marchiori 2001][2001MarchioriM_LatoraV]). Economical in this sense does not simply mean parsimonious or cheap; it is more closely related to the common-sense notion of \u201cvalue for money\u201d. Topologically clustered nodes are anatomically co-located and thereby nearly minimize wiring cost. But small-world brain networks are not naturally lattices and if they are computationally rewired strictly to minimize wiring cost then brain networks are topologically penalized, losing integrative capacity indexed by increased characteristic path length and thus reduced small-worldness scalar \\sigma The economical idea is that brain networks have been selected by the competition between a pressure to minimize biological cost versus a pressure to maximize topological integration. More formally, P_{i,j} \\sim f(d_{i,j})f(k_{i,j}) \\tag{1} , the probability of a connection between nodes i and j , P_{i,j} , is a product of: a function of the physical distance in mm between nodes d_{i,j} \u2014often used as a proxy for wiring cost \u2014and a function of the topological relationship between nodes k_{i,j} . Typically, the functions of cost and topology are each parameterized by a single parameter, for example, simple exponential and power law functions. Several variants of this approach have been published, exploring a range of different topological relationships k_{i,j} between nodes, for example, clustering and homophily ( Betzel and others 2016a ; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). Economical models can generally reproduce the small world properties of brain networks quite realistically: clustering and path length are both increased as a function of the cost parameter (Avena-Koenigsberger and others 2014). In other words, as the cost penalty becomes the dominant factor predicting the probability of a connection between nodes, economical models generate increasingly lattice-like networks, with strong spatial and topological clustering of connected nodes, approximating in the limit the minimal cost configuration of the network. The emergence of more integrative network features\u2014such as hubs mediating many intermodular connections\u2014typically depends on some degree of relaxation of the cost penalty (reduced distance parameter) relative to the parameter controlling the importance of (integrative) topological relationships between nodes in predicting their connectivity. Thus, small-world networks can be generated by economical models for a certain range of the two parameters controlling the competitive factors of (wiring) cost and (topological) value . Small-Worldness Is Not the Whole Story \u00b6 Before getting further into the details of small-worldness, as we do below in relation to recent tract-tracing results, it is important to acknowledge that the specific metrics of path length \\Lambda and clustering \\Gamma introduced by Watts and Strogatz (1998) , and the small-worldness scalar derived from them \\sigma = \\frac{\\Gamma}{\\Lambda} ( Humphries and others 2006 ), are a few global topological metrics that have been of central importance to the growth of complex network science generally. But more than 15 years after the first discovery of small-world properties in brain networks, the field of connectomics now extends into many other areas of topological analysis. There is much important recent work on topological properties like degree distribution and hubness ( Achard and others 2006 ), modularity ([Bassett and others 2011b][2011GraftonST_BassettDS]; [Chen and others 2008][2008EvansAC_ChenZJ]; Mattar and others 2015 ; [Meunier and others 2009][2009BullmoreE_MeunierD]; [Simon 1962][1962SimonH]; Sporns and Betzel 2016 ; [Stoop and others 2013][2013StoopR_StoopR]), core/periphery organization ([Bassett and others 2013][2013GraftonST_BassettDS]; Senden and others 2014 ; [van den Heuvel and Sporns 2011][2011SpornsO_VanDenHeuvelMP]), controllability ([Betzel and others 2016b][2016BassettDS_BetzelRF]; [Gu and others 2015][2015KahnAE_GuS]; Muldoon and others 2016b ) and navigability ( Guly\u00e1s and others 2015 ) that are not simply related to small-worldness. It is nothing like a complete description of the brain to say it is small world; we now turn to a more technical discussion of the evidence for small-worldness as a common property of nervous systems. 3. Challenges to Small-Worldness \u00b6 About 3 to 4 years ago, an important series of papers began to be published that could be regarded as \u201cblack swans\u201d refuting the general importance of small-worldness in an understanding of brain networks ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]; Markov and others 2013 ; [Markov and others 2014][2014SalletJ_MarkovNT]; [Song and others 2014][2014WangXJ_SongHF]): Previous studies of low density inter-areal graphs and apparent small-world properties are challenged by data that reveal high-density cortical graphs in which economy of connections is achieved by weight heterogeneity and distance-weight correlations. ( Markov and others 2013 ) Recent connectomic tract tracing reveals that, contrary to what was previously thought, the cortical inter-areal network has high density . This finding leads to a necessary revision of the relevance of some of the graph theoretical notions, such as the small world property . . ., that have been claimed to characterise the inter-areal cortical network. ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]) These remarks carried weight because they were based on sophisticated and highly sensitive measurements of mammalian cortical connectivity ( Fig. 4 ). In each one of multiple carefully standardized experiments in the macaque monkey, a fluorescent tracer was injected into a (target) cortical region where it was taken up by synaptic terminals and actively transported to the cell bodies of neurons projecting to the target region. When the animal\u2019s brain was subsequently examined microscopically, the retrograde transport of the tracer from the injection site resulted in a fluorescent signal in the (source) regions of cortex that were directly connected to the target region. The basic technology of anatomical tract-tracing had been used by neuroanatomists since the late 20th century; but in the first decades of the 21st century it was possible to increase the scale and precision of the measurements dramatically, enabling the construction of connectivity matrices that summarized the strength or weight of axonal projections between a large number of cortical areas. These next-generation tract-tracing data thus represented a new standard of knowledge about mammalian cortical connectivity, that was more continuously quantified than the binary or ordinal rating of connectivity from traditional tract-tracing experiments ([Stephan and others 2001][2001K\u00f6tterR_StephanKE]), and much less ambiguously related to the cellular substrates of brain networks than the statistical measures of functional connectivity ([Achard and others 2006][2006BullmoreET_AchardS]; [Zhang and others 2016][2016BassettDS_ZhangZ]) and structural covariance ([Alexander-Bloch and others 2013][2013BullmoreET_Alexander-BlochA]; [Bassett and others 2008][2008Meyer-LindenbergA_BassettDS]) used to build graphs from human neuroimaging data. It is clearly important to understand in some detail how the topology of brain networks can be modelled in contemporary tract-tracing data from the macaque (and subsequently the mouse ([Oh and others 2014][2014MortrudMT_OhSW]; [Rubinov and others 2015][2015BullmoreET_RubinovM])) and what these results tell us about the small-worldness of brain networks. Figure 4. High density of the macaque cortical graph excludes sparse small world architecture. (A) Comparison of the average shortest path length and density of the macaque cortical graph from (Markov and others 2013) with the graphs of previous studies (Felleman and Van Essen 1991; Honey and others 2007; Jouve and others 1998; Markov and others 2012; Modha and Singh 2010; Young 1993). Sequential removal of weak connections causes an increase in the path length. Black triangle: macaque cortical graph from Markov and others (2013); gray area: 95% confidence interval following random removal of connections from the macaque cortical graph from Markov and others (2013). Jouve et al., 1998 predicted indicates values of the graph inferred using the published algorithm. (B) Effect of density on Watts and Strogatz\u2019s formalization of a small-world network. Clustering and path length variations generated by edge rewiring with probability range indicated on the x-axis applied to regular lattices of increasingly higher densities. The pie charts show graph density encoded via colors for path length ( L and clustering coefficient (C). The y-axis indicates the path length ratio ( \\frac{L_p}{L_o} ) and clustering ratio ( \\frac{C_p}{C_o} ) of the randomly rewired network, where L_o and C_o are the path length and clustering of the regular lattice, respectively. The variables L_p and C_p are the same quantities measured for the network rewired with probability P . Hence, for each density value indicated in the L and C pie charts, the corresponding \\frac{L_p}{L_o} and \\frac{C_p}{C_o} curves can be identified. Three diagrams below the x-axis indicate the lattice (left), sparsely rewired (middle), and the randomized (right) networks. (C) The small-world coefficient \u03c3 (Humphries and others 2006) corresponding to each lattice rewiring. Color code is the same as in panel (B). Dashed lines in (B) and (C) indicate 42% and 48% density levels, respectively. Reproduced with permission from Markov and others (2013). Binary Graphs \u00b6 In general, a node represents a component of a system and an edge represents a connection or interaction between two nodes. Mathematically, we can capture these ideas with a graph G = (V,E) composed of a node set V and an edge set E ( Bollob\u00e1s 1979 , 1985 ). We store this information in an association or weight matrix W , whose ij th element indicates the strength or weight w_{i,j} of the edge between node i and node j A simple way of building a graph from such an association matrix is to apply a threshold \\tau to each element of the matrix, such that if w_{i,j} \u2265 \\tau then an edge is drawn between the corresponding nodes, but if w_{i,j} < \\tau no edge is drawn ([Achard and others 2006][2006BullmoreET_AchardS]). This thresholding operation thus binarizes the weight matrix and converts the continuously variable edge weights to either 1 (suprathreshold) or 0 (subthreshold). It was on this basis that almost all brain graphs were constructed in the 15 years or so following the seminal small-world analysis of a binary graph representing the cellular connectome of C. elegans ( Watts and Strogatz 1998 ). Most of the neuroimaging evidence for small-worldness in human brain networks, for example, is based on analysis of binary graphs constructed by thresholding a correlation coefficient or equivalent estimator of the weight of functional or structural connectivity or structural covariance between regions i and j ( van Wijk and others 2010 ). It is well recognized that construction of binary graphs represents an extreme simplification of brain networks; indeed, a binary undirected graph of homogenous nodes is as simple as it gets in graph theory ([Bassett and others 2012a][2012LimKO_BassettDS]). However, this approach has historically been preferred in neuroimaging because of limited signal-to-noise ratio in the data ([Achard and others 2006][2006BullmoreET_AchardS]). By varying the threshold \\tau used to construct a binary graph from a continuous weight matrix, the connection density of the network is made denser or sparser. If the threshold is low and many weak weights are added to the graph as edges then the connection density will increase; if the threshold is high and only the strongest weights are represented as edges, then the connection density will decrease. The connection density D is quantified by the number of edges E in the graph as a proportion of the total number of edges in a fully connected network of the same number of nodes N : D=\\frac{E}{\\frac{N^2\u2212N}{2}} Often, this proportion is translated into a percentage. In many neuroimaging studies, the threshold is set to a large value to control for the high levels of noise in MRI data, resulting in connection densities in the range 5% to 30% ( Lynall and others 2010 ). In many of the first generation tract tracing studies, the connectivity data were collected on a binary or ordinal scale, and not all possible connections had been experimentally measured, so these data were naturally modelled as binary graphs with connection densities ~30% a value that was constrained by the completeness and quality of the data ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). The small-world topology of a binary brain graph is defined by estimating two parameters in the data, path length L and clustering C ( Fig. 2A ), and comparing each of these observed parameters to their distributions under a specified null model ( Humphries and others 2006 ). More specifically, L = \\frac{1}{N}\\sum l_{i,j} is the global or characteristic path length, where l_{i,j} is the shortest path (geodesic) between nodes i] and j ; and C = \\frac{1}{N}\\sum c_{i,j} ( c_i := \\frac{L_i}{\\frac{k_i(k_i-1)}{2}} ) is the global clustering coefficient , where c_{i,j} is the number of closed triangular motifs including node i . Each of these parameters is normalized by its value in a binary graph representing the null hypothesis. For example, if the null hypothesis is that clustering of brain networks C_{\\text{brain}} is no different from the clustering of a random graph, then it is reasonable to generate an Erd\u00f6s\u2013Reny\u00ed graph for N nodes and D connection density, measure the clustering coefficient in the random graph C_{\\text{random}}] , and use the ratio between brain and random graph clustering coefficients as a test statistic for non-random clustering. We note that there are many other possible ways in which a null model could be sampled, besides using the classical Erd\u00f6s\u2013Reny\u00ed model, and this is an active area of methodological research ( Muldoon and others 2016a ). However, in general one can define the normalized clustering coefficient as \\Gamma = \\frac{C_{\\text{brain}}}{C_{\\text{random}}} . Likewise, the path length of the brain graph can be normalized by its value in a comparable random graph \\Lambda = \\frac{L_{\\text{brain}}}{L_{\\text{random}}} . A small-worldness scalar can then be simply defined as \\sigma = \\frac{\\Gamma}{\\Lambda} . With these definitions, small-world networks will have \\sigma > 1 , \\Gamma > 1 and \\Lambda \\sim 1 ( Humphries and others 2006 ). Weighted Graphs \u00b6 Although binary graph analysis has predominated to date in analysis of brain networks, this certainly does not represent the methodological limit of graph theory for connectomics. For example, provided the data are of sufficient quality, there is no need to threshold the weight matrix to estimate topological properties like clustering, path length, and small-worldness. Indeed, while the binarization procedure was common in early applications of graph theory to neural data ( van Wijk and others 2010 ), it remains fundamentally agnostic to architectural principles that may be encoded in edge weights ( Rubinov and Sporns 2011 ). This realization has more generally motivated the field to develop methods that remain sensitive to the patterns of weights on the edges ([Ginestet and others 2011][2011SimmonsA_GinestetCE]), and to the topologies present in weak versus strong weights ( Rubinov and Sporns 2011 ). These efforts have included the development of alternative thresholding schemes ([Bassett and others 2012a][2012LimKO_BassettDS]; [Lohse and others 2014][2014CarlsonJM_LohseC]) and fully weighted graph analysis ([Bassett and others 2011b][2011GraftonST_BassettDS]; Rubinov and Sporns 2011 ). The mathematical tools exist to estimate and simulate the topological properties of weighted networks, and analysis of weighted networks is akin to studying the geometry of the graph, rather than simply its topology ([Bassett and others 2012b][2012PorterMA_BassettDS]; [Bassett and others 2013][2013GraftonST_BassettDS]). For example, weighted analogues of binary metrics of clustering, path length and small-worldness can be defined formally ( Fig. 2B ). First, the weighted clustering coefficient of node i can be defined as C_{\\text{weighted}} = \\frac{1}{k_i(k_i\u22121)} \\sum_{j,k}(\\hat{w}_{ij} \\hat{w}_{jk} \\hat{w}_{ik})^{\\frac{1}{3}} , where k_i is the number of edges connected to node i , or degree of node i ([Onnela and others 2005][2005KaskiK_OnnelaJP]) (but see also [Barrat and others 2004][2004VespignaniA_BarratA]; [Zhang and Horvath 2005][2005HorvathS_ZhangB] for other similar definitions). The weighted path length can be defined as L_{\\text{weighted}} = \\frac{1}{N(N\u22121)} \\sum_{i \\neq j} \\delta_{ij} , where the topological distance between two nodes is given by \\delta_{ij} = \\frac{1}{w_{ij}} ([Newman 2001][2001NewmanMEJ]). These two statistics can be combined to construct a weighted metric of small-worldness ([Bola\u00f1os and others 2013][2013AviyenteS_Bola\u00f1osM]): \\sigma_{\\text{weighted}} = \\frac{\\Gamma_{\\text{weighted}}}{\\Lambda_{\\text{weighted}}} . With these definitions, small-world networks will have \\Gamma_{\\text{weighted}} > 1 , \\Lambda_{\\text{weighted}} \\sim 1 , and \\sigma_{\\text{weighted}} > 1 ( Humphries and others 2006 ). The Small-World Propensity \u00b6 There are several important limitations to the definitions of small-worldness described in the previous sections. First, the small-world scalar \\sigma (whether binary or weighted) can be greater than 1 even in cases when the normalized path length is much greater than one; because it is defined as a ratio, if \\gamma \\gg 1 and \\lambda > 1 , the scalar \\sigma > 1 This means that a small-world network will always have \\sigma > 1 , but not all networks with \\sigma > 1 will be small-world (some of them may have greater path length than random graphs). Second, the measure is strongly driven by the density of the graph, and denser networks will naturally have smaller values of \\sigma even if they are in fact generated from an identical small-world model. To address these and other limitations, Muldoon and colleagues recently developed a metric called the small-world propensity . Specifically, the small-world propensity, \\phi , reflects the deviation of a network\u2019s clustering coefficient, C_{\\text{brain}}] , and characteristic path length, L_{\\text{brain}} , from both lattice ( C_{\\text{lattice}} , L_{\\text{lattice}} ) and random ( C_{\\text{random}} , L_{\\text{random}} ) networks constructed with the same number of nodes and the same degree distribution: \\phi = 1 \u2212 \\sqrt{\\frac{\\Delta^2_C+\\Delta^2_L}{2}} , where \\Delta C = \\frac{C_{\\text{lattice}}\u2212C_{\\text{brain}}}{C_{\\text{lattice}}\u2212C_{\\text{random}}} and \\Delta L = \\frac{L_{\\text{brain}}\u2212L_{\\text{random}}}{L_{\\text{lattice}}\u2212L_{\\text{random}}} . The ratio \\Delta_{C/L} represents the fractional deviation of the metric C_{\\text{brain}} or L_{\\text{brain}} ) from its respective null model (a lattice or random network). This quantity can be calculated for binary networks (using binary definitions of clustering and path length) or for weighted networks (using weighted definitions of clustering and path length). Networks are considered small-world if they have small-world propensity 0.4 < \\phi \u2264 1 . However, this metric should be viewed as a continuous metric of small-worldness rather than a hard threshold ( Muldoon and others 2016a ). Importantly, the small-world propensity overcomes several limitations of previous scalar definitions of small-worldness ( Muldoon and others 2016a ). First, it can incorporate weighted estimates of both the clustering coefficient and path-length, thus being generally applicable to any neural data that can be represented as a weighted network. Second, it is density independent, meaning that it can be used to compare the relative small-worldness between two networks that have very different densities from one another. Third, the metric is informed by spatially-constrained null models ([Bassett and others 2015][2015DanielsKE_BassettDS]; [Expert and others 2011][2011LambiotteR_ExpertP]; [Papadopoulus and others 2016][2016BassettDS_Papadopoulus]) in which nodes have physical locations and the edges that correspond to the smallest Euclidean distance between nodes are assigned the highest weights ([Barth\u00e9lemy 2011][2011Barth\u00e9lemyM]) ( Fig. 5 ). Figure 5. Small-world propensity in weighted networks. Here, we illustrate an example of a generative small-world model, and its utility in estimating an empirical network\u2019s small-world propensity. (A) We can extend the concept of a Watts\u2013Strogatz model to weighted graphs by first building a lattice in which the edges are weighted by distance such that edges between spatially neighboring nodes have more strongly weighted than edges between spatially distant nodes. These edge weights can then be rewired with a probability, P, to create a weighted small-world network. (B) Weighted clustering coefficient and weighted path length can be estimated as a function of the rewiring parameter, P, and used to derive the small-world propensity of the graph compared with random and lattice benchmarks (Eq. 11). (C) Weighted small-world propensity calculated for the same network as in panel (B). Error bars represent the standard error of the mean calculated over 50 simulations, and the shaded regions represent the range denoted as small-world. (D) Weighted small-world propensity as a function of network density for a graph of 1000 nodes. Reproduced with permission from Muldoon and others (2016a). 4. Twenty-First Century Tract-Tracing \u00b6 The scale and quality of contemporary tract-tracing data, in both the macaque and the mouse, represents a step change in terms of sensitivity in detecting anatomical connections, or axonal projections, between cortical areas. Using retrograde tracer experiments it has proven possible to demonstrate reliably that pairs of regions in the macaque brain may be connected by one or a few axonal projections. Likewise, anterograde tracer experiments in the mouse have demonstrated that the minimal detectable weight of connectivity between cortical regions, that just exceeds the noise threshold, is equivalent to the projection of one or a few axons (Ypma and Bullmore 2016). This high sensitivity has led immediately to the recognition of a large number of weak and previously unreported axonal connections. In the macaque, it was estimated that 36% of connections identified by contemporary tract tracing were so-called new found projections (NFPs) that had not been described in the prior literature (Markov and others 2014). The existence of so many weak connections is reflected in the log normal distributions of connectivity weight, ranging over five to six orders of magnitude, in both the macaque and the mouse (Ercsey-Ravasz and others 2013; Oh and others 2014). In short, tract-tracing can now resolve connections approximately equivalent to a single axonal projection and approximately a million times weaker than the strongest anatomical connections or white matter tracts. How can we use graph theory to model the network organization of such highly sensitive, highly variable data? Perhaps the simplest approach, borrowing from prior studies of less high quality datasets, is to apply a threshold and convert the log-normal weight matrix into a binary adjacency matrix. If the threshold is defined by the noise distribution of the measurements then it will be very close to zero for these sophisticated experiments, and correspondingly the connection density of the binary graph will be high. In the macaque, the connection density of a binary graph of 29 visual cortical areas was estimated to be 66% (Markov and others 2013), considerably higher than historical estimates in the range of 25% to 45% (Fellemen and Van Essen 1991). In the mouse, the connection density of a binary graph of 308 areas of the whole cortex was estimated to be 53% (Rubinov and others 2015). In other words, the binary graphs generated from 21st century tract-tracing data are about twice as dense as the much sparser networks derived from human neuroimaging and 20th century tract-tracing. They are also considerably denser than brain networks constructed at a finer grained (ultimately cellular) resolution. For example, the connection density of the C elegans nervous system, which is still the only completely mapped synaptic connectome, is about 12%. It is easy to see that the connection density of a binary network depends on the number of neurons comprising each node. In the limit, if the nervous system is parcellated into two large nodes the connection density will certainly be 100%; as the same system is parcellated into a larger number of smaller nodes its connection density will monotonically decrease (Bassett and others 2011a; Zalesky and others 2010). Thus, the current interval estimate of mammalian cortical connection density ~55% to 65% is conditional both on the anatomical resolution of the parcellation scheme used to define the nodes and the sensitivity of the tract-tracing methods used to estimate the weights of the edges. Small-Worldness of Binary Tract-Tracing Networks \u00b6 Having constructed a high-density binary graph from tract-tracing data on mammalian cortex, it is straightforward to estimate its clustering and path length, using the same metrics as for sparser binary graphs. However, simply because there is a larger number of connections in the denser network, its clustering will be considerably higher (there will be more closed triangular motifs) and its path length will be shorter (there will be more direct, pairwise connections) than a sparser network. Indeed, the clustering and path length of any binary graph at 60% connection density will be close to the maximal clustering and minimal path length of a fully connected graph; and therefore the clustering and path length of a 60% dense brain network will be very similar to the clustering and path length of a 60% random network (Bassett and others 2009). This means that when clustering and path length in brain networks are normalized by their corresponding values in equally dense random networks, the scaled metrics \u0393 and \u039b will both be close to 1, and the small-world scalar \u03c3 will be close to its critical value of 1 (Markov and others 2013). For the macaque, at 66% connection density, \u0393=1.21\u00b10.014 , \u039b=1.00\u00b10.000 , and \u03c3=1.21\u00b10.014 ; for the mouse, at 53% connection density, \u0393=1.31\u00b10.004 , \u039b=1.00\u00b10.000 , and \u03c3=1.31\u00b10.004 (all given in mean \u00b1 standard deviation; Fig. 6A and C; Table 1). Since small-worldness has been traditionally defined as \u03c3>1 , these results suggest that dense binary graphs constructed from tract tracing data are small-world, although the macaque is more similar to a random network than the mouse. Figure 6. Binary and weighted small-worldness in mouse and macaque connectomes. For the macaque connectome reported in Markov and others (2013), we show (A) the binary network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (B) we show a weighted network analysis for the same data. For the mouse connectome reported in Rubinov and others (2015), we show (C) the weighted network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (D), we show a binary network analysis for the same data. In the boxplots, the gray dotted line shows the threshold value of \u03c3 = 1, and the purple area shows the range of values of 0.4 < \u03c6 \u2264 1 in which a network is considered small-world. These results do not look like a \u201cblack swan\u201d that refutes universal claims that the brain always embodies small-world network topology. Nor do they undermine the credibility of previous studies demonstrating small-worldness in sparser brain graphs. However, our view is that binary graph models are very unlikely to be an optimal strategy for network analysis of tract-tracing data, because they fail to take account of the extraordinary range of connectivity weights, distributed log normally over 6 orders of magnitude, that has been discovered in mammalian cortical networks (Ercsey-Ravasz and others 2013). The weakest connection between cortical areas is about a million times less weighted than the strongest connection: does it really make sense to set all these weights equivalently to 1 as edges in a binary graph? To ask the question is to answer it. Small-Worldness of Weighted Tract-Tracing Networks \u00b6 A weighted small-world analysis is easily done for these data (Fig. 6B and D). The weighted clustering and weighted path length metrics (Eqs. 8 and 9) are estimated directly from the weight matrices, and the ratio of weighted clustering to weighted path length is the scalar summary of weighted small-worldness \u03c3_{\\text{weighted}}>1 . In Figure 6, we directly compare binary and weighted graph theoretical results for the mouse (Oh and others 2014; Rubinov and others 2015) and macaque (Markov and others 2013) connectomes. Compared with the results of binary graph analysis, both mouse and macaque networks have increased clustering for the weighted graph analysis, and \u03c3 is increased for the macaque (see Table 1). The weighted graph of the mouse connectome is similarly small-world compared to the weighted macaque graph, as measured by \u03c3 , but is significantly more small-world as measured by the small-world propensity \u03d5 . However, classical estimates of small-worldness may depend in a non-trivial way on the density of the graph. This relationship becomes obvious if we estimate the topology of both weighted graphs as a function of connection density (Fig. 7). The classical small-world scalar \u03c3 is greatest when it is estimated for a sparse graph comprising less than 20% to 30% of the most strongly connected edges, and decreases progressively as the graph becomes denser. This might suggest that the macaque connectome seems less small-world than the mouse simply because it is denser. However, the small world propensity \u03d5 has the useful property that it is independent of network density and it is significantly greater, indicating more small-worldness, for the mouse than the macaque. This could be related to differences between the datasets in number of cortical areas and completeness of cortical coverage: the macaque dataset comprises fewer nodes of mostly visual cortex than the larger number of nodes across the whole mouse cortex. Figure 7. Dependence of small-world characteristics on network density. (A) Macaque and (B) mouse connectivity matrices in their natural state (left), as well as after thresholding to retain the 5% strongest (middle) or 25% strongest (right) connections. Weighted small-world metrics including the normalized clustering coefficient (\u0393), normalized path length (\u039b), small-world index (\u03c3), and small-world propensity (\u03c6) as a function of network density for the (C) macaque and (D) mouse connectivity matrices. Weighted Small-Worldness and the Role of Edge Weights \u00b6 Why does a weighted graph analysis provide stronger evidence for non-random clustering than a binary graph analysis applied to the same tract-tracing data? The most strongly weighted connections generally span the shortest physical distances between cortical areas (Ercsey-Ravasz and others 2013; Klimm and others 2014; Rubinov and others 2015). This is not surprising based on what we know about the importance of cost constraints on brain organization (Bassett and others 2009; Bassett and others 2010; Bullmore and Sporns 2012; Fornito and others 2011). Strong connectivity weights indicate a large number of axonal projections, a big bandwidth bundle, perhaps macroscopically visible as a white matter tract. Building and resourcing a high-bandwidth axonal signaling bundle is a significant biological cost that will increase as a function of connection distance: it is parsimonious to wire high bandwidth over short distances. Short distance connections are not only strongly weighted but also topologically clustered. So the strongest weights in both cortical networks define a topologically segregated and anatomically localized organization. A map of the sub-network formed by the strongest weights shows spatial and topological clusters of regions (Fig. 8). In the mouse, the strongly weighted clusters each comprise functionally specialized areas of cortex (visual, motor, etc.) that are known to be densely interconnected and anatomically localized (Rubinov and others 2015; Ypma and Bullmore 2016). Thus it is not surprising that weighting the topological analysis of mammalian cortical networks will provide stronger evidence for non-random clustering than unweighted analysis of binary graphs. Figure 8. The existence of weak links and their topology in the mouse connectome. Here, we show the properties of the 5% weakest and 5% strongest edges of the mouse cortical network. (A, B) Axial view of the mouse cortical network, red dots represent brain regions, blue lines represent the connections between them. Drawn are the (A) 5% weakest or (B) 5% strongest edges. Dot size corresponds to degree, the total number of incoming and outgoing edges connected to a node. In (B), the three nodes with highest degree have been labeled as follows: VISp, primary visual area; MOp, primary motor area; SSs, supplemental somatosensory area. The strong connections are spatially organized, mainly connecting spatially adjacent or contralaterally homologous regions. The weak connections span longer distances and are topologically more random than the strongest connections. (C) The distance distributions for (blue) the 5% weakest edges, (red) the 5% strongest edges, and (black) a random graph of the same size and connection density. (D) The degree distributions for the weakest and strongest connections of the mouse connectome, and a comparable random graph, color-coded as in panel (C). Reproduced with permission from Ypma and Bullmore (2016). The most weakly weighted connections are an area of active, ongoing research (discussed in more detail below) and it is inevitable that there is still much to learn about a feature of network organization\u2014replicable but very weak connections between large cortical areas\u2014that had not been measurable until recent advances in tract-tracing methodology. However, it is clear that weaker connections tend to subtend longer distances, and can be either more topologically random (Ypma and Bullmore 2016) than or similarly topologically organized to strong connections (Bassett and others 2012a). We conclude that graph theoretical analysis of tract-tracing connectomes should respect the quality of the data and use weighted topological metrics to reflect the wide ranging variation in anatomical connectivity, from single fibers to major tracts, that is now measurable in the mammalian brain (Wang and Kennedy 2016). Weighted graph analysis demonstrates clearly that both the macaque and mouse connectomes are small-world networks, as are the human, cat, and nematode brains (Muldoon and others 2016a). Binary graph analysis has usefully measured high connection density, due to the existence of many new anatomical connections, but binarization of these data is not the best way to understand their complex topology and its economical embedding in anatomical space (Bassett and others 2011a; Bassett and others 2012a; Klimm and others 2014; Rubinov and others 2015; Rubinov and Sporns 2011). Future studies will likely also pay more attention to the fact that most tract-tracing markers are axonally transported only in one direction: anterograde or retrograde. This means that the weight matrix could be modelled more completely as a weighted and directed graph, representing a further evolution in the use of graph theoretical methods to capture a richer and biologically more meaningful model of brain network organization than can be provided by binary graphs of unweighted and undirected edges. 5. The Utility of Weak Connections \u00b6 At this juncture, one might naturally ask, \u201cFrom a neuroscientific perspective, do we need techniques that account for edge weights? Do these weights indeed capture information of relevance for cognition and behavior?\u201d Neuroanatomical data suggest that the weights of structural connections may be driven by developmental growth rules (Ercsey-Ravasz and others 2013; Kaiser and Hilgetag 2006; Klimm and others 2014; Lohse and others 2014; Markov and others 2013), energetic and metabolic constraints (Bassett and others 2010), and physical limitations on the volume of neural systems, particularly brains encapsulated by bone (Sherbondy and others 2009). Yet the role of these edge weights in neural computations (Schneidman and others 2006) and higher order cognition has been less well studied. Recent studies have begun to elucidate the role of edge weights\u2014and particularly of weak connections\u2014in human cognition. In resting-state fMRI data, weak functional connections from lateral prefrontal cortex to regions within and outside the frontoparietal network have been shown to display individual differences in strength that predict individual differences in fluid intelligence (Cole and others 2012). The same general relationship was observed in a separate study in which individual differences in moderately weak, long-distance functional connections at rest were strongly correlated with full scale, verbal, and performance IQ (Santarnecchi and others 2014). Neither of these correlations were observed when considering strong connections. Indeed, the utility of weak edges appears to extend to psychiatric illness, where the highly organized topology of weak functional connections\u2014but not strong functional connections\u2014in resting-state fMRI were able to classify people with schizophrenia from healthy controls with high accuracy and specificity (Bassett and others 2012a). Interestingly, individual differences in these weak connections were significantly correlated with individual differences in cognitive scores and symptomatology. Together these results demonstrate that, indeed, methods that are sensitive to the strength (or weakness) of individual connections are imperative for progress to be made in understanding individual differences in cognitive abilities, and their alteration in psychiatric disease. Importantly, the utility of weak connections is not only evident at the large scale in human brains but also at the neuronal scale as measured in non-human species. In an influential article published in 2006 with Bialek and colleagues, Schneidman demonstrated that weak pairwise correlations implied strongly correlated network states in a neural population, suggesting the presence of strong collective behavior (Schneidman and others 2006). This result was initially counterintuitive as one might expect that weak correlations would be associated with the lack of collective behavior. However, the original observation has withstood the test of time, and has been validated in several additional studies including work at the level of tract tracing in macaque monkeys (Goulas and others 2015). Intuitively, the juxtaposition of weak correlations and cohesive, collective behavior is thought to be driven by the underlying sparsity of neuronal interactions (Ganmor and others 2011b), which contain a few non-trivial higher-order interaction terms (Ganmor and others 2011a). Indeed, these higher-order interactions are the topic of some interest both from a computational neuroscience perspective (Giusti and others 2016; Sizemore and others 2016), and from the perspective of neural coding (Giusti and others 2015). But perhaps the claim that weak connections are critically important for our understanding of neural systems should not be particularly surprising. Indeed, it is in fact an old story, first published at the inception of network science. In 1973, Granovetter wrote a seminal paper, titled \u201cThe strength of weak ties,\u201d which highlighted the critical importance of weakly connected components in global system dynamics (Granovetter 1973). Such weak connections are ubiquitous in many systems, from physician interactions (Bridewell and Das 2011) to ecosystem webs (Ulanowicz and others 2014) and atmospheric pathways (Lee and Su 2014). Looking forward, critical open questions lie in how these weak connections drive global dynamics, and how one can intervene in a system to manipulate those processes (Betzel and others 2016b; Gu and others 2015; Muldoon and others 2016b). Acknowledging the role of weak connections, weighted small-world organization plays a critical role in system functions that are particularly relevant to neural systems: including coherence, computation, and control and robustness (Novkovic and others 2016). Perhaps the most commonly studied function afforded by small-world architecture is the ability to transmit information, a characteristic that is common in networks of coupled oscillators (Barahona and Pecora 2002; Hong and others 2002; Nishikawa and others 2003) (although see Atay and others 2006, for a few notable exceptions). This capability supports enhanced computational power (Lago-Fern\u00e1ndez and others 2000), via swift flow and transport (Hwang and others 2010). In dynamic networks, oscillators coupled on small-world networks are much more sensitive to link changes than their random network counterparts (Kohar and others 2014), the time taken to reach synchronization is lowered, and the synchronized state is less stable over time, potentially enabling greater diversity of function. When such a system has both small-world topology and geometry, it directly impacts the network\u2019s ability to speed or slow spreading (Karsai and others 2011), a potentially useful characteristic for resilience to dementia which is thought to be caused by the spread of prions (Raj and others 2012; Raj and others 2015). The value of small-world architecture is not limited to its support of synchronization and information flow. Instead, it also supports a wide range of computations in neural circuits. From early neural network studies, it is clear that the exact topology of connectivity patterns between network elements directly supports trade-offs in the network\u2019s ability to learn new information versus retain old information in memory (Hermundstad and others 2011). When these patterns are organized in a small-world manner, evidence suggests that local computations can be integrated across distributed cell assemblies to support functions as diverse as somatosensation (Zippo and others 2013) and olfaction (Imam and others 2012). The mechanism by which small-worlds support these computations may stem from the fact that their topological structure tends to contain both large cavities and high-dimensional cliques (Sizemore and others 2016), which when embedded in a physical space can strongly constrain the geometric properties of the computation (Giusti and others 2015). While small-world structure can offer non-trivial advantages in terms of both communication and computation, it also directly informs the sorts of interventions that one could use to guide network dynamics and by extension system function. Indeed, computational studies have demonstrated that small-world network architecture requires specific control strategies if one wishes to stem the propagation of seizure activity (Ching and others 2012), control the spread of viruses (Kleczkowski and others 2012), or enhance recovery following injury (H\u00fcbler and Buchman 2008). To gain an intuition for how topology impacts control, we can consider the broad-scale degree distribution also characteristic of brain networks. Based on the Laplacian spectrum, one can observe that weakly connected nodes have the greatest potential to push the system into distant states, far away on an energy landscape (Pasqualetti and others 2014); conversely, strongly connected hubs have the greatest potential to push the system into many local states, nearby on the energy landscape (Gu and others 2015). Thus, control energy (such as that provided by brain stimulation) may be targeted to different locations in a small-world brain network to affect a specific change in brain dynamics (Muldoon and others 2016b). 6. Conclusions \u00b6 Small-worldness remains an important and viable concept in network neuroscience. Nearly 20 years on from the first analysis of the complex topology of a binary graph representing the nervous system of C. elegans, it has been established that small-worldness is a nearly universal and functionally valuable property of nervous systems economically embedded in anatomical space. Recent advances in tract-tracing connectomics do not refute small-worldness; rather they considerably enrich and deepen our understanding of what it means in the brain. The extraordinary precision of contemporary tract tracing, and the important discovery that mammalian cortical networks are denser than expected, mandates the adoption of more sophisticated techniques for weighted graph theoretical modeling of interareal connectomes. On this basis, we expect the next 10 years to yield further insights into the functional value of weak as well as strong connections in brain networks with weighted small-worldness. \u00b6 ref","title":"190311 Bassett, D. S., Bullmore, E.T. 2017"},{"location":"190311_BassettDS_BullmoreET2017/#contents","text":"Abstract Small Worlds, Watts and Strogatz Small-World Brain Graphs What Have We (Not) Learnt Since 2006? Universality Economical Small-World Networks Small-Worldness Is Not the Whole Story Challenges to Small-Worldness Binary Graphs Weighted Graphs The Small-World Propensity Twenty-First Century Tract-Tracing Small-Worldness of Binary Tract-Tracing Networks Small-Worldness of Weighted Tract-Tracing Networks Weighted Small-Worldness and the Role of Edge Weights The Utility of Weak Connections Conclusions","title":"Contents"},{"location":"190311_BassettDS_BullmoreET2017/#0_abstract","text":"It is nearly 20 years since the concept of a small-world network was first quantitatively defined, by a combination of high clustering and short path length; and about 10 years since this metric of complex network topology began to be widely applied to analysis of neuroimaging and other neuroscience data as part of the rapid growth of the new field of connectomics. Here, we review briefly the foundational concepts of graph theoretical estimation and generation of small-world networks. We take stock of some of the key developments in the field in the past decade and we consider in some detail the implications of recent studies using high-resolution tract-tracing methods to map the anatomical networks of the macaque and the mouse. In doing so, we draw attention to the important methodological distinction between topological analysis of binary or unweighted graphs, which have provided a popular but simple approach to brain network analysis in the past, and the topology of weighted graphs, which retain more biologically relevant information and are more appropriate to the increasingly sophisticated data on brain connectivity emerging from contemporary tract-tracing and other imaging studies. We conclude by highlighting some possible future trends in the further development of weighted small-worldness as part of a deeper and broader understanding of the topology and the functional value of the strong and weak links between areas of mammalian cortex.","title":"0. Abstract"},{"location":"190311_BassettDS_BullmoreET2017/#1_small_worlds_watts_and_strogatz","text":"Small-worldness now seems to be a ubiquitous characteristic of many complex systems; but its first, and still most familiar, appearance was in the form of social networks. We know that as individual agents (nodes) in a social network, we are connected by strong familial and friendship ties (edges) to a relatively few people who are likely also strongly connected to each other, forming a social clique, family or tribe. Yet we also know that we can travel far away from our tribal network, to physically remote cultures and places, and sometimes be surprised there to meet people\u2014often \u201cfriends-of-friends\u201d\u2014who are quite closely connected to our home tribe: \u201cit\u2019s a small world,\u201d we say. This common intuition was experimentally investigated by Milgram (1967) , who asked people in the Midwest of the United States (Omaha, Nebraska) to forward a letter addressed to an unknown individual in Boston by posting it to the friend or acquaintance in their social network that they thought might know someone else who would know the addressee ( Fig. 1 ). It was discovered, on average over multiple trials of this procedure, that the letters successfully reaching Boston had been passed through 6 intermediate postings, which was considered much less than expected given the geographical distance between source and target addresses. In the language of graph theory, the characteristic path length of Milgram\u2019s social networks was short. Figure 1. An illustration of the shortest path between Omaha and Boston in Milgram\u2019s social network experiment, published in Psychology Today in 1967 . Here, the results of multiple experiments are represented as a composite shortest path between the source (a person in Omaha) and the target (a person in Boston). A letter addressed to the target was given to the source, who was asked to send it on (with the same instructions) to the friend or acquaintance that they thought was most likely to know the target, or someone else who might know the target personally. It was found that most letters that eventually reached the correct address in Boston passed through six intermediaries between source and target (denoted 1st remove, 2nd remove, etc.), popularizing the notion that each of us is separated by no more than \u201csix degrees of freedom\u201d from any other individual in a geographically distributed social network. Reproduced with permission from Milgram (1967) . Famously, Watts and Strogatz (1998) combined this concept of path length (the minimum number of edges needed to make a connection between nodes) with a measure of topological clustering or cliquishness of edges between nodes ( Fig. 2 ). More formally, clustering measures the probability that the nodes j and k which are both directly connected to node i are also directly connected to each other; this is equivalent to measuring the proportion of closed triangular three-node motifs in a network ( Sporns and K\u00f6tter 2004 ). Watts and Strogatz (WS) explored the behavior of path length and clustering in a simple generative model (henceforth the WS model ) ( Fig. 3 ). Starting with a binary lattice network of N nodes each connected to the same number of nearest neighbors, by edges of identical weight (unity), the WS model iteratively rewires the lattice by randomly deleting an existing edge, between nodes i and j and replacing it by a new edge between node i and any node k \\neq j . They found that as the probability of random rewiring was incrementally increased from zero, so that the original lattice was progressively randomized, sparsely rewired networks demonstrated both high clustering (like a lattice) and short path length (like a random graph). By analogy to social networks, these algorithmically generated graphs were called small-world networks. Figure 2. Diagrams of clustering and path length in binary and weighted networks. (A) In a binary network, all edges have the same weight, and that is a weight equal to unity. In this example of a binary graph, if one wishes to walk along the shortest path from the orange node to the green node, then one would choose to walk along the edges highlighted in red, rather than along the edges highlighted in blue. We also note that the clustering coefficient of the green node is equal to 1 (all neighbors are also connected to each other to form a closed triangular motif), while the clustering coefficient of the orange node is =1 (only three out of five neighbors are also connected to each other). (B) In a weighted graph, edges can have different weights. In this example, edges have weights of \\frac{3}{3} = 1 , \\frac{2}{3} = 0.66 , and \\frac{1}{3} = 0.33 . If one wishes to traverse the graph from the orange node to the green node along the shortest path, one would choose to follow the path along the edges with weight equal to unity (stronger weights are equivalent to shorter topological distance). Note also that because the edges are now weighted, neither the orange nor the green nodes has a clustering coefficient equal to unity. Figure 3. The Watts\u2013Strogatz model and the generation of small-world networks. The canonical model of a small-world network is that described by Duncan Watts and Steve Strogatz in their 1998 article in Nature. The model begins with a regular lattice network in which each node is placed along the circumference of a circle, and is connected to its k nearest neighbors on that circle. Then, with probability p edges are rewired uniformly at random such that (1) at p = 0 the network is a lattice and (2) at p = 1 the network is random. Interestingly, at intermediate values of p the network has so-called \u201csmall-world\u201d characteristics with significant local clustering (from the lattice model) and short average path length facilitated by the topological short-cuts created during the random rewiring procedure. Because this architecture can be defined mathematically, small-world graphs have proven fundamental in understanding game theory ( Li and Cao 2009 ) and even testing analytical results in subfields of mathematics ( Konishi and Hara 2011 ). Yet, while this work provided a qualitative model of a small-world graph, it did not give a statistic to measure the degree of small-worldness in a particular data set. As a simple scalar measure of \u201csmall-worldness,\u201d Humphries and colleagues defined the small-world index, \u03c3 to be the ratio of the clustering coefficient (normalized by that expected in a random graph) to the average shortest path length (also normalized by that expected in a random graph) ( Humphries and others 2006 ). The intuition here is that this index should be large (in particular, \\sigma > 1 when the clustering coefficient is much greater than expected in the random graph, and the average shortest path length is comparable to that expected in a random graph. Since this initial definition, other extensions have been proposed and utilized ( Telesford and others 2011 ; Toppi and others 2012 ), building on the same general notions. In addition to introducing this generative model, Watts and Strogatz (1998) also showed how small-worldness could be estimated in naturally occurring networks. The hybrid combination of high clustering and short path length that emerged in sparsely rewired WS networks was proposed as a general quantitative measure of small-worldness in other networks. It was shown immediately that a nervous system was among the real-world networks that shared the small-world pattern of topological organization. Using data on the synaptic and gap junction connectivity between all N = 302 neurons in the nervous system of Caenorhabditis elegans ( White and others 1986 ), a binary undirected graph was constructed representing each neuron as an identical node and each synapse (~5000) or gap junction (~600) as an identical, unweighted and undirected edge between nodes. This graph of about 5600 edges between 302 nodes was sparsely connected: only about 12% of the maximum possible number of synaptic connections, \\frac{N^2-N}{2} = 45451 , actually existed. Compared with a random graph of \\frac{N^2-N}{2} = 45451 nodes, C. elegans had high clustering \\Gamma \\sim 5.6 and short path length \\Lambda \\sim 1.18 . Thus the C. elegans connectome was small-world, in the same quantitative sense as the networks generated by the WS model at low rewiring probabilities, less than 10%. But note that does not necessarily mean that the C. elegans connectome was biologically generated by the WS algorithm of random rewiring of established connections (axonal projections) between neurons. To put it another way, the WS model can generate small-world networks but not all small-world networks were generated by a WS model. (And the WS model does not seem like a biologically plausible generative model for brain networks ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]).)","title":"1. Small Worlds, Watts and Strogatz"},{"location":"190311_BassettDS_BullmoreET2017/#small-world_brain_graphs","text":"Following the small-world analysis of C. elegans, pioneering topological studies of mammalian cortical networks used databases of tract-tracing experiments to demonstrate that the cat and macaque interareal anatomical networks shared similar small-world properties of short path length and high clustering ([Hilgetag and Kaiser 2004][2004KaiserM_HilgetagCC]; [Sporns and Zwi 2004][2004ZwiJD_SpornsO]). The first graph theoretical studies of neuroimaging data demonstrated that large-scale interareal networks of functional and structural connectivity in the human brain also had small-world properties ([Bassett and others 2006][2006BullmoreE_BassettDS]; [Salvador and others 2005][2005BullmoreE_SalvadorR]; [Vaessen and others 2010][2010BackesWH_VaessenMJ]). These and other seminal discoveries were central to the emergence of connectomics as a major growth point of network neuroscience ([Sporns and others 2005][2005K\u00f6tterR_SpornsO]). About 10 years ago, we reviewed these and other data in support of the idea that the brain is a small world network ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). Here, we aim to take another look at the concept of small-worldness, one or two decades since it was first formulated quantitatively and applied to brain network analysis at microscopic and macroscopic scales of anatomical resolution. First, we review some of the key questions about small-worldness that have been a focus of work in the period 2006\u20132016; then we review the technical evidence for small-worldness in high resolution tract-tracing data from the macaque and the mouse; finally, we highlight some likely trends in the further evolution of small-worldness as part of a deeper understanding of the topology of weighted brain graphs.","title":"Small-World Brain Graphs"},{"location":"190311_BassettDS_BullmoreET2017/#2_what_have_we_not_learnt_since_2006","text":"We have learnt a lot about complex topological organization of nervous systems since 2006, as evidenced by rapid growth in research articles, reviews, and citations related to \u201cbrain graphs\u201d and \u201cconnectomes\u201d ([Bullmore and Bassett 2011][2011BassettDS_Bullmore]; [Bullmore and Sporns 2009][2009SpornsO_BullmoreE]; [Pessoa 2014][2014PessoaL]); by the publication of several textbooks ([Fornito and others 2016][2016BullmoreET_FornitoA]; Sporns 2011 ); and by the recent launch of new specialist journals for network neuroscience. This emerging field of brain topology has grown much bigger than the foundational concept of small-worldness. But what have we learnt more specifically about brain small-worldness since 2006, and what do we still have to learn?","title":"2. What Have We (Not) Learnt Since 2006?"},{"location":"190311_BassettDS_BullmoreET2017/#universality","text":"There is no doubt that small-worldness\u2014the combination of non-random clustering with near-random path length\u2014has been very frequently reported across a wide range of neuroscience studies. Small-world topology has been highly replicated across multiple species and scales from structural and functional MRI studies of large-scale brain networks in humans to multielectrode array recordings of cellular networks in cultures ([Bettencourt and others 2007][2007GrossGW_BettencourtLM]) and intact animals ([van den Heuvel and others 2016][2016SpornsO_VanDenHeuvelMP]). It seems reasonable to conclude that small-worldness is at least very common in network neuroscience; but is it a universal property of nervous systems? Universality is a strong claim and difficult to affirm conclusively. As Popper noted in his philosophy of science by hypothetical refutation ([Popper 1963][1963PopperKR]), the universal hypothesis that \u201call swans are white\u201d can only be affirmed conclusively by a complete survey of every swan in the world. Whereas it can be immediately and decisively refuted by the observation of a single black swan. Similarly, the claim that all brains have small-world topology has not yet been (and never will be) affirmed by a complete connectomic mapping of every brain in the world. Some apparent counter-examples of brain networks that do not have small-world topology have been reported and deserve careful consideration as possible Popperian black swans ([see below][]). However, we can provisionally conclude that enough evidence has amassed to judge that small-worldness is a nearly universal property of nervous systems. Indeed, it seems likely that brains are only one of a large \u201cuniversality class\u201d of small-world networks comprising also many other non-neural or non-biological complex systems. Such near-universality of small-worldness, or any other brain network parameter, has a number of implications. First, near-universality implies self-similarity . If the macroscale interareal network of the human brain is small-world, as is the microscale interneuronal network of the worm or the fly, then we should expect also that the microscale interneuronal network of the human brain is small-world. Self-similarity of small-worldness would be indexed by scale invariance of network path length and clustering parameters as the anatomical resolution \u201czooms in\u201d from macro- to microscales. Although there is abundant evidence for scaling, fractal or self-similar statistics in many aspects of brain network topology ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Klimm and others 2014][2014MuchaPJ_KlimmF]), experimental data do not yet exist that could support a multiscale, macro-to-micro analysis of small-worldness (and other network properties) in the same (human or mammalian) nervous system ( Bassett and Siebenhuhner 2013 ). Second, near-universality suggests some very general selection pressures might be operative on the evolution and development of nervous systems across scales and species. This line of thinking has led to the formulation of generative models that can simulate brain networks by some probabilistic growth rule or genetic algorithm. It has been found that simple generative models, that add edges to a network based on the spatial distance and the topological relationships between nodes, can recapitulate small-worldness and many other properties of the connectome on the basis of two (spatial and topological) parameters ([Betzel and others 2016a][2016VanDenHeuvelM_BetzelRF]; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). This serves as a reminder that the network phenotype of small-worldness can be generated by many different mechanisms and the biological mechanisms controlling formation of small-world properties in brain networks currently remain unknown. Third, and from a somewhat more controversial perspective, universality might seem tantamount to triviality . If the brain is everywhere small-world, and so are almost all other complex systems in real life ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]; [Bullmore and others 2009][2009SucklingJ_BullmoreET]; [Gaiteri and others 2014][2014SibilleE_GaiteriC]; [Moslonka-Lefebvre and others 2011][2011PautassoM_Moslonka-LefebvreM]; [Sizemore and others 2016][2016BassettDS_SizemoreA]) (for a few exceptions, see [Koschutzki and others 2010][2010SchreiberF_KoschutzkiD]), then what is the small-worldness of the brain telling us that\u2019s of any interest specifically to neuroscience? There are two main answers to this important question, as we discuss in more detail below: (1) studies have recently succeeded in linking network topological metrics to biological concepts, like wiring cost ([Bassett and others 2010][2010BullmoreET_BassettDS]; [Bassett and others 2011a][2011GraftonST_BassettDS]; [Bullmore and Sporns 2012][2012SpornsO_BullomoreET]; [Rubinov and others 2015][2015BullmoreET_RubinovM]), and to biological phenotypes, like neuronal density ([A\u0107imovi\u0107 and others 2015][2015LinneML_A\u0107imovi\u0107J]; [van den Heuvel and others 2015][2015deReusMA_VanDenHeuvel]) or gene expression ([Fulcher and Fornito 2016][2016FornitoA_FulcherBD]) and (2) small-worldness is not the whole story of brain network organization ([Wang and Kennedy 2016][2016KennedyH_WangXJ]).","title":"Universality"},{"location":"190311_BassettDS_BullmoreET2017/#economical_small-world_networks","text":"At the risk of stating the obvious, small-worldness is a purely topological quantity that tells us nothing about the physical layout of the nodes or edges that constitute the graph ([Bassett and others 2010][2010BassettDS]; [Pessoa 2014][2014PessoaL]). However, it is equally obvious that brain networks are embedded in anatomical space ([Bassett and others 2011a][2011BassettDS]; [Klimm and others 2014][2014KlimmF]; [Lohse and others 2014][2014LohseC]). Somehow the abstract, dimensionless topology of small-worldness must be reconciled to the anatomy of the brain. It turns out that the small-world topology of brain networks is (almost) always economically embedded in physical space ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Kaiser and Hilgetag 2006][2006HilgetagCC_KaiserM]). For both clustering and path length, the two topological metrics combined in the hybrid small-world estimator, there is a strong relationship with brain anatomical space ([Bassett and others 2010][2010BassettDS]; [Bassett and others 2011a][2011BassettDS]; [Rubinov and others 2015][2015RubinovM]). The edges between clustered nodes tend to be shorter distance whereas the edges that mediate topological short cuts tend to traverse longer anatomical distances. Interpreting the Euclidean distance between brain regional nodes or neurons as a proxy for the wiring cost, that is, the total biological cost of building a physical connection and maintaining communication between nodes, it has been argued that the brain is an economical small-world network ([Bullmore and Sporns 2012][2012SpornsO_BullmoreE]; [Latora and Marchiori 2001][2001MarchioriM_LatoraV]). Economical in this sense does not simply mean parsimonious or cheap; it is more closely related to the common-sense notion of \u201cvalue for money\u201d. Topologically clustered nodes are anatomically co-located and thereby nearly minimize wiring cost. But small-world brain networks are not naturally lattices and if they are computationally rewired strictly to minimize wiring cost then brain networks are topologically penalized, losing integrative capacity indexed by increased characteristic path length and thus reduced small-worldness scalar \\sigma The economical idea is that brain networks have been selected by the competition between a pressure to minimize biological cost versus a pressure to maximize topological integration. More formally, P_{i,j} \\sim f(d_{i,j})f(k_{i,j}) \\tag{1} , the probability of a connection between nodes i and j , P_{i,j} , is a product of: a function of the physical distance in mm between nodes d_{i,j} \u2014often used as a proxy for wiring cost \u2014and a function of the topological relationship between nodes k_{i,j} . Typically, the functions of cost and topology are each parameterized by a single parameter, for example, simple exponential and power law functions. Several variants of this approach have been published, exploring a range of different topological relationships k_{i,j} between nodes, for example, clustering and homophily ( Betzel and others 2016a ; [V\u00e9rtes and others 2012][2012BullmoreET_V\u00e9rtesPE]; [V\u00e9rtes and others 2014][2014BullmoreET_V\u00e9rtesPE]). Economical models can generally reproduce the small world properties of brain networks quite realistically: clustering and path length are both increased as a function of the cost parameter (Avena-Koenigsberger and others 2014). In other words, as the cost penalty becomes the dominant factor predicting the probability of a connection between nodes, economical models generate increasingly lattice-like networks, with strong spatial and topological clustering of connected nodes, approximating in the limit the minimal cost configuration of the network. The emergence of more integrative network features\u2014such as hubs mediating many intermodular connections\u2014typically depends on some degree of relaxation of the cost penalty (reduced distance parameter) relative to the parameter controlling the importance of (integrative) topological relationships between nodes in predicting their connectivity. Thus, small-world networks can be generated by economical models for a certain range of the two parameters controlling the competitive factors of (wiring) cost and (topological) value .","title":"Economical Small-World Networks"},{"location":"190311_BassettDS_BullmoreET2017/#small-worldness_is_not_the_whole_story","text":"Before getting further into the details of small-worldness, as we do below in relation to recent tract-tracing results, it is important to acknowledge that the specific metrics of path length \\Lambda and clustering \\Gamma introduced by Watts and Strogatz (1998) , and the small-worldness scalar derived from them \\sigma = \\frac{\\Gamma}{\\Lambda} ( Humphries and others 2006 ), are a few global topological metrics that have been of central importance to the growth of complex network science generally. But more than 15 years after the first discovery of small-world properties in brain networks, the field of connectomics now extends into many other areas of topological analysis. There is much important recent work on topological properties like degree distribution and hubness ( Achard and others 2006 ), modularity ([Bassett and others 2011b][2011GraftonST_BassettDS]; [Chen and others 2008][2008EvansAC_ChenZJ]; Mattar and others 2015 ; [Meunier and others 2009][2009BullmoreE_MeunierD]; [Simon 1962][1962SimonH]; Sporns and Betzel 2016 ; [Stoop and others 2013][2013StoopR_StoopR]), core/periphery organization ([Bassett and others 2013][2013GraftonST_BassettDS]; Senden and others 2014 ; [van den Heuvel and Sporns 2011][2011SpornsO_VanDenHeuvelMP]), controllability ([Betzel and others 2016b][2016BassettDS_BetzelRF]; [Gu and others 2015][2015KahnAE_GuS]; Muldoon and others 2016b ) and navigability ( Guly\u00e1s and others 2015 ) that are not simply related to small-worldness. It is nothing like a complete description of the brain to say it is small world; we now turn to a more technical discussion of the evidence for small-worldness as a common property of nervous systems.","title":"Small-Worldness Is Not the Whole Story"},{"location":"190311_BassettDS_BullmoreET2017/#3_challenges_to_small-worldness","text":"About 3 to 4 years ago, an important series of papers began to be published that could be regarded as \u201cblack swans\u201d refuting the general importance of small-worldness in an understanding of brain networks ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]; Markov and others 2013 ; [Markov and others 2014][2014SalletJ_MarkovNT]; [Song and others 2014][2014WangXJ_SongHF]): Previous studies of low density inter-areal graphs and apparent small-world properties are challenged by data that reveal high-density cortical graphs in which economy of connections is achieved by weight heterogeneity and distance-weight correlations. ( Markov and others 2013 ) Recent connectomic tract tracing reveals that, contrary to what was previously thought, the cortical inter-areal network has high density . This finding leads to a necessary revision of the relevance of some of the graph theoretical notions, such as the small world property . . ., that have been claimed to characterise the inter-areal cortical network. ([Knoblauch and others 2016][2016ToroczkaiZ_KnoblauchK]) These remarks carried weight because they were based on sophisticated and highly sensitive measurements of mammalian cortical connectivity ( Fig. 4 ). In each one of multiple carefully standardized experiments in the macaque monkey, a fluorescent tracer was injected into a (target) cortical region where it was taken up by synaptic terminals and actively transported to the cell bodies of neurons projecting to the target region. When the animal\u2019s brain was subsequently examined microscopically, the retrograde transport of the tracer from the injection site resulted in a fluorescent signal in the (source) regions of cortex that were directly connected to the target region. The basic technology of anatomical tract-tracing had been used by neuroanatomists since the late 20th century; but in the first decades of the 21st century it was possible to increase the scale and precision of the measurements dramatically, enabling the construction of connectivity matrices that summarized the strength or weight of axonal projections between a large number of cortical areas. These next-generation tract-tracing data thus represented a new standard of knowledge about mammalian cortical connectivity, that was more continuously quantified than the binary or ordinal rating of connectivity from traditional tract-tracing experiments ([Stephan and others 2001][2001K\u00f6tterR_StephanKE]), and much less ambiguously related to the cellular substrates of brain networks than the statistical measures of functional connectivity ([Achard and others 2006][2006BullmoreET_AchardS]; [Zhang and others 2016][2016BassettDS_ZhangZ]) and structural covariance ([Alexander-Bloch and others 2013][2013BullmoreET_Alexander-BlochA]; [Bassett and others 2008][2008Meyer-LindenbergA_BassettDS]) used to build graphs from human neuroimaging data. It is clearly important to understand in some detail how the topology of brain networks can be modelled in contemporary tract-tracing data from the macaque (and subsequently the mouse ([Oh and others 2014][2014MortrudMT_OhSW]; [Rubinov and others 2015][2015BullmoreET_RubinovM])) and what these results tell us about the small-worldness of brain networks. Figure 4. High density of the macaque cortical graph excludes sparse small world architecture. (A) Comparison of the average shortest path length and density of the macaque cortical graph from (Markov and others 2013) with the graphs of previous studies (Felleman and Van Essen 1991; Honey and others 2007; Jouve and others 1998; Markov and others 2012; Modha and Singh 2010; Young 1993). Sequential removal of weak connections causes an increase in the path length. Black triangle: macaque cortical graph from Markov and others (2013); gray area: 95% confidence interval following random removal of connections from the macaque cortical graph from Markov and others (2013). Jouve et al., 1998 predicted indicates values of the graph inferred using the published algorithm. (B) Effect of density on Watts and Strogatz\u2019s formalization of a small-world network. Clustering and path length variations generated by edge rewiring with probability range indicated on the x-axis applied to regular lattices of increasingly higher densities. The pie charts show graph density encoded via colors for path length ( L and clustering coefficient (C). The y-axis indicates the path length ratio ( \\frac{L_p}{L_o} ) and clustering ratio ( \\frac{C_p}{C_o} ) of the randomly rewired network, where L_o and C_o are the path length and clustering of the regular lattice, respectively. The variables L_p and C_p are the same quantities measured for the network rewired with probability P . Hence, for each density value indicated in the L and C pie charts, the corresponding \\frac{L_p}{L_o} and \\frac{C_p}{C_o} curves can be identified. Three diagrams below the x-axis indicate the lattice (left), sparsely rewired (middle), and the randomized (right) networks. (C) The small-world coefficient \u03c3 (Humphries and others 2006) corresponding to each lattice rewiring. Color code is the same as in panel (B). Dashed lines in (B) and (C) indicate 42% and 48% density levels, respectively. Reproduced with permission from Markov and others (2013).","title":"3. Challenges to Small-Worldness"},{"location":"190311_BassettDS_BullmoreET2017/#binary_graphs","text":"In general, a node represents a component of a system and an edge represents a connection or interaction between two nodes. Mathematically, we can capture these ideas with a graph G = (V,E) composed of a node set V and an edge set E ( Bollob\u00e1s 1979 , 1985 ). We store this information in an association or weight matrix W , whose ij th element indicates the strength or weight w_{i,j} of the edge between node i and node j A simple way of building a graph from such an association matrix is to apply a threshold \\tau to each element of the matrix, such that if w_{i,j} \u2265 \\tau then an edge is drawn between the corresponding nodes, but if w_{i,j} < \\tau no edge is drawn ([Achard and others 2006][2006BullmoreET_AchardS]). This thresholding operation thus binarizes the weight matrix and converts the continuously variable edge weights to either 1 (suprathreshold) or 0 (subthreshold). It was on this basis that almost all brain graphs were constructed in the 15 years or so following the seminal small-world analysis of a binary graph representing the cellular connectome of C. elegans ( Watts and Strogatz 1998 ). Most of the neuroimaging evidence for small-worldness in human brain networks, for example, is based on analysis of binary graphs constructed by thresholding a correlation coefficient or equivalent estimator of the weight of functional or structural connectivity or structural covariance between regions i and j ( van Wijk and others 2010 ). It is well recognized that construction of binary graphs represents an extreme simplification of brain networks; indeed, a binary undirected graph of homogenous nodes is as simple as it gets in graph theory ([Bassett and others 2012a][2012LimKO_BassettDS]). However, this approach has historically been preferred in neuroimaging because of limited signal-to-noise ratio in the data ([Achard and others 2006][2006BullmoreET_AchardS]). By varying the threshold \\tau used to construct a binary graph from a continuous weight matrix, the connection density of the network is made denser or sparser. If the threshold is low and many weak weights are added to the graph as edges then the connection density will increase; if the threshold is high and only the strongest weights are represented as edges, then the connection density will decrease. The connection density D is quantified by the number of edges E in the graph as a proportion of the total number of edges in a fully connected network of the same number of nodes N : D=\\frac{E}{\\frac{N^2\u2212N}{2}} Often, this proportion is translated into a percentage. In many neuroimaging studies, the threshold is set to a large value to control for the high levels of noise in MRI data, resulting in connection densities in the range 5% to 30% ( Lynall and others 2010 ). In many of the first generation tract tracing studies, the connectivity data were collected on a binary or ordinal scale, and not all possible connections had been experimentally measured, so these data were naturally modelled as binary graphs with connection densities ~30% a value that was constrained by the completeness and quality of the data ([Bassett and Bullmore 2006][2006BullmoreE_BassettDS]). The small-world topology of a binary brain graph is defined by estimating two parameters in the data, path length L and clustering C ( Fig. 2A ), and comparing each of these observed parameters to their distributions under a specified null model ( Humphries and others 2006 ). More specifically, L = \\frac{1}{N}\\sum l_{i,j} is the global or characteristic path length, where l_{i,j} is the shortest path (geodesic) between nodes i] and j ; and C = \\frac{1}{N}\\sum c_{i,j} ( c_i := \\frac{L_i}{\\frac{k_i(k_i-1)}{2}} ) is the global clustering coefficient , where c_{i,j} is the number of closed triangular motifs including node i . Each of these parameters is normalized by its value in a binary graph representing the null hypothesis. For example, if the null hypothesis is that clustering of brain networks C_{\\text{brain}} is no different from the clustering of a random graph, then it is reasonable to generate an Erd\u00f6s\u2013Reny\u00ed graph for N nodes and D connection density, measure the clustering coefficient in the random graph C_{\\text{random}}] , and use the ratio between brain and random graph clustering coefficients as a test statistic for non-random clustering. We note that there are many other possible ways in which a null model could be sampled, besides using the classical Erd\u00f6s\u2013Reny\u00ed model, and this is an active area of methodological research ( Muldoon and others 2016a ). However, in general one can define the normalized clustering coefficient as \\Gamma = \\frac{C_{\\text{brain}}}{C_{\\text{random}}} . Likewise, the path length of the brain graph can be normalized by its value in a comparable random graph \\Lambda = \\frac{L_{\\text{brain}}}{L_{\\text{random}}} . A small-worldness scalar can then be simply defined as \\sigma = \\frac{\\Gamma}{\\Lambda} . With these definitions, small-world networks will have \\sigma > 1 , \\Gamma > 1 and \\Lambda \\sim 1 ( Humphries and others 2006 ).","title":"Binary Graphs"},{"location":"190311_BassettDS_BullmoreET2017/#weighted_graphs","text":"Although binary graph analysis has predominated to date in analysis of brain networks, this certainly does not represent the methodological limit of graph theory for connectomics. For example, provided the data are of sufficient quality, there is no need to threshold the weight matrix to estimate topological properties like clustering, path length, and small-worldness. Indeed, while the binarization procedure was common in early applications of graph theory to neural data ( van Wijk and others 2010 ), it remains fundamentally agnostic to architectural principles that may be encoded in edge weights ( Rubinov and Sporns 2011 ). This realization has more generally motivated the field to develop methods that remain sensitive to the patterns of weights on the edges ([Ginestet and others 2011][2011SimmonsA_GinestetCE]), and to the topologies present in weak versus strong weights ( Rubinov and Sporns 2011 ). These efforts have included the development of alternative thresholding schemes ([Bassett and others 2012a][2012LimKO_BassettDS]; [Lohse and others 2014][2014CarlsonJM_LohseC]) and fully weighted graph analysis ([Bassett and others 2011b][2011GraftonST_BassettDS]; Rubinov and Sporns 2011 ). The mathematical tools exist to estimate and simulate the topological properties of weighted networks, and analysis of weighted networks is akin to studying the geometry of the graph, rather than simply its topology ([Bassett and others 2012b][2012PorterMA_BassettDS]; [Bassett and others 2013][2013GraftonST_BassettDS]). For example, weighted analogues of binary metrics of clustering, path length and small-worldness can be defined formally ( Fig. 2B ). First, the weighted clustering coefficient of node i can be defined as C_{\\text{weighted}} = \\frac{1}{k_i(k_i\u22121)} \\sum_{j,k}(\\hat{w}_{ij} \\hat{w}_{jk} \\hat{w}_{ik})^{\\frac{1}{3}} , where k_i is the number of edges connected to node i , or degree of node i ([Onnela and others 2005][2005KaskiK_OnnelaJP]) (but see also [Barrat and others 2004][2004VespignaniA_BarratA]; [Zhang and Horvath 2005][2005HorvathS_ZhangB] for other similar definitions). The weighted path length can be defined as L_{\\text{weighted}} = \\frac{1}{N(N\u22121)} \\sum_{i \\neq j} \\delta_{ij} , where the topological distance between two nodes is given by \\delta_{ij} = \\frac{1}{w_{ij}} ([Newman 2001][2001NewmanMEJ]). These two statistics can be combined to construct a weighted metric of small-worldness ([Bola\u00f1os and others 2013][2013AviyenteS_Bola\u00f1osM]): \\sigma_{\\text{weighted}} = \\frac{\\Gamma_{\\text{weighted}}}{\\Lambda_{\\text{weighted}}} . With these definitions, small-world networks will have \\Gamma_{\\text{weighted}} > 1 , \\Lambda_{\\text{weighted}} \\sim 1 , and \\sigma_{\\text{weighted}} > 1 ( Humphries and others 2006 ).","title":"Weighted Graphs"},{"location":"190311_BassettDS_BullmoreET2017/#the_small-world_propensity","text":"There are several important limitations to the definitions of small-worldness described in the previous sections. First, the small-world scalar \\sigma (whether binary or weighted) can be greater than 1 even in cases when the normalized path length is much greater than one; because it is defined as a ratio, if \\gamma \\gg 1 and \\lambda > 1 , the scalar \\sigma > 1 This means that a small-world network will always have \\sigma > 1 , but not all networks with \\sigma > 1 will be small-world (some of them may have greater path length than random graphs). Second, the measure is strongly driven by the density of the graph, and denser networks will naturally have smaller values of \\sigma even if they are in fact generated from an identical small-world model. To address these and other limitations, Muldoon and colleagues recently developed a metric called the small-world propensity . Specifically, the small-world propensity, \\phi , reflects the deviation of a network\u2019s clustering coefficient, C_{\\text{brain}}] , and characteristic path length, L_{\\text{brain}} , from both lattice ( C_{\\text{lattice}} , L_{\\text{lattice}} ) and random ( C_{\\text{random}} , L_{\\text{random}} ) networks constructed with the same number of nodes and the same degree distribution: \\phi = 1 \u2212 \\sqrt{\\frac{\\Delta^2_C+\\Delta^2_L}{2}} , where \\Delta C = \\frac{C_{\\text{lattice}}\u2212C_{\\text{brain}}}{C_{\\text{lattice}}\u2212C_{\\text{random}}} and \\Delta L = \\frac{L_{\\text{brain}}\u2212L_{\\text{random}}}{L_{\\text{lattice}}\u2212L_{\\text{random}}} . The ratio \\Delta_{C/L} represents the fractional deviation of the metric C_{\\text{brain}} or L_{\\text{brain}} ) from its respective null model (a lattice or random network). This quantity can be calculated for binary networks (using binary definitions of clustering and path length) or for weighted networks (using weighted definitions of clustering and path length). Networks are considered small-world if they have small-world propensity 0.4 < \\phi \u2264 1 . However, this metric should be viewed as a continuous metric of small-worldness rather than a hard threshold ( Muldoon and others 2016a ). Importantly, the small-world propensity overcomes several limitations of previous scalar definitions of small-worldness ( Muldoon and others 2016a ). First, it can incorporate weighted estimates of both the clustering coefficient and path-length, thus being generally applicable to any neural data that can be represented as a weighted network. Second, it is density independent, meaning that it can be used to compare the relative small-worldness between two networks that have very different densities from one another. Third, the metric is informed by spatially-constrained null models ([Bassett and others 2015][2015DanielsKE_BassettDS]; [Expert and others 2011][2011LambiotteR_ExpertP]; [Papadopoulus and others 2016][2016BassettDS_Papadopoulus]) in which nodes have physical locations and the edges that correspond to the smallest Euclidean distance between nodes are assigned the highest weights ([Barth\u00e9lemy 2011][2011Barth\u00e9lemyM]) ( Fig. 5 ). Figure 5. Small-world propensity in weighted networks. Here, we illustrate an example of a generative small-world model, and its utility in estimating an empirical network\u2019s small-world propensity. (A) We can extend the concept of a Watts\u2013Strogatz model to weighted graphs by first building a lattice in which the edges are weighted by distance such that edges between spatially neighboring nodes have more strongly weighted than edges between spatially distant nodes. These edge weights can then be rewired with a probability, P, to create a weighted small-world network. (B) Weighted clustering coefficient and weighted path length can be estimated as a function of the rewiring parameter, P, and used to derive the small-world propensity of the graph compared with random and lattice benchmarks (Eq. 11). (C) Weighted small-world propensity calculated for the same network as in panel (B). Error bars represent the standard error of the mean calculated over 50 simulations, and the shaded regions represent the range denoted as small-world. (D) Weighted small-world propensity as a function of network density for a graph of 1000 nodes. Reproduced with permission from Muldoon and others (2016a).","title":"The Small-World Propensity"},{"location":"190311_BassettDS_BullmoreET2017/#4_twenty-first_century_tract-tracing","text":"The scale and quality of contemporary tract-tracing data, in both the macaque and the mouse, represents a step change in terms of sensitivity in detecting anatomical connections, or axonal projections, between cortical areas. Using retrograde tracer experiments it has proven possible to demonstrate reliably that pairs of regions in the macaque brain may be connected by one or a few axonal projections. Likewise, anterograde tracer experiments in the mouse have demonstrated that the minimal detectable weight of connectivity between cortical regions, that just exceeds the noise threshold, is equivalent to the projection of one or a few axons (Ypma and Bullmore 2016). This high sensitivity has led immediately to the recognition of a large number of weak and previously unreported axonal connections. In the macaque, it was estimated that 36% of connections identified by contemporary tract tracing were so-called new found projections (NFPs) that had not been described in the prior literature (Markov and others 2014). The existence of so many weak connections is reflected in the log normal distributions of connectivity weight, ranging over five to six orders of magnitude, in both the macaque and the mouse (Ercsey-Ravasz and others 2013; Oh and others 2014). In short, tract-tracing can now resolve connections approximately equivalent to a single axonal projection and approximately a million times weaker than the strongest anatomical connections or white matter tracts. How can we use graph theory to model the network organization of such highly sensitive, highly variable data? Perhaps the simplest approach, borrowing from prior studies of less high quality datasets, is to apply a threshold and convert the log-normal weight matrix into a binary adjacency matrix. If the threshold is defined by the noise distribution of the measurements then it will be very close to zero for these sophisticated experiments, and correspondingly the connection density of the binary graph will be high. In the macaque, the connection density of a binary graph of 29 visual cortical areas was estimated to be 66% (Markov and others 2013), considerably higher than historical estimates in the range of 25% to 45% (Fellemen and Van Essen 1991). In the mouse, the connection density of a binary graph of 308 areas of the whole cortex was estimated to be 53% (Rubinov and others 2015). In other words, the binary graphs generated from 21st century tract-tracing data are about twice as dense as the much sparser networks derived from human neuroimaging and 20th century tract-tracing. They are also considerably denser than brain networks constructed at a finer grained (ultimately cellular) resolution. For example, the connection density of the C elegans nervous system, which is still the only completely mapped synaptic connectome, is about 12%. It is easy to see that the connection density of a binary network depends on the number of neurons comprising each node. In the limit, if the nervous system is parcellated into two large nodes the connection density will certainly be 100%; as the same system is parcellated into a larger number of smaller nodes its connection density will monotonically decrease (Bassett and others 2011a; Zalesky and others 2010). Thus, the current interval estimate of mammalian cortical connection density ~55% to 65% is conditional both on the anatomical resolution of the parcellation scheme used to define the nodes and the sensitivity of the tract-tracing methods used to estimate the weights of the edges.","title":"4. Twenty-First Century Tract-Tracing"},{"location":"190311_BassettDS_BullmoreET2017/#small-worldness_of_binary_tract-tracing_networks","text":"Having constructed a high-density binary graph from tract-tracing data on mammalian cortex, it is straightforward to estimate its clustering and path length, using the same metrics as for sparser binary graphs. However, simply because there is a larger number of connections in the denser network, its clustering will be considerably higher (there will be more closed triangular motifs) and its path length will be shorter (there will be more direct, pairwise connections) than a sparser network. Indeed, the clustering and path length of any binary graph at 60% connection density will be close to the maximal clustering and minimal path length of a fully connected graph; and therefore the clustering and path length of a 60% dense brain network will be very similar to the clustering and path length of a 60% random network (Bassett and others 2009). This means that when clustering and path length in brain networks are normalized by their corresponding values in equally dense random networks, the scaled metrics \u0393 and \u039b will both be close to 1, and the small-world scalar \u03c3 will be close to its critical value of 1 (Markov and others 2013). For the macaque, at 66% connection density, \u0393=1.21\u00b10.014 , \u039b=1.00\u00b10.000 , and \u03c3=1.21\u00b10.014 ; for the mouse, at 53% connection density, \u0393=1.31\u00b10.004 , \u039b=1.00\u00b10.000 , and \u03c3=1.31\u00b10.004 (all given in mean \u00b1 standard deviation; Fig. 6A and C; Table 1). Since small-worldness has been traditionally defined as \u03c3>1 , these results suggest that dense binary graphs constructed from tract tracing data are small-world, although the macaque is more similar to a random network than the mouse. Figure 6. Binary and weighted small-worldness in mouse and macaque connectomes. For the macaque connectome reported in Markov and others (2013), we show (A) the binary network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (B) we show a weighted network analysis for the same data. For the mouse connectome reported in Rubinov and others (2015), we show (C) the weighted network, a random graph of the same size and density, and the estimated small-world parameters \u0393 (normalized clustering coefficient), \u039b (normalized path length), \u03c3 (classical small-world scalar) and \u03c6 (small world propensity). In panel (D), we show a binary network analysis for the same data. In the boxplots, the gray dotted line shows the threshold value of \u03c3 = 1, and the purple area shows the range of values of 0.4 < \u03c6 \u2264 1 in which a network is considered small-world. These results do not look like a \u201cblack swan\u201d that refutes universal claims that the brain always embodies small-world network topology. Nor do they undermine the credibility of previous studies demonstrating small-worldness in sparser brain graphs. However, our view is that binary graph models are very unlikely to be an optimal strategy for network analysis of tract-tracing data, because they fail to take account of the extraordinary range of connectivity weights, distributed log normally over 6 orders of magnitude, that has been discovered in mammalian cortical networks (Ercsey-Ravasz and others 2013). The weakest connection between cortical areas is about a million times less weighted than the strongest connection: does it really make sense to set all these weights equivalently to 1 as edges in a binary graph? To ask the question is to answer it.","title":"Small-Worldness of Binary Tract-Tracing Networks"},{"location":"190311_BassettDS_BullmoreET2017/#small-worldness_of_weighted_tract-tracing_networks","text":"A weighted small-world analysis is easily done for these data (Fig. 6B and D). The weighted clustering and weighted path length metrics (Eqs. 8 and 9) are estimated directly from the weight matrices, and the ratio of weighted clustering to weighted path length is the scalar summary of weighted small-worldness \u03c3_{\\text{weighted}}>1 . In Figure 6, we directly compare binary and weighted graph theoretical results for the mouse (Oh and others 2014; Rubinov and others 2015) and macaque (Markov and others 2013) connectomes. Compared with the results of binary graph analysis, both mouse and macaque networks have increased clustering for the weighted graph analysis, and \u03c3 is increased for the macaque (see Table 1). The weighted graph of the mouse connectome is similarly small-world compared to the weighted macaque graph, as measured by \u03c3 , but is significantly more small-world as measured by the small-world propensity \u03d5 . However, classical estimates of small-worldness may depend in a non-trivial way on the density of the graph. This relationship becomes obvious if we estimate the topology of both weighted graphs as a function of connection density (Fig. 7). The classical small-world scalar \u03c3 is greatest when it is estimated for a sparse graph comprising less than 20% to 30% of the most strongly connected edges, and decreases progressively as the graph becomes denser. This might suggest that the macaque connectome seems less small-world than the mouse simply because it is denser. However, the small world propensity \u03d5 has the useful property that it is independent of network density and it is significantly greater, indicating more small-worldness, for the mouse than the macaque. This could be related to differences between the datasets in number of cortical areas and completeness of cortical coverage: the macaque dataset comprises fewer nodes of mostly visual cortex than the larger number of nodes across the whole mouse cortex. Figure 7. Dependence of small-world characteristics on network density. (A) Macaque and (B) mouse connectivity matrices in their natural state (left), as well as after thresholding to retain the 5% strongest (middle) or 25% strongest (right) connections. Weighted small-world metrics including the normalized clustering coefficient (\u0393), normalized path length (\u039b), small-world index (\u03c3), and small-world propensity (\u03c6) as a function of network density for the (C) macaque and (D) mouse connectivity matrices.","title":"Small-Worldness of Weighted Tract-Tracing Networks"},{"location":"190311_BassettDS_BullmoreET2017/#weighted_small-worldness_and_the_role_of_edge_weights","text":"Why does a weighted graph analysis provide stronger evidence for non-random clustering than a binary graph analysis applied to the same tract-tracing data? The most strongly weighted connections generally span the shortest physical distances between cortical areas (Ercsey-Ravasz and others 2013; Klimm and others 2014; Rubinov and others 2015). This is not surprising based on what we know about the importance of cost constraints on brain organization (Bassett and others 2009; Bassett and others 2010; Bullmore and Sporns 2012; Fornito and others 2011). Strong connectivity weights indicate a large number of axonal projections, a big bandwidth bundle, perhaps macroscopically visible as a white matter tract. Building and resourcing a high-bandwidth axonal signaling bundle is a significant biological cost that will increase as a function of connection distance: it is parsimonious to wire high bandwidth over short distances. Short distance connections are not only strongly weighted but also topologically clustered. So the strongest weights in both cortical networks define a topologically segregated and anatomically localized organization. A map of the sub-network formed by the strongest weights shows spatial and topological clusters of regions (Fig. 8). In the mouse, the strongly weighted clusters each comprise functionally specialized areas of cortex (visual, motor, etc.) that are known to be densely interconnected and anatomically localized (Rubinov and others 2015; Ypma and Bullmore 2016). Thus it is not surprising that weighting the topological analysis of mammalian cortical networks will provide stronger evidence for non-random clustering than unweighted analysis of binary graphs. Figure 8. The existence of weak links and their topology in the mouse connectome. Here, we show the properties of the 5% weakest and 5% strongest edges of the mouse cortical network. (A, B) Axial view of the mouse cortical network, red dots represent brain regions, blue lines represent the connections between them. Drawn are the (A) 5% weakest or (B) 5% strongest edges. Dot size corresponds to degree, the total number of incoming and outgoing edges connected to a node. In (B), the three nodes with highest degree have been labeled as follows: VISp, primary visual area; MOp, primary motor area; SSs, supplemental somatosensory area. The strong connections are spatially organized, mainly connecting spatially adjacent or contralaterally homologous regions. The weak connections span longer distances and are topologically more random than the strongest connections. (C) The distance distributions for (blue) the 5% weakest edges, (red) the 5% strongest edges, and (black) a random graph of the same size and connection density. (D) The degree distributions for the weakest and strongest connections of the mouse connectome, and a comparable random graph, color-coded as in panel (C). Reproduced with permission from Ypma and Bullmore (2016). The most weakly weighted connections are an area of active, ongoing research (discussed in more detail below) and it is inevitable that there is still much to learn about a feature of network organization\u2014replicable but very weak connections between large cortical areas\u2014that had not been measurable until recent advances in tract-tracing methodology. However, it is clear that weaker connections tend to subtend longer distances, and can be either more topologically random (Ypma and Bullmore 2016) than or similarly topologically organized to strong connections (Bassett and others 2012a). We conclude that graph theoretical analysis of tract-tracing connectomes should respect the quality of the data and use weighted topological metrics to reflect the wide ranging variation in anatomical connectivity, from single fibers to major tracts, that is now measurable in the mammalian brain (Wang and Kennedy 2016). Weighted graph analysis demonstrates clearly that both the macaque and mouse connectomes are small-world networks, as are the human, cat, and nematode brains (Muldoon and others 2016a). Binary graph analysis has usefully measured high connection density, due to the existence of many new anatomical connections, but binarization of these data is not the best way to understand their complex topology and its economical embedding in anatomical space (Bassett and others 2011a; Bassett and others 2012a; Klimm and others 2014; Rubinov and others 2015; Rubinov and Sporns 2011). Future studies will likely also pay more attention to the fact that most tract-tracing markers are axonally transported only in one direction: anterograde or retrograde. This means that the weight matrix could be modelled more completely as a weighted and directed graph, representing a further evolution in the use of graph theoretical methods to capture a richer and biologically more meaningful model of brain network organization than can be provided by binary graphs of unweighted and undirected edges.","title":"Weighted Small-Worldness and the Role of Edge Weights"},{"location":"190311_BassettDS_BullmoreET2017/#5_the_utility_of_weak_connections","text":"At this juncture, one might naturally ask, \u201cFrom a neuroscientific perspective, do we need techniques that account for edge weights? Do these weights indeed capture information of relevance for cognition and behavior?\u201d Neuroanatomical data suggest that the weights of structural connections may be driven by developmental growth rules (Ercsey-Ravasz and others 2013; Kaiser and Hilgetag 2006; Klimm and others 2014; Lohse and others 2014; Markov and others 2013), energetic and metabolic constraints (Bassett and others 2010), and physical limitations on the volume of neural systems, particularly brains encapsulated by bone (Sherbondy and others 2009). Yet the role of these edge weights in neural computations (Schneidman and others 2006) and higher order cognition has been less well studied. Recent studies have begun to elucidate the role of edge weights\u2014and particularly of weak connections\u2014in human cognition. In resting-state fMRI data, weak functional connections from lateral prefrontal cortex to regions within and outside the frontoparietal network have been shown to display individual differences in strength that predict individual differences in fluid intelligence (Cole and others 2012). The same general relationship was observed in a separate study in which individual differences in moderately weak, long-distance functional connections at rest were strongly correlated with full scale, verbal, and performance IQ (Santarnecchi and others 2014). Neither of these correlations were observed when considering strong connections. Indeed, the utility of weak edges appears to extend to psychiatric illness, where the highly organized topology of weak functional connections\u2014but not strong functional connections\u2014in resting-state fMRI were able to classify people with schizophrenia from healthy controls with high accuracy and specificity (Bassett and others 2012a). Interestingly, individual differences in these weak connections were significantly correlated with individual differences in cognitive scores and symptomatology. Together these results demonstrate that, indeed, methods that are sensitive to the strength (or weakness) of individual connections are imperative for progress to be made in understanding individual differences in cognitive abilities, and their alteration in psychiatric disease. Importantly, the utility of weak connections is not only evident at the large scale in human brains but also at the neuronal scale as measured in non-human species. In an influential article published in 2006 with Bialek and colleagues, Schneidman demonstrated that weak pairwise correlations implied strongly correlated network states in a neural population, suggesting the presence of strong collective behavior (Schneidman and others 2006). This result was initially counterintuitive as one might expect that weak correlations would be associated with the lack of collective behavior. However, the original observation has withstood the test of time, and has been validated in several additional studies including work at the level of tract tracing in macaque monkeys (Goulas and others 2015). Intuitively, the juxtaposition of weak correlations and cohesive, collective behavior is thought to be driven by the underlying sparsity of neuronal interactions (Ganmor and others 2011b), which contain a few non-trivial higher-order interaction terms (Ganmor and others 2011a). Indeed, these higher-order interactions are the topic of some interest both from a computational neuroscience perspective (Giusti and others 2016; Sizemore and others 2016), and from the perspective of neural coding (Giusti and others 2015). But perhaps the claim that weak connections are critically important for our understanding of neural systems should not be particularly surprising. Indeed, it is in fact an old story, first published at the inception of network science. In 1973, Granovetter wrote a seminal paper, titled \u201cThe strength of weak ties,\u201d which highlighted the critical importance of weakly connected components in global system dynamics (Granovetter 1973). Such weak connections are ubiquitous in many systems, from physician interactions (Bridewell and Das 2011) to ecosystem webs (Ulanowicz and others 2014) and atmospheric pathways (Lee and Su 2014). Looking forward, critical open questions lie in how these weak connections drive global dynamics, and how one can intervene in a system to manipulate those processes (Betzel and others 2016b; Gu and others 2015; Muldoon and others 2016b). Acknowledging the role of weak connections, weighted small-world organization plays a critical role in system functions that are particularly relevant to neural systems: including coherence, computation, and control and robustness (Novkovic and others 2016). Perhaps the most commonly studied function afforded by small-world architecture is the ability to transmit information, a characteristic that is common in networks of coupled oscillators (Barahona and Pecora 2002; Hong and others 2002; Nishikawa and others 2003) (although see Atay and others 2006, for a few notable exceptions). This capability supports enhanced computational power (Lago-Fern\u00e1ndez and others 2000), via swift flow and transport (Hwang and others 2010). In dynamic networks, oscillators coupled on small-world networks are much more sensitive to link changes than their random network counterparts (Kohar and others 2014), the time taken to reach synchronization is lowered, and the synchronized state is less stable over time, potentially enabling greater diversity of function. When such a system has both small-world topology and geometry, it directly impacts the network\u2019s ability to speed or slow spreading (Karsai and others 2011), a potentially useful characteristic for resilience to dementia which is thought to be caused by the spread of prions (Raj and others 2012; Raj and others 2015). The value of small-world architecture is not limited to its support of synchronization and information flow. Instead, it also supports a wide range of computations in neural circuits. From early neural network studies, it is clear that the exact topology of connectivity patterns between network elements directly supports trade-offs in the network\u2019s ability to learn new information versus retain old information in memory (Hermundstad and others 2011). When these patterns are organized in a small-world manner, evidence suggests that local computations can be integrated across distributed cell assemblies to support functions as diverse as somatosensation (Zippo and others 2013) and olfaction (Imam and others 2012). The mechanism by which small-worlds support these computations may stem from the fact that their topological structure tends to contain both large cavities and high-dimensional cliques (Sizemore and others 2016), which when embedded in a physical space can strongly constrain the geometric properties of the computation (Giusti and others 2015). While small-world structure can offer non-trivial advantages in terms of both communication and computation, it also directly informs the sorts of interventions that one could use to guide network dynamics and by extension system function. Indeed, computational studies have demonstrated that small-world network architecture requires specific control strategies if one wishes to stem the propagation of seizure activity (Ching and others 2012), control the spread of viruses (Kleczkowski and others 2012), or enhance recovery following injury (H\u00fcbler and Buchman 2008). To gain an intuition for how topology impacts control, we can consider the broad-scale degree distribution also characteristic of brain networks. Based on the Laplacian spectrum, one can observe that weakly connected nodes have the greatest potential to push the system into distant states, far away on an energy landscape (Pasqualetti and others 2014); conversely, strongly connected hubs have the greatest potential to push the system into many local states, nearby on the energy landscape (Gu and others 2015). Thus, control energy (such as that provided by brain stimulation) may be targeted to different locations in a small-world brain network to affect a specific change in brain dynamics (Muldoon and others 2016b).","title":"5. The Utility of Weak Connections"},{"location":"190311_BassettDS_BullmoreET2017/#6_conclusions","text":"Small-worldness remains an important and viable concept in network neuroscience. Nearly 20 years on from the first analysis of the complex topology of a binary graph representing the nervous system of C. elegans, it has been established that small-worldness is a nearly universal and functionally valuable property of nervous systems economically embedded in anatomical space. Recent advances in tract-tracing connectomics do not refute small-worldness; rather they considerably enrich and deepen our understanding of what it means in the brain. The extraordinary precision of contemporary tract tracing, and the important discovery that mammalian cortical networks are denser than expected, mandates the adoption of more sophisticated techniques for weighted graph theoretical modeling of interareal connectomes. On this basis, we expect the next 10 years to yield further insights into the functional value of weak as well as strong connections in brain networks with weighted small-worldness.","title":"6. Conclusions"},{"location":"190410_DeAngelisD_DiazS_2019/","text":"19-04-10 Decision-Making in Agent-Based Modeling: A Current Review and Future Prospectus \u00b6 Original | Mendeley ToC \u00b6 00. Abstract 01. Introduction 02. Decisions in Classical Population Models 03. Agent-based Modeling in Ecology 04. Movement Decisions and Their Consequences 04.01. When to Move? 04.02. Where to Move? 04.03. Collective Movement Behavior 05. Foraging Decisions and Population Interactions 06. Social Interactions in Populations 07. Developments in the Modeling of Decisions in Population Models 08. Prospectus 00. Abstract \u00b6 00.P01 \u00b6 All basic processes of ecological populations involve decisions; when and where to move, when and what to eat, and whether to fight or flee. Yet decisions and the underlying principles of decision-making have been difficult to integrate into the classical population-level models of ecology. Certainly, there is a long history of modeling individuals' searching behavior, diet selection, or conflict dynamics within social interactions. When all the individuals are given certain simple rules to govern their decision-making processes, the resultant population\u2013level models have yielded important generalizations and theory. But it is also recognized that such models do not represent the way real individuals decide on actions. Factors that influence a decision include the organism's environment with its dynamic rewards and risks , the complex internal state of the organism , and its imperfect knowledge of the environment . In the case of animals, it may also involve complex social factors, and experience and learning , which vary among individuals. The way that all factors are weighed and processed to lead to decisions is a major area of behavioral theory. 00.P02 \u00b6 Individual- / Agent-based model (IBM / ABM) While classic population-level modeling is limited in its ability to integrate decision-making in its actual complexity, the development of individual- or agent-based models (IBM/ABMs) (we use ABM throughout to designate both \u201cagent-based modeling\u201d and an \u201cagent-based model\u201d) has opened the possibility of describing the way that decisions are made, and their effects, in minute detail. Over the years, these models have increased in size and complexity. Current ABMs can simulate thousands of individuals in realistic environments, and with highly detailed internal physiology, perception and ability to process the perceptions and make decisions based on those and their internal states . The implementation of decision-making in ABMs ranges from fairly simple to highly complex; the process of an individual deciding on an action can occur through the use of logical and simple (if-then) rules to more sophisticated neural networks and genetic algorithms . The purpose of this paper is to give an overview of the ways in which decisions are integrated into a variety of ABMs and to give a prospectus on the future of modeling of decisions in ABMs. 01. Introduction \u00b6 01.P01 \u00b6 classical models: randomly moving w/ growth, reproducitin, mortality, interaction w/ env & other organisms e.g., logistric population model Lotka-Volterra predator-prey and competition model reaction-diffusion partial differential equation (PDE) model decision-making by individuals is unimportant? decision continually made e.g., single-celled animals (paramecia) ( 2009_Wagner ) social ants ( 2006_Deneubourg_Detrain ) The role of decision-making by individual organisms is largely ignored in the classical mathematical models of ecology, such as the logistic population models , Lotka-Volterra predator-prey and competition models , and their many variations. These models, as well as their extensions to space, as reaction-diffusion partial differential equation (PDE) models , treat organisms as randomly moving atoms, with added features of growth, reproduction, mortality, and interactions with their environment and other organisms. Because the classical models of ecological populations have been successful in revealing much about ecological systems, one might ask if decision-making by individuals is unimportant enough that it can be ignored at the population level, at least for simple organisms. But something like decisions are continually made, even by organisms perceived as simple. Bray (2009) notes that single-celled animals, like swimming paramecia, \u201ccontinually encounter different situations\u2026 and have to evaluate their options and assign priorities.\u201d Wagner (2009) asks \u201cDoes the bacterium choose to change direction?\u201d and concludes that this may be a matter of perspective. In more complex organisms, such as social ants, \u201ceach individual is a sensitive unit which can process a lot of information\u201d ( Detrain and Deneubourg, 2006 ). These actions are programmed into the organism's DNA, and are unconscious, but clearly decisions are being made, and we will follow Ydenberg (2010) (cited in Rypstra et al., 2015 ) in calling a decision \u201cwherever one or two (or more) options is/are selected .\u201d 01.P02 \u00b6 How important these decisions are at the population level and above is therefore an important question, even for organisms of low cognitive ability. Our perspective in this review is from that of population and community ecology and how modeling helps link individual behaviors to phenomena at the level of collections of individuals. We begin with a brief overview of how decisions have been incorporated in some classical analytic models of ecology. Then we introduce agent-based modeling (ABM) and describe how it has been used to simulate decision-making in individual movement, foraging behavior, population interactions, and social interactions within populations. This is not intended to be comprehensive, but to touch on a variety of ways ABM is used. Finally, we discuss more recent developments in modeling decisions within population models and present a prospectus for future directions. 02. Decisions in Classical Population Models \u00b6 02.P01 \u00b6 individual decision \u21d2 population < individual fitness \u21d0 ecological context The use of modeling to address the question of the effect of individual decisions on population level dynamics developed more slowly than modeling of the converse question of how ecological context influences the effects of decisions on individual fitness. The latter has been explored by behavioral ecologists using classical population models for several decades, focusing especially on decisions regarding foraging movement and its effects on the fitness of individuals. Additionally, some models have been able to incorporate decision-making when modeling collections of individuals, whole populations, and even multiple populations. 02.P02 \u00b6 models kinesis model (decisions on when to speed up/slow down/turn in response to detected local cond) restricted are search patch DSVM (Dynamic State Variable Modeling) Movement of animals toward favorable conditions could be decomposed into simple decisions on directed movement, or taxis, in relation to light, temperature, or resource gradients. Because it may be difficult for organisms to detect gradients, but possible to assess conditions at a current location, many models focused on kinesis, which involves decisions on when to speed up, slow down, or turn in response to detected local conditions (e.g., Gunn and Fraenkel, 1961 ; Sch\u00f6ne, 1984 ; Bell, 1991 ; Gr\u00fcnbaum, 1999 ; Gautestad, 2016 ). Other models used \u201crestricted area search\u201d in which organisms evaluate conditions within a limited area before making a movement choice (e.g., Humston et al., 2004 ). Movement decisions can be combined with decisions on settling at a place or leaving it, for which a variety of modeling approaches are used ( Lima and Zollner, 1996 ). At one extreme, animals may simply move in one direction until they find a spot to settle, or they may use spatial memory and learning to gain knowledge of the landscape such that they can choose the nearest detectable habitat patch ( Fahrig, 1988 ). Decisions on leaving a patch may increase as the level of resources is depleted. Mathematical theory has been applied to predict the optimal time to leave a patch, depending on its resource level relative to other patches and travel costs ( Charnov, 1976 ), or to predict what succession of patches, with varying risks and rewards, to choose in order to maximize fitness over longer times ( Mangel and Clark, 1988 ; Houston and McNamara, 1999 ; Clark and Mangel, 2000 ), an approach referred to as Dynamic State Variable Modeling (DSVM) . 02.P03 \u00b6 advective-dffusion model ( Skalski and Gilliam (2000) ) e.g., fish formation purposeful kinesis population density gradients ( Flierl (1999) ) The step from modeling individuals to modeling collections of individuals could in some cases be done using mathematical models, in which all individuals follow the same basic rules that could be incorporated into PDEs. For example, Skalski and Gilliam (2000) used an advective -diffusion model to simulate patterns of fish formation in which the only decisions involved swimming fast or slow and having an upstream directional bias rather than pure random movement. However, many observed movement patterns of collectives, such as of flocks of birds, schools of fish, swarms of insects, and patterns formed by herding mammals, are more complex. Modeling these patterns requires more than movement decisions based on abiotic conditions, but they can be approximated when the PDEs also incorporate terms that represent decisions to move up or down population density gradients . Such individual movement behavior differs from random walk and can result in various patterns of collections of organisms ( Patterson et al., 2008 ). \u201c Purposeful kinesis \u201d can alter diffusive patterns ( Gorban and \u00c7abukoglu, 2018 ), leading to positive density-dependent diffusion, or \u201c super-diffusion ,\u201d and other variations on diffusion through dependence on population density ( Topaz and Bertozzi, 2004 ; Lutscher, 2008 ; Almeida et al., 2015 ; Tilles and Petrovskii, 2016 ). For example, the phenomenon of clustering, in insect swarms and fish shoals, can occur when individuals accelerate in the direction of a positive density gradient ( Tyutyunov et al., 2004 ). Flierl et al. (1999) provide a general review of mathematical modeling of collective behavior. 02.P04 \u00b6 IFD (Ideal free distribution) e.g., predator-induced defenses Holling type 2 functional responses The influence of individual decisions on the level of whole populations and multi-population systems can also be studied when decision-making is incorporated into models of classical ecology, if the decisions are limited to simple rules, such as optimization of fitness in foraging ( MacArthur and Pianka, 1966 ; Charnov, 1976 ), or in diet selection ( Pulliam, 1974 ) and life history ( Roff, 1992 ). For example, if all individuals foraging on a spatial environment of habitat patches with different resource levels are assumed to move among them until no further movement would increase their fitness, the population would reach what is called an Ideal Free Distribution (IFD) ( Fretwell and Lucas, 1969 ). The IFD concept applies to a wider range of decisions, such as the choice that organisms in a population have in allocating resources in different proportions to foraging, defense, reproduction, etc. A particular example is that of predator-induced defenses , which are known to exist in many ecological systems and may involve changes in both morphology and behavior. The defended individuals are still edible but less so than undefended prey and, as a tradeoff, have lower resource intake rates than undefended prey. Recently it has been shown that the existence of inducible defenses in species at the bottom or middle of the food chain can affect the stability of the food chain, as well as the ability of the top predator to exert top-down control on the system ( Vos et al., 2004 ). In DeAngelis et al. (2007) , a system containing a predator, a prey with an inducible defense, and a resource of the prey, was studied using a differential equation model, with Holling type 2 functional responses describing the trophic interactions. The prey could choose between allocation to induced defense, with the tradeoff of lower resource uptake. The prey population evolved to switching between defense and non-defense, leading to a balance in which certain proportions of prey were in the undefended state and the rest in the defended state, the proportions depending on predator density. The prey in each state had equal fitness, so this was an IFD. This strategy of the prey influenced the populations of the whole food chain. 02.P05 \u00b6 game theory Mathematical models have also represented simple decisions on allocation of time and energy to foraging vs. avoiding or defending against predators ( Abrams, 1982 , 1993 ; Lima and Dill, 1990 ; Abrams and Matsuda, 1993 ; Lima and Zollner, 1996 ; Werner and Peacor, 2003 ). Other models, using differential equations, describe a forager in an environment of several prey, in which the forager could choose which prey to feed on, based on a maximization of long-term food intake (e.g., Feng et al., 2009 ). Another approach of classical mathematical theory in ecology is game theory , which can be used to determine the optimal strategy of an individual when the expected pay-off of a decision (e.g., to \u201cfight or flee\u201d when in a confrontation) depends on the decisions made by other individuals ( Riechert and Hammerstein, 1983 ). In all these cases models showed that optimal decisions taken by members of the populations can have large ecosystem consequences. 03. Agent-based Modeling in Ecology \u00b6 03.P01 \u00b6 2 trends: variability w/i populations in behaviors e.g., behavioral tendencies, personality ABM (Agent-based modeling) The fact that mathematical or analytic models based on a few simple decision rules can explain even complex patterns is remarkable, and such modeling is still a lively and important area of theory. But behaviorists recognized that the capacity of these simple models to represent the real decisions of organisms, determined by a multitude of inputs, was limited, and that inclusion of decision-making was important ( Dill, 1987 ). Two trends in ecology that are relevant to that problem have accelerated over the past two decades. One trend is the increasing recognition of marked variability within populations, not just in age, size, or stage, but also in behaviors of individuals within given classes. In fact, individuals across many taxa appear to have their own personalities, or temporally consistent \u201c behavioral tendencies \u201d ( Biro and Stamps, 2008 ; Beekman and Jordan, 2017 ). The prevalence of behavioral differences, or different personalities, that exists across animal taxa was reviewed in a pivotal paper ( Bolnick et al., 2003 ). Personality differences such as \u201cbold\u201d vs. \u201cconservative\u201d behavior in fish (e.g., Blake et al., 2018 ) exist and involve various aspects of behavior, such as responses to intra-and inter-specific competition ( Ara\u00fajo et al., 2011 ; Dall et al., 2012 ), and tradeoffs between growth and mortality ( Biro and Stamps, 2008 ) and early and late reproduction ( Wolf et al., 2011 ). Bolnick et al. (2011) discuss several ways in which this individual-level specialization can affect community dynamics, which is an impetus to including individual differences in models. Variation in individuals, and therefore in their possible decision-making, casts further doubt on the capability of analytic models to adequately represent real populations. The second trend is the rapid spread of individual- or agent-based modeling (ABM) , which has become an established approach with numerous modeling platforms and is encompassed by a vast literature. The latter development may provide a solution to the impasse of creating models of sophistication comparable to what is known about decision-making. 03.P02 \u00b6 ABM simulate autonomous \"agents\" state var representing internal states (behavioral states) unique history of interactions w/ env & other agents ABMs are ideally suited to accounting for individual differences in organisms. First applied to tree communities ( Botkin et al., 1972 ), in the last few decades ABMs have become well established in all areas of ecology. ABMs simulate the interactions of autonomous \u201cagents,\u201d generally representing individual organisms or other real-world entities, with other agents and with the external environment ( DeAngelis and Mooij, 2005 ). In ABMs, every individual of a population can, in principle, be simulated to almost any level of detail. Each agent may have state variables representing internal states, including behavioral states, and each can have a unique history of interactions with its environment and other agents ( DeAngelis and Grimm, 2014 ). Agent-based modeling attempts to capture the variation among individuals that is relevant to the questions being addressed. In particular, it can incorporate what is known about individual decision-making to explore the consequences in population and community models ( Parunak et al., 1998 ; Railsback, 2001 ; Vincenot, 2018 ). 03.P03 \u00b6 deterministic probabilistic An ABM can be used where decisions are complex and/or are in a setting of populations or communities. The simplest and most straightforward way to represent individual decision-making in an ABM is to utilize logical rules following the \u201cif-then\u201d structure. The behavior of an individual can be modeled when the \u201cif\u201d part contains a condition, and the proceeding \u201cthen\u201d part presents the individual's response. The rules governing decision-making processes can be set up in various ways. Strictly logical, deterministic rules would assign only one possible behavior to an individual in a particular circumstance ( Grimm and Railsback, 2005 ). Alternatively, a rule may be probabilistic , with a different probability for each choice in an array of possible actions in response to some stimulus. Rules may also be a combination of probabilistic and deterministic. For many animals, however, decision-making processes may be more complex, and can be influenced by many variables including the internal state of the individual, that may vary among individuals. As will be discussed later, ABMs are increasingly being used in situations where organisms are assumed to have incomplete knowledge of the surrounding environment, and differences in perception and navigation capacities, but are able to employ cognitive skills to make choices that are good proximate decisions, albeit less than optimal. In this way, ABMs are able to encapsulate the basic underlying principles of the decision-making process in a more realistic way than classical models, and may more effectively represent the way individuals actually decide on actions and how the resulting behaviors shape population-level processes ( Figure 1 ). Figure 1 \u00b6 individual complexity proximate decision-making, uncerteainty, emergence Figure 1. Schematic representation of the models and methodologies utilized to model decision-making in ecology. Along the horizontal axis, these models and methodologies are able to introduce greater individual complexity and represent more complex interactions. Along the vertical axis, the models' entities can be characterized by more proximate decision-making . There is more uncertainty introduced along this axis, but also a greater level of emergence , or higher-level behaviors resulting from lower-level interactions. 04. Movement Decisions and Their Consequences \u00b6 04.P01 \u00b6 compoponents: internal state physiological psychological external env spatially-explicit navigational capacity motion capacity ABMs have been used extensively to simulate the movement behavior of both terrestrial and marine organisms, due in large part to their flexibility in incorporating the various components that influence an individual's movement through space. These components include an individual's internal state, external environment, and navigational and motion capabilities ( Tang and Bennett, 2010 ; Figure 2 ). ABMs can represent a variety of dynamic physiological and psychological state variables comprising an organism's internal state, including commonly used bioenergetic variables (Tang and Bennett, 2010). When simulating movement through space, the external environment is generally spatially-explicit , often represented as grid cells, such that the location of agents and environmental attributes and the spatial relationship between them is explicitly defined ( Duning, 1995 ; Bauer and Klaassen, 2013 ). Figure 2 \u00b6 Figure 2. Schematic illustrating the major components influencing animal movement, all of which can be represented within an ABM ( Baguette et al., 2014 ). 04.P02 \u00b6 2 important decisions: when to move where to move By portraying an individual's dynamic environment and internal state in detail, ABMs can capture the two important decisions that an individual in motion must continually make: when to move , and where to move . The models can also be used to derive the consequences of many organisms moving and interacting with each other at the same time, which gives rise to spatial patterns. 04.01. When to Move? \u00b6 04.01.P01 \u00b6 internal state cond of the current area presence of competition & predation temporal changes prompt agents moement e.g., trout, bear ABMs for movement generally incorporate rules that dictate when an individual agent decides to move from its current location. For real-world organisms, the onset of movement at fine scales may depend on the individual's current internal state, including its physiological and psychological conditions, the condition of the current area that the individual is in, and the presence of competition and predation ( Semeniuk et al., 2011 ; Martin et al., 2013 ; Doherty and Driscoll, 2017 ). As such, ABMs simulating the fine-scale movement of individuals often keep track of temporal changes in the individual agent's internal state and its local surroundings . These changes generally prompt agent movement if they somehow increase (or at the very least, not decrease) the agent's fitness. For example, brown and rainbow trout agents decide to move from their position if the previous day's calculated ratio of mortality risk to growth is greater than expected. Thus, at a given time step, fish agents might move to maximize the ratio of growth to risk of mortality ( Van Winkle et al., 1998 ). Bear agents in an ABM simulating human-bear interactions decide to move to a new location if their current location has a low amount of food, or if they are threatened by human activity ( Marley et al., 2017 ). 04.01.P02 \u00b6 changes of many individuals through time influence movement e.g., green woodhoopoe An individual's decision on when to move may be influenced by the individual's life cycle and social factors. ABMs can track important biological changes of many individuals through time and be utilized to explore how these changes influence movement when placed within a social context. For example, Neuert et al. (1995) developed a model of the territorial group-living green woodhoopoe ( Phoeniculus purpureus ) to address the question of when a subdominant (and thus non-breeding) individual should decide to leave the group and scout for a territory on which it could breed, vs. waiting around to become high enough in status to breed at its natal site. In the model, the decision was based on its own age and rank, where the rank is correlated with age . The simple decision trait of higher propensity to go on scouting forays with increasing age provided the best agreement with empirical data. 04.01.P03 \u00b6 temporal changes in recoucese availability other factors e.g., timing of pink-footed geese migration are influenced by factors: minimal body stores maximal stores date temperature plant phenology fixed duration of stay Large scale movement, such as migration, may be triggered by temporal changes in resource availability ( Van Moorter et al., 2013 ) along with a number of other factors, and ABMs have been utilized to explore the potential decision-rules that may dictate the timing of such behavior for various species. For example, ( Duriez et al., 2009 ) assumed the following factors to be important for the timing of pink-footed geese migration; (1) having minimal body stores, (2) having maximal stores, (3) date, (4) temperature, (5) plant phenology, and (6) fixed duration of stay. The authors found that decision-rules related to food resources were important for dictating the onset of migration, but later in the season, decision-rules related to the geese agents' internal clocks and date are likely used. 04.01.P04 \u00b6 range expansion of population 3 phases initial probability of offspring dispersing from natal cell depending on population density transfer probability dependind on landscape composition of the cell & neghboring animals settlement probability depending on habitat suitability, presence of potential mate, density of conspeceifics Movement is also involved in the range expansion of a population , and Bocedi et al. (2014) modeled range expansion by considering three phases. Within the ABM, there is an initial probability of offspring dispersing from a natal cell , which can depend on population density. Then there is \u201c transfer probability ,\u201d the direction of which is weighted by the costs of moving to each adjacent cell, which depends on landscape composition of the cell and neighboring animals. In the final phase, four alternative strategies were compared to determine which best described settlement probability , and each of which was based on a combination of several factors, including habitat suitability, presence of a potential mate, and density of conspecifics . 04.02. Where to Move? \u00b6 04.02.P01 \u00b6 navigation capacity: links internal state and ecternal var Many organisms can process information about their environment and make movement decisions to satisfy internal desires. Changes in the internal state of an organism may result in changes in the organism's goals, movement decisions, and subsequent movement behavior ( Tang and Bennett, 2010 ). When deciding where to move, mobile animals rely on their navigation capacity, which links the animals' internal states and external variables and manifests itself as either non-oriented, oriented, or memory-driven movement ( Nathan et al., 2008 ; Doherty and Driscoll, 2017 ). The ABM framework lends itself to representing dynamic environmental cues, particularly when the spatiotemporal relationships between the agent and the environment is explicitly represented, as in spatially explicit ABMs. The internal states of individuals can be represented as dynamic state variables and integrated with the cognitive capabilities of individuals, which allows model agents to assess various movement decisions within complex landscapes and ultimately decide on their next destination ( Tang and Bennett, 2010 ). Additionally, some stochasticity in selecting an area to move to is incorporated within many ABMs by using a combination of probabilistic and logical rules, reflecting imperfect knowledge of the environment and perception capabilities. 04.02.P02 \u00b6 e.g., caribous destination cell influenced by daily energetic state reproductive energy requirement predation risk jaguars energy reserves (internal state) hippopotamus Many ABMs simulating animal movement explicitly represent and track various components of an individual agent's internal state in detail, often resulting in movement characteristics that closely mimic those of real-world organisms. For example, Semeniuk et al. (2012) explored potential habitat-selection strategies employed by woodland caribou in response to industrial features in the landscape and represented the caribou's internal states by primarily tracking an individual's energy gain and loss. The caribou's decision on selecting a destination cell was influenced by its daily energetic state, reproductive energy requirement, and predation risk. The way that the internal state of the agent influenced movement varied for each alternative habitat-selection strategy. The authors found that the behavioral strategy concerned with balancing daily energy intake, conserving energy for reproduction, and minimizing predation risk agreed with real-world data better than the other strategies ( Semeniuk et al., 2012 ). Watkins et al. (2015) kept track of energy reserves of jaguar agents within a model landscape representing central Belize . The landscape cells were characterized by attributes including food availability, the presence of marks from other jaguars, and the presence of roads. When agents decide to move, their decision-making process concerning cell selection depends on the habitat attributes underlying a specific cell and the agent's internal state, namely, energy reserves . The individual jaguar's energy reserve levels modulate the preference of different attributes; consequently, agents with high energy reserve levels may decide to move to a cell that does not necessarily have high food availability; see also Lewison and Carter (2004) for an ABM simulating hippopotamus foraging behavior. 04.02.P03 \u00b6 resistance cost When information on the dynamics of an individual's internal state is lacking, it may be appropriate to simulate movement by using simple decision rules based on empirical observations of the resistance that different habitat types may confer to the movement of real-world individuals. For example, Aben et al. (2014) developed and explored the effectiveness of an ABM in simulating forest bird movement, in which a bird agent's selection of a cell (habitat area) to move to at any given time step was partially determined by the land-cover class that characterized a given cell. Land-cover classes conferring more resistance to movement were given higher \u201ccost\u201d values . Spatial cells characterized as having a low cost to birds that are moving through the cells had a higher probability of being selected than cells characterized as having a high cost. Similarly, simple decisions rules governing the selection of destination cells may be based on the quality of the surrounding environment, such that agents generally move toward preferred or favorable areas. In an ABM developed to estimate landscape connectivity for bighorn sheep, each cell in the landscape is represented by landscape attributes including its proximity to escape terrain and the presence of roads ( Allen et al., 2016 ). Bighorn sheep agents have a higher probability of moving to cells closer to escape terrain and away from roads as these cells represent more favorable habitats to real-world bighorn sheep. The characteristics of the surrounding environment also played a major role for agents deciding on a destination in ABMs simulating movement for tiger ( Kanagaraj et al., 2013 ), tortoise ( Anad\u00f3n et al., 2012 ), and capercaillie ( Graf et al., 2007 ). 04.03. Collective Movement Behavior \u00b6 04.03.P01 \u00b6 single-agent multi-agent key to understanding collective behavior: principles of the bahavioral algorithms followed by individual how info flows between animals Many of the above models of animal movement are single-agent ABMs, as opposed to multi-agent ABMs used to simulate collections of interacting individuals. One of the enigmas of natural history is the striking coordinated collective movements of various taxa (birds, insects, fish, mammals, etc.). Investigation of these phenomena is based on the recognition that the movement characteristics of individuals are often influenced by the spatial position and movement of conspecifics, which results in collective movement ( Krause et al., 2010 ). Individuals must make decisions on the initialization and direction of movement, and these decisions somehow incorporate the states of their neighbors. As Sumpter (2006) notes, the key to understanding collective behavior lies in identifying the principles of the behavioral algorithms followed by individual animals and how information flows between the animals . The decision-making process of individuals involved in collective movement has been represented by a spectrum of simple to complex rules within ABMs. 04.03.P02 \u00b6 fish school ( Huth and Wissel (1992) ) rupulsion () attraction () Couzin 2002 : repulsion (move away from nearby neighbors) alignment (adopt the same direction) attractin (avoid becoming isolated) Both Sumpter (2006) and Couzin and Krause (2003) provide reviews of the use of ABM in describing collective behaviors of organisms based on relatively small sets of decision rules. In an early use of a spatially explicit ABM, Huth and Wissel (1992) showed that the complex movement of fish schools could be described by a few rules; the fish tend to keep a certain distance between themselves and a number of its close neighbors and move parallel to other fish when they are within a certain range. The fish agents decide to move away from neighbors if they are too close (repulsion) or align themselves with their neighbors if they are too far (attraction). Size sorting also occurs in fish schools, which may be represented by simple rules, where individuals tend to actively stay with individuals of similar size and avoid those of a different size ( Hemelrijk and Kunz, 2005 ). Couzin et al. (2002) proposed a similarly simple model in which individual animals followed three rules of thumb; (1) move away from very nearby neighbors, (2) adopt the same direction as those that are close by, and (3) avoid becoming isolated; thus, the authors utilized the simple rules of repulsion, alignment, and attraction. In taking into account the location of an individual's neighbors at all times, the authors' model was able to reproduce collective behavior often seen in nature, including swarm ing and torus behavior. 04.03.P03 \u00b6 Gueron 1996 : stress zone attraction zone neutral zone rear zone Hoare 2004 : \"zones of interactions\" influence the size of groups Dynamics of a small herd of mammal browsers were described by a more complex decision tree than those used by Huth and Wissel (1992) , and Couzin et al. (2002) . Gueron et al. (1996) integrated a combination of \u201cstress zones,\u201d \u201cattraction zones,\u201d \u201cneutral zones,\u201d and \u201c rear zones\u201d in the decision-making process of individuals moving within a herd. The various zones corresponded to specific distances extending from each individual, and each individual could invade the zones of other individuals. Probabilities and directions of movement depended on the type of zone an individual invaded. For example, if an individual sensed another individual within its \u201cstress zone,\u201d the individual slowed down to avoid collision. If neighbors were all on one side of the individual, the individual moved toward neighbors. Even though this sort of collective behavior is a much looser kind of group cohesion than fish schooling, the ABM showed that by simply integrating information about an individual's neighbors into the decision rules, many patterns of collective movement could emerge. Hoare et al. (2004) used a similar model to explain the group size distributions of fish, where dynamic \u201czones of interactions\u201d strongly influence the size of groups in simulations. 05. Foraging Decisions and Population Interactions \u00b6 05.P01 \u00b6 temporal / spatial patterns The use of ABM in modeling foraging behavior changes the emphasis from predicting the fitness of the individual based on its decisions to predicting temporal and spatial patterns formed by large collections of individuals foraging and competing for resources (though sometimes interacting positively). 05.P02 \u00b6 observations of real population in real locations A common, though not universal, characteristic of ABMs is that they are based on observations of real populations in real locations. Thus, the assumptions built into the models constitute hypotheses that can be tested against observational data. A few models will be mentioned here: Barnacle geese (Branta leucopsis) in Helgeland, Norway, white-fronted geese (Anser albifrons) in Lake Miyajimanuma, Japan, oystercatchers (Haematopus ostralegus) in the Exe estuary, England, salmonids in Little Jones Creek, California, USA, and dusky dolphins (Lagenorrhyncus obscurus) and killer whales (Orcinus orca) near Kaikoura, New Zealand. 05.P03 \u00b6 geese Bioenergetics model Kanarek et al. (2008) modeled barnacle geese that return to the same group of islands each year as a migration stop. The geese are assumed to vary in age, energy reserves, genetic disposition, and spatial memory of previously visited locations. A goose chooses a specific island based on a combination of factors; constraint due to rank in the dominance hierarchy, memories of previously visited sites and past reproductive success, inherited genetic influences toward site faithfulness, and knowledge of the available biomass density. Within the island the goose chooses to forage on a particular patch until its intake rate drops below a certain amount. Once a goose decides to leave a patch, it uses its knowledge to choose where to go next, assigning a score to a patch based on several factors. A bioenergetics model keeps track of the body mass of the goose, and the goose leaves the staging area either when a threshold of the amount of energy stored or when the end of the stopover period is reached. Each year that a goose returns to the same patch, its familiarity and ability to locate food increases as well as its hierarchical rank. 05.P04 \u00b6 dominance hierarchy A dominance hierarchy is also incorporated into the ABM of a migratory bird, the oystercatcher , feeding on mussels in a tidal estuary in England during the winter ( Stillman et al., 1997 ). A population of individual oystercatchers was modeled on a two-dimensional patch, and the individuals were assigned to places in a dominance hierarchy. When two individuals come within a certain distance, and one is handling prey, the other might attack to attempt to steal the prey, prompting the other to fight back, with the dominant individual always winning. The alternative behavior is avoidance, an action taken with higher probability by less dominant individuals. Avoidance subtracts less time from foraging than fighting. The authors assumed that individuals could calculate the costs vs. benefits of a given action and decide accordingly. The results of the model show, in accord with observations, much less incidence of interference than equivalent analytic models, because, given the differences in rank that are incorporated in the ABM, dominants wasted less time avoiding subdominants, and subdominants avoided fighting. 05.P05 \u00b6 white-fronted geese choosing patches already occcupied by conspecifiecs In the case of white-fronted geese, an ABM explored possible positive interactions among the geese ( Amano et al., 2006 ). The model tested the hypothesis that geese foraging in patches usually have no prior information on patch quality before making a choice but can benefit from choosing patches already occupied by conspecifics; i.e., the presence of conspecifics in the patch has the potential positive effect of decreasing the need for vigilance , although it also causes prey to be depleted faster. The authors tested the hypothesis and found that it was better than alternatives foraging models at describing empirical data, indicating that the assumptions are realistic bases of foraging decisions. 05.P06 \u00b6 tradeoff: starvation vs. predation EM (expected maturity) : maximize growth & survival In the classical DSVM theory mentioned earlier, tradeoffs are assumed between risks of starvation and predation. Many ABM foraging models also incorporate these risks. Railsback and Harvey (2002) used ABM to predict empirical patterns of habitat selection in drift-feeding juvenile trout foraging in a stream, based on assumptions concerning their decisions to choose habitat to forage in, given a range of empirical factors; e.g., water depth, velocity, available food, in the stream environment. The risks involved in deciding to move to a given location were starvation, predation by terrestrial animals, predation by fish, high water velocity (which requires energy to stay in place), stranding , and competition. A bioenergetics model was used to follow daily growth, and daily predation mortality depended on the size of the fish and water depth at its location, which gave the current \u201cstate\u201d of the individual. Three foraging strategies were compared, based on the agent's state and prediction of conditions over the near term (thus the authors label their approach \u201cstate and prediction theory\u201d). The strategy that compared well with all observed patterns involved daily decisions that maximized, over a longer term, the \u201cexpected maturity\u201d (EM) , or the product of predicted survival from starvation and other mortality risks and the fraction of reproductive size reached over the period of the 75-day simulation. The EM strategy, in which the fish agent maximizes both growth and survival, gives better results than other maximization criteria, such as maximizing only growth or only survival probability, used in many classical models. 05.P07 \u00b6 These studies all illustrate the ability of ABMs not only to incorporate high amounts of the complexity of real ecological systems and their ability to compare output with empirically observed patterns, but also to derive more realistic pictures of the decision-making process . 05.P08 \u00b6 importance of behavioral decisions of the predator Lima (2002) noted the importance of the behavioral decisions of the predator when studying predator-prey interactions, rather than treating predators as \u201cunresponsive black boxes.\u201d Accordingly, ABMs have also been used to extend the focus of predation risk beyond merely the prey strategy to include feedbacks from the predator or from competitors. A detailed investigation of tradeoffs of dusky dolphins choosing between feeding and avoiding predation by killer whales is given in a spatially explicit ABM of Srinivasan et al. (2010) . The authors explored the fitness costs and benefits of various escape strategies potentially used by dusky dolphins, including the time spent hiding in a refuge from predation and degree of vigilance when feeding. The model included counter strategies of the amount of time a killer whale would wait for a dolphin to emerge from a refuge , so the model contains a version of game theory. Another example of feedback effects, in this case coming from competition, is illustrated by a model of Peacor et al. (2007) . Individual foragers and a common resource for which they compete were simulated, both with and without the presence of a predator. The density of competing foragers affected the tradeoff in foraging. Under low density of competitors, the forager spent 40% of its time eating, but the time eating decreased to 7% when the forager density was high. This outcome resulted from the decreased benefits of foraging, as the resource was reduced by competitors. Such an outcome could not easily be anticipated or incorporated in simple models. A species' response to predation risk can affect whole food chains. An illustration of how behaviors of herbivores can modify top-down effects of predators on plants is the ABM of Schmitz and Booth (1997) for a tri-trophic chain (spiders, grasshoppers, and a food base of two plant types, grass and a herbaceous plant) of individual organisms on a spatial lattice. The spiders could affect the grasshoppers directly, by predation, and indirectly, by causing grasshoppers within a certain radius to move from grass to the safer herb sites. The model allowed the relative effects of direct predation and the predator avoidance by grasshoppers to be examined individually. The model predicted long-term field observations of the predator's top-down effect and showed the importance of including the behavioral response of the herbivore. 06. Social Interactions in Populations \u00b6 06.P01 \u00b6 reproduction parental care territorial / home range defense Social behavior requires decisions involving, among other things, reproduction, parental care, and territorial or home range defense. These, plus a wider range of activities, have been represented in ABMs. 06.P02 \u00b6 population dynamics is influenced by landscaoe structure & life history trait ( Ye (2014) ) Several studies have addressed the question of when to reproduce, incorporating external factors such as population density and the availability and quality of resources in an area ( Stewart et al., 2005 ; Brouwer et al., 2015 ). When deciding to reproduce, individual agents within an ABM often consider information of the surrounding environment. An individual agent's decision to reproduce is frequently represented in a logistical or probabilistic manner, in which agents may decide to reproduce if they meet some requirement or threshold. For example, Ye et al. (2014) used a grid-based spatially explicit ABM to gain insight on how the population dynamics of a virtual species is influenced by landscape structure and various life history traits. In the ABM, the resource share of an individual reflects the number of individuals in a spatial cell and the cell's underlying habitat quality. Only individual agents with a resource fraction over a given threshold may reproduce in a given time step with a given probability, reflecting the importance of competition and the quality of the surrounding habitat in driving reproductive success. 06.P03 \u00b6 The agent's internal state also influences its decision to reproduce. Hancock et al. (2005) developed an ABM to predict the population dynamics of Bornean bearded pigs, tracking the \u201cfatness index\u201d of each pig agent in the model. The reproduction status of each agent varied according to their respective fatness index, which in turn depended on the available food supply and the density of the pig population. A pig agent decides to reproduce if its fatness index is above a specific threshold, such that agents in a large population with low food supply are unlikely to reproduce due to low fatness indices (Hancock et al., 2005). Fish agents within another ABM also took their internal state into consideration in deciding when to spawn. The ratio of actual weight to expected weight of a fish of a given length (a \u201ccondition index\u201d) is tracked for each fish agent, and only agents with ratios over 0.75 were able to spawn (Rashleigh and Grossman, 2005). 06.P04 \u00b6 After reproduction, decisions must be made on parental care. Different strategies of parental investment reflect variation in the decision-making process of individuals, and ultimately have consequences for the dynamics of the population. Decisions involved in provisioning of offspring by bluebirds were studied by Davis et al. (1999), from the point of view of survival of the offspring under different conditions of food availability in the environment. The authors performed model simulations for different levels of food availability by assuming four different strategies on feeding decisions: (1) Feed the smallest first, (2) feed the largest first, or (3) feed the hungriest first, in each case going to the next larger, smaller, or less hungry, respectively, if the offspring representing the first choice was full. These choices were compared to random feeding (strategy 4). Environmental conditions determined which strategy maximizes the nest success, in terms of total biomass of surviving fledglings, with strategies (1), (2), and (3) performing best, successively, with increasing food availability. The random strategy was never the best. 06.P05 \u00b6 The formation and maintenance of territories and home ranges occurs in many taxa. ABMs can simulate the spatiotemporal changes in resource availability throughout the environment and interactions between various agents through time, potentially elucidating the mechanisms underlying territory and home range dynamics. In order to aid conservation planning for the tiger (Panthera tigris) by providing an estimate of population number, location and size of territories, Carter et al. (2015), modeled territory formation of both females and males using a spatially explicit ABM. Females at 3 years of age were assumed to move to a site within 33 km of their natal site, and at least 2 km away from any other female, and to establish the center of her territory where prey density was highest. If the female failed to find such a site, she lowered the limit on neighboring females from 2 to 1 km. A female was assumed to be able to sense the total prey in her territory and in neighboring spatial cells. She could try to add habitat cells to their territories based on a few decision rules, including prey availability and the status of the neighboring female tiger's hierarchical status (avoiding cells near a female of higher status, which is correlated with age). When possible, she adds habitat cells to her territory until the total amount of prey reaches a certain threshold. Another spatially explicit model of home range dynamics is that of Wang and Grimm (2007) for the common shrew (Sorex araneus). Shrews were assumed to continually adjust their territories by sensing the amount of food in a cell of the habitat and the presence of other individuals. Individuals tried to optimize their home range by preferentially selecting cells with high food sources and avoiding cells occupied by other individuals. Acquisition and release of cells followed an optimization procedure, in which the shrew is assumed capable of ranking all the cells in its territory and releasing the worst ones in favor of adding new ones. These simple model rules predicted realistic territories for both tigers and shrews. 07. Developments in the Modeling of Decisions in Population Models \u00b6 07.P01 \u00b6 The papers reviewed above show that ABM is an important departure from earlier mathematical approaches, which, collectively, have been critical theoretical frameworks over the past few decades. In many earlier approaches, the individual was assumed to make a sequence of decisions through time that would result in optimal fitness at some end time. These approaches often assume knowledge of future conditions and ignore the feedbacks on the individuals through changes in the environment created by the individual's own decisions. To incorporate the feedbacks and uncertainty that are present in real ecological systems, a trend in ABMs has been to simulate how proximate decisions are made based on the individual's current state and short-term predictions. This is modeled differently by different modeling groups. As noted earlier, Railsback and Harvey (2002) (see also Railsback et al., 1999) use a \u201cstate and prediction theory\u201d strategy for individuals, which estimate an \u201cexpected survival\u201d over a specified time horizon over which little appreciable change in the environment is expected, considering the risks of predation, starvation, etc., and choosing the behavior that achieves the best fitness over that time. Giske and his group (e.g., Eliassen et al., 2016) find the adaptive behaviors for situations encountered through selection in genetic algorithms (GA) and artificial neural networks (ANNs). Other ABM modeling approaches include the computational system Digital Organisms in a Virtual Ecosystem (DOVE), in which a GA is used to allow phenotypic plasticity to evolve and interact with a dynamic environment (Peacor et al., 2007). 07.P02 \u00b6 Borrowing concepts from machine learning and artificial life studies, modelers such as those noted above have utilized ANN and GA (together referred to as individual-based neural-network-genetic algorithm, or ING techniques) to represent the decision-making processes and consequent behaviors of organisms. Instead of fixed rules governing the way an organism makes decisions, ING techniques are flexible approaches that use principles of neurobiology and natural selection to solve optimization problems, resulting in individuals making adaptive decisions (Huse et al., 1999; Hamblin, 2013). ABMs that use genetic algorithms to govern decisions initialize a population of individuals with different solutions to a given optimization problem. Optimization problems may include finding the decision that will maximize fitness during foraging, or the probability of survival when selecting a patch (Hamblin, 2013). The individuals with the best solutions to the problem are allowed to reproduce through various commonly recognized selection operators, and reproduction continues for many generations. Trebitz (1991) was an early user of a GA-type approach to estimate the optimal spawning time for largemouth bass. In the model a female that spawned too early in spring risked the loss of eggs or larvae due to random cold snaps, while if it spawned too late it risked zooplankton being depleted by the offspring of earlier spawning females. 07.P03 \u00b6 ANNs \u201ctrain\u201d the weights of input data by continuously modifying these weights until the resulting decisions and subsequent behaviors of individuals reach a specific fitness measure. In this way, ANNs and the way they capture the decision-making process of individuals mirror brain functions by \u201clearning\u201d from the outputs of the various input data weight modifications. Once an ANN is trained, it can be used on new input data to determine the decisions and behaviors that satisfy the specific fitness measure (Huse et al., 1999; Lek and Gu\u00e9gan, 1999). In GAs and ANNs, as in the more fixed logical and probabilistic decision-making rules within ABMs discussed earlier, the decision-making processes of individuals are generally geared to optimize some sort of fitness measure, but the decision-making strategy evolves through selection. 07.P04 \u00b6 ANNs have been increasingly used to represent the decision-making processes of organisms, particularly with respect to movement through space. As the decisions of real-world individuals are driven by neuronal responses to internal and external stimuli, neural networks are well-suited to represent decision-making in an individual agent (Eliassen et al., 2016). Once a fitness criterion is identified for a given ABM, the ANN can be trained to find the input weights that correspond to outputs best fitting the fitness criterion (Huse et al., 1999). ANN training can occur in several ways, but here we focus on training with GAs, an approach common within ABMs utilizing ANNs. GAs are optimization tools that utilize the principles of crossing over and mutation to essentially \u201cevolve\u201d the decision-making processes of individuals, without the need for probabilistic or logistical rules. 07.P05 \u00b6 For example, Okunishi et al. (2009) developed an ABM to simulate the growth and large-scale movements of Japanese sardines with the goal of exploring the effects of climate change on the population dynamics of the species, primarily its distribution and production in the North Pacific. The authors utilized an ANN to represent the way fish decide on movement directions when migrating to spawn. The inputs to the ANN were environmental factors known to influence the migration of sardines and included ocean current speed, the distance from land, and the experienced temperature change during subsequent days. Back propagation (a training method where weights are assigned to the various inputs based on a training data set) was one method used to train the ANN on spawning migration trajectories from actual sardines. The authors also used a GA to find the sardine offspring's input weights that would produce optimal outputs through crossing over and mutation of the parent sardine's weights. In training the ANN with a GA, the sardines' decisions representing swimming directions were allowed to evolve through many simulations; subsequently, the authors were able to find optimal combinations of weights that produce realistic migration trajectories. They found that combining back propagation with a GA to determine input weights produced the most realistic migration trajectories, indicating that utilizing the principles underlying the GA (in tandem with observed movement data) adequately captures the decision-making process of migrating sardines (Okunishi et al., 2009). 07.P06 \u00b6 Morales et al. (2005) utilized several ANNs, each in combination with a GA, to determine efficient movement decisions for elk within a spatially explicit ABM. Movement decisions to be made by the elk included when to switch behaviors from foraging to exploring, where to move if foraging or exploring, and which type of plant to consume at a given time if foraging. The fitness measures that characterized the efficiency of elk agent decisions were energy gain for fat reserves and survival probability associated with predation risks. Percent body fat, forage biomass, and local predation risks were a few of the ANN inputs representing both internal and external movement stimuli. Rather than setting specific rules to guide the decision-making processes, the authors allowed the process to evolve over many generations. 07.P07 \u00b6 ANNs were also used to model movement in Mueller and Fagan (2008), who noted three basic types of pattern resulting from movement (or lack thereof): sedentary, migratory, and nomadism. The authors assumed that landscape structure drove individual-level movement types and that there were four gradients in hierarchical order; resource abundance, spatial configuration of resources, temporal variability of resource locations, and temporal predictability of resources. Mueller et al. (2011) followed up by using ANNs to evolutionarily train model organisms to use and combine different types of information representing the different movement behaviors (memory, oriented, and non-oriented movements). 240 individual animals were simulated moving across landscape and making choices of patches and, in doing so, depleting exhaustible resources. Different movement strategies involving combinations of these movement types worked better on different landscape types. After 5,000 generations with inheritance, survivors using unique movement strategies had optimized fitness on particular landscapes. 07.P08 \u00b6 In incorporating optimization of fitness, classic analytic models omit consideration of the immediate, or proximate, complexities that organisms encounter. This allows the integration of individual strategies with ultimate fitness, but the proximate mechanisms through which organisms solve problems are largely ignored (Sih et al., 2004; Fawcett et al., 2013; Eliassen et al., 2016). Animals need to be able to respond quickly and adequately in situations they have never experienced before; that is, the world is too complex for evolution to produce rules for every possible circumstance. It is likely that animals will evolve rules that perform well on average in their natural environment (McNamara and Houston, 2009). One of the basic advantages of ABMs is their ability to incorporate such heuristic, or rule of thumb, decision-making to perform well in complex environments. The GA approach allows one to simulate how such rules evolve. 07.P09 \u00b6 Toward this end Giske et al. (2013) formulated a model based on recent insights from a range of empirical disciplines that sheds light on the processes involved with decision making. In vertebrates, multipurpose rules are arbitrated through an \u201cemotion system,\u201d which describes the integration of information, motivation, and physiological state in determining physiological and behavioral outcomes. These outcomes affect the survival, growth, development, space use, and life history of the organism. Giske et al. (2013) modeled fish, where the fish had the choice of moving among different depths. Fish at deeper depths were generally safer from predation, but food was more limited, though uncertainty was entered into the model by letting both food levels and predation risk vary stochastically from fish generation to generation. The authors employed the survival-circuit concept (LeDoux, 2012), in which emotions form the basis for decisions, and they contribute to the survival or fitness of the organism. The first half of the survival-circuit is \u201cemotional appraisal.\u201d It starts with sensory input, considers motivational impact related to developmental stage, and may potentially activate the organism into a \u201cglobal organismic state,\u201d such that the whole organism is focusing on the situation. The second half of the survival circuit; the \u201cemotional response,\u201d consists of physiological responses and behavior. Physiological activation enables the organism to focus its sensory attention, brain activity, and potentially also bodily functions, such as heartbeat and muscle tension, toward the present situation. Fear and hunger are the emotions, and the ABM includes sensory inputs. The options for a fish are to stay at its current depth or move a short distance upward or downward, as determined by equations incorporating hunger and fear. On the basis of its global organismic state, the motivated fish behaves in a way that maximizes its net neuronal response. 07.P10 \u00b6 The model of Giske et al. (2013) predicted the same general type of spatial distribution patterns that classic optimization and game models produce, such as diel vertical migration with some extension around the average. But the model of Giske et al. (2013) differed from the classic models by allowing the fish to respond on immediate timescales to food and perceived risk, as opposed to optimization criteria involving only long-term goals. For example, danger is avoided because of an evolved proximate preference to stay with others or in darker waters when afraid, while danger is largely ignored when hungry. This is important in a fluctuating environment, where simple rules of thumb couple proximate constraints in determining behavior, with long-term adaptive value. 08. Prospectus \u00b6 08.P01 \u00b6 In his book \u201cSociobiology,\u201d Wilson (1975) foresaw the gradual merger of population biology and behavioral ecology and the growing importance of neurophysiology to the latter (Wilson's Figure 1.2). These developments have been accelerated through ABM and ANN, particularly through using ABM as a means through which population and community dynamics emerge from the adaptive traits of individuals, including \u201chow individuals make decisions in response to other individuals, the environment, or changes in themselves\u201d (Grimm and Railsback, 2005). 08.P02 \u00b6 Future development of ABM in modeling decision-making in ecology will be influenced by the continued refinement of modeling methodology and increases in needed data for parameterizing ABMs. Two of the modeling methodologies, the \u201cstate and predict\u201d (Railsback and Harvey, 2002) and ING (Eliassen et al., 2016) continue to be developed. In particular, a cognitive architecture based on the survival-circuit concept (LeDoux 2012) forms a general modeling framework for linking decision-making to neurobiological mechanisms (Bach and Dayan, 2017; Landsr\u00f8d, 2017; Budaev et al., 2018). Other approaches to providing rules of thumb for individuals, such as Fuzzy Cognitive Maps (FCM), based on Fuzzy Set Theory (Berkes and Berkes, 2009), are also being used in evolving predator-prey systems. In addition, off the shelf ABM modeling programs such as NetLogo and Ecobeaker are making development and use of ABM easier (e.g., Railsback and Grimm, 2011). Concerning relevant data, ABMs generally require a large amount of data at the individual level and site level. Such data are generally only available for ecological systems that have been intensively studied. But technology is rapidly increasing data collection capabilities, and the availability of remote sensing data and the ease with which these data are integrated within the ABM framework facilitates the development of decision rules based on a plethora of environmental attributes that potentially drive animal decisions. This may facilitate wider use of ABM, though this will require more coordinated efforts in making data broadly available to the modeling community (Hampton et al., 2013). 08.P03 \u00b6 Recent advancements in technological tools should facilitate the parameterization of future ABMs developed for simulating dispersal, migration, and local-scale movements of animal groups. In particular, developments in GPS telemetry (and satellite telemetry in general) and biotelemetry devices have allowed for remote tracking of an individual's movement, physiological state, and behavior over long periods of time at relatively fine temporal resolutions (Cooke et al., 2004). This is especially useful considering the challenges ecologists face when attempting to directly observe and study free-ranging animals in a non-invasive manner. Basic movement parameters, including step length and turning angle distributions, can be derived from a time series of relocation data obtained from GPS and satellite telemetry and together shed light onto the limits of an individual's motion capacity. Subsequently, parameters driving agent movement within an ABM can be made more accurate. Bioenergetic models used to track agents' growth and reproduction within ABMs can be further parametrized with information gained from biotelemetry monitoring of undisturbed animals in their natural environments. The wealth of data generated for many species thus far should allow ecologists to more accurately model how individuals decide on an action in response to dynamic internal and external variables. What will most define the future success of ABM will be its ability to address complex questions that are difficult for traditional approaches. These questions include, among many others, the origin of collective actions, the interactions of tradeoffs at the individual level and the whole ecological system, the dynamics of complex social systems, and applications to conservation and management issues. 08.P04 \u00b6 How collective actions such as migration are initiated, usually by a small fraction of leaders, has been discussed as resulting from differences in experience within populations (Ferno et al., 1998; Krause et al., 2000). Collective actions that benefit the whole group, but which may have negative fitness consequences for individuals, have long attracted attention, but are still not well-understood and may profit from use of ABM. An example is \u201cmobbing action\u201d by birds against nest threats. Unrelated birds and even birds of different species may be involved in these aggressive actions against a threat to nests, such as hawks or owls. While there is clear benefit to each individual in protecting its nest, there are also costs, such as becoming a victim of the threat or revealing the location of one's nest. Wheatcroft and Price (2018), in an empirical/theoretical study, proposed that collective action in this case can build up among birds whose nest locations are distributed across various distances from the threat. Birds nearest the nest respond first, and others that have nests farther and farther away decide to join as the risk to any individual declines with the growing size of the mob. The authors used a mathematical model with two individuals to analyze the amount of investment in each in aggressiveness to the threat. While the mathematical model provides insight, this is the type of problem where an ABM, which can simulate the actions of many birds with individual variation, could provide output directly comparable with observations of mobbing behavior. 08.P05 \u00b6 Social interactions of at least moderate complexity already occur in several of the models noted above, in which social hierarchies constrain actions, but extension to much more complex interactions is possible using ABM. For example, in many primate troops the type and level of interaction between individuals, whether positive (cooperation) or negative (e.g., fighting) depends on genetic relatedness. Because there is kin-recognition, any interaction, such as a fight between two unrelated individuals, can give rise to a larger complex of interactions involving their relatives (Cheney and Seyfarth, 1992). Modeling the behavior of such societies may require ABM to represent a network of relevant interactions. 08.P06 \u00b6 Behavioral ecologists and modelers like to think that their work can have some useful applications in conservation and wildlife management. A review by Caro (2007) of application of ecological theory found little evidence that many of the applications proposed so far by behavioral ecology, based on hypothetical conditions, would be effective in practice. However, ABM may be able to bridge the gap between theory and real conservation issues (Wood et al., 2015). For example, to elucidate whether specific predation strategies influenced the nesting success of waterfowl, Ringelman (2014) simulated the movements of different predators in a landscape with clumped and dispersed nests. The results of the ABM suggested that managers concerned with nesting success of waterfowl should consider the various behavioral strategies used by nest predators when creating and managing potential nesting habitat. The results highlight the practical applications of ABMs, particularly when utilized as tools for wildlife ecology investigations (some examples of which are detailed in McLane et al., 2011). 08.P07 \u00b6 The above are all areas in which ABM incorporating decision-making has room to expand. ABM is still a young field, and its potential for contributing to the understanding of how decisions are made by animals and how they affect whole ecosystems has yet to be fully exploited. \u00b6 img{width: 51%; float: right;}","title":"190410 DeAngelis D., Diaz S., 2019"},{"location":"190410_DeAngelisD_DiazS_2019/#toc","text":"00. Abstract 01. Introduction 02. Decisions in Classical Population Models 03. Agent-based Modeling in Ecology 04. Movement Decisions and Their Consequences 04.01. When to Move? 04.02. Where to Move? 04.03. Collective Movement Behavior 05. Foraging Decisions and Population Interactions 06. Social Interactions in Populations 07. Developments in the Modeling of Decisions in Population Models 08. Prospectus","title":"ToC"},{"location":"190410_DeAngelisD_DiazS_2019/#00_abstract","text":"","title":"00. Abstract"},{"location":"190410_DeAngelisD_DiazS_2019/#00p01","text":"All basic processes of ecological populations involve decisions; when and where to move, when and what to eat, and whether to fight or flee. Yet decisions and the underlying principles of decision-making have been difficult to integrate into the classical population-level models of ecology. Certainly, there is a long history of modeling individuals' searching behavior, diet selection, or conflict dynamics within social interactions. When all the individuals are given certain simple rules to govern their decision-making processes, the resultant population\u2013level models have yielded important generalizations and theory. But it is also recognized that such models do not represent the way real individuals decide on actions. Factors that influence a decision include the organism's environment with its dynamic rewards and risks , the complex internal state of the organism , and its imperfect knowledge of the environment . In the case of animals, it may also involve complex social factors, and experience and learning , which vary among individuals. The way that all factors are weighed and processed to lead to decisions is a major area of behavioral theory.","title":"00.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#00p02","text":"Individual- / Agent-based model (IBM / ABM) While classic population-level modeling is limited in its ability to integrate decision-making in its actual complexity, the development of individual- or agent-based models (IBM/ABMs) (we use ABM throughout to designate both \u201cagent-based modeling\u201d and an \u201cagent-based model\u201d) has opened the possibility of describing the way that decisions are made, and their effects, in minute detail. Over the years, these models have increased in size and complexity. Current ABMs can simulate thousands of individuals in realistic environments, and with highly detailed internal physiology, perception and ability to process the perceptions and make decisions based on those and their internal states . The implementation of decision-making in ABMs ranges from fairly simple to highly complex; the process of an individual deciding on an action can occur through the use of logical and simple (if-then) rules to more sophisticated neural networks and genetic algorithms . The purpose of this paper is to give an overview of the ways in which decisions are integrated into a variety of ABMs and to give a prospectus on the future of modeling of decisions in ABMs.","title":"00.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#01_introduction","text":"","title":"01. Introduction"},{"location":"190410_DeAngelisD_DiazS_2019/#01p01","text":"classical models: randomly moving w/ growth, reproducitin, mortality, interaction w/ env & other organisms e.g., logistric population model Lotka-Volterra predator-prey and competition model reaction-diffusion partial differential equation (PDE) model decision-making by individuals is unimportant? decision continually made e.g., single-celled animals (paramecia) ( 2009_Wagner ) social ants ( 2006_Deneubourg_Detrain ) The role of decision-making by individual organisms is largely ignored in the classical mathematical models of ecology, such as the logistic population models , Lotka-Volterra predator-prey and competition models , and their many variations. These models, as well as their extensions to space, as reaction-diffusion partial differential equation (PDE) models , treat organisms as randomly moving atoms, with added features of growth, reproduction, mortality, and interactions with their environment and other organisms. Because the classical models of ecological populations have been successful in revealing much about ecological systems, one might ask if decision-making by individuals is unimportant enough that it can be ignored at the population level, at least for simple organisms. But something like decisions are continually made, even by organisms perceived as simple. Bray (2009) notes that single-celled animals, like swimming paramecia, \u201ccontinually encounter different situations\u2026 and have to evaluate their options and assign priorities.\u201d Wagner (2009) asks \u201cDoes the bacterium choose to change direction?\u201d and concludes that this may be a matter of perspective. In more complex organisms, such as social ants, \u201ceach individual is a sensitive unit which can process a lot of information\u201d ( Detrain and Deneubourg, 2006 ). These actions are programmed into the organism's DNA, and are unconscious, but clearly decisions are being made, and we will follow Ydenberg (2010) (cited in Rypstra et al., 2015 ) in calling a decision \u201cwherever one or two (or more) options is/are selected .\u201d","title":"01.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#01p02","text":"How important these decisions are at the population level and above is therefore an important question, even for organisms of low cognitive ability. Our perspective in this review is from that of population and community ecology and how modeling helps link individual behaviors to phenomena at the level of collections of individuals. We begin with a brief overview of how decisions have been incorporated in some classical analytic models of ecology. Then we introduce agent-based modeling (ABM) and describe how it has been used to simulate decision-making in individual movement, foraging behavior, population interactions, and social interactions within populations. This is not intended to be comprehensive, but to touch on a variety of ways ABM is used. Finally, we discuss more recent developments in modeling decisions within population models and present a prospectus for future directions.","title":"01.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#02_decisions_in_classical_population_models","text":"","title":"02. Decisions in Classical Population Models"},{"location":"190410_DeAngelisD_DiazS_2019/#02p01","text":"individual decision \u21d2 population < individual fitness \u21d0 ecological context The use of modeling to address the question of the effect of individual decisions on population level dynamics developed more slowly than modeling of the converse question of how ecological context influences the effects of decisions on individual fitness. The latter has been explored by behavioral ecologists using classical population models for several decades, focusing especially on decisions regarding foraging movement and its effects on the fitness of individuals. Additionally, some models have been able to incorporate decision-making when modeling collections of individuals, whole populations, and even multiple populations.","title":"02.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#02p02","text":"models kinesis model (decisions on when to speed up/slow down/turn in response to detected local cond) restricted are search patch DSVM (Dynamic State Variable Modeling) Movement of animals toward favorable conditions could be decomposed into simple decisions on directed movement, or taxis, in relation to light, temperature, or resource gradients. Because it may be difficult for organisms to detect gradients, but possible to assess conditions at a current location, many models focused on kinesis, which involves decisions on when to speed up, slow down, or turn in response to detected local conditions (e.g., Gunn and Fraenkel, 1961 ; Sch\u00f6ne, 1984 ; Bell, 1991 ; Gr\u00fcnbaum, 1999 ; Gautestad, 2016 ). Other models used \u201crestricted area search\u201d in which organisms evaluate conditions within a limited area before making a movement choice (e.g., Humston et al., 2004 ). Movement decisions can be combined with decisions on settling at a place or leaving it, for which a variety of modeling approaches are used ( Lima and Zollner, 1996 ). At one extreme, animals may simply move in one direction until they find a spot to settle, or they may use spatial memory and learning to gain knowledge of the landscape such that they can choose the nearest detectable habitat patch ( Fahrig, 1988 ). Decisions on leaving a patch may increase as the level of resources is depleted. Mathematical theory has been applied to predict the optimal time to leave a patch, depending on its resource level relative to other patches and travel costs ( Charnov, 1976 ), or to predict what succession of patches, with varying risks and rewards, to choose in order to maximize fitness over longer times ( Mangel and Clark, 1988 ; Houston and McNamara, 1999 ; Clark and Mangel, 2000 ), an approach referred to as Dynamic State Variable Modeling (DSVM) .","title":"02.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#02p03","text":"advective-dffusion model ( Skalski and Gilliam (2000) ) e.g., fish formation purposeful kinesis population density gradients ( Flierl (1999) ) The step from modeling individuals to modeling collections of individuals could in some cases be done using mathematical models, in which all individuals follow the same basic rules that could be incorporated into PDEs. For example, Skalski and Gilliam (2000) used an advective -diffusion model to simulate patterns of fish formation in which the only decisions involved swimming fast or slow and having an upstream directional bias rather than pure random movement. However, many observed movement patterns of collectives, such as of flocks of birds, schools of fish, swarms of insects, and patterns formed by herding mammals, are more complex. Modeling these patterns requires more than movement decisions based on abiotic conditions, but they can be approximated when the PDEs also incorporate terms that represent decisions to move up or down population density gradients . Such individual movement behavior differs from random walk and can result in various patterns of collections of organisms ( Patterson et al., 2008 ). \u201c Purposeful kinesis \u201d can alter diffusive patterns ( Gorban and \u00c7abukoglu, 2018 ), leading to positive density-dependent diffusion, or \u201c super-diffusion ,\u201d and other variations on diffusion through dependence on population density ( Topaz and Bertozzi, 2004 ; Lutscher, 2008 ; Almeida et al., 2015 ; Tilles and Petrovskii, 2016 ). For example, the phenomenon of clustering, in insect swarms and fish shoals, can occur when individuals accelerate in the direction of a positive density gradient ( Tyutyunov et al., 2004 ). Flierl et al. (1999) provide a general review of mathematical modeling of collective behavior.","title":"02.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#02p04","text":"IFD (Ideal free distribution) e.g., predator-induced defenses Holling type 2 functional responses The influence of individual decisions on the level of whole populations and multi-population systems can also be studied when decision-making is incorporated into models of classical ecology, if the decisions are limited to simple rules, such as optimization of fitness in foraging ( MacArthur and Pianka, 1966 ; Charnov, 1976 ), or in diet selection ( Pulliam, 1974 ) and life history ( Roff, 1992 ). For example, if all individuals foraging on a spatial environment of habitat patches with different resource levels are assumed to move among them until no further movement would increase their fitness, the population would reach what is called an Ideal Free Distribution (IFD) ( Fretwell and Lucas, 1969 ). The IFD concept applies to a wider range of decisions, such as the choice that organisms in a population have in allocating resources in different proportions to foraging, defense, reproduction, etc. A particular example is that of predator-induced defenses , which are known to exist in many ecological systems and may involve changes in both morphology and behavior. The defended individuals are still edible but less so than undefended prey and, as a tradeoff, have lower resource intake rates than undefended prey. Recently it has been shown that the existence of inducible defenses in species at the bottom or middle of the food chain can affect the stability of the food chain, as well as the ability of the top predator to exert top-down control on the system ( Vos et al., 2004 ). In DeAngelis et al. (2007) , a system containing a predator, a prey with an inducible defense, and a resource of the prey, was studied using a differential equation model, with Holling type 2 functional responses describing the trophic interactions. The prey could choose between allocation to induced defense, with the tradeoff of lower resource uptake. The prey population evolved to switching between defense and non-defense, leading to a balance in which certain proportions of prey were in the undefended state and the rest in the defended state, the proportions depending on predator density. The prey in each state had equal fitness, so this was an IFD. This strategy of the prey influenced the populations of the whole food chain.","title":"02.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#02p05","text":"game theory Mathematical models have also represented simple decisions on allocation of time and energy to foraging vs. avoiding or defending against predators ( Abrams, 1982 , 1993 ; Lima and Dill, 1990 ; Abrams and Matsuda, 1993 ; Lima and Zollner, 1996 ; Werner and Peacor, 2003 ). Other models, using differential equations, describe a forager in an environment of several prey, in which the forager could choose which prey to feed on, based on a maximization of long-term food intake (e.g., Feng et al., 2009 ). Another approach of classical mathematical theory in ecology is game theory , which can be used to determine the optimal strategy of an individual when the expected pay-off of a decision (e.g., to \u201cfight or flee\u201d when in a confrontation) depends on the decisions made by other individuals ( Riechert and Hammerstein, 1983 ). In all these cases models showed that optimal decisions taken by members of the populations can have large ecosystem consequences.","title":"02.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#03_agent-based_modeling_in_ecology","text":"","title":"03. Agent-based Modeling in Ecology"},{"location":"190410_DeAngelisD_DiazS_2019/#03p01","text":"2 trends: variability w/i populations in behaviors e.g., behavioral tendencies, personality ABM (Agent-based modeling) The fact that mathematical or analytic models based on a few simple decision rules can explain even complex patterns is remarkable, and such modeling is still a lively and important area of theory. But behaviorists recognized that the capacity of these simple models to represent the real decisions of organisms, determined by a multitude of inputs, was limited, and that inclusion of decision-making was important ( Dill, 1987 ). Two trends in ecology that are relevant to that problem have accelerated over the past two decades. One trend is the increasing recognition of marked variability within populations, not just in age, size, or stage, but also in behaviors of individuals within given classes. In fact, individuals across many taxa appear to have their own personalities, or temporally consistent \u201c behavioral tendencies \u201d ( Biro and Stamps, 2008 ; Beekman and Jordan, 2017 ). The prevalence of behavioral differences, or different personalities, that exists across animal taxa was reviewed in a pivotal paper ( Bolnick et al., 2003 ). Personality differences such as \u201cbold\u201d vs. \u201cconservative\u201d behavior in fish (e.g., Blake et al., 2018 ) exist and involve various aspects of behavior, such as responses to intra-and inter-specific competition ( Ara\u00fajo et al., 2011 ; Dall et al., 2012 ), and tradeoffs between growth and mortality ( Biro and Stamps, 2008 ) and early and late reproduction ( Wolf et al., 2011 ). Bolnick et al. (2011) discuss several ways in which this individual-level specialization can affect community dynamics, which is an impetus to including individual differences in models. Variation in individuals, and therefore in their possible decision-making, casts further doubt on the capability of analytic models to adequately represent real populations. The second trend is the rapid spread of individual- or agent-based modeling (ABM) , which has become an established approach with numerous modeling platforms and is encompassed by a vast literature. The latter development may provide a solution to the impasse of creating models of sophistication comparable to what is known about decision-making.","title":"03.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#03p02","text":"ABM simulate autonomous \"agents\" state var representing internal states (behavioral states) unique history of interactions w/ env & other agents ABMs are ideally suited to accounting for individual differences in organisms. First applied to tree communities ( Botkin et al., 1972 ), in the last few decades ABMs have become well established in all areas of ecology. ABMs simulate the interactions of autonomous \u201cagents,\u201d generally representing individual organisms or other real-world entities, with other agents and with the external environment ( DeAngelis and Mooij, 2005 ). In ABMs, every individual of a population can, in principle, be simulated to almost any level of detail. Each agent may have state variables representing internal states, including behavioral states, and each can have a unique history of interactions with its environment and other agents ( DeAngelis and Grimm, 2014 ). Agent-based modeling attempts to capture the variation among individuals that is relevant to the questions being addressed. In particular, it can incorporate what is known about individual decision-making to explore the consequences in population and community models ( Parunak et al., 1998 ; Railsback, 2001 ; Vincenot, 2018 ).","title":"03.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#03p03","text":"deterministic probabilistic An ABM can be used where decisions are complex and/or are in a setting of populations or communities. The simplest and most straightforward way to represent individual decision-making in an ABM is to utilize logical rules following the \u201cif-then\u201d structure. The behavior of an individual can be modeled when the \u201cif\u201d part contains a condition, and the proceeding \u201cthen\u201d part presents the individual's response. The rules governing decision-making processes can be set up in various ways. Strictly logical, deterministic rules would assign only one possible behavior to an individual in a particular circumstance ( Grimm and Railsback, 2005 ). Alternatively, a rule may be probabilistic , with a different probability for each choice in an array of possible actions in response to some stimulus. Rules may also be a combination of probabilistic and deterministic. For many animals, however, decision-making processes may be more complex, and can be influenced by many variables including the internal state of the individual, that may vary among individuals. As will be discussed later, ABMs are increasingly being used in situations where organisms are assumed to have incomplete knowledge of the surrounding environment, and differences in perception and navigation capacities, but are able to employ cognitive skills to make choices that are good proximate decisions, albeit less than optimal. In this way, ABMs are able to encapsulate the basic underlying principles of the decision-making process in a more realistic way than classical models, and may more effectively represent the way individuals actually decide on actions and how the resulting behaviors shape population-level processes ( Figure 1 ).","title":"03.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#figure_1","text":"individual complexity proximate decision-making, uncerteainty, emergence Figure 1. Schematic representation of the models and methodologies utilized to model decision-making in ecology. Along the horizontal axis, these models and methodologies are able to introduce greater individual complexity and represent more complex interactions. Along the vertical axis, the models' entities can be characterized by more proximate decision-making . There is more uncertainty introduced along this axis, but also a greater level of emergence , or higher-level behaviors resulting from lower-level interactions.","title":"Figure 1"},{"location":"190410_DeAngelisD_DiazS_2019/#04_movement_decisions_and_their_consequences","text":"","title":"04. Movement Decisions and Their Consequences"},{"location":"190410_DeAngelisD_DiazS_2019/#04p01","text":"compoponents: internal state physiological psychological external env spatially-explicit navigational capacity motion capacity ABMs have been used extensively to simulate the movement behavior of both terrestrial and marine organisms, due in large part to their flexibility in incorporating the various components that influence an individual's movement through space. These components include an individual's internal state, external environment, and navigational and motion capabilities ( Tang and Bennett, 2010 ; Figure 2 ). ABMs can represent a variety of dynamic physiological and psychological state variables comprising an organism's internal state, including commonly used bioenergetic variables (Tang and Bennett, 2010). When simulating movement through space, the external environment is generally spatially-explicit , often represented as grid cells, such that the location of agents and environmental attributes and the spatial relationship between them is explicitly defined ( Duning, 1995 ; Bauer and Klaassen, 2013 ).","title":"04.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#figure_2","text":"Figure 2. Schematic illustrating the major components influencing animal movement, all of which can be represented within an ABM ( Baguette et al., 2014 ).","title":"Figure 2"},{"location":"190410_DeAngelisD_DiazS_2019/#04p02","text":"2 important decisions: when to move where to move By portraying an individual's dynamic environment and internal state in detail, ABMs can capture the two important decisions that an individual in motion must continually make: when to move , and where to move . The models can also be used to derive the consequences of many organisms moving and interacting with each other at the same time, which gives rise to spatial patterns.","title":"04.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0401_when_to_move","text":"","title":"04.01. When to Move?"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p01","text":"internal state cond of the current area presence of competition & predation temporal changes prompt agents moement e.g., trout, bear ABMs for movement generally incorporate rules that dictate when an individual agent decides to move from its current location. For real-world organisms, the onset of movement at fine scales may depend on the individual's current internal state, including its physiological and psychological conditions, the condition of the current area that the individual is in, and the presence of competition and predation ( Semeniuk et al., 2011 ; Martin et al., 2013 ; Doherty and Driscoll, 2017 ). As such, ABMs simulating the fine-scale movement of individuals often keep track of temporal changes in the individual agent's internal state and its local surroundings . These changes generally prompt agent movement if they somehow increase (or at the very least, not decrease) the agent's fitness. For example, brown and rainbow trout agents decide to move from their position if the previous day's calculated ratio of mortality risk to growth is greater than expected. Thus, at a given time step, fish agents might move to maximize the ratio of growth to risk of mortality ( Van Winkle et al., 1998 ). Bear agents in an ABM simulating human-bear interactions decide to move to a new location if their current location has a low amount of food, or if they are threatened by human activity ( Marley et al., 2017 ).","title":"04.01.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p02","text":"changes of many individuals through time influence movement e.g., green woodhoopoe An individual's decision on when to move may be influenced by the individual's life cycle and social factors. ABMs can track important biological changes of many individuals through time and be utilized to explore how these changes influence movement when placed within a social context. For example, Neuert et al. (1995) developed a model of the territorial group-living green woodhoopoe ( Phoeniculus purpureus ) to address the question of when a subdominant (and thus non-breeding) individual should decide to leave the group and scout for a territory on which it could breed, vs. waiting around to become high enough in status to breed at its natal site. In the model, the decision was based on its own age and rank, where the rank is correlated with age . The simple decision trait of higher propensity to go on scouting forays with increasing age provided the best agreement with empirical data.","title":"04.01.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p03","text":"temporal changes in recoucese availability other factors e.g., timing of pink-footed geese migration are influenced by factors: minimal body stores maximal stores date temperature plant phenology fixed duration of stay Large scale movement, such as migration, may be triggered by temporal changes in resource availability ( Van Moorter et al., 2013 ) along with a number of other factors, and ABMs have been utilized to explore the potential decision-rules that may dictate the timing of such behavior for various species. For example, ( Duriez et al., 2009 ) assumed the following factors to be important for the timing of pink-footed geese migration; (1) having minimal body stores, (2) having maximal stores, (3) date, (4) temperature, (5) plant phenology, and (6) fixed duration of stay. The authors found that decision-rules related to food resources were important for dictating the onset of migration, but later in the season, decision-rules related to the geese agents' internal clocks and date are likely used.","title":"04.01.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#0401p04","text":"range expansion of population 3 phases initial probability of offspring dispersing from natal cell depending on population density transfer probability dependind on landscape composition of the cell & neghboring animals settlement probability depending on habitat suitability, presence of potential mate, density of conspeceifics Movement is also involved in the range expansion of a population , and Bocedi et al. (2014) modeled range expansion by considering three phases. Within the ABM, there is an initial probability of offspring dispersing from a natal cell , which can depend on population density. Then there is \u201c transfer probability ,\u201d the direction of which is weighted by the costs of moving to each adjacent cell, which depends on landscape composition of the cell and neighboring animals. In the final phase, four alternative strategies were compared to determine which best described settlement probability , and each of which was based on a combination of several factors, including habitat suitability, presence of a potential mate, and density of conspecifics .","title":"04.01.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#0402_where_to_move","text":"","title":"04.02. Where to Move?"},{"location":"190410_DeAngelisD_DiazS_2019/#0402p01","text":"navigation capacity: links internal state and ecternal var Many organisms can process information about their environment and make movement decisions to satisfy internal desires. Changes in the internal state of an organism may result in changes in the organism's goals, movement decisions, and subsequent movement behavior ( Tang and Bennett, 2010 ). When deciding where to move, mobile animals rely on their navigation capacity, which links the animals' internal states and external variables and manifests itself as either non-oriented, oriented, or memory-driven movement ( Nathan et al., 2008 ; Doherty and Driscoll, 2017 ). The ABM framework lends itself to representing dynamic environmental cues, particularly when the spatiotemporal relationships between the agent and the environment is explicitly represented, as in spatially explicit ABMs. The internal states of individuals can be represented as dynamic state variables and integrated with the cognitive capabilities of individuals, which allows model agents to assess various movement decisions within complex landscapes and ultimately decide on their next destination ( Tang and Bennett, 2010 ). Additionally, some stochasticity in selecting an area to move to is incorporated within many ABMs by using a combination of probabilistic and logical rules, reflecting imperfect knowledge of the environment and perception capabilities.","title":"04.02.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#0402p02","text":"e.g., caribous destination cell influenced by daily energetic state reproductive energy requirement predation risk jaguars energy reserves (internal state) hippopotamus Many ABMs simulating animal movement explicitly represent and track various components of an individual agent's internal state in detail, often resulting in movement characteristics that closely mimic those of real-world organisms. For example, Semeniuk et al. (2012) explored potential habitat-selection strategies employed by woodland caribou in response to industrial features in the landscape and represented the caribou's internal states by primarily tracking an individual's energy gain and loss. The caribou's decision on selecting a destination cell was influenced by its daily energetic state, reproductive energy requirement, and predation risk. The way that the internal state of the agent influenced movement varied for each alternative habitat-selection strategy. The authors found that the behavioral strategy concerned with balancing daily energy intake, conserving energy for reproduction, and minimizing predation risk agreed with real-world data better than the other strategies ( Semeniuk et al., 2012 ). Watkins et al. (2015) kept track of energy reserves of jaguar agents within a model landscape representing central Belize . The landscape cells were characterized by attributes including food availability, the presence of marks from other jaguars, and the presence of roads. When agents decide to move, their decision-making process concerning cell selection depends on the habitat attributes underlying a specific cell and the agent's internal state, namely, energy reserves . The individual jaguar's energy reserve levels modulate the preference of different attributes; consequently, agents with high energy reserve levels may decide to move to a cell that does not necessarily have high food availability; see also Lewison and Carter (2004) for an ABM simulating hippopotamus foraging behavior.","title":"04.02.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0402p03","text":"resistance cost When information on the dynamics of an individual's internal state is lacking, it may be appropriate to simulate movement by using simple decision rules based on empirical observations of the resistance that different habitat types may confer to the movement of real-world individuals. For example, Aben et al. (2014) developed and explored the effectiveness of an ABM in simulating forest bird movement, in which a bird agent's selection of a cell (habitat area) to move to at any given time step was partially determined by the land-cover class that characterized a given cell. Land-cover classes conferring more resistance to movement were given higher \u201ccost\u201d values . Spatial cells characterized as having a low cost to birds that are moving through the cells had a higher probability of being selected than cells characterized as having a high cost. Similarly, simple decisions rules governing the selection of destination cells may be based on the quality of the surrounding environment, such that agents generally move toward preferred or favorable areas. In an ABM developed to estimate landscape connectivity for bighorn sheep, each cell in the landscape is represented by landscape attributes including its proximity to escape terrain and the presence of roads ( Allen et al., 2016 ). Bighorn sheep agents have a higher probability of moving to cells closer to escape terrain and away from roads as these cells represent more favorable habitats to real-world bighorn sheep. The characteristics of the surrounding environment also played a major role for agents deciding on a destination in ABMs simulating movement for tiger ( Kanagaraj et al., 2013 ), tortoise ( Anad\u00f3n et al., 2012 ), and capercaillie ( Graf et al., 2007 ).","title":"04.02.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#0403_collective_movement_behavior","text":"","title":"04.03. Collective Movement Behavior"},{"location":"190410_DeAngelisD_DiazS_2019/#0403p01","text":"single-agent multi-agent key to understanding collective behavior: principles of the bahavioral algorithms followed by individual how info flows between animals Many of the above models of animal movement are single-agent ABMs, as opposed to multi-agent ABMs used to simulate collections of interacting individuals. One of the enigmas of natural history is the striking coordinated collective movements of various taxa (birds, insects, fish, mammals, etc.). Investigation of these phenomena is based on the recognition that the movement characteristics of individuals are often influenced by the spatial position and movement of conspecifics, which results in collective movement ( Krause et al., 2010 ). Individuals must make decisions on the initialization and direction of movement, and these decisions somehow incorporate the states of their neighbors. As Sumpter (2006) notes, the key to understanding collective behavior lies in identifying the principles of the behavioral algorithms followed by individual animals and how information flows between the animals . The decision-making process of individuals involved in collective movement has been represented by a spectrum of simple to complex rules within ABMs.","title":"04.03.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#0403p02","text":"fish school ( Huth and Wissel (1992) ) rupulsion () attraction () Couzin 2002 : repulsion (move away from nearby neighbors) alignment (adopt the same direction) attractin (avoid becoming isolated) Both Sumpter (2006) and Couzin and Krause (2003) provide reviews of the use of ABM in describing collective behaviors of organisms based on relatively small sets of decision rules. In an early use of a spatially explicit ABM, Huth and Wissel (1992) showed that the complex movement of fish schools could be described by a few rules; the fish tend to keep a certain distance between themselves and a number of its close neighbors and move parallel to other fish when they are within a certain range. The fish agents decide to move away from neighbors if they are too close (repulsion) or align themselves with their neighbors if they are too far (attraction). Size sorting also occurs in fish schools, which may be represented by simple rules, where individuals tend to actively stay with individuals of similar size and avoid those of a different size ( Hemelrijk and Kunz, 2005 ). Couzin et al. (2002) proposed a similarly simple model in which individual animals followed three rules of thumb; (1) move away from very nearby neighbors, (2) adopt the same direction as those that are close by, and (3) avoid becoming isolated; thus, the authors utilized the simple rules of repulsion, alignment, and attraction. In taking into account the location of an individual's neighbors at all times, the authors' model was able to reproduce collective behavior often seen in nature, including swarm ing and torus behavior.","title":"04.03.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#0403p03","text":"Gueron 1996 : stress zone attraction zone neutral zone rear zone Hoare 2004 : \"zones of interactions\" influence the size of groups Dynamics of a small herd of mammal browsers were described by a more complex decision tree than those used by Huth and Wissel (1992) , and Couzin et al. (2002) . Gueron et al. (1996) integrated a combination of \u201cstress zones,\u201d \u201cattraction zones,\u201d \u201cneutral zones,\u201d and \u201c rear zones\u201d in the decision-making process of individuals moving within a herd. The various zones corresponded to specific distances extending from each individual, and each individual could invade the zones of other individuals. Probabilities and directions of movement depended on the type of zone an individual invaded. For example, if an individual sensed another individual within its \u201cstress zone,\u201d the individual slowed down to avoid collision. If neighbors were all on one side of the individual, the individual moved toward neighbors. Even though this sort of collective behavior is a much looser kind of group cohesion than fish schooling, the ABM showed that by simply integrating information about an individual's neighbors into the decision rules, many patterns of collective movement could emerge. Hoare et al. (2004) used a similar model to explain the group size distributions of fish, where dynamic \u201czones of interactions\u201d strongly influence the size of groups in simulations.","title":"04.03.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#05_foraging_decisions_and_population_interactions","text":"","title":"05. Foraging Decisions and Population Interactions"},{"location":"190410_DeAngelisD_DiazS_2019/#05p01","text":"temporal / spatial patterns The use of ABM in modeling foraging behavior changes the emphasis from predicting the fitness of the individual based on its decisions to predicting temporal and spatial patterns formed by large collections of individuals foraging and competing for resources (though sometimes interacting positively).","title":"05.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#05p02","text":"observations of real population in real locations A common, though not universal, characteristic of ABMs is that they are based on observations of real populations in real locations. Thus, the assumptions built into the models constitute hypotheses that can be tested against observational data. A few models will be mentioned here: Barnacle geese (Branta leucopsis) in Helgeland, Norway, white-fronted geese (Anser albifrons) in Lake Miyajimanuma, Japan, oystercatchers (Haematopus ostralegus) in the Exe estuary, England, salmonids in Little Jones Creek, California, USA, and dusky dolphins (Lagenorrhyncus obscurus) and killer whales (Orcinus orca) near Kaikoura, New Zealand.","title":"05.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#05p03","text":"geese Bioenergetics model Kanarek et al. (2008) modeled barnacle geese that return to the same group of islands each year as a migration stop. The geese are assumed to vary in age, energy reserves, genetic disposition, and spatial memory of previously visited locations. A goose chooses a specific island based on a combination of factors; constraint due to rank in the dominance hierarchy, memories of previously visited sites and past reproductive success, inherited genetic influences toward site faithfulness, and knowledge of the available biomass density. Within the island the goose chooses to forage on a particular patch until its intake rate drops below a certain amount. Once a goose decides to leave a patch, it uses its knowledge to choose where to go next, assigning a score to a patch based on several factors. A bioenergetics model keeps track of the body mass of the goose, and the goose leaves the staging area either when a threshold of the amount of energy stored or when the end of the stopover period is reached. Each year that a goose returns to the same patch, its familiarity and ability to locate food increases as well as its hierarchical rank.","title":"05.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#05p04","text":"dominance hierarchy A dominance hierarchy is also incorporated into the ABM of a migratory bird, the oystercatcher , feeding on mussels in a tidal estuary in England during the winter ( Stillman et al., 1997 ). A population of individual oystercatchers was modeled on a two-dimensional patch, and the individuals were assigned to places in a dominance hierarchy. When two individuals come within a certain distance, and one is handling prey, the other might attack to attempt to steal the prey, prompting the other to fight back, with the dominant individual always winning. The alternative behavior is avoidance, an action taken with higher probability by less dominant individuals. Avoidance subtracts less time from foraging than fighting. The authors assumed that individuals could calculate the costs vs. benefits of a given action and decide accordingly. The results of the model show, in accord with observations, much less incidence of interference than equivalent analytic models, because, given the differences in rank that are incorporated in the ABM, dominants wasted less time avoiding subdominants, and subdominants avoided fighting.","title":"05.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#05p05","text":"white-fronted geese choosing patches already occcupied by conspecifiecs In the case of white-fronted geese, an ABM explored possible positive interactions among the geese ( Amano et al., 2006 ). The model tested the hypothesis that geese foraging in patches usually have no prior information on patch quality before making a choice but can benefit from choosing patches already occupied by conspecifics; i.e., the presence of conspecifics in the patch has the potential positive effect of decreasing the need for vigilance , although it also causes prey to be depleted faster. The authors tested the hypothesis and found that it was better than alternatives foraging models at describing empirical data, indicating that the assumptions are realistic bases of foraging decisions.","title":"05.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#05p06","text":"tradeoff: starvation vs. predation EM (expected maturity) : maximize growth & survival In the classical DSVM theory mentioned earlier, tradeoffs are assumed between risks of starvation and predation. Many ABM foraging models also incorporate these risks. Railsback and Harvey (2002) used ABM to predict empirical patterns of habitat selection in drift-feeding juvenile trout foraging in a stream, based on assumptions concerning their decisions to choose habitat to forage in, given a range of empirical factors; e.g., water depth, velocity, available food, in the stream environment. The risks involved in deciding to move to a given location were starvation, predation by terrestrial animals, predation by fish, high water velocity (which requires energy to stay in place), stranding , and competition. A bioenergetics model was used to follow daily growth, and daily predation mortality depended on the size of the fish and water depth at its location, which gave the current \u201cstate\u201d of the individual. Three foraging strategies were compared, based on the agent's state and prediction of conditions over the near term (thus the authors label their approach \u201cstate and prediction theory\u201d). The strategy that compared well with all observed patterns involved daily decisions that maximized, over a longer term, the \u201cexpected maturity\u201d (EM) , or the product of predicted survival from starvation and other mortality risks and the fraction of reproductive size reached over the period of the 75-day simulation. The EM strategy, in which the fish agent maximizes both growth and survival, gives better results than other maximization criteria, such as maximizing only growth or only survival probability, used in many classical models.","title":"05.P06"},{"location":"190410_DeAngelisD_DiazS_2019/#05p07","text":"These studies all illustrate the ability of ABMs not only to incorporate high amounts of the complexity of real ecological systems and their ability to compare output with empirically observed patterns, but also to derive more realistic pictures of the decision-making process .","title":"05.P07"},{"location":"190410_DeAngelisD_DiazS_2019/#05p08","text":"importance of behavioral decisions of the predator Lima (2002) noted the importance of the behavioral decisions of the predator when studying predator-prey interactions, rather than treating predators as \u201cunresponsive black boxes.\u201d Accordingly, ABMs have also been used to extend the focus of predation risk beyond merely the prey strategy to include feedbacks from the predator or from competitors. A detailed investigation of tradeoffs of dusky dolphins choosing between feeding and avoiding predation by killer whales is given in a spatially explicit ABM of Srinivasan et al. (2010) . The authors explored the fitness costs and benefits of various escape strategies potentially used by dusky dolphins, including the time spent hiding in a refuge from predation and degree of vigilance when feeding. The model included counter strategies of the amount of time a killer whale would wait for a dolphin to emerge from a refuge , so the model contains a version of game theory. Another example of feedback effects, in this case coming from competition, is illustrated by a model of Peacor et al. (2007) . Individual foragers and a common resource for which they compete were simulated, both with and without the presence of a predator. The density of competing foragers affected the tradeoff in foraging. Under low density of competitors, the forager spent 40% of its time eating, but the time eating decreased to 7% when the forager density was high. This outcome resulted from the decreased benefits of foraging, as the resource was reduced by competitors. Such an outcome could not easily be anticipated or incorporated in simple models. A species' response to predation risk can affect whole food chains. An illustration of how behaviors of herbivores can modify top-down effects of predators on plants is the ABM of Schmitz and Booth (1997) for a tri-trophic chain (spiders, grasshoppers, and a food base of two plant types, grass and a herbaceous plant) of individual organisms on a spatial lattice. The spiders could affect the grasshoppers directly, by predation, and indirectly, by causing grasshoppers within a certain radius to move from grass to the safer herb sites. The model allowed the relative effects of direct predation and the predator avoidance by grasshoppers to be examined individually. The model predicted long-term field observations of the predator's top-down effect and showed the importance of including the behavioral response of the herbivore.","title":"05.P08"},{"location":"190410_DeAngelisD_DiazS_2019/#06_social_interactions_in_populations","text":"","title":"06. Social Interactions in Populations"},{"location":"190410_DeAngelisD_DiazS_2019/#06p01","text":"reproduction parental care territorial / home range defense Social behavior requires decisions involving, among other things, reproduction, parental care, and territorial or home range defense. These, plus a wider range of activities, have been represented in ABMs.","title":"06.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#06p02","text":"population dynamics is influenced by landscaoe structure & life history trait ( Ye (2014) ) Several studies have addressed the question of when to reproduce, incorporating external factors such as population density and the availability and quality of resources in an area ( Stewart et al., 2005 ; Brouwer et al., 2015 ). When deciding to reproduce, individual agents within an ABM often consider information of the surrounding environment. An individual agent's decision to reproduce is frequently represented in a logistical or probabilistic manner, in which agents may decide to reproduce if they meet some requirement or threshold. For example, Ye et al. (2014) used a grid-based spatially explicit ABM to gain insight on how the population dynamics of a virtual species is influenced by landscape structure and various life history traits. In the ABM, the resource share of an individual reflects the number of individuals in a spatial cell and the cell's underlying habitat quality. Only individual agents with a resource fraction over a given threshold may reproduce in a given time step with a given probability, reflecting the importance of competition and the quality of the surrounding habitat in driving reproductive success.","title":"06.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#06p03","text":"The agent's internal state also influences its decision to reproduce. Hancock et al. (2005) developed an ABM to predict the population dynamics of Bornean bearded pigs, tracking the \u201cfatness index\u201d of each pig agent in the model. The reproduction status of each agent varied according to their respective fatness index, which in turn depended on the available food supply and the density of the pig population. A pig agent decides to reproduce if its fatness index is above a specific threshold, such that agents in a large population with low food supply are unlikely to reproduce due to low fatness indices (Hancock et al., 2005). Fish agents within another ABM also took their internal state into consideration in deciding when to spawn. The ratio of actual weight to expected weight of a fish of a given length (a \u201ccondition index\u201d) is tracked for each fish agent, and only agents with ratios over 0.75 were able to spawn (Rashleigh and Grossman, 2005).","title":"06.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#06p04","text":"After reproduction, decisions must be made on parental care. Different strategies of parental investment reflect variation in the decision-making process of individuals, and ultimately have consequences for the dynamics of the population. Decisions involved in provisioning of offspring by bluebirds were studied by Davis et al. (1999), from the point of view of survival of the offspring under different conditions of food availability in the environment. The authors performed model simulations for different levels of food availability by assuming four different strategies on feeding decisions: (1) Feed the smallest first, (2) feed the largest first, or (3) feed the hungriest first, in each case going to the next larger, smaller, or less hungry, respectively, if the offspring representing the first choice was full. These choices were compared to random feeding (strategy 4). Environmental conditions determined which strategy maximizes the nest success, in terms of total biomass of surviving fledglings, with strategies (1), (2), and (3) performing best, successively, with increasing food availability. The random strategy was never the best.","title":"06.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#06p05","text":"The formation and maintenance of territories and home ranges occurs in many taxa. ABMs can simulate the spatiotemporal changes in resource availability throughout the environment and interactions between various agents through time, potentially elucidating the mechanisms underlying territory and home range dynamics. In order to aid conservation planning for the tiger (Panthera tigris) by providing an estimate of population number, location and size of territories, Carter et al. (2015), modeled territory formation of both females and males using a spatially explicit ABM. Females at 3 years of age were assumed to move to a site within 33 km of their natal site, and at least 2 km away from any other female, and to establish the center of her territory where prey density was highest. If the female failed to find such a site, she lowered the limit on neighboring females from 2 to 1 km. A female was assumed to be able to sense the total prey in her territory and in neighboring spatial cells. She could try to add habitat cells to their territories based on a few decision rules, including prey availability and the status of the neighboring female tiger's hierarchical status (avoiding cells near a female of higher status, which is correlated with age). When possible, she adds habitat cells to her territory until the total amount of prey reaches a certain threshold. Another spatially explicit model of home range dynamics is that of Wang and Grimm (2007) for the common shrew (Sorex araneus). Shrews were assumed to continually adjust their territories by sensing the amount of food in a cell of the habitat and the presence of other individuals. Individuals tried to optimize their home range by preferentially selecting cells with high food sources and avoiding cells occupied by other individuals. Acquisition and release of cells followed an optimization procedure, in which the shrew is assumed capable of ranking all the cells in its territory and releasing the worst ones in favor of adding new ones. These simple model rules predicted realistic territories for both tigers and shrews.","title":"06.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#07_developments_in_the_modeling_of_decisions_in_population_models","text":"","title":"07. Developments in the Modeling of Decisions in Population Models"},{"location":"190410_DeAngelisD_DiazS_2019/#07p01","text":"The papers reviewed above show that ABM is an important departure from earlier mathematical approaches, which, collectively, have been critical theoretical frameworks over the past few decades. In many earlier approaches, the individual was assumed to make a sequence of decisions through time that would result in optimal fitness at some end time. These approaches often assume knowledge of future conditions and ignore the feedbacks on the individuals through changes in the environment created by the individual's own decisions. To incorporate the feedbacks and uncertainty that are present in real ecological systems, a trend in ABMs has been to simulate how proximate decisions are made based on the individual's current state and short-term predictions. This is modeled differently by different modeling groups. As noted earlier, Railsback and Harvey (2002) (see also Railsback et al., 1999) use a \u201cstate and prediction theory\u201d strategy for individuals, which estimate an \u201cexpected survival\u201d over a specified time horizon over which little appreciable change in the environment is expected, considering the risks of predation, starvation, etc., and choosing the behavior that achieves the best fitness over that time. Giske and his group (e.g., Eliassen et al., 2016) find the adaptive behaviors for situations encountered through selection in genetic algorithms (GA) and artificial neural networks (ANNs). Other ABM modeling approaches include the computational system Digital Organisms in a Virtual Ecosystem (DOVE), in which a GA is used to allow phenotypic plasticity to evolve and interact with a dynamic environment (Peacor et al., 2007).","title":"07.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#07p02","text":"Borrowing concepts from machine learning and artificial life studies, modelers such as those noted above have utilized ANN and GA (together referred to as individual-based neural-network-genetic algorithm, or ING techniques) to represent the decision-making processes and consequent behaviors of organisms. Instead of fixed rules governing the way an organism makes decisions, ING techniques are flexible approaches that use principles of neurobiology and natural selection to solve optimization problems, resulting in individuals making adaptive decisions (Huse et al., 1999; Hamblin, 2013). ABMs that use genetic algorithms to govern decisions initialize a population of individuals with different solutions to a given optimization problem. Optimization problems may include finding the decision that will maximize fitness during foraging, or the probability of survival when selecting a patch (Hamblin, 2013). The individuals with the best solutions to the problem are allowed to reproduce through various commonly recognized selection operators, and reproduction continues for many generations. Trebitz (1991) was an early user of a GA-type approach to estimate the optimal spawning time for largemouth bass. In the model a female that spawned too early in spring risked the loss of eggs or larvae due to random cold snaps, while if it spawned too late it risked zooplankton being depleted by the offspring of earlier spawning females.","title":"07.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#07p03","text":"ANNs \u201ctrain\u201d the weights of input data by continuously modifying these weights until the resulting decisions and subsequent behaviors of individuals reach a specific fitness measure. In this way, ANNs and the way they capture the decision-making process of individuals mirror brain functions by \u201clearning\u201d from the outputs of the various input data weight modifications. Once an ANN is trained, it can be used on new input data to determine the decisions and behaviors that satisfy the specific fitness measure (Huse et al., 1999; Lek and Gu\u00e9gan, 1999). In GAs and ANNs, as in the more fixed logical and probabilistic decision-making rules within ABMs discussed earlier, the decision-making processes of individuals are generally geared to optimize some sort of fitness measure, but the decision-making strategy evolves through selection.","title":"07.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#07p04","text":"ANNs have been increasingly used to represent the decision-making processes of organisms, particularly with respect to movement through space. As the decisions of real-world individuals are driven by neuronal responses to internal and external stimuli, neural networks are well-suited to represent decision-making in an individual agent (Eliassen et al., 2016). Once a fitness criterion is identified for a given ABM, the ANN can be trained to find the input weights that correspond to outputs best fitting the fitness criterion (Huse et al., 1999). ANN training can occur in several ways, but here we focus on training with GAs, an approach common within ABMs utilizing ANNs. GAs are optimization tools that utilize the principles of crossing over and mutation to essentially \u201cevolve\u201d the decision-making processes of individuals, without the need for probabilistic or logistical rules.","title":"07.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#07p05","text":"For example, Okunishi et al. (2009) developed an ABM to simulate the growth and large-scale movements of Japanese sardines with the goal of exploring the effects of climate change on the population dynamics of the species, primarily its distribution and production in the North Pacific. The authors utilized an ANN to represent the way fish decide on movement directions when migrating to spawn. The inputs to the ANN were environmental factors known to influence the migration of sardines and included ocean current speed, the distance from land, and the experienced temperature change during subsequent days. Back propagation (a training method where weights are assigned to the various inputs based on a training data set) was one method used to train the ANN on spawning migration trajectories from actual sardines. The authors also used a GA to find the sardine offspring's input weights that would produce optimal outputs through crossing over and mutation of the parent sardine's weights. In training the ANN with a GA, the sardines' decisions representing swimming directions were allowed to evolve through many simulations; subsequently, the authors were able to find optimal combinations of weights that produce realistic migration trajectories. They found that combining back propagation with a GA to determine input weights produced the most realistic migration trajectories, indicating that utilizing the principles underlying the GA (in tandem with observed movement data) adequately captures the decision-making process of migrating sardines (Okunishi et al., 2009).","title":"07.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#07p06","text":"Morales et al. (2005) utilized several ANNs, each in combination with a GA, to determine efficient movement decisions for elk within a spatially explicit ABM. Movement decisions to be made by the elk included when to switch behaviors from foraging to exploring, where to move if foraging or exploring, and which type of plant to consume at a given time if foraging. The fitness measures that characterized the efficiency of elk agent decisions were energy gain for fat reserves and survival probability associated with predation risks. Percent body fat, forage biomass, and local predation risks were a few of the ANN inputs representing both internal and external movement stimuli. Rather than setting specific rules to guide the decision-making processes, the authors allowed the process to evolve over many generations.","title":"07.P06"},{"location":"190410_DeAngelisD_DiazS_2019/#07p07","text":"ANNs were also used to model movement in Mueller and Fagan (2008), who noted three basic types of pattern resulting from movement (or lack thereof): sedentary, migratory, and nomadism. The authors assumed that landscape structure drove individual-level movement types and that there were four gradients in hierarchical order; resource abundance, spatial configuration of resources, temporal variability of resource locations, and temporal predictability of resources. Mueller et al. (2011) followed up by using ANNs to evolutionarily train model organisms to use and combine different types of information representing the different movement behaviors (memory, oriented, and non-oriented movements). 240 individual animals were simulated moving across landscape and making choices of patches and, in doing so, depleting exhaustible resources. Different movement strategies involving combinations of these movement types worked better on different landscape types. After 5,000 generations with inheritance, survivors using unique movement strategies had optimized fitness on particular landscapes.","title":"07.P07"},{"location":"190410_DeAngelisD_DiazS_2019/#07p08","text":"In incorporating optimization of fitness, classic analytic models omit consideration of the immediate, or proximate, complexities that organisms encounter. This allows the integration of individual strategies with ultimate fitness, but the proximate mechanisms through which organisms solve problems are largely ignored (Sih et al., 2004; Fawcett et al., 2013; Eliassen et al., 2016). Animals need to be able to respond quickly and adequately in situations they have never experienced before; that is, the world is too complex for evolution to produce rules for every possible circumstance. It is likely that animals will evolve rules that perform well on average in their natural environment (McNamara and Houston, 2009). One of the basic advantages of ABMs is their ability to incorporate such heuristic, or rule of thumb, decision-making to perform well in complex environments. The GA approach allows one to simulate how such rules evolve.","title":"07.P08"},{"location":"190410_DeAngelisD_DiazS_2019/#07p09","text":"Toward this end Giske et al. (2013) formulated a model based on recent insights from a range of empirical disciplines that sheds light on the processes involved with decision making. In vertebrates, multipurpose rules are arbitrated through an \u201cemotion system,\u201d which describes the integration of information, motivation, and physiological state in determining physiological and behavioral outcomes. These outcomes affect the survival, growth, development, space use, and life history of the organism. Giske et al. (2013) modeled fish, where the fish had the choice of moving among different depths. Fish at deeper depths were generally safer from predation, but food was more limited, though uncertainty was entered into the model by letting both food levels and predation risk vary stochastically from fish generation to generation. The authors employed the survival-circuit concept (LeDoux, 2012), in which emotions form the basis for decisions, and they contribute to the survival or fitness of the organism. The first half of the survival-circuit is \u201cemotional appraisal.\u201d It starts with sensory input, considers motivational impact related to developmental stage, and may potentially activate the organism into a \u201cglobal organismic state,\u201d such that the whole organism is focusing on the situation. The second half of the survival circuit; the \u201cemotional response,\u201d consists of physiological responses and behavior. Physiological activation enables the organism to focus its sensory attention, brain activity, and potentially also bodily functions, such as heartbeat and muscle tension, toward the present situation. Fear and hunger are the emotions, and the ABM includes sensory inputs. The options for a fish are to stay at its current depth or move a short distance upward or downward, as determined by equations incorporating hunger and fear. On the basis of its global organismic state, the motivated fish behaves in a way that maximizes its net neuronal response.","title":"07.P09"},{"location":"190410_DeAngelisD_DiazS_2019/#07p10","text":"The model of Giske et al. (2013) predicted the same general type of spatial distribution patterns that classic optimization and game models produce, such as diel vertical migration with some extension around the average. But the model of Giske et al. (2013) differed from the classic models by allowing the fish to respond on immediate timescales to food and perceived risk, as opposed to optimization criteria involving only long-term goals. For example, danger is avoided because of an evolved proximate preference to stay with others or in darker waters when afraid, while danger is largely ignored when hungry. This is important in a fluctuating environment, where simple rules of thumb couple proximate constraints in determining behavior, with long-term adaptive value.","title":"07.P10"},{"location":"190410_DeAngelisD_DiazS_2019/#08_prospectus","text":"","title":"08. Prospectus"},{"location":"190410_DeAngelisD_DiazS_2019/#08p01","text":"In his book \u201cSociobiology,\u201d Wilson (1975) foresaw the gradual merger of population biology and behavioral ecology and the growing importance of neurophysiology to the latter (Wilson's Figure 1.2). These developments have been accelerated through ABM and ANN, particularly through using ABM as a means through which population and community dynamics emerge from the adaptive traits of individuals, including \u201chow individuals make decisions in response to other individuals, the environment, or changes in themselves\u201d (Grimm and Railsback, 2005).","title":"08.P01"},{"location":"190410_DeAngelisD_DiazS_2019/#08p02","text":"Future development of ABM in modeling decision-making in ecology will be influenced by the continued refinement of modeling methodology and increases in needed data for parameterizing ABMs. Two of the modeling methodologies, the \u201cstate and predict\u201d (Railsback and Harvey, 2002) and ING (Eliassen et al., 2016) continue to be developed. In particular, a cognitive architecture based on the survival-circuit concept (LeDoux 2012) forms a general modeling framework for linking decision-making to neurobiological mechanisms (Bach and Dayan, 2017; Landsr\u00f8d, 2017; Budaev et al., 2018). Other approaches to providing rules of thumb for individuals, such as Fuzzy Cognitive Maps (FCM), based on Fuzzy Set Theory (Berkes and Berkes, 2009), are also being used in evolving predator-prey systems. In addition, off the shelf ABM modeling programs such as NetLogo and Ecobeaker are making development and use of ABM easier (e.g., Railsback and Grimm, 2011). Concerning relevant data, ABMs generally require a large amount of data at the individual level and site level. Such data are generally only available for ecological systems that have been intensively studied. But technology is rapidly increasing data collection capabilities, and the availability of remote sensing data and the ease with which these data are integrated within the ABM framework facilitates the development of decision rules based on a plethora of environmental attributes that potentially drive animal decisions. This may facilitate wider use of ABM, though this will require more coordinated efforts in making data broadly available to the modeling community (Hampton et al., 2013).","title":"08.P02"},{"location":"190410_DeAngelisD_DiazS_2019/#08p03","text":"Recent advancements in technological tools should facilitate the parameterization of future ABMs developed for simulating dispersal, migration, and local-scale movements of animal groups. In particular, developments in GPS telemetry (and satellite telemetry in general) and biotelemetry devices have allowed for remote tracking of an individual's movement, physiological state, and behavior over long periods of time at relatively fine temporal resolutions (Cooke et al., 2004). This is especially useful considering the challenges ecologists face when attempting to directly observe and study free-ranging animals in a non-invasive manner. Basic movement parameters, including step length and turning angle distributions, can be derived from a time series of relocation data obtained from GPS and satellite telemetry and together shed light onto the limits of an individual's motion capacity. Subsequently, parameters driving agent movement within an ABM can be made more accurate. Bioenergetic models used to track agents' growth and reproduction within ABMs can be further parametrized with information gained from biotelemetry monitoring of undisturbed animals in their natural environments. The wealth of data generated for many species thus far should allow ecologists to more accurately model how individuals decide on an action in response to dynamic internal and external variables. What will most define the future success of ABM will be its ability to address complex questions that are difficult for traditional approaches. These questions include, among many others, the origin of collective actions, the interactions of tradeoffs at the individual level and the whole ecological system, the dynamics of complex social systems, and applications to conservation and management issues.","title":"08.P03"},{"location":"190410_DeAngelisD_DiazS_2019/#08p04","text":"How collective actions such as migration are initiated, usually by a small fraction of leaders, has been discussed as resulting from differences in experience within populations (Ferno et al., 1998; Krause et al., 2000). Collective actions that benefit the whole group, but which may have negative fitness consequences for individuals, have long attracted attention, but are still not well-understood and may profit from use of ABM. An example is \u201cmobbing action\u201d by birds against nest threats. Unrelated birds and even birds of different species may be involved in these aggressive actions against a threat to nests, such as hawks or owls. While there is clear benefit to each individual in protecting its nest, there are also costs, such as becoming a victim of the threat or revealing the location of one's nest. Wheatcroft and Price (2018), in an empirical/theoretical study, proposed that collective action in this case can build up among birds whose nest locations are distributed across various distances from the threat. Birds nearest the nest respond first, and others that have nests farther and farther away decide to join as the risk to any individual declines with the growing size of the mob. The authors used a mathematical model with two individuals to analyze the amount of investment in each in aggressiveness to the threat. While the mathematical model provides insight, this is the type of problem where an ABM, which can simulate the actions of many birds with individual variation, could provide output directly comparable with observations of mobbing behavior.","title":"08.P04"},{"location":"190410_DeAngelisD_DiazS_2019/#08p05","text":"Social interactions of at least moderate complexity already occur in several of the models noted above, in which social hierarchies constrain actions, but extension to much more complex interactions is possible using ABM. For example, in many primate troops the type and level of interaction between individuals, whether positive (cooperation) or negative (e.g., fighting) depends on genetic relatedness. Because there is kin-recognition, any interaction, such as a fight between two unrelated individuals, can give rise to a larger complex of interactions involving their relatives (Cheney and Seyfarth, 1992). Modeling the behavior of such societies may require ABM to represent a network of relevant interactions.","title":"08.P05"},{"location":"190410_DeAngelisD_DiazS_2019/#08p06","text":"Behavioral ecologists and modelers like to think that their work can have some useful applications in conservation and wildlife management. A review by Caro (2007) of application of ecological theory found little evidence that many of the applications proposed so far by behavioral ecology, based on hypothetical conditions, would be effective in practice. However, ABM may be able to bridge the gap between theory and real conservation issues (Wood et al., 2015). For example, to elucidate whether specific predation strategies influenced the nesting success of waterfowl, Ringelman (2014) simulated the movements of different predators in a landscape with clumped and dispersed nests. The results of the ABM suggested that managers concerned with nesting success of waterfowl should consider the various behavioral strategies used by nest predators when creating and managing potential nesting habitat. The results highlight the practical applications of ABMs, particularly when utilized as tools for wildlife ecology investigations (some examples of which are detailed in McLane et al., 2011).","title":"08.P06"},{"location":"190410_DeAngelisD_DiazS_2019/#08p07","text":"The above are all areas in which ABM incorporating decision-making has room to expand. ABM is still a young field, and its potential for contributing to the understanding of how decisions are made by animals and how they affect whole ecosystems has yet to be fully exploited.","title":"08.P07"},{"location":"190427_JaafraY_LaurentJL_2018/","text":"19-04-27 \u00b6 Original | Mendeley 00. Abstract \u00b6 Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are gen- erally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. De- signing such architectures requires significant human expertise, substantial computation time and doesnt always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search. 01. Introduction \u00b6 01.P01 \u00b6 \u201dA neuron is nothing more than a switch with information input and output. The switch will be activated if there are enough stimuli of other neurons hitting the information input. Then, at the information output, a pulse is sent to, for example, other neurons \u201d [1] . Brain-inspired machine learning imitates in a simplified manner the hierarchical operating mode of biological neurons [2] . The concept of artificial neural networks (ANN) achieved a huge progress from its first theoretical proposal in the 1950s until the recent considerable outcomes of deep learning. In computer vision and more specifically in classification tasks, CNN, which we will examine in this review, are among the most popular deep learning techniques since they are outperforming humans in some vision complex tasks [3] . 01.P02 \u00b6 advances: backpropagation algorithm large training datasets / computational resources CNN differ in that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer The origin of CNN that were initially established by [4] goes back to the 1950s with the advent of \u201dperceptron\u201d, the first neural network prototyped by Frank Rosenblatt. However, neural network models were not extensively used until recently, after researchers overcame certain limits. Among these advances we can mention the generalization of perceptrons to many layers [5], the emergence of backpropagation algorithm as an appropriate training method for such architectures [6] and, mainly, the availability of large training datasets and computational resources to learn millions of parameters. CNN differ from classical neural networks in the fact that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer . This selective connection endow the network with the ability to operate, implicitly, hierarchical features extraction. For an image classification case, the first hidden layer can visualize edges, the second a specific shape and so on until the final layer that will identify the object. 01.P03 \u00b6 convolusion pooling fully connected multiple choices: num / order of layers hyperparameter (receptive fild size, stride) in this paper overview of deep learning history of CNN architectures several methods for automating CNN design according to search optimizaiton architecture design methods search acceleration techniques future works CNN architecture consists of several types of layers including convolution, pooling, and fully connected. The network expert has to make multiple choices while designing a CNN such as the number and ordering of layers , the hyperparameters for each type of layer ( receptive field size , stride , etc.). Thus, selecting the appropriate architecture and related hyperparameters requires a trial and error manual search process mainly directed by intuition and experience. Additionally, the number of available choices makes the selection space of CNN architectures extremely wide and impossible for an exhaustive manual exploration. Many research effort in meta-modeling tries to minimize human intervention in designing neural network architectures. In this paper, we first give a general overview and define the field of deep learning. We then briefly survey the history of CNN architectures. In the following section we review several methods for automating CNN design according to three dimensions: search optimization , architecture design methods (plain or modular) and search acceleration techniques . Finally, we conclude the article with a discussion of future works. 02. Background \u00b6 02.P01 \u00b6 Before embarking with CNN, we will introduce in this section some basic generalities about artificial networks and deep learning. 02.01. Artificial Neural Networks \u00b6 02.01.P01 \u00b6 ANN are a major field of artificial intelligence that attempts to replicate human brain processing. Three types of neural layers distinguish an ANN: input, output and hidden layers. The latter operate transitional representa- tions of the input data evolving from low level features (lines and edges) to higher ones (complex patterns) as far as deeper layers are reached. Figure 1 provide an example of ANN involving classical fully-connected layers where every neuron is connected to all ones of the previous layer. Figure 1 \u00b6 [![Fig.1][fig_01]][fig_01] Figure 1: Artificial neural network, containing an input layer, an output layer and two hidden layers. 02.01.P02 \u00b6 weight bias During training, an ANN aims at learning two types of parameters that will condition its predictive performance. First, connection weights that assess to which extent a neuron result will impact the output of higher level neuron. Second, the bias which is a global estimator of a feature presence across all inputs. Hence, a neuron output can be formalized through a linear combination of weighted inputs and associated bias: \\text{output} = \\Big( \\sum_i{ \\text{input}_i \u2217 \\text{weight}_i} \\Big) + \\text{bias} 02.01.P03 \u00b6 activation fn ReLU ( Rectified Linear Unit ) In order to allow the network operating non-linear transformations, an activation function is applied to the previous output. Equation 1 presents an example of such transformation using one of the most common and efficient activation function which is the Rectified Linear Unit ( ReLU ) [7]: f(x) = \\max(x, 0) \\tag{1} 02.02. Deep learning \u00b6 02.02.P01 \u00b6 Deep learning: ML processing w/i multi-layer ANN loss function evalation backpropagation The concept of deep learning refers to machine learning processing within multi-layer ANN [8]. The training of these networks relies on a loss function evaluation . For example, in supervised learning the loss is assimilated to the matching accuracy between ANN predictions and real expected outputs. An iterative update procedure is implemented to adjust network parameters according to loss function computed gradient. This procedure is called Backpropagation since parameters updates are spread from final layers to initial ones. Deep learning implies a certain number of challenges such as vanish- ing/exploding gradient and overfitting. The solutions to these problems will be discussed when developing CNN design architectures in next sections. 03. CNN Layers \u00b6 CNN are widely used in a great number of pattern and image recognition problems. Three main characteristics are making this deep learning technique successful and suitable to visual data. First, local receptive fields perfectly reflect image data specificity to be correlated locally and uncorrelated in global segments. Second, shared weights allows a substantial parameter re- duction without altering image processing since the convolution is applicable to the whole image. Last, grid-structured image enable pooling operations that simplify data without losing useful information [9]. 03.01. Convolutional Layer \u00b6 The convolutional layer is the basic CNN unit that has been inspired by physiological research evidence of hierarchical processing in the visual cor- tex of mammals [10]. Simple cells detect primitive attributes while more compound structures are subsequently extracted by complex cells. Thus, convolutional layer consists of a set of feature maps issued from convolving different filters (kernels) with an input image or previous layer output [11]. The 2-dimensional maps are stacked together to produce the resulting vol- ume of the convolutional layer. This process reduces drastically the network complexity since the neurons of a same feature map share the same weights and bias maintaining a low number of parameters to learn [12]. The hyperparameters characterizing a convolutional layer are the depth F (number of filters), the stride S (filter movement from a receptive field to the next one) and the zero padding P to control input size [13]. Assuming that the filter size ( (\\text{height}, \\text{width}, \\text{depth}) = (h, w, D), the dimensions of the feature maps generated can be obtained according to: \\begin{pmatrix} H_1 & W_1 & F \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{(H + 2P \u2212 h)}{S} + 1 & \\frac{(W + 2P \u2212 w)}{S} + 1 & F \\\\ \\end{pmatrix} Where (H, W, D) is the size (\\text{height}, \\text{width}, \\text{depth}) of the input image. 03.02. Pooling Layer \u00b6 CNN architectures generally alternate convolution and pooling layers. The latter have the purpose of reducing network complexity and avoid the problem of overfitting. At biological level, pooling is assimilated to the behav- ior of cortical complex cells that reveal a certain degree of position invariance. A pooling layer neuron is connected to a region of the previous layer by per- forming a non-parameterized function. Thus it differs from convolution as it doesn\u2019t have learnable weights or bias and additionally, it keeps the same depth of the previous layer. Max pooling [14] is one of the most common type of pooling that consists in retaining the maximum value of a neurons cluster. It means that max pooling is detecting if a given feature has been identified in a receptive field without recording the exact location [9]. 03.03. Fully connected Layer \u00b6 The convolution layers identify local features in the input data such as edges and shapes. The Fully connected layer operates the high level reasoning (classification for image case) by combining information from all the previous layers. As in a regular ANN, neurons at this level are fully connected to all ones in the previous layer. A softmax loss layer is then used to compute the probability distribution of the CNN final outputs. 04. CNN Architecture History \u00b6 This section presents the most influential hand-crafted CNN architectures that have impacted the recent work on automatic architecture design. Most of them won at least one of the \u201dImageNet Large Scale Visual Recognition Competition\u201d (ILSVRC) challenges [3]. 04.01. LeNet \u00b6 As mentioned previously, LeNet [7] was the innovative work that intro- duced convolutional networks. The model was experimented successfully to classify handwritten digits without any preprocessing of the input image (of size 32 \u2217 32 pixels). LeNet architecture is illustrated in figure 2. It consists of an input and an output layers of respective sizes 32 \u2217 32 and 10 as well as 6 hidden Layers. The basic idea of this design is to operate multiple con- volutions (3) with pooling in-between (2) then transmitting the final signal via a fully-connected layer toward the output layer. Unfortunately, due to the lack of adequate training data and computing power, it wasn\u2019t possible to extend this architecture to more complex applications. [![fig.2][fig_02]][fig_02] Figure 2: Architecture of LeNet-5 [7]. 04.02. AlexNet \u00b6 AlexNet [11] is one of the most influential deep CNN that won the ILSVRC (Imagenet Large Scale Visual Recognition Challenge) competitions in 2012. As shown in figure 3, it is not much different from LeNet. Never- theless, the corresponding architecture is deeper with 8 layers in total, 5 con- volutional and 3 fully connected. The effective contribution of AlexNet lies in several design and training specificities. First, it introduced the Rectified Linear Unit (ReLU) nonlinearity which helped to overcome the problem of vanishing gradient and boosted a faster training. Furthermore, AlexNet im- plements a dropout step [15] that consists in setting to zero a predefined per- centage of layers\u2019 parameters. This technique decreases learned parameters and controls neurons correlation in order to limit overfitting impact. Third, training process convergence is accelerated with momentum and conditional learning rate decrease (e.g. when learning stagnates). Finally, training data volume is increased artificially by generating variations of the original images that are shifted randomly. Thus the network learning is enhanced with the use of invariant representations of the data. [![fig.3][fig_03]][fig_03] Figure 3: An illustration of AlexNet [11]. 04.03. VGGNet \u00b6 Submitted for the ILSVRC 2014, VGGNet [16] won the second place and demonstrated that deeper architectures achieve better results. Indeed, with its 19 hidden layers, it was much deeper than previous convolutional networks. In order to allow an increase in depth without an exponential growth of the parameters number, smaller convolution filters (3\u22173) were used in all layers (e.g. lower size than the 11 \u2217 11 filters adopted in AlexNet). An additional advantage of using smaller filters consists in reducing overlapping scanned pixels which results in feature maps with more local details [17]. 04.04. GoogLeNet \u00b6 Since it has been demonstrated that a CNN architecture size is positively correlated to its performance, recent efforts focus on how to increase the depth of a CNN while keeping an acceptable number of parameters. Winner of ILSVRC 2014, GoogLeNet [18] innovated network design by replacing the classical strategy of alternating convolutional and pooling layers with stacked Inception Modules depicted in figure 4. Despite being deeper than VGGNet with 22 hidden layers, GoogLeNet requires outstandingly fewer parameters due to this sparse connection technique. Within an inception module, several convolutions with different scales and pooling are performed in parallel then concatenated in one single layer. This enables the CNN to detect patterns of various sizes within the same layer and avoid heavy parameters redundancies [18]. GoogLeNet hidden layers consist of 3 convolutions, 9 inception blocks (2 layers deep each one) and one fully connected. [![Fig.4][fig_04]][fig_04] Figure 4: Inception module [18]. 04.05. ResNet \u00b6 Deep Residual Network (ResNet) [19], was the first neural network to ex- ceed human-level accuracy in ImageNet Challenge (ILSVRC 2015). Thanks to residual connections, such kind of architecture went deeper and was im- plemented with multiple versions of 34, 50, 101 and 152 layers. Indeed, one of the difficulties with very deep networks training is the vanishing gradi- ent during error backpropagation which penalizes the appropriate update of earlier layers weights. ResNet main contribution consists in dividing con- volutional layers into residual blocks. Each block is bypassed by a residual (skip) connection that forwards the block input using an identity mapping. The final output is the summation of the block output and the mapped input as illustrated in figure 5. By adding skip connections, backpropagation can be operated without any interference with previous layers which allows to prevent vanishing gra- dient and train very deep architectures. ResNet-101 consists of one convolu- tional layer followed by 33 residual blocks (3 layers deep each one), and one fully connected layer. [![Fig.5][fig_05]][fig_05] Figure 5: Residual learning: a building block [19] 04.06. More Networks \u00b6 After ResNet [19] success, which exceeded human-level accuracy in (ILSVRC 2015), the so-called modern hand-crafted CNN are still being designed on the basis of previous models looking for more efficiency and lower training time. Inception-v4 [20] is a new release of GoogLeNet that involved many more layers than the initial version. Inception-ResNet [20] is built as a combi- nation of an Inception network and a ResNet, joining inception blocks and residual connections. The last example of this section is DenseNet (Dense Convolutional Networks) [21] where each dense block layer is connected via skip connections to all subsequent ones allowing the learning of new features. 05. Meta-modeling for CNN automatic architecture design \u00b6 Meta-modeling for neural network architectures design aims at reduc- ing the intervention of human expertise in this process. The earliest meta- modeling methods were based on genetic algorithms and Bayesian optimiza- tion then more recently, reinforcement learning became among the most im- plemented approaches [22]. 05.01. Context of automation \u00b6 The performance of a neural network and particularly a CNN mainly depends on the setting of the model structure, the training process, and the data representation. All of these variables are controlled through a number of hyperparameters and impact the learning process to a large extent. In order to achieve an optimal performance of CNN, these hyperparameters including the depth of the network, learning rates, layer type, number of units per layer, dropout rates, etc., should be then carefully tuned. On the other hand, the advent of deeper and more complex modern architectures (see section 4) is increasing the number and the types of hyperparameters. Hence, tuning step and more generally CNN architectures search become very expensive and heavy for an expert trial-and-error procedure. Additionally, CNN parameters setting is considered as a black-box [23] optimization problem because of the unknown nature of the mapping between the architecture, the performance, and the learning task. In this context, au- tomatic design solutions are highly required and instigates a large volume of research. The task of CNN hyperparameters tuning has been handled through meta-modeling that consists in applying machine learning models for designing CNN architectures. Three meta-modeling approaches are gen- erally used in the literature of architecture search and will be described in the next paragraph: bayesian optimization, evolutionary algorithms and re- inforcement learning. 05.02. Meta-controllers \u00b6 Meta-modeling approaches perform iterative selection from the hyperpa- rameters space and build associated architectures that are then trained and evaluated. Accuracies records are fed to meta-modeling controllers (meta- controllers) to guide next architectures sampling. Meta-controllers for CNN design are mainly based on bayesian optimization ([24], [25]), evolutionary algorithms ([26], [27]) or more recently on reinforcement learning ([28], [29]). Bayesian optimization is an efficient way to optimize black-box objective functions f : X \\rightarrow R that are slow to evaluate [30]. It aims at finding an input x = \\arg \\min_{x \\in X} f(x) that globally minimizes f where in the context of a machine learning algorithm, x refers to the set of hyperparameters to optimize. The problem with this kind of optimization is that evaluating the objective function is very costly due to the great number of hyperparameters and the complex nature of models like deep neural networks. In order to overcome this problem, bayesian approaches propose probabilistic surrogate reconstruction of the objective function p(f|D) where D is a set of past observations. The evaluation of the empirical function is much cheaper than the true objective function [31]. Some of the most used probabilistic surrogate (regression) models are gaussian processes [32], random forests [33] and tree- structured Parzen estimator [24]. Briefly, the processing of a bayesian optimization consists in building an empirical (probabilistic) model of the objective function. Then, iteratively, the model identifies a set of optimal hyperparameters for which the objective function returns corresponding results (e.g. loss values). Each feedback al- lows the update of the surrogate model and the guidance of hyperparameters predictions until the process reaches a termination condition. Evolutionary algorithms present another strategy of hyperparameters op- timization that modifies a set of candidate solutions (population) on the basis of a number of rules (operators). Following an iterative procedure of muta- tion, crossover and selection [34], an evolutionary algorithm initializes, in a first step, a set of N random networks to create a primary population. The second step consists in introducing a fitness function to score each net- work through its classification accuracy and keep the top ranked networks to construct the next generation. The evolutionary process continues until a termination criteria is met, which is generally defined as the maximum num- ber of allowed generations. One of the advantage of evolutionary algorithms is the adaptation to complex combination of discrete (layer type) and contin- uous (learning rate) hyperparameters which is suitable to neuronal network optimization models [35]. An important approach for goal-oriented optimization is reinforcement learning (RL) inspired from behaviorist psychology [36]. The frame of RL is an agent learning through interaction with its environment (figure 6). Thus the agent adapts its behavior (transition to a state s_{t+1} ) on the basis of observed consequences (rewards) of an action at taken in state s_t . The agent purpose is to learn a policy \\pi that is able to identify the optimal sequence of actions maximizing the expected cumulative rewards. The environment return reinforces the agent to select new actions to improve learning process, hence the name of reinforcement learning. [![Fig.6][fig_06]][fig_06] Figure 6: Illustration of the RL process. The methods developed to resolve reinforcement tasks are based on value functions, policy search or a combination of both strategies (actor-critic methods) [37]. Value function methods consist in estimating the expected reward value R when reaching a given state s and following a policy \\pi : V ^\\pi(s) = \\mathbb{E} [\\mathfrak{R}|s, \\pi] A recursive form of this function is particularly used in recent Q-learning [38] models assigned to CNN architecture design ([39], [40]): Q(s_t, a_t) = Q(s_t, a_t) + \\alpha[r_{t + 1} + \\gamma \\text{max}_a Q(s_{t+1}, a) \u2212 Q(s_t, a_t)] Where s_t is a current state, a_t is a current action, \\alpha is the learning rate, r_{t+1} is the reward earned when transitioning from time t to the next and \\gamma is the discount rate. In contrast to value function methods, policy search methods do not implement a value function and apply, instead, a gradient-based procedure to identify directly an optimal policy \\pi^\u2217 . In this context, deep reinforcement learning is achieved when deep neural networks are used to approximate one of the reinforcement learning components : value function, policy or reward function [41]. Among the active fields of designing CNN architectures through deep reinforcement learning, recurrent neural networks (RNN) arise as a valuable model that handles a set of tasks such as hyperparameters prediction ([28], [29]). In fact, a RNN operates sequentially involving hidden units to store processing history, which allows the reinforcement learning to profit from past observations. Long short term memory networks (LSTM), a variant of RNN, offers a more efficient way of evolving conditionally on the basis of previous elements. 06. Neural Architecture Search \u00b6 Various strategies have been developed to operate CNN architectures de- sign for the majority of which reinforcement learning has been selected as meta-controller. This section is assigned to review in detail most recent promising automatic search approaches differentiated according to search spaces specificities and complexity level. 06.01. Plain Architecture Design \u00b6 Some architecture search approaches focus on designing plain CNN which consists exclusively of conventional layers, mainly convolution, pooling and fully-connected. The resulting research space is relatively simple and the approaches contribution lies almost entirely in the design strategy. 06.01.01. MetaQNN \u00b6 MetaQNN model [39] relies on Q-learning, a type of reinforcement learn- ing (refer to previous section for more details), to sequentially select network layers and their parameters among a finite space. This method implies, first, the definition of each learning agent state as a layer with all associated relevant parameters. As an example, 5 layers are depicted in figure 7: convo- lution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM). [![Fig.7][fig_07]][fig_07] Figure 7: State space possible parameters [39]. Second, the agent action space is assimilated to the possible layers the agent may move to given a certain number of constraints set intentionally, for the majority, to enable faster convergence. Figure 8 illustrates a set of state and action spaces and an eventual agent path to design a CNN architecture. MetaQNN was evaluated competitive with similar and different hand-crafted CNN architectures as with existing automated network design methods. [![Fig.8][fig_08]][fig_08] Figure 8: An illustration of the full state and action space (a) and a path that the agent has chosen (b) [39]. 06.01.02. NAS \u00b6 Using reinforcement learning, [28] train a recurrent neural network to generate convolutional architectures. Figure 9 shows a RNN controller gener- ating sequentially CNN parameters associated to convolutional layers. Every sequence output is predicted by a softmax classifier then used as input of the next sequence. The parameters set consists of filter height and width, stride height and width and the number of filters per layer. The design of an ar- chitecture takes an end once the number of layers reaches a predefined value that increases all along training. The accuracy of the designed architecture is fed as a reward to train the RNN controller through reinforcement learning in order to maximize the expected validation accuracy of the next architectures. The experimentation of the global approach achieved competitive results on CIFAR-10 and Penn Treebank datasets. [![Fig.9][fig_09]][fig_09] Figure 9: Illustration of the way the agent used to select hyperparameters [28]. 06.01.03. EAS \u00b6 In their very recent work Efficient Architecture Search, [42] implement network transformation techniques that allow reusing pre-existing models and efficiently exploring search space for automatic architecture design. This novel approach differs from the previous ones in the definition of reinforce- ment learning states and actions. The state is the current network archi- tecture while the action involves network transformation operations such as adding, enlarging and deleting layers. Starting point architectures used in ex- periments are plain CNN which only consist of convolutional, fully-connected and pooling layers. EAS approach is inspired from Net2Net technique intro- duced in [43] and based on the idea of building deeper student network to reproduce the same processing of an associated teacher network. As shown in figure 10, an encoder network implemented with bidirectional recurrent neural network [44] feeds actors network with given architectures. The se- lected actor networks performs 2 types of transformation: widening layers in terms of units and filters and inserting new layers. EAS outperforms sim- ilar state-of-the-art models designed either manually or automatically with the attractive advantage of using relatively much smaller computational re- sources. [![Fig.10][fig_10]][fig_10] Figure 10: A meta-controller operation for network transformation [42]. 06.02. Modular Architecture Design \u00b6 Most of recent work on neural architecture search is based on more com- plex modular (multi-branch) structures inspired by modern architectures pre- sented in section 4. Rather than operating the tedious search over entire networks, this second set of approaches focus on finding building blocks sim- ilarly to the ones used in, e.g. GoogLeNet and ResNet models. These multi- branch elements are then stacked repetitively involving skip connections to build the final deep architecture. As we will see through the models detailed in this section, \u201dblock-wise\u201d architecture design reduces drastically search space speeding up search process, enhances generated networks performance and gives them more transferable ability through minor adaptation. 06.02.01. BlockQNN \u00b6 One of the first approaches implementing block-wise architecture search, BlockQNN [40] automatically builds convolutional neworks using Q-Learning reinforcement technique [45] with epsilon-greedy as exploration strategy [46]. The block structure is similar to ResNet and Inception (GoogLeNet) modern networks since it contains shortcut connections and multi-branch layer com- binations. The search space of the approach is reduced given that the focus is switched to explore network blocks rather than designing the entire network. The block search space is detailed in figure 11 and consists of 5 parameters: a layer index (its position in the block), an operation type (selected among 7 types commonly used), a kernel size and 2 predecessors layers indexes. Fig- ure 12 depicts 2 different samples of blocks, one with multi-branch structure and the second showing a skip connection. As described in previous sections, the Q-learning model includes an agent, states and actions, where the state represents the current layer of the agent and the action refers to the transi- tion to the next layer. On the basis of defined blocks, the complete network is constructed by stacking them sequentially N times. [![Fig.11][fig_11]][fig_11] Figure 11: Network structure code space [40]. 06.02.02. PNAS \u00b6 Progressive neural architecture search [47] proposes to explore the space of modular structures starting from simple models then evolving to more complex ones, discarding underperforming structures as learning progresses. The modular structure in this approach is called a cell and consists of a fixed number of blocks. Each block is a combination of 2 operators among 8 selected ones such as identity, pooling and convolution. A cell structure is learned first then it\u2019s stacked N times in order to build the resulting CNN. The main contribution of PNAS lies in the optimization of the search process by avoiding direct search in the entire space of cells. This was made possible with the use of a sequential model-based optimization (SMBO) strategy. [![Fig.12][fig_12]][fig_12] Figure 12: Representative block exemplars with their network structure codes [40]. The initial step consists in building, training and evaluating all possible 1- block cells. Then the cell is expanded to 2-block size exploding the number of total combinations. The innovation brought by PNAS is to predict the performance of the second level cells by training a RNN (predictor) on the performance of previous level ones. Only the K best cells (i.e. most promising ones) are transferred to the next step of cell size expansion. This process is repeated until the maximum allowed blocks number is reached. With an accuracy comparable to NAS [28] approach, PNAS is up to 5 times faster using a cell maximum size of 5 blocks and K equal to 256. This result is due to the fact that performance prediction takes much less time than full training of designed cells. The best cell architecture is shown in figure 13. 06.02.03. ENAS \u00b6 Efficient neural architecture search [29] comes in the continuity of previous work NAS [28] and PNAS [47]. It explores a cell-based search space through a controller RNN trained with reinforcement learning. The cell structure is similar to PNAS model where block concept is replaced with a node that consists of 2 operations and two skip connections. The controller RNN man- ages thus 2 types of decisions at each node. First it identifies 2 previous nodes to connect to, allowing the cell to set skip connections. Second, the controller selects 2 operations to implement among a set of 1 identity, 2 depth [![Fig.13][fig_13]][fig_13] wise-separable convolutions of filter sizes 3 \u2217 3 and 5 \u2217 5 [48], max pooling and average pooling both of size 3 \u2217 3. Within each node, the operations results are added in order to constitute an input for the next node. Figure 14 illustrates the design of a 4-node cell. At the end, the entire CNN is built by stacking N times convolutional cells. Another contribution of ENAS consists in sampling mini-batches from validation dataset to train designed models. The models with the best ac- curacy are then trained on the entire validation dataset. Additionally, the approach efficiency is greatly improved by implementing a weight sharing strategy. Each node has its own parameters (used when involved operations are activated) that are shared through inheritance by the generated child models. The latter are hence not trained from scratch saving a considerable processing time. ENAS provides competitive results on CIFAR-10 and Penn Treebank datasets. It specifically takes much less time to build the convolu- tion cells than previous approaches that adopt the same strategy of designing modular structures then stack them to obtain a final CNN. 06.02.04. EAS With Path Level Transformation \u00b6 A developed version of EAS [42] which adopts network transformation for efficient CNN architecture search is presented in [49]. The new ap- proach tackle the constraint of only performing plain architecture modifi- cation (layer-level), e.g. adding (removing) units, filters and layers, by using path-level transformation operations. The proposed model is similar to ([42]) [![Fig.14][fig_14]][fig_14] where the reinforcement learning meta-controller samples network transfor- mation actions to build new architectures. The latter are then trained and re- sulting accuracies are used as reward to update the meta-controller. However, certain changes have been implemented in order to adapt search methods to the tree-structured architecture space: using a tree-structured LSTM, ([50]) as meta-controller, defining a new action space consisting of feature maps allocation schemes (replication, skip), merge schemes (add, concatenation, none) and primitive operations (convolution, identity, depthwise-separable convolution, etc.). Figure 15 presents an example of transformation decisions operated by the meta-controller. Experimenting with ResNet and DenseNet architectures as base input, the path level transformation approach achieves competitive performance with state-of-the-art models maintaining low com- putational resources comparable to EAS approach ones. [![Fig.15][fig_15]][fig_15] Figure 15: Path-level transformation: from a single layer to a tree-structured motif [49]. 06.03. Architecture search accelerators \u00b6 Reinforcement learning methods have been applied successfully to design neural networks. Although multi-branch structures and skip connections improves the efficiency of architectures automatic search, the latter is still computationally expensive (hundreds of GPU hours), time consuming and requires further acceleration of learning process. Thus, in addition to the methods assigned to architectural search optimization and complex compo- nent building, some techniques are developed to speed up learning and are depicted in the current section. Early stopping strategy proposed in [40] enables fast convergence of the learning agent while maintaining an acceptable level of efficiency. This is pos- sible by taking into account intermediate rewards ignored in previous works (set to zero delaying reinforcement learning convergence [36]. In such case, the agent stops searching in an early training phase as the accuracy rewards reach higher levels in fewer iterations. The reward function is redefined in order to include designed block complexity and density and avoid possible poor accuracy resulting from training early stopping. A second technique is presented in [40] which consists of a distributed asynchronous framework assembling 3 nodes with different functions. The master node is the place where block structures are sampled by agent. Then, in the controller node, the entire network is built from generated blocks and transmitted to multiple compute nodes for training. The framework is a kind of simplified parameter-server [51] and allows the parallel training of designed networks in each compute nodes. Hence, the whole design and learning processing is operated in multiple machines and GPUs. [28] uses the same parameter server scheme with replication of controllers in order to train various architectures in parallel. As seen previously, reinforcement learning policies use explored architec- tures performance as a guiding reward for controllers updates. Training and evaluating every sampled architecture (among hundreds) on validation data is responsible for most of computational load. Extracting architecture perfor- mance was consequently subject to several estimation attempts. A number of approaches focus on performance prediction on the basis of past observa- tions. Most of such techniques are based on learning curve extrapolation [25] and surrogate models using RNN predictor [52] that aim at predicting and eliminating poor architectures before full training. Another idea to estimate performance and rank designed architectures is to use simplified (proxy) met- rics for training such as data subsets (mini-batches) [29] and down-sampled data (like images with lower resolution) [53]. Network transformation is one of the more recent techniques assigned to accelerate neural architecture search ([49], [54]). It consists in train- ing explored architectures reusing previously trained or existing networks. This modeling feature allows to address a limitation of reinforcement learn- ing approaches where training is performed with a random initialization of weights. Thus, extending network morphisms [55] to initiate architecture search through the transfer of experience and knowledge reflected by reused weights enables the framework to scrutinize the search space efficiently. Although the techniques presented above have saved substantial compu- tational resources for neural architecture search, there is still more effort needed to examine the extent of bias impact of such techniques on the search process. Indeed, it\u2019s crucial to assure that modifications brought through re-sampled data, discarded cases and early convergence do not influence the models original predictions. Further studies are thus required to verify that learning accelerators do not have amplified effect on approaches predictions and validation accuracies. 07. Conclusion \u00b6 The review of recent work trend on automatic design of CNN architectures raised some methodological options that are adopted by the majority of built approaches. Despite some attempts to use design meta-controllers based on evolutionary algorithms ([26], [27]) and Bayesian optimization ([25], [56]), reinforcement learning has shown promising empirical results and stands as the preferred strategy to train design controllers [57]. Another common conception option is the introduction of multi-branch (modular) structures as an elementary component of the entire network which restricts the search space to block/cell level. The plain network design is generally kept as a first step of proposed approaches application ([28], [42]) given that it leads to simple networks and allows to focus on the method itself before switching to more complex structures with modular design ([29], [49]). A third option used in design approaches at a lower scale is the prediction of explored architectures rewards before full training the most promising ones ([25], [29]). This training acceleration technique is implemented for performance improvement purpose and requires further attention to control possible bias impact on the models behavior. The success of current reinforcement-learning-based approaches to design CNN architectures is widely proven especially for image classification tasks. However, it is achieved at the cost of high computational resources despite the acceleration attempts of most of recent models. Such fact is preventing indi- vidual researchers and small research entities (companies and laboratories) from fully access to this innovative technology [42]. Hence, deeper and more revolutionary optimizing methods are required to practically operate CNN automatic design. Transformation approaches based on extended network morphisms [49] are among the first attempts in this direction that achieved drastic decrease in computational cost and demonstrated generalization ca- pacity. Additional future directions to control automatic design complexity is to develop methods for multi-task problems [58] and weights sharing [59] in order to benefit from knowledge transfer contributions. References \u00b6 [D. Kriesel, A Brief Introduction to Neural Networks, 2007.][2007_KrieselD] [V. Sze, Y. Chen, T. Yang, J. S. Emer, Efficient processing of deep neural networks: A tutorial and survey, Proceedings of the IEEE 105 (12) (2017) 2295\u20132329.] [O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei, Imagenet large scale visual recognition challenge, Int. J. Comput. Vision 115 (3) (2015) 211\u2013252.] [Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, L. D. Jackel, Handwritten digit recognition with a back- propagation network, in: D. S. Touretzky (Ed.), Advances in Neural Information Processing Systems 2, Morgan-Kaufmann, 1990, pp. 396\u2013 404.] [M. Minsky, S. Papert, Perceptrons: An Introduction to Computational Geometry, MIT Press, Cambridge, MA, USA, 1969.] [D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning representations by back-propagating errors, Nature 323 (1986) 533\u2013536.] [Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (11) (1998) 2278\u20132324.] [K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best multi-stage architecture for object recognition?, in: 2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 2146\u20132153.] [M. A. Nielsen, Neural Networks and Deep Learning, Determination Press, 2018.] [D. Hubel, T. Wiesel, Receptive fields, binocular interaction, and func- tional architecture in the cat\u2019s visual cortex, Journal of Physiology 160 (1962) 106\u2013154.] [A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1097\u20131105.] [H. Wu, X. Gu, Max-pooling dropout for regularization of convolutional neural networks, in: Neural Information Processing - 22nd International Conference, ICONIP 2015, Istanbul, Turkey, November 9-12, 2015, Pro- ceedings, Part I, 2015, pp. 46\u201354.] [I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016.] [M. Zeiler, R. Fergus, Stochastic pooling for regularization of deep con- volutional neural networks, in: Proceedings of the International Confer- ence on Learning Representations (ICLR), 2013.] [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, J. Mach. Learn. Res. 15 (1) (2014) 1929\u20131958.] [K. Simonyan, A. Zisserman, Very deep convolutional networks for large- scale image recognition, in: Proceedings of the International Conference on Learning Representations (ICLR), 2015.] [M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Com- puter Vision \u2013 ECCV 2014, Springer International Publishing, Cham, 2014, pp. 818\u2013833.] [C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.] [K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.] [C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi, Inception-v4, inception- resnet and the impact of residual connections on learning, in: S. P. Singh, S. Markovitch (Eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., AAAI Press, 2017, pp. 4278\u20134284.] [G. Huang, Z. Liu, L. v. d. Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261\u20132269.] [B. Baker, O. Gupta, R. Raskar, N. Naik, Accelerating neural architec- ture search using performance prediction, in: International Conference on Learning Representations, Workshop, 2018.] [Y. Bengio, A. Courville, P. Vincent, Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell. 35 (8) (2013) 1798\u20131828.] [J. S. Bergstra, R. Bardenet, Y. Bengio, B. K \u0301egl, Algorithms for hyper- parameter optimization, in: Advances in Neural Information Processing Systems 24, Curran Associates, Inc., 2011, pp. 2546\u20132554.] [T. Domhan, J. T. Springenberg, F. Hutter, Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves, in: Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI\u201915, AAAI Press, 2015, pp. 3460\u20133468.] [K. O. Stanley, D. B. D\u2019Ambrosio, J. Gauci, A hypercube-based encoding for evolving large-scale neural networks, Artif. Life 15 (2) (2009) 185\u2013 212.] [M. Suganuma, S. Shirakawa, T. Nagao, A genetic programming ap- proach to designing convolutional neural network architectures, in: Pro- ceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201917, ACM, New York, NY, USA, 2017, pp. 497\u2013504.] [B. Zoph, Q. V. Le, Neural architecture search with reinforcement learn- ing, in: Proceedings of the International Conference on Learning Rep- resentations (ICLR), 2017.] [H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, Efficient neural architec- ture search via parameters sharing, in: J. Dy, A. Krause (Eds.), Proceed- ings of the 35th International Conference on Machine Learning, Vol. 80, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 4095\u20134104.] [E. Brochu, T. Brochu, N. de Freitas, A bayesian interactive optimization approach to procedural animation design, in: Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association, Goslar Germany, Germany, 2010, pp. 103\u2013 112.] [A. Klein, S. Falkner, S. Bartels, P. Hennig, F. Hutter, Fast bayesian optimization of machine learning hyperparameters on large datasets, in: Proceedings of the 20th International Conference on Artificial Intelli- gence and Statistics (AISTATS 2017), Vol. 54 of Proceedings of Machine Learning Research, PMLR, 2017, pp. 528\u2013536.] [C. E. Rasmussen, C. K. I. Williams, Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning), The MIT Press, 2005.] [L. Breiman, Random forests, Mach. Learn. 45 (1) (2001) 5\u201332.] [A. E. Eiben, J. E. Smith, Introduction to Evolutionary Computing, 2nd Edition, Springer Publishing Company, Incorporated, 2015.] [E. Dufourq, B. A. Bassett, Eden: Evolutionary deep networks for ef- ficient machine learning, in: 2017 Pattern Recognition Association of South Africa and Robotics and Mechatronics (PRASA-RobMech), 2017, pp. 110\u2013115.] [R. S. Sutton, A. G. Barto, Reinforcement learning - an introduction, Adaptive computation and machine learning, MIT Press, 1998.] [K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, Deep reinforcement learning: A brief survey, IEEE Signal Processing Maga- zine 34 (6) (2017) 26\u201338.] [C. J. C. H. Watkins, P. Dayan, Q-learning, Machine Learning 8 (3) (1992) 279\u2013292.] [B. Baker, O. Gupta, N. Naik, R. Raskar, Designing neural network architectures using reinforcement learning, in: Proceedings of the Inter- national Conference on Learning Representations (ICLR), 2017.] [Z. Zhong, J. Yan, W. Wu, J. Shao, C.-L. Liu, Practical block-wise neural network architecture generation, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.] [F. Tan, P. Yan, X. Guan, Deep reinforcement learning: From q-learning to deep q-learning, in: D. Liu, S. Xie, Y. Li, D. Zhao, E.-S. M. El-Alfy (Eds.), Neural Information Processing, Springer International Publish- ing, Cham, 2017, pp. 475\u2013483.] [H. Cai, T. Chen, W. Zhang, Y. Yu, J. Wang, Efficient architecture search by network transformation, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innova- tive Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI- 18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2787\u20132794.] [T. Chen, I. Goodfellow, J. Shlens, Net2Net: Accelerating learning via knowledge transfer, in: International Conference on Learning Represen- tations (ICLR), 2016.] [M. Schuster, K. Paliwal, Bidirectional recurrent neural networks, Trans. Sig. Proc. 45 (11) (1997) 2673\u20132681.] [C. J. C. H. Watkins, Learning from delayed rewards, Ph.D. thesis, King\u2019s College, Cambridge, UK (1989).] [V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (2015) 529\u2013533.] [C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, K. Murphy, Progressive neural architecture search, in: V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (Eds.), Computer Vision \u2013 ECCV 2018, Springer International Publishing, Cham, 2018, pp. 19\u201335.] [F. Chollet, Xception: Deep learning with depthwise separable convo- lutions, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1800\u20131807.] [H. Cai, J. Yang, W. Zhang, S. Han, Y. Yu, Path-level network trans- formation for efficient architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stock- holmsmssan, Stockholm Sweden, 2018, pp. 678\u2013687.] [K. S. Tai, R. Socher, C. D. Manning, Improved semantic representations from tree-structured long short-term memory networks, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, 2015, pp. 1556\u20131566.] [J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng, Large scale distributed deep networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1223\u20131231.] [H. Liu, K. Simonyan, O. Vinyals, C. Fernando, K. Kavukcuoglu, Hierar- chical representations for efficient architecture search, in: International Conference on Learning Representations (ICLR), 2018.] [T. Hinz, N. Navarro-Guerrero, S. Magg, S. Wermter, Speeding up the Hyperparameter Optimization of Deep Convolutional Neural Networks, International Journal of Computational Intelligence and Applications 17 (2).] [F. H. Thomas Elsken, Jan Hendrik Metzen, Simple and efficient archi- tecture search for convolutional neural networks, in: Proceedings of the International Conference on Learning Representations (ICLR), 2018.] [T. Wei, C. Wang, C. W. Chen, Modularized morphing of neural net- works, in: International Conference on Learning Representations, Work- shop, 2017.] [H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, F. Hutter, To- wards automatically-tuned neural networks, in: F. Hutter, L. Kotthoff, J. Vanschoren (Eds.), Proceedings of the Workshop on Automatic Ma- chine Learning, Vol. 64, PMLR, New York, New York, USA, 2016, pp. 58\u201365.] [J. Perez-Rua, M. Baccouche, S. Pateux, Efficient progressive neural ar- chitecture search, in: British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, BMVA Press, 2018, p. 150.] [J. Liang, E. Meyerson, R. Miikkulainen, Evolutionary architecture search for deep multitask networks, in: Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201918, ACM, New York, NY, USA, 2018, pp. 466\u2013473.] [G. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, Q. Le, Un- derstanding and simplifying one-shot architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 550\u2013559.] \u00b6 img{width: 50%; float: right;}","title":"190427 Jaafra, Y., Laurent, J.L. 2018"},{"location":"190427_JaafraY_LaurentJL_2018/#00_abstract","text":"Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are gen- erally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. De- signing such architectures requires significant human expertise, substantial computation time and doesnt always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.","title":"00. Abstract"},{"location":"190427_JaafraY_LaurentJL_2018/#01_introduction","text":"","title":"01. Introduction"},{"location":"190427_JaafraY_LaurentJL_2018/#01p01","text":"\u201dA neuron is nothing more than a switch with information input and output. The switch will be activated if there are enough stimuli of other neurons hitting the information input. Then, at the information output, a pulse is sent to, for example, other neurons \u201d [1] . Brain-inspired machine learning imitates in a simplified manner the hierarchical operating mode of biological neurons [2] . The concept of artificial neural networks (ANN) achieved a huge progress from its first theoretical proposal in the 1950s until the recent considerable outcomes of deep learning. In computer vision and more specifically in classification tasks, CNN, which we will examine in this review, are among the most popular deep learning techniques since they are outperforming humans in some vision complex tasks [3] .","title":"01.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#01p02","text":"advances: backpropagation algorithm large training datasets / computational resources CNN differ in that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer The origin of CNN that were initially established by [4] goes back to the 1950s with the advent of \u201dperceptron\u201d, the first neural network prototyped by Frank Rosenblatt. However, neural network models were not extensively used until recently, after researchers overcame certain limits. Among these advances we can mention the generalization of perceptrons to many layers [5], the emergence of backpropagation algorithm as an appropriate training method for such architectures [6] and, mainly, the availability of large training datasets and computational resources to learn millions of parameters. CNN differ from classical neural networks in the fact that the connectivity of a hidden layer neuron is limited to a subset of neurons in the previous layer . This selective connection endow the network with the ability to operate, implicitly, hierarchical features extraction. For an image classification case, the first hidden layer can visualize edges, the second a specific shape and so on until the final layer that will identify the object.","title":"01.P02"},{"location":"190427_JaafraY_LaurentJL_2018/#01p03","text":"convolusion pooling fully connected multiple choices: num / order of layers hyperparameter (receptive fild size, stride) in this paper overview of deep learning history of CNN architectures several methods for automating CNN design according to search optimizaiton architecture design methods search acceleration techniques future works CNN architecture consists of several types of layers including convolution, pooling, and fully connected. The network expert has to make multiple choices while designing a CNN such as the number and ordering of layers , the hyperparameters for each type of layer ( receptive field size , stride , etc.). Thus, selecting the appropriate architecture and related hyperparameters requires a trial and error manual search process mainly directed by intuition and experience. Additionally, the number of available choices makes the selection space of CNN architectures extremely wide and impossible for an exhaustive manual exploration. Many research effort in meta-modeling tries to minimize human intervention in designing neural network architectures. In this paper, we first give a general overview and define the field of deep learning. We then briefly survey the history of CNN architectures. In the following section we review several methods for automating CNN design according to three dimensions: search optimization , architecture design methods (plain or modular) and search acceleration techniques . Finally, we conclude the article with a discussion of future works.","title":"01.P03"},{"location":"190427_JaafraY_LaurentJL_2018/#02_background","text":"","title":"02. Background"},{"location":"190427_JaafraY_LaurentJL_2018/#02p01","text":"Before embarking with CNN, we will introduce in this section some basic generalities about artificial networks and deep learning.","title":"02.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#0201_artificial_neural_networks","text":"","title":"02.01. Artificial Neural Networks"},{"location":"190427_JaafraY_LaurentJL_2018/#0201p01","text":"ANN are a major field of artificial intelligence that attempts to replicate human brain processing. Three types of neural layers distinguish an ANN: input, output and hidden layers. The latter operate transitional representa- tions of the input data evolving from low level features (lines and edges) to higher ones (complex patterns) as far as deeper layers are reached. Figure 1 provide an example of ANN involving classical fully-connected layers where every neuron is connected to all ones of the previous layer.","title":"02.01.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#figure_1","text":"[![Fig.1][fig_01]][fig_01] Figure 1: Artificial neural network, containing an input layer, an output layer and two hidden layers.","title":"Figure 1"},{"location":"190427_JaafraY_LaurentJL_2018/#0201p02","text":"weight bias During training, an ANN aims at learning two types of parameters that will condition its predictive performance. First, connection weights that assess to which extent a neuron result will impact the output of higher level neuron. Second, the bias which is a global estimator of a feature presence across all inputs. Hence, a neuron output can be formalized through a linear combination of weighted inputs and associated bias: \\text{output} = \\Big( \\sum_i{ \\text{input}_i \u2217 \\text{weight}_i} \\Big) + \\text{bias}","title":"02.01.P02"},{"location":"190427_JaafraY_LaurentJL_2018/#0201p03","text":"activation fn ReLU ( Rectified Linear Unit ) In order to allow the network operating non-linear transformations, an activation function is applied to the previous output. Equation 1 presents an example of such transformation using one of the most common and efficient activation function which is the Rectified Linear Unit ( ReLU ) [7]: f(x) = \\max(x, 0) \\tag{1}","title":"02.01.P03"},{"location":"190427_JaafraY_LaurentJL_2018/#0202_deep_learning","text":"","title":"02.02. Deep learning"},{"location":"190427_JaafraY_LaurentJL_2018/#0202p01","text":"Deep learning: ML processing w/i multi-layer ANN loss function evalation backpropagation The concept of deep learning refers to machine learning processing within multi-layer ANN [8]. The training of these networks relies on a loss function evaluation . For example, in supervised learning the loss is assimilated to the matching accuracy between ANN predictions and real expected outputs. An iterative update procedure is implemented to adjust network parameters according to loss function computed gradient. This procedure is called Backpropagation since parameters updates are spread from final layers to initial ones. Deep learning implies a certain number of challenges such as vanish- ing/exploding gradient and overfitting. The solutions to these problems will be discussed when developing CNN design architectures in next sections.","title":"02.02.P01"},{"location":"190427_JaafraY_LaurentJL_2018/#03_cnn_layers","text":"CNN are widely used in a great number of pattern and image recognition problems. Three main characteristics are making this deep learning technique successful and suitable to visual data. First, local receptive fields perfectly reflect image data specificity to be correlated locally and uncorrelated in global segments. Second, shared weights allows a substantial parameter re- duction without altering image processing since the convolution is applicable to the whole image. Last, grid-structured image enable pooling operations that simplify data without losing useful information [9].","title":"03. CNN Layers"},{"location":"190427_JaafraY_LaurentJL_2018/#0301_convolutional_layer","text":"The convolutional layer is the basic CNN unit that has been inspired by physiological research evidence of hierarchical processing in the visual cor- tex of mammals [10]. Simple cells detect primitive attributes while more compound structures are subsequently extracted by complex cells. Thus, convolutional layer consists of a set of feature maps issued from convolving different filters (kernels) with an input image or previous layer output [11]. The 2-dimensional maps are stacked together to produce the resulting vol- ume of the convolutional layer. This process reduces drastically the network complexity since the neurons of a same feature map share the same weights and bias maintaining a low number of parameters to learn [12]. The hyperparameters characterizing a convolutional layer are the depth F (number of filters), the stride S (filter movement from a receptive field to the next one) and the zero padding P to control input size [13]. Assuming that the filter size ( (\\text{height}, \\text{width}, \\text{depth}) = (h, w, D), the dimensions of the feature maps generated can be obtained according to: \\begin{pmatrix} H_1 & W_1 & F \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{(H + 2P \u2212 h)}{S} + 1 & \\frac{(W + 2P \u2212 w)}{S} + 1 & F \\\\ \\end{pmatrix} Where (H, W, D) is the size (\\text{height}, \\text{width}, \\text{depth}) of the input image.","title":"03.01. Convolutional Layer"},{"location":"190427_JaafraY_LaurentJL_2018/#0302_pooling_layer","text":"CNN architectures generally alternate convolution and pooling layers. The latter have the purpose of reducing network complexity and avoid the problem of overfitting. At biological level, pooling is assimilated to the behav- ior of cortical complex cells that reveal a certain degree of position invariance. A pooling layer neuron is connected to a region of the previous layer by per- forming a non-parameterized function. Thus it differs from convolution as it doesn\u2019t have learnable weights or bias and additionally, it keeps the same depth of the previous layer. Max pooling [14] is one of the most common type of pooling that consists in retaining the maximum value of a neurons cluster. It means that max pooling is detecting if a given feature has been identified in a receptive field without recording the exact location [9].","title":"03.02. Pooling Layer"},{"location":"190427_JaafraY_LaurentJL_2018/#0303_fully_connected_layer","text":"The convolution layers identify local features in the input data such as edges and shapes. The Fully connected layer operates the high level reasoning (classification for image case) by combining information from all the previous layers. As in a regular ANN, neurons at this level are fully connected to all ones in the previous layer. A softmax loss layer is then used to compute the probability distribution of the CNN final outputs.","title":"03.03. Fully connected Layer"},{"location":"190427_JaafraY_LaurentJL_2018/#04_cnn_architecture_history","text":"This section presents the most influential hand-crafted CNN architectures that have impacted the recent work on automatic architecture design. Most of them won at least one of the \u201dImageNet Large Scale Visual Recognition Competition\u201d (ILSVRC) challenges [3].","title":"04. CNN Architecture History"},{"location":"190427_JaafraY_LaurentJL_2018/#0401_lenet","text":"As mentioned previously, LeNet [7] was the innovative work that intro- duced convolutional networks. The model was experimented successfully to classify handwritten digits without any preprocessing of the input image (of size 32 \u2217 32 pixels). LeNet architecture is illustrated in figure 2. It consists of an input and an output layers of respective sizes 32 \u2217 32 and 10 as well as 6 hidden Layers. The basic idea of this design is to operate multiple con- volutions (3) with pooling in-between (2) then transmitting the final signal via a fully-connected layer toward the output layer. Unfortunately, due to the lack of adequate training data and computing power, it wasn\u2019t possible to extend this architecture to more complex applications. [![fig.2][fig_02]][fig_02] Figure 2: Architecture of LeNet-5 [7].","title":"04.01. LeNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0402_alexnet","text":"AlexNet [11] is one of the most influential deep CNN that won the ILSVRC (Imagenet Large Scale Visual Recognition Challenge) competitions in 2012. As shown in figure 3, it is not much different from LeNet. Never- theless, the corresponding architecture is deeper with 8 layers in total, 5 con- volutional and 3 fully connected. The effective contribution of AlexNet lies in several design and training specificities. First, it introduced the Rectified Linear Unit (ReLU) nonlinearity which helped to overcome the problem of vanishing gradient and boosted a faster training. Furthermore, AlexNet im- plements a dropout step [15] that consists in setting to zero a predefined per- centage of layers\u2019 parameters. This technique decreases learned parameters and controls neurons correlation in order to limit overfitting impact. Third, training process convergence is accelerated with momentum and conditional learning rate decrease (e.g. when learning stagnates). Finally, training data volume is increased artificially by generating variations of the original images that are shifted randomly. Thus the network learning is enhanced with the use of invariant representations of the data. [![fig.3][fig_03]][fig_03] Figure 3: An illustration of AlexNet [11].","title":"04.02. AlexNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0403_vggnet","text":"Submitted for the ILSVRC 2014, VGGNet [16] won the second place and demonstrated that deeper architectures achieve better results. Indeed, with its 19 hidden layers, it was much deeper than previous convolutional networks. In order to allow an increase in depth without an exponential growth of the parameters number, smaller convolution filters (3\u22173) were used in all layers (e.g. lower size than the 11 \u2217 11 filters adopted in AlexNet). An additional advantage of using smaller filters consists in reducing overlapping scanned pixels which results in feature maps with more local details [17].","title":"04.03. VGGNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0404_googlenet","text":"Since it has been demonstrated that a CNN architecture size is positively correlated to its performance, recent efforts focus on how to increase the depth of a CNN while keeping an acceptable number of parameters. Winner of ILSVRC 2014, GoogLeNet [18] innovated network design by replacing the classical strategy of alternating convolutional and pooling layers with stacked Inception Modules depicted in figure 4. Despite being deeper than VGGNet with 22 hidden layers, GoogLeNet requires outstandingly fewer parameters due to this sparse connection technique. Within an inception module, several convolutions with different scales and pooling are performed in parallel then concatenated in one single layer. This enables the CNN to detect patterns of various sizes within the same layer and avoid heavy parameters redundancies [18]. GoogLeNet hidden layers consist of 3 convolutions, 9 inception blocks (2 layers deep each one) and one fully connected. [![Fig.4][fig_04]][fig_04] Figure 4: Inception module [18].","title":"04.04. GoogLeNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0405_resnet","text":"Deep Residual Network (ResNet) [19], was the first neural network to ex- ceed human-level accuracy in ImageNet Challenge (ILSVRC 2015). Thanks to residual connections, such kind of architecture went deeper and was im- plemented with multiple versions of 34, 50, 101 and 152 layers. Indeed, one of the difficulties with very deep networks training is the vanishing gradi- ent during error backpropagation which penalizes the appropriate update of earlier layers weights. ResNet main contribution consists in dividing con- volutional layers into residual blocks. Each block is bypassed by a residual (skip) connection that forwards the block input using an identity mapping. The final output is the summation of the block output and the mapped input as illustrated in figure 5. By adding skip connections, backpropagation can be operated without any interference with previous layers which allows to prevent vanishing gra- dient and train very deep architectures. ResNet-101 consists of one convolu- tional layer followed by 33 residual blocks (3 layers deep each one), and one fully connected layer. [![Fig.5][fig_05]][fig_05] Figure 5: Residual learning: a building block [19]","title":"04.05. ResNet"},{"location":"190427_JaafraY_LaurentJL_2018/#0406_more_networks","text":"After ResNet [19] success, which exceeded human-level accuracy in (ILSVRC 2015), the so-called modern hand-crafted CNN are still being designed on the basis of previous models looking for more efficiency and lower training time. Inception-v4 [20] is a new release of GoogLeNet that involved many more layers than the initial version. Inception-ResNet [20] is built as a combi- nation of an Inception network and a ResNet, joining inception blocks and residual connections. The last example of this section is DenseNet (Dense Convolutional Networks) [21] where each dense block layer is connected via skip connections to all subsequent ones allowing the learning of new features.","title":"04.06. More Networks"},{"location":"190427_JaafraY_LaurentJL_2018/#05_meta-modeling_for_cnn_automatic_architecture_design","text":"Meta-modeling for neural network architectures design aims at reduc- ing the intervention of human expertise in this process. The earliest meta- modeling methods were based on genetic algorithms and Bayesian optimiza- tion then more recently, reinforcement learning became among the most im- plemented approaches [22].","title":"05. Meta-modeling for CNN automatic architecture design"},{"location":"190427_JaafraY_LaurentJL_2018/#0501_context_of_automation","text":"The performance of a neural network and particularly a CNN mainly depends on the setting of the model structure, the training process, and the data representation. All of these variables are controlled through a number of hyperparameters and impact the learning process to a large extent. In order to achieve an optimal performance of CNN, these hyperparameters including the depth of the network, learning rates, layer type, number of units per layer, dropout rates, etc., should be then carefully tuned. On the other hand, the advent of deeper and more complex modern architectures (see section 4) is increasing the number and the types of hyperparameters. Hence, tuning step and more generally CNN architectures search become very expensive and heavy for an expert trial-and-error procedure. Additionally, CNN parameters setting is considered as a black-box [23] optimization problem because of the unknown nature of the mapping between the architecture, the performance, and the learning task. In this context, au- tomatic design solutions are highly required and instigates a large volume of research. The task of CNN hyperparameters tuning has been handled through meta-modeling that consists in applying machine learning models for designing CNN architectures. Three meta-modeling approaches are gen- erally used in the literature of architecture search and will be described in the next paragraph: bayesian optimization, evolutionary algorithms and re- inforcement learning.","title":"05.01. Context of automation"},{"location":"190427_JaafraY_LaurentJL_2018/#0502_meta-controllers","text":"Meta-modeling approaches perform iterative selection from the hyperpa- rameters space and build associated architectures that are then trained and evaluated. Accuracies records are fed to meta-modeling controllers (meta- controllers) to guide next architectures sampling. Meta-controllers for CNN design are mainly based on bayesian optimization ([24], [25]), evolutionary algorithms ([26], [27]) or more recently on reinforcement learning ([28], [29]). Bayesian optimization is an efficient way to optimize black-box objective functions f : X \\rightarrow R that are slow to evaluate [30]. It aims at finding an input x = \\arg \\min_{x \\in X} f(x) that globally minimizes f where in the context of a machine learning algorithm, x refers to the set of hyperparameters to optimize. The problem with this kind of optimization is that evaluating the objective function is very costly due to the great number of hyperparameters and the complex nature of models like deep neural networks. In order to overcome this problem, bayesian approaches propose probabilistic surrogate reconstruction of the objective function p(f|D) where D is a set of past observations. The evaluation of the empirical function is much cheaper than the true objective function [31]. Some of the most used probabilistic surrogate (regression) models are gaussian processes [32], random forests [33] and tree- structured Parzen estimator [24]. Briefly, the processing of a bayesian optimization consists in building an empirical (probabilistic) model of the objective function. Then, iteratively, the model identifies a set of optimal hyperparameters for which the objective function returns corresponding results (e.g. loss values). Each feedback al- lows the update of the surrogate model and the guidance of hyperparameters predictions until the process reaches a termination condition. Evolutionary algorithms present another strategy of hyperparameters op- timization that modifies a set of candidate solutions (population) on the basis of a number of rules (operators). Following an iterative procedure of muta- tion, crossover and selection [34], an evolutionary algorithm initializes, in a first step, a set of N random networks to create a primary population. The second step consists in introducing a fitness function to score each net- work through its classification accuracy and keep the top ranked networks to construct the next generation. The evolutionary process continues until a termination criteria is met, which is generally defined as the maximum num- ber of allowed generations. One of the advantage of evolutionary algorithms is the adaptation to complex combination of discrete (layer type) and contin- uous (learning rate) hyperparameters which is suitable to neuronal network optimization models [35]. An important approach for goal-oriented optimization is reinforcement learning (RL) inspired from behaviorist psychology [36]. The frame of RL is an agent learning through interaction with its environment (figure 6). Thus the agent adapts its behavior (transition to a state s_{t+1} ) on the basis of observed consequences (rewards) of an action at taken in state s_t . The agent purpose is to learn a policy \\pi that is able to identify the optimal sequence of actions maximizing the expected cumulative rewards. The environment return reinforces the agent to select new actions to improve learning process, hence the name of reinforcement learning. [![Fig.6][fig_06]][fig_06] Figure 6: Illustration of the RL process. The methods developed to resolve reinforcement tasks are based on value functions, policy search or a combination of both strategies (actor-critic methods) [37]. Value function methods consist in estimating the expected reward value R when reaching a given state s and following a policy \\pi : V ^\\pi(s) = \\mathbb{E} [\\mathfrak{R}|s, \\pi] A recursive form of this function is particularly used in recent Q-learning [38] models assigned to CNN architecture design ([39], [40]): Q(s_t, a_t) = Q(s_t, a_t) + \\alpha[r_{t + 1} + \\gamma \\text{max}_a Q(s_{t+1}, a) \u2212 Q(s_t, a_t)] Where s_t is a current state, a_t is a current action, \\alpha is the learning rate, r_{t+1} is the reward earned when transitioning from time t to the next and \\gamma is the discount rate. In contrast to value function methods, policy search methods do not implement a value function and apply, instead, a gradient-based procedure to identify directly an optimal policy \\pi^\u2217 . In this context, deep reinforcement learning is achieved when deep neural networks are used to approximate one of the reinforcement learning components : value function, policy or reward function [41]. Among the active fields of designing CNN architectures through deep reinforcement learning, recurrent neural networks (RNN) arise as a valuable model that handles a set of tasks such as hyperparameters prediction ([28], [29]). In fact, a RNN operates sequentially involving hidden units to store processing history, which allows the reinforcement learning to profit from past observations. Long short term memory networks (LSTM), a variant of RNN, offers a more efficient way of evolving conditionally on the basis of previous elements.","title":"05.02. Meta-controllers"},{"location":"190427_JaafraY_LaurentJL_2018/#06_neural_architecture_search","text":"Various strategies have been developed to operate CNN architectures de- sign for the majority of which reinforcement learning has been selected as meta-controller. This section is assigned to review in detail most recent promising automatic search approaches differentiated according to search spaces specificities and complexity level.","title":"06. Neural Architecture Search"},{"location":"190427_JaafraY_LaurentJL_2018/#0601_plain_architecture_design","text":"Some architecture search approaches focus on designing plain CNN which consists exclusively of conventional layers, mainly convolution, pooling and fully-connected. The resulting research space is relatively simple and the approaches contribution lies almost entirely in the design strategy.","title":"06.01. Plain Architecture Design"},{"location":"190427_JaafraY_LaurentJL_2018/#060101_metaqnn","text":"MetaQNN model [39] relies on Q-learning, a type of reinforcement learn- ing (refer to previous section for more details), to sequentially select network layers and their parameters among a finite space. This method implies, first, the definition of each learning agent state as a layer with all associated relevant parameters. As an example, 5 layers are depicted in figure 7: convo- lution (C), pooling (P), fully connected (FC), global average pooling (GAP), and softmax (SM). [![Fig.7][fig_07]][fig_07] Figure 7: State space possible parameters [39]. Second, the agent action space is assimilated to the possible layers the agent may move to given a certain number of constraints set intentionally, for the majority, to enable faster convergence. Figure 8 illustrates a set of state and action spaces and an eventual agent path to design a CNN architecture. MetaQNN was evaluated competitive with similar and different hand-crafted CNN architectures as with existing automated network design methods. [![Fig.8][fig_08]][fig_08] Figure 8: An illustration of the full state and action space (a) and a path that the agent has chosen (b) [39].","title":"06.01.01. MetaQNN"},{"location":"190427_JaafraY_LaurentJL_2018/#060102_nas","text":"Using reinforcement learning, [28] train a recurrent neural network to generate convolutional architectures. Figure 9 shows a RNN controller gener- ating sequentially CNN parameters associated to convolutional layers. Every sequence output is predicted by a softmax classifier then used as input of the next sequence. The parameters set consists of filter height and width, stride height and width and the number of filters per layer. The design of an ar- chitecture takes an end once the number of layers reaches a predefined value that increases all along training. The accuracy of the designed architecture is fed as a reward to train the RNN controller through reinforcement learning in order to maximize the expected validation accuracy of the next architectures. The experimentation of the global approach achieved competitive results on CIFAR-10 and Penn Treebank datasets. [![Fig.9][fig_09]][fig_09] Figure 9: Illustration of the way the agent used to select hyperparameters [28].","title":"06.01.02. NAS"},{"location":"190427_JaafraY_LaurentJL_2018/#060103_eas","text":"In their very recent work Efficient Architecture Search, [42] implement network transformation techniques that allow reusing pre-existing models and efficiently exploring search space for automatic architecture design. This novel approach differs from the previous ones in the definition of reinforce- ment learning states and actions. The state is the current network archi- tecture while the action involves network transformation operations such as adding, enlarging and deleting layers. Starting point architectures used in ex- periments are plain CNN which only consist of convolutional, fully-connected and pooling layers. EAS approach is inspired from Net2Net technique intro- duced in [43] and based on the idea of building deeper student network to reproduce the same processing of an associated teacher network. As shown in figure 10, an encoder network implemented with bidirectional recurrent neural network [44] feeds actors network with given architectures. The se- lected actor networks performs 2 types of transformation: widening layers in terms of units and filters and inserting new layers. EAS outperforms sim- ilar state-of-the-art models designed either manually or automatically with the attractive advantage of using relatively much smaller computational re- sources. [![Fig.10][fig_10]][fig_10] Figure 10: A meta-controller operation for network transformation [42].","title":"06.01.03. EAS"},{"location":"190427_JaafraY_LaurentJL_2018/#0602_modular_architecture_design","text":"Most of recent work on neural architecture search is based on more com- plex modular (multi-branch) structures inspired by modern architectures pre- sented in section 4. Rather than operating the tedious search over entire networks, this second set of approaches focus on finding building blocks sim- ilarly to the ones used in, e.g. GoogLeNet and ResNet models. These multi- branch elements are then stacked repetitively involving skip connections to build the final deep architecture. As we will see through the models detailed in this section, \u201dblock-wise\u201d architecture design reduces drastically search space speeding up search process, enhances generated networks performance and gives them more transferable ability through minor adaptation.","title":"06.02. Modular Architecture Design"},{"location":"190427_JaafraY_LaurentJL_2018/#060201_blockqnn","text":"One of the first approaches implementing block-wise architecture search, BlockQNN [40] automatically builds convolutional neworks using Q-Learning reinforcement technique [45] with epsilon-greedy as exploration strategy [46]. The block structure is similar to ResNet and Inception (GoogLeNet) modern networks since it contains shortcut connections and multi-branch layer com- binations. The search space of the approach is reduced given that the focus is switched to explore network blocks rather than designing the entire network. The block search space is detailed in figure 11 and consists of 5 parameters: a layer index (its position in the block), an operation type (selected among 7 types commonly used), a kernel size and 2 predecessors layers indexes. Fig- ure 12 depicts 2 different samples of blocks, one with multi-branch structure and the second showing a skip connection. As described in previous sections, the Q-learning model includes an agent, states and actions, where the state represents the current layer of the agent and the action refers to the transi- tion to the next layer. On the basis of defined blocks, the complete network is constructed by stacking them sequentially N times. [![Fig.11][fig_11]][fig_11] Figure 11: Network structure code space [40].","title":"06.02.01. BlockQNN"},{"location":"190427_JaafraY_LaurentJL_2018/#060202_pnas","text":"Progressive neural architecture search [47] proposes to explore the space of modular structures starting from simple models then evolving to more complex ones, discarding underperforming structures as learning progresses. The modular structure in this approach is called a cell and consists of a fixed number of blocks. Each block is a combination of 2 operators among 8 selected ones such as identity, pooling and convolution. A cell structure is learned first then it\u2019s stacked N times in order to build the resulting CNN. The main contribution of PNAS lies in the optimization of the search process by avoiding direct search in the entire space of cells. This was made possible with the use of a sequential model-based optimization (SMBO) strategy. [![Fig.12][fig_12]][fig_12] Figure 12: Representative block exemplars with their network structure codes [40]. The initial step consists in building, training and evaluating all possible 1- block cells. Then the cell is expanded to 2-block size exploding the number of total combinations. The innovation brought by PNAS is to predict the performance of the second level cells by training a RNN (predictor) on the performance of previous level ones. Only the K best cells (i.e. most promising ones) are transferred to the next step of cell size expansion. This process is repeated until the maximum allowed blocks number is reached. With an accuracy comparable to NAS [28] approach, PNAS is up to 5 times faster using a cell maximum size of 5 blocks and K equal to 256. This result is due to the fact that performance prediction takes much less time than full training of designed cells. The best cell architecture is shown in figure 13.","title":"06.02.02. PNAS"},{"location":"190427_JaafraY_LaurentJL_2018/#060203_enas","text":"Efficient neural architecture search [29] comes in the continuity of previous work NAS [28] and PNAS [47]. It explores a cell-based search space through a controller RNN trained with reinforcement learning. The cell structure is similar to PNAS model where block concept is replaced with a node that consists of 2 operations and two skip connections. The controller RNN man- ages thus 2 types of decisions at each node. First it identifies 2 previous nodes to connect to, allowing the cell to set skip connections. Second, the controller selects 2 operations to implement among a set of 1 identity, 2 depth [![Fig.13][fig_13]][fig_13] wise-separable convolutions of filter sizes 3 \u2217 3 and 5 \u2217 5 [48], max pooling and average pooling both of size 3 \u2217 3. Within each node, the operations results are added in order to constitute an input for the next node. Figure 14 illustrates the design of a 4-node cell. At the end, the entire CNN is built by stacking N times convolutional cells. Another contribution of ENAS consists in sampling mini-batches from validation dataset to train designed models. The models with the best ac- curacy are then trained on the entire validation dataset. Additionally, the approach efficiency is greatly improved by implementing a weight sharing strategy. Each node has its own parameters (used when involved operations are activated) that are shared through inheritance by the generated child models. The latter are hence not trained from scratch saving a considerable processing time. ENAS provides competitive results on CIFAR-10 and Penn Treebank datasets. It specifically takes much less time to build the convolu- tion cells than previous approaches that adopt the same strategy of designing modular structures then stack them to obtain a final CNN.","title":"06.02.03. ENAS"},{"location":"190427_JaafraY_LaurentJL_2018/#060204_eas_with_path_level_transformation","text":"A developed version of EAS [42] which adopts network transformation for efficient CNN architecture search is presented in [49]. The new ap- proach tackle the constraint of only performing plain architecture modifi- cation (layer-level), e.g. adding (removing) units, filters and layers, by using path-level transformation operations. The proposed model is similar to ([42]) [![Fig.14][fig_14]][fig_14] where the reinforcement learning meta-controller samples network transfor- mation actions to build new architectures. The latter are then trained and re- sulting accuracies are used as reward to update the meta-controller. However, certain changes have been implemented in order to adapt search methods to the tree-structured architecture space: using a tree-structured LSTM, ([50]) as meta-controller, defining a new action space consisting of feature maps allocation schemes (replication, skip), merge schemes (add, concatenation, none) and primitive operations (convolution, identity, depthwise-separable convolution, etc.). Figure 15 presents an example of transformation decisions operated by the meta-controller. Experimenting with ResNet and DenseNet architectures as base input, the path level transformation approach achieves competitive performance with state-of-the-art models maintaining low com- putational resources comparable to EAS approach ones. [![Fig.15][fig_15]][fig_15] Figure 15: Path-level transformation: from a single layer to a tree-structured motif [49].","title":"06.02.04. EAS With Path Level Transformation"},{"location":"190427_JaafraY_LaurentJL_2018/#0603_architecture_search_accelerators","text":"Reinforcement learning methods have been applied successfully to design neural networks. Although multi-branch structures and skip connections improves the efficiency of architectures automatic search, the latter is still computationally expensive (hundreds of GPU hours), time consuming and requires further acceleration of learning process. Thus, in addition to the methods assigned to architectural search optimization and complex compo- nent building, some techniques are developed to speed up learning and are depicted in the current section. Early stopping strategy proposed in [40] enables fast convergence of the learning agent while maintaining an acceptable level of efficiency. This is pos- sible by taking into account intermediate rewards ignored in previous works (set to zero delaying reinforcement learning convergence [36]. In such case, the agent stops searching in an early training phase as the accuracy rewards reach higher levels in fewer iterations. The reward function is redefined in order to include designed block complexity and density and avoid possible poor accuracy resulting from training early stopping. A second technique is presented in [40] which consists of a distributed asynchronous framework assembling 3 nodes with different functions. The master node is the place where block structures are sampled by agent. Then, in the controller node, the entire network is built from generated blocks and transmitted to multiple compute nodes for training. The framework is a kind of simplified parameter-server [51] and allows the parallel training of designed networks in each compute nodes. Hence, the whole design and learning processing is operated in multiple machines and GPUs. [28] uses the same parameter server scheme with replication of controllers in order to train various architectures in parallel. As seen previously, reinforcement learning policies use explored architec- tures performance as a guiding reward for controllers updates. Training and evaluating every sampled architecture (among hundreds) on validation data is responsible for most of computational load. Extracting architecture perfor- mance was consequently subject to several estimation attempts. A number of approaches focus on performance prediction on the basis of past observa- tions. Most of such techniques are based on learning curve extrapolation [25] and surrogate models using RNN predictor [52] that aim at predicting and eliminating poor architectures before full training. Another idea to estimate performance and rank designed architectures is to use simplified (proxy) met- rics for training such as data subsets (mini-batches) [29] and down-sampled data (like images with lower resolution) [53]. Network transformation is one of the more recent techniques assigned to accelerate neural architecture search ([49], [54]). It consists in train- ing explored architectures reusing previously trained or existing networks. This modeling feature allows to address a limitation of reinforcement learn- ing approaches where training is performed with a random initialization of weights. Thus, extending network morphisms [55] to initiate architecture search through the transfer of experience and knowledge reflected by reused weights enables the framework to scrutinize the search space efficiently. Although the techniques presented above have saved substantial compu- tational resources for neural architecture search, there is still more effort needed to examine the extent of bias impact of such techniques on the search process. Indeed, it\u2019s crucial to assure that modifications brought through re-sampled data, discarded cases and early convergence do not influence the models original predictions. Further studies are thus required to verify that learning accelerators do not have amplified effect on approaches predictions and validation accuracies.","title":"06.03. Architecture search accelerators"},{"location":"190427_JaafraY_LaurentJL_2018/#07_conclusion","text":"The review of recent work trend on automatic design of CNN architectures raised some methodological options that are adopted by the majority of built approaches. Despite some attempts to use design meta-controllers based on evolutionary algorithms ([26], [27]) and Bayesian optimization ([25], [56]), reinforcement learning has shown promising empirical results and stands as the preferred strategy to train design controllers [57]. Another common conception option is the introduction of multi-branch (modular) structures as an elementary component of the entire network which restricts the search space to block/cell level. The plain network design is generally kept as a first step of proposed approaches application ([28], [42]) given that it leads to simple networks and allows to focus on the method itself before switching to more complex structures with modular design ([29], [49]). A third option used in design approaches at a lower scale is the prediction of explored architectures rewards before full training the most promising ones ([25], [29]). This training acceleration technique is implemented for performance improvement purpose and requires further attention to control possible bias impact on the models behavior. The success of current reinforcement-learning-based approaches to design CNN architectures is widely proven especially for image classification tasks. However, it is achieved at the cost of high computational resources despite the acceleration attempts of most of recent models. Such fact is preventing indi- vidual researchers and small research entities (companies and laboratories) from fully access to this innovative technology [42]. Hence, deeper and more revolutionary optimizing methods are required to practically operate CNN automatic design. Transformation approaches based on extended network morphisms [49] are among the first attempts in this direction that achieved drastic decrease in computational cost and demonstrated generalization ca- pacity. Additional future directions to control automatic design complexity is to develop methods for multi-task problems [58] and weights sharing [59] in order to benefit from knowledge transfer contributions.","title":"07. Conclusion"},{"location":"190427_JaafraY_LaurentJL_2018/#references","text":"[D. Kriesel, A Brief Introduction to Neural Networks, 2007.][2007_KrieselD] [V. Sze, Y. Chen, T. Yang, J. S. Emer, Efficient processing of deep neural networks: A tutorial and survey, Proceedings of the IEEE 105 (12) (2017) 2295\u20132329.] [O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei, Imagenet large scale visual recognition challenge, Int. J. Comput. Vision 115 (3) (2015) 211\u2013252.] [Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, L. D. Jackel, Handwritten digit recognition with a back- propagation network, in: D. S. Touretzky (Ed.), Advances in Neural Information Processing Systems 2, Morgan-Kaufmann, 1990, pp. 396\u2013 404.] [M. Minsky, S. Papert, Perceptrons: An Introduction to Computational Geometry, MIT Press, Cambridge, MA, USA, 1969.] [D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning representations by back-propagating errors, Nature 323 (1986) 533\u2013536.] [Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (11) (1998) 2278\u20132324.] [K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best multi-stage architecture for object recognition?, in: 2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 2146\u20132153.] [M. A. Nielsen, Neural Networks and Deep Learning, Determination Press, 2018.] [D. Hubel, T. Wiesel, Receptive fields, binocular interaction, and func- tional architecture in the cat\u2019s visual cortex, Journal of Physiology 160 (1962) 106\u2013154.] [A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1097\u20131105.] [H. Wu, X. Gu, Max-pooling dropout for regularization of convolutional neural networks, in: Neural Information Processing - 22nd International Conference, ICONIP 2015, Istanbul, Turkey, November 9-12, 2015, Pro- ceedings, Part I, 2015, pp. 46\u201354.] [I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016.] [M. Zeiler, R. Fergus, Stochastic pooling for regularization of deep con- volutional neural networks, in: Proceedings of the International Confer- ence on Learning Representations (ICLR), 2013.] [N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A simple way to prevent neural networks from overfitting, J. Mach. Learn. Res. 15 (1) (2014) 1929\u20131958.] [K. Simonyan, A. Zisserman, Very deep convolutional networks for large- scale image recognition, in: Proceedings of the International Conference on Learning Representations (ICLR), 2015.] [M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Com- puter Vision \u2013 ECCV 2014, Springer International Publishing, Cham, 2014, pp. 818\u2013833.] [C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.] [K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.] [C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi, Inception-v4, inception- resnet and the impact of residual connections on learning, in: S. P. Singh, S. Markovitch (Eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., AAAI Press, 2017, pp. 4278\u20134284.] [G. Huang, Z. Liu, L. v. d. Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261\u20132269.] [B. Baker, O. Gupta, R. Raskar, N. Naik, Accelerating neural architec- ture search using performance prediction, in: International Conference on Learning Representations, Workshop, 2018.] [Y. Bengio, A. Courville, P. Vincent, Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell. 35 (8) (2013) 1798\u20131828.] [J. S. Bergstra, R. Bardenet, Y. Bengio, B. K \u0301egl, Algorithms for hyper- parameter optimization, in: Advances in Neural Information Processing Systems 24, Curran Associates, Inc., 2011, pp. 2546\u20132554.] [T. Domhan, J. T. Springenberg, F. Hutter, Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves, in: Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI\u201915, AAAI Press, 2015, pp. 3460\u20133468.] [K. O. Stanley, D. B. D\u2019Ambrosio, J. Gauci, A hypercube-based encoding for evolving large-scale neural networks, Artif. Life 15 (2) (2009) 185\u2013 212.] [M. Suganuma, S. Shirakawa, T. Nagao, A genetic programming ap- proach to designing convolutional neural network architectures, in: Pro- ceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201917, ACM, New York, NY, USA, 2017, pp. 497\u2013504.] [B. Zoph, Q. V. Le, Neural architecture search with reinforcement learn- ing, in: Proceedings of the International Conference on Learning Rep- resentations (ICLR), 2017.] [H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, Efficient neural architec- ture search via parameters sharing, in: J. Dy, A. Krause (Eds.), Proceed- ings of the 35th International Conference on Machine Learning, Vol. 80, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 4095\u20134104.] [E. Brochu, T. Brochu, N. de Freitas, A bayesian interactive optimization approach to procedural animation design, in: Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association, Goslar Germany, Germany, 2010, pp. 103\u2013 112.] [A. Klein, S. Falkner, S. Bartels, P. Hennig, F. Hutter, Fast bayesian optimization of machine learning hyperparameters on large datasets, in: Proceedings of the 20th International Conference on Artificial Intelli- gence and Statistics (AISTATS 2017), Vol. 54 of Proceedings of Machine Learning Research, PMLR, 2017, pp. 528\u2013536.] [C. E. Rasmussen, C. K. I. Williams, Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning), The MIT Press, 2005.] [L. Breiman, Random forests, Mach. Learn. 45 (1) (2001) 5\u201332.] [A. E. Eiben, J. E. Smith, Introduction to Evolutionary Computing, 2nd Edition, Springer Publishing Company, Incorporated, 2015.] [E. Dufourq, B. A. Bassett, Eden: Evolutionary deep networks for ef- ficient machine learning, in: 2017 Pattern Recognition Association of South Africa and Robotics and Mechatronics (PRASA-RobMech), 2017, pp. 110\u2013115.] [R. S. Sutton, A. G. Barto, Reinforcement learning - an introduction, Adaptive computation and machine learning, MIT Press, 1998.] [K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, Deep reinforcement learning: A brief survey, IEEE Signal Processing Maga- zine 34 (6) (2017) 26\u201338.] [C. J. C. H. Watkins, P. Dayan, Q-learning, Machine Learning 8 (3) (1992) 279\u2013292.] [B. Baker, O. Gupta, N. Naik, R. Raskar, Designing neural network architectures using reinforcement learning, in: Proceedings of the Inter- national Conference on Learning Representations (ICLR), 2017.] [Z. Zhong, J. Yan, W. Wu, J. Shao, C.-L. Liu, Practical block-wise neural network architecture generation, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.] [F. Tan, P. Yan, X. Guan, Deep reinforcement learning: From q-learning to deep q-learning, in: D. Liu, S. Xie, Y. Li, D. Zhao, E.-S. M. El-Alfy (Eds.), Neural Information Processing, Springer International Publish- ing, Cham, 2017, pp. 475\u2013483.] [H. Cai, T. Chen, W. Zhang, Y. Yu, J. Wang, Efficient architecture search by network transformation, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innova- tive Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI- 18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2787\u20132794.] [T. Chen, I. Goodfellow, J. Shlens, Net2Net: Accelerating learning via knowledge transfer, in: International Conference on Learning Represen- tations (ICLR), 2016.] [M. Schuster, K. Paliwal, Bidirectional recurrent neural networks, Trans. Sig. Proc. 45 (11) (1997) 2673\u20132681.] [C. J. C. H. Watkins, Learning from delayed rewards, Ph.D. thesis, King\u2019s College, Cambridge, UK (1989).] [V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis, Human-level control through deep reinforcement learning, Nature 518 (2015) 529\u2013533.] [C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, K. Murphy, Progressive neural architecture search, in: V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (Eds.), Computer Vision \u2013 ECCV 2018, Springer International Publishing, Cham, 2018, pp. 19\u201335.] [F. Chollet, Xception: Deep learning with depthwise separable convo- lutions, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1800\u20131807.] [H. Cai, J. Yang, W. Zhang, S. Han, Y. Yu, Path-level network trans- formation for efficient architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stock- holmsmssan, Stockholm Sweden, 2018, pp. 678\u2013687.] [K. S. Tai, R. Socher, C. D. Manning, Improved semantic representations from tree-structured long short-term memory networks, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, 2015, pp. 1556\u20131566.] [J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng, Large scale distributed deep networks, in: Proceedings of the 25th Interna- tional Conference on Neural Information Processing Systems - Volume 1, Curran Associates Inc., USA, 2012, pp. 1223\u20131231.] [H. Liu, K. Simonyan, O. Vinyals, C. Fernando, K. Kavukcuoglu, Hierar- chical representations for efficient architecture search, in: International Conference on Learning Representations (ICLR), 2018.] [T. Hinz, N. Navarro-Guerrero, S. Magg, S. Wermter, Speeding up the Hyperparameter Optimization of Deep Convolutional Neural Networks, International Journal of Computational Intelligence and Applications 17 (2).] [F. H. Thomas Elsken, Jan Hendrik Metzen, Simple and efficient archi- tecture search for convolutional neural networks, in: Proceedings of the International Conference on Learning Representations (ICLR), 2018.] [T. Wei, C. Wang, C. W. Chen, Modularized morphing of neural net- works, in: International Conference on Learning Representations, Work- shop, 2017.] [H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, F. Hutter, To- wards automatically-tuned neural networks, in: F. Hutter, L. Kotthoff, J. Vanschoren (Eds.), Proceedings of the Workshop on Automatic Ma- chine Learning, Vol. 64, PMLR, New York, New York, USA, 2016, pp. 58\u201365.] [J. Perez-Rua, M. Baccouche, S. Pateux, Efficient progressive neural ar- chitecture search, in: British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, BMVA Press, 2018, p. 150.] [J. Liang, E. Meyerson, R. Miikkulainen, Evolutionary architecture search for deep multitask networks, in: Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201918, ACM, New York, NY, USA, 2018, pp. 466\u2013473.] [G. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, Q. Le, Un- derstanding and simplifying one-shot architecture search, in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stockholmsmssan, Stockholm Sweden, 2018, pp. 550\u2013559.]","title":"References"}]}